From c282aaa19aa30ac5c2119ed24195a4b6b0b573b7 Mon Sep 17 00:00:00 2001
From: Vadim Pasternak <vadimp@mellanox.com>
Date: Mon, 1 Aug 2016 06:59:31 +0000
Subject: [patch] mellanox asic support over sdk

---
 linux/drivers/hwmon/Kconfig                      |    2 +
 linux/drivers/hwmon/Makefile                     |    2 +
 linux/drivers/hwmon/mellanox/Kconfig             |   34 +
 linux/drivers/hwmon/mellanox/Makefile            |   11 +
 linux/drivers/hwmon/mellanox/alloc.c             |  256 +
 linux/drivers/hwmon/mellanox/alloc.h             |   42 +
 linux/drivers/hwmon/mellanox/cmd.c               | 1141 ++++
 linux/drivers/hwmon/mellanox/cq.c                | 1296 ++++
 linux/drivers/hwmon/mellanox/cq.h                |  133 +
 linux/drivers/hwmon/mellanox/dq.c                |  991 +++
 linux/drivers/hwmon/mellanox/dq.h                |   65 +
 linux/drivers/hwmon/mellanox/eq.c                |  495 ++
 linux/drivers/hwmon/mellanox/eq.h                |   97 +
 linux/drivers/hwmon/mellanox/fw.c                | 5330 ++++++++++++++++
 linux/drivers/hwmon/mellanox/fw.h                |   25 +
 linux/drivers/hwmon/mellanox/ib.h                |   62 +
 linux/drivers/hwmon/mellanox/icm.c               |  185 +
 linux/drivers/hwmon/mellanox/icm.h               |   93 +
 linux/drivers/hwmon/mellanox/intf.c              |  200 +
 linux/drivers/hwmon/mellanox/mlnx-asic-drv.c     | 2322 +++++++
 linux/drivers/hwmon/mellanox/mlnx-asic-i2c.c     | 2395 ++++++++
 linux/drivers/hwmon/mellanox/mlnx-common-drv.h   |   30 +
 linux/drivers/hwmon/mellanox/reset.c             |  175 +
 linux/drivers/hwmon/mellanox/sx.h                |  641 ++
 linux/drivers/hwmon/mellanox/sx_core_main.c      | 7096 ++++++++++++++++++++++
 linux/drivers/hwmon/mellanox/sx_dpt.c            | 2041 +++++++
 linux/drivers/hwmon/mellanox/sx_dpt.h            |  217 +
 linux/drivers/hwmon/mellanox/sx_proc.c           | 2461 ++++++++
 linux/drivers/hwmon/mellanox/sx_proc.h           |   43 +
 linux/drivers/hwmon/mellanox/sx_sgmii.h          |   87 +
 linux/include/linux/mlx_sx/cmd.h                 |  221 +
 linux/include/linux/mlx_sx/device.h              |  308 +
 linux/include/linux/mlx_sx/driver.h              |  200 +
 linux/include/linux/mlx_sx/kernel_user.h         | 6143 +++++++++++++++++++
 linux/include/linux/mlx_sx/mlx_sx/device.h       |  308 +
 linux/include/linux/mlx_sx/mlx_sx/driver.h       |  200 +
 linux/include/linux/mlx_sx/mlx_sx/sx_i2c_if.h    |   35 +
 linux/include/linux/mlx_sx/sx_i2c_if.h           |   35 +
 linux/include/linux/mlx_sx/sx_vtca_kernel_user.h |  149 +
 39 files changed, 35567 insertions(+)
 create mode 100644 linux/drivers/hwmon/mellanox/Kconfig
 create mode 100644 linux/drivers/hwmon/mellanox/Makefile
 create mode 100644 linux/drivers/hwmon/mellanox/alloc.c
 create mode 100644 linux/drivers/hwmon/mellanox/alloc.h
 create mode 100644 linux/drivers/hwmon/mellanox/cmd.c
 create mode 100644 linux/drivers/hwmon/mellanox/cq.c
 create mode 100644 linux/drivers/hwmon/mellanox/cq.h
 create mode 100644 linux/drivers/hwmon/mellanox/dq.c
 create mode 100644 linux/drivers/hwmon/mellanox/dq.h
 create mode 100644 linux/drivers/hwmon/mellanox/eq.c
 create mode 100644 linux/drivers/hwmon/mellanox/eq.h
 create mode 100644 linux/drivers/hwmon/mellanox/fw.c
 create mode 100644 linux/drivers/hwmon/mellanox/fw.h
 create mode 100644 linux/drivers/hwmon/mellanox/ib.h
 create mode 100644 linux/drivers/hwmon/mellanox/icm.c
 create mode 100644 linux/drivers/hwmon/mellanox/icm.h
 create mode 100644 linux/drivers/hwmon/mellanox/intf.c
 create mode 100644 linux/drivers/hwmon/mellanox/mlnx-asic-drv.c
 create mode 100644 linux/drivers/hwmon/mellanox/mlnx-asic-i2c.c
 create mode 100644 linux/drivers/hwmon/mellanox/mlnx-common-drv.h
 create mode 100644 linux/drivers/hwmon/mellanox/reset.c
 create mode 100644 linux/drivers/hwmon/mellanox/sx.h
 create mode 100644 linux/drivers/hwmon/mellanox/sx_core_main.c
 create mode 100644 linux/drivers/hwmon/mellanox/sx_dpt.c
 create mode 100644 linux/drivers/hwmon/mellanox/sx_dpt.h
 create mode 100644 linux/drivers/hwmon/mellanox/sx_proc.c
 create mode 100644 linux/drivers/hwmon/mellanox/sx_proc.h
 create mode 100644 linux/drivers/hwmon/mellanox/sx_sgmii.h
 create mode 100644 linux/include/linux/mlx_sx/cmd.h
 create mode 100644 linux/include/linux/mlx_sx/device.h
 create mode 100644 linux/include/linux/mlx_sx/driver.h
 create mode 100644 linux/include/linux/mlx_sx/kernel_user.h
 create mode 100644 linux/include/linux/mlx_sx/mlx_sx/device.h
 create mode 100644 linux/include/linux/mlx_sx/mlx_sx/driver.h
 create mode 100644 linux/include/linux/mlx_sx/mlx_sx/sx_i2c_if.h
 create mode 100644 linux/include/linux/mlx_sx/sx_i2c_if.h
 create mode 100644 linux/include/linux/mlx_sx/sx_vtca_kernel_user.h

diff --git a/linux/drivers/hwmon/Kconfig b/linux/drivers/hwmon/Kconfig
index eb7de13..eb36b45 100644
--- a/linux/drivers/hwmon/Kconfig
+++ b/linux/drivers/hwmon/Kconfig
@@ -1817,4 +1817,6 @@ config SENSORS_ATK0110
 
 endif # ACPI
 
+source drivers/hwmon/mellanox/Kconfig
+
 endif # HWMON
diff --git a/linux/drivers/hwmon/Makefile b/linux/drivers/hwmon/Makefile
index e33768d..fd31f9f 100644
--- a/linux/drivers/hwmon/Makefile
+++ b/linux/drivers/hwmon/Makefile
@@ -165,5 +165,7 @@ obj-$(CONFIG_SENSORS_WM8350)	+= wm8350-hwmon.o
 
 obj-$(CONFIG_PMBUS)		+= pmbus/
 
+obj-$(CONFIG_MELLANOX)      	+= mellanox/
+
 ccflags-$(CONFIG_HWMON_DEBUG_CHIP) := -DDEBUG
 
diff --git a/linux/drivers/hwmon/mellanox/Kconfig b/linux/drivers/hwmon/mellanox/Kconfig
new file mode 100644
index 0000000..70d3204
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/Kconfig
@@ -0,0 +1,34 @@
+#
+# Mellanox chip configuration
+#
+
+menuconfig MELLANOX
+	tristate "Mellanox support"
+	depends on I2C
+	default n
+	help
+	  Say yes here for Mellanox hwmon support.
+
+
+if MELLANOX
+
+config MELLANOX_I2C_BACKEND
+	tristate "Mellanox I2C backend"
+	default y
+	help
+	  Say yes for Mellanox I2C backend driver support
+
+
+config MELLANOX_ASIC_CORE
+	tristate "Mellanox ASIC core"
+	default y
+	help
+	  Say yes for Mellanox ASIC core support
+
+config MELLANOX_ASIC_HWMON
+	tristate "Mellanox ASIC hwmon"
+	default y
+	help
+	  Say yes for Mellanox ASIC hwmon support
+
+endif # MELLANOX
diff --git a/linux/drivers/hwmon/mellanox/Makefile b/linux/drivers/hwmon/mellanox/Makefile
new file mode 100644
index 0000000..0d9b652
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/Makefile
@@ -0,0 +1,11 @@
+#
+# Makefile for Mellanox chip drivers.
+#
+
+obj-$(CONFIG_MELLANOX_I2C_BACKEND)		+= mlnx-asic-i2c.o
+obj-$(CONFIG_MELLANOX_ASIC_CORE)                += sx_core.o
+sx_core-y                      			:= sx_core_main.o cq.o intf.o dq.o eq.o \
+                                   		alloc.o cmd.o reset.o \
+                                   		fw.o icm.o sx_dpt.o sx_proc.o
+obj-$(CONFIG_MELLANOX_ASIC_HWMON)		+= mlnx-asic-drv.o
+
diff --git a/linux/drivers/hwmon/mellanox/alloc.c b/linux/drivers/hwmon/mellanox/alloc.c
new file mode 100644
index 0000000..bcd24af
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/alloc.c
@@ -0,0 +1,256 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+
+/************************************************
+ * Includes
+ ***********************************************/
+
+#include <asm/atomic.h>
+#include <linux/pci.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/bitmap.h>
+#include <linux/dma-mapping.h>
+#include <linux/vmalloc.h>
+#include <linux/mlx_sx/device.h>
+#include "alloc.h"
+#include "sx.h"
+
+/************************************************
+ * Functions
+ ***********************************************/
+
+u32 sx_bitmap_test(struct sx_bitmap *bitmap, u32 obj)
+{
+	u32 ret;
+	unsigned long flags;
+
+	spin_lock_irqsave(&bitmap->lock, flags);
+
+	if (obj >= bitmap->max) {
+		ret = 0;
+	} else if (test_bit(obj, bitmap->table)) {
+		ret = 1;
+	} else {
+		ret = 0;
+	}
+
+	spin_unlock_irqrestore(&bitmap->lock, flags);
+
+	return ret;
+}
+
+u32 sx_bitmap_set(struct sx_bitmap *bitmap, u32 obj)
+{
+	u32 ret = obj;
+	unsigned long flags;
+
+	spin_lock_irqsave(&bitmap->lock, flags);
+
+	if (obj >= bitmap->max) {
+		ret = -1;
+	} else if (test_bit(obj, bitmap->table)) {
+		ret = -1;
+	} else {
+		set_bit(obj, bitmap->table);
+	}
+
+	spin_unlock_irqrestore(&bitmap->lock, flags);
+
+	return ret;
+}
+
+u32 sx_bitmap_alloc(struct sx_bitmap *bitmap)
+{
+	u32 obj;
+	unsigned long flags;
+
+	spin_lock_irqsave(&bitmap->lock, flags);
+
+	obj = find_next_zero_bit(bitmap->table, bitmap->max, 0);
+	if (obj >= bitmap->max) {
+		obj = -1;
+	} else {
+		set_bit(obj, bitmap->table);
+	}
+
+	spin_unlock_irqrestore(&bitmap->lock, flags);
+
+	return obj;
+}
+
+void sx_bitmap_free(struct sx_bitmap *bitmap, u32 obj)
+{
+	unsigned long flags;
+
+	WARN_ON(obj >= bitmap->max);
+
+	spin_lock_irqsave(&bitmap->lock, flags);
+	clear_bit(obj, bitmap->table);
+	spin_unlock_irqrestore(&bitmap->lock, flags);
+}
+
+/*
+ * Handling for queue buffers -- we allocate a bunch of memory and
+ * register it in a memory region at Switch virtual address 0.  If the
+ * requested size is > max_direct, we split the allocation into
+ * multiple pages, so we don't require too much contiguous memory.
+ */
+int sx_buf_alloc(struct sx_dev *dev, int size, int max_direct,
+		 struct sx_buf *buf, int init_val)
+{
+	dma_addr_t t = 0;
+
+	if (size <= max_direct) {
+		buf->nbufs        = 1;
+		buf->npages       = (size + PAGE_SIZE - 1) / PAGE_SIZE;
+		buf->page_shift   = PAGE_SHIFT;
+		if (!dev->pdev) {
+			buf->u.direct.buf = kmalloc(size, GFP_KERNEL);
+		} else {
+			buf->u.direct.buf = dma_alloc_coherent(&dev->pdev->dev, size, &t, GFP_KERNEL);
+		}
+
+		if (!buf->u.direct.buf) {
+			return -ENOMEM;
+		}
+
+		buf->u.direct.map = t;
+
+
+		memset(buf->u.direct.buf, init_val, size);
+	} else {
+		int i;
+
+		buf->nbufs       = (size + PAGE_SIZE - 1) / PAGE_SIZE;
+		buf->npages      = buf->nbufs;
+		buf->page_shift  = PAGE_SHIFT;
+		buf->u.page_list = kzalloc(buf->nbufs*sizeof(*buf->u.page_list), GFP_KERNEL);
+		if (!buf->u.page_list) {
+			return -ENOMEM;
+		}
+
+		for (i = 0; i < buf->nbufs; ++i) {
+			if (!dev->pdev) {
+				buf->u.page_list[i].buf =
+						kmalloc(PAGE_SIZE, GFP_KERNEL);
+			} else {
+				buf->u.page_list[i].buf =
+						dma_alloc_coherent(&dev->pdev->dev, PAGE_SIZE, &t, GFP_KERNEL);
+			}
+
+			if (!buf->u.page_list[i].buf) {
+				goto err_free;
+			}
+
+			buf->u.page_list[i].map = t;
+
+			memset(buf->u.page_list[i].buf, init_val, PAGE_SIZE);
+		}
+
+		if (BITS_PER_LONG == 64) {
+			struct page **pages;
+			pages = kmalloc(sizeof *pages * buf->nbufs, GFP_KERNEL);
+			if (!pages)
+				goto err_free;
+			for (i = 0; i < buf->nbufs; ++i) {
+				pages[i] =
+					virt_to_page(buf->u.page_list[i].buf);
+			}
+
+			buf->u.direct.buf = vmap(pages, buf->nbufs,
+						 VM_MAP, PAGE_KERNEL);
+			kfree(pages);
+			if (!buf->u.direct.buf) {
+				goto err_free;
+			}
+		}
+	}
+
+	return 0;
+
+err_free:
+	sx_buf_free(dev, size, buf);
+
+	return -ENOMEM;
+}
+
+
+void sx_fill_page_list(__be64 *dma_addrs, struct sx_buf *buf)
+{
+	int i;
+	dma_addr_t map;
+
+	if (buf->nbufs == 1) {
+		map = buf->u.direct.map;
+		for (i = 0; i < buf->npages; i++) {
+			*dma_addrs++ = cpu_to_be64(map & 0xfffffffffffff000ULL);
+			map += PAGE_SIZE;
+		}
+	} else {
+		for (i = 0; i < buf->nbufs; ++i) {
+			*dma_addrs++ = cpu_to_be64(buf->u.page_list[i].map
+					& 0xfffffffffffff000ULL);
+		}
+	}
+}
+
+void sx_buf_free(struct sx_dev *dev, int size, struct sx_buf *buf)
+{
+	int i;
+
+	if (buf->nbufs == 1) {
+		if (!dev->pdev) {
+			kfree(buf->u.direct.buf);
+		} else {
+			dma_free_coherent(&dev->pdev->dev, size, buf->u.direct.buf,
+					buf->u.direct.map);
+		}
+	} else {
+		if (BITS_PER_LONG == 64)
+			vunmap(buf->u.direct.buf);
+
+		for (i = 0; i < buf->nbufs; ++i) {
+			if (buf->u.page_list[i].buf) {
+				if (!dev->pdev) {
+					kfree(buf->u.page_list[i].buf);
+				} else {
+					dma_free_coherent(&dev->pdev->dev, PAGE_SIZE,
+							buf->u.page_list[i].buf,
+							buf->u.page_list[i].map);
+				}
+			}
+		}
+
+		kfree(buf->u.page_list);
+	}
+}
+
+
+int sx_bitmap_init(struct sx_bitmap *bitmap, u32 num)
+{
+	if (num > (8 * sizeof bitmap->table))
+		return -EINVAL;
+
+	bitmap->max  = num;
+	memset(bitmap->table, 0, sizeof bitmap->table);
+	spin_lock_init(&bitmap->lock);
+
+	return 0;
+}
+
+/************************************************
+ *                  EOF                         *
+ ***********************************************/
+
diff --git a/linux/drivers/hwmon/mellanox/alloc.h b/linux/drivers/hwmon/mellanox/alloc.h
new file mode 100644
index 0000000..bb464cc
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/alloc.h
@@ -0,0 +1,42 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+
+#ifndef ALLOC_H_
+#define ALLOC_H_
+
+/************************************************
+ * Includes
+ ***********************************************/
+
+#include "sx.h"
+
+/************************************************
+ * Function prototypes
+ ***********************************************/
+u32 sx_bitmap_test(struct sx_bitmap *bitmap, u32 obj);
+u32 sx_bitmap_set(struct sx_bitmap *bitmap, u32 obj);
+u32  sx_bitmap_alloc(struct sx_bitmap *bitmap);
+void sx_bitmap_free(struct sx_bitmap *bitmap, u32 obj);
+int sx_buf_alloc(struct sx_dev *dev, int size, int max_direct,
+		 struct sx_buf *buf, int init_val);
+void sx_fill_page_list(__be64 *dma_addrs, struct sx_buf *buf);
+void sx_buf_free(struct sx_dev *dev, int size, struct sx_buf *buf);
+void sx_bitmap_free(struct sx_bitmap *bitmap, u32 obj);
+int sx_bitmap_init(struct sx_bitmap *bitmap, u32 num);
+
+#endif /* ALLOC_H_ */
+
+/************************************************
+ *                  EOF                         *
+ ***********************************************/
diff --git a/linux/drivers/hwmon/mellanox/cmd.c b/linux/drivers/hwmon/mellanox/cmd.c
new file mode 100644
index 0000000..f9cb53d
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/cmd.c
@@ -0,0 +1,1141 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+/************************************************
+ * Includes
+ ***********************************************/
+
+#include <linux/sched.h>
+#include <linux/pci.h>
+#include <linux/errno.h>
+#include <linux/io.h>
+#include <linux/mlx_sx/cmd.h>
+#include <linux/delay.h>
+#include "sx.h"
+#include "dq.h"
+#include "cq.h"
+#include "sx_dpt.h"
+#include "sx_proc.h"
+
+/************************************************
+ *  Definitions
+ ***********************************************/
+
+#define CMD_POLL_TOKEN 0xffff
+
+/************************************************
+ *  Globals
+ ***********************************************/
+
+extern struct sx_globals sx_glb;
+extern int i2c_cmd_dump;
+extern int i2c_cmd_op;
+extern int i2c_cmd_reg_id;
+extern int i2c_cmd_dump_cnt;
+
+/* for simulator only */
+static int (*cmd_ifc_stub_func)(void *rxbuff, void *txbuf, int size,
+		u8 op_modifier, u16 opcode, u32 input_modifier, u16 token);
+
+/************************************************
+ * Functions                      	*
+ ***********************************************/
+
+void mem_blk_dump(char *name, u8 *data, int len)
+{
+	int i;
+	u8 *buf = (void *)data;
+	int cnt = len;
+
+	printk( "======= %s =========\n",name);
+	for (i = 0; i < cnt; i++) {
+		if (i == 0 || (i%4 == 0))
+			printk( "\n0x%04x : ", i);
+		printk( " 0x%02x", buf[i]);
+	}
+
+	printk(KERN_INFO "\n");
+}
+
+void register_ver_cmd_ifc_stub(int (*func)(void *rxbuff, void *txbuf, int size,
+		u8 op_modifier, u16 opcode, u32 input_modifier, u16 token))
+{
+	cmd_ifc_stub_func = func;
+}
+EXPORT_SYMBOL(register_ver_cmd_ifc_stub);
+
+enum {
+	/* command completed successfully: */
+	CMD_STAT_OK		= 0x00,
+	/* Internal error (such as a bus error)
+	 * occurred while processing command: */
+	CMD_STAT_INTERNAL_ERR	= 0x01,
+	/* Operation/command not supported or opcode modifier not supported: */
+	CMD_STAT_BAD_OP		= 0x02,
+	/* Parameter not supported or parameter out of range: */
+	CMD_STAT_BAD_PARAM	= 0x03,
+	/* System not enabled or bad system state: */
+	CMD_STAT_BAD_SYS_STATE	= 0x04,
+	/* Attempt to access reserved or unallocaterd resource: */
+	CMD_STAT_BAD_RESOURCE	= 0x05,
+	/* Requested resource is currently executing a command,
+	 * or is otherwise busy: */
+	CMD_STAT_RESOURCE_BUSY	= 0x06,
+	/* Required capability exceeds device limits: */
+	CMD_STAT_EXCEED_LIM	= 0x08,
+	/* Resource is not in the appropriate state or ownership: */
+	CMD_STAT_BAD_RES_STATE	= 0x09,
+	/* Index out of range: */
+	CMD_STAT_BAD_INDEX	= 0x0a,
+	/* FW image corrupted: */
+	CMD_STAT_BAD_NVMEM	= 0x0b,
+	/* Bad management packet (silently discarded): */
+	CMD_STAT_BAD_PKT	= 0x30,
+};
+
+enum {
+	HCR_IN_PARAM_OFFSET	= 0x00,
+	HCR_IN_MODIFIER_OFFSET	= 0x08,
+	HCR_OUT_PARAM_OFFSET	= 0x0c,
+	HCR_TOKEN_OFFSET	= 0x14,
+	HCR_STATUS_OFFSET	= 0x18,
+	HCR_OPMOD_SHIFT		= 12,
+	HCR_E_BIT		= 22,
+	HCR_GO_BIT		= 23
+};
+
+enum {
+#ifdef NO_PCI
+	GO_BIT_TIMEOUT_MSECS		= 10,
+	I2C_GO_BIT_TIMEOUT_MSECS	= 10
+#else
+	GO_BIT_TIMEOUT_MSECS		= 10000,
+	I2C_GO_BIT_TIMEOUT_MSECS	= 30000
+#endif
+};
+
+enum {
+	SX_HCR_BASE		= 0x71000,
+	SX_HCR_SIZE		=  0x0001c,
+	SX_HCR2_BASE	= 0x72000,		/* for i2c command interface */
+	SX_HCR2_SIZE	 = 0x0001c,
+};
+
+struct sx_cmd_context {
+	struct completion	done;
+	int			result;
+	int			next;
+	u64			out_param;
+	u16			token;
+	u16			opcode;
+};
+
+static const char *cmd_str(u16 opcode)
+{
+	switch (opcode) {
+	case SX_CMD_MAP_FA:
+		return "SX_CMD_MAP_FA";
+	case SX_CMD_UNMAP_FA:
+		return "SX_CMD_UNMAP_FA";
+	case SX_CMD_QUERY_FW:
+		return "SX_CMD_QUERY_FW";
+	case SX_CMD_QUERY_BOARDINFO:
+		return "SX_CMD_QUERY_BOARDINFO";
+	case SX_CMD_QUERY_AQ_CAP:
+		return "SX_CMD_QUERY_AQ_CAP";
+	case SX_CMD_CONFIG_PROFILE:
+		return "SX_CMD_CONFIG_PROFILE";
+	case SX_CMD_ACCESS_REG:
+		return "SX_CMD_ACCESS_REG";
+	case SX_CMD_CONF_PORT:
+		return "SX_CMD_CONF_PORT";
+	case SX_CMD_INIT_PORT:
+		return "SX_CMD_INIT_PORT";
+	case SX_CMD_CLOSE_PORT:
+		return "SX_CMD_CLOSE_PORT";
+	case SX_CMD_SW2HW_DQ:
+		return "SX_CMD_SW2HW_DQ";
+	case SX_CMD_HW2SW_DQ:
+		return "SX_CMD_HW2SW_DQ";
+	case SX_CMD_2ERR_DQ:
+		return "SX_CMD_2ERR_DQ";
+	case SX_CMD_QUERY_DQ:
+		return "SX_CMD_QUERY_DQ";
+	case SX_CMD_SW2HW_CQ:
+		return "SX_CMD_SW2HW_CQ";
+	case SX_CMD_HW2SW_CQ:
+		return "SX_CMD_HW2SW_CQ";
+	case SX_CMD_QUERY_CQ:
+		return "SX_CMD_QUERY_CQ";
+	case SX_CMD_SW2HW_EQ:
+		return "SX_CMD_SW2HW_EQ";
+	case SX_CMD_HW2SW_EQ:
+		return "SX_CMD_HW2SW_EQ";
+	case SX_CMD_QUERY_EQ:
+		return "SX_CMD_QUERY_EQ";
+	case SX_CMD_INIT_MAD_DEMUX:
+		return "SX_CMD_INIT_MAD_DEMUX";
+	case SX_CMD_MAD_IFC:
+		return "SX_CMD_MAD_IFC";
+	default:
+		return "Unknown command";
+	}
+}
+
+static int sx_status_to_errno(u8 status)
+{
+	static const int trans_table[] = {
+		[CMD_STAT_INTERNAL_ERR]	  = -EIO,
+		[CMD_STAT_BAD_OP]	  = -EPERM,
+		[CMD_STAT_BAD_PARAM]	  = -EINVAL,
+		[CMD_STAT_BAD_SYS_STATE]  = -ENXIO,
+		[CMD_STAT_BAD_RESOURCE]	  = -EBADF,
+		[CMD_STAT_RESOURCE_BUSY]  = -EBUSY,
+		[CMD_STAT_EXCEED_LIM]	  = -ENOMEM,
+		[CMD_STAT_BAD_RES_STATE]  = -EBADF,
+		[CMD_STAT_BAD_INDEX]	  = -EBADF,
+		[CMD_STAT_BAD_NVMEM]	  = -EFAULT,
+		[CMD_STAT_BAD_PKT]	  = -EINVAL,
+	};
+
+	if (status >= ARRAY_SIZE(trans_table) ||
+	    (status != CMD_STAT_OK && trans_table[status] == 0))
+		return -EIO;
+
+	return trans_table[status];
+}
+
+static int cmd_get_hcr_pci(struct sx_dev *dev, int offset)
+{
+	u32 status = 0;
+
+	if (dev->pdev)
+		status = __raw_readl(sx_priv(dev)->cmd.hcr + offset);
+
+	return status;
+}
+
+static int cmd_get_hcr_i2c(int sx_dev_id, int offset, int *err)
+{
+//#ifdef NO_PCI
+//	return 0;
+//#endif
+	return sx_dpt_i2c_readl(sx_dev_id, SX_HCR2_BASE + offset, err); 
+}
+
+static int cmd_get_hcr_sgmii(int sx_dev_id, int offset, int *err)
+{
+#ifdef NO_PCI
+	return 0;
+#else
+	return sx_glb.sx_sgmii.cr_space_readl(sx_dev_id, SX_HCR2_BASE + offset, err);
+#endif
+}
+
+static u32 cmd_get_hcr_reg(struct sx_dev *dev, int sx_dev_id,
+			int offset, int cmd_path, int *err)
+{
+	u32 reg = 0xffff;
+
+	if (cmd_path == DPT_PATH_I2C) {
+		reg = be32_to_cpu(cmd_get_hcr_i2c(sx_dev_id, offset, err));
+	} else if (cmd_path == DPT_PATH_SGMII) {
+		reg = be32_to_cpu(cmd_get_hcr_sgmii(sx_dev_id, offset, err));
+	} else if (cmd_path == DPT_PATH_PCI_E) {
+		reg = be32_to_cpu((__force __be32)cmd_get_hcr_pci(dev, offset));
+		*err = 0;
+	} else {
+		printk(KERN_CRIT "%s(): Error: unsupported cmd_path %d \n",
+			   __func__, cmd_path);
+		*err = -EINVAL;
+	}
+
+	return reg;
+}
+
+static int cmd_pending(struct sx_dev *dev, int sx_dev_id, int cmd_path, int *err)
+{
+	u32 status;
+	status = cmd_get_hcr_reg(dev, sx_dev_id, HCR_STATUS_OFFSET, cmd_path, err);
+
+	return status & (1 << HCR_GO_BIT);
+}
+
+static int wait_for_cmd_pending(struct sx_dev *dev, int sx_dev_id, int cmd_path, u16 op, int timeout)
+{
+	int err = 0;
+	unsigned long end = 0;
+	unsigned long start = 0;
+
+#ifdef INCREASED_TIMEOUT
+	msleep(1000);
+#endif
+
+	start = jiffies;
+	end = msecs_to_jiffies(timeout * 10) + start;
+
+	while (cmd_pending(dev, sx_dev_id, cmd_path, &err) || (err != 0)) {
+		if (time_after_eq(jiffies, end)) {
+#ifdef INCREASED_TIMEOUT
+			end = msecs_to_jiffies((timeout * 4000) + 1000) + jiffies;
+			sx_warn(dev, "INCREASED_TIMEOUT is set, Skipping timeout. op=%d, timeout=%d, start=%lu, end=%lu\n", op, timeout, start, end);
+			cond_resched();
+			continue;
+#else
+			sx_warn(dev, "Go bit not cleared, op=%d, timeout=%d\n", op, timeout);
+			return -ETIMEDOUT;
+#endif
+		}
+		cond_resched();
+	}
+
+	return err;
+}
+
+static int sx_cmd_post_pci(struct sx_dev *dev, struct sx_cmd_mailbox *in_mb,
+		struct sx_cmd_mailbox *out_mb, u32 in_modifier, u8 op_modifier,
+		u16 op, u16 token, int event)
+{
+	struct sx_cmd *cmd = &sx_priv(dev)->cmd;
+	int ret = -EAGAIN;
+	u64 in_param = in_mb ? in_mb->dma : 0;
+	u64 out_param = out_mb ? out_mb->dma : 0;
+	u32 __iomem *hcr = cmd->hcr;
+	int err = 0;
+
+	mutex_lock(&cmd->hcr_mutex);
+
+	err = wait_for_cmd_pending(dev, dev->device_id, DPT_PATH_PCI_E, op, (event ? GO_BIT_TIMEOUT_MSECS : 0));
+	if ( -ETIMEDOUT == err ) {
+		dev->dev_stuck = 1;
+		goto out;
+	}
+
+	/*
+	 * We use writel (instead of something like memcpy_toio)
+	 * because writes of less than 32 bits to the HCR don't work
+	 * (and some architectures such as ia64 implement memcpy_toio
+	 * in terms of writeb).
+	 */
+	__raw_writel((__force u32) cpu_to_be32(in_param >> 32),
+								hcr + 0);
+	__raw_writel((__force u32) cpu_to_be32(in_param & 0xfffffffful),
+								hcr + 1);
+	__raw_writel((__force u32) cpu_to_be32(in_modifier),
+								hcr + 2);
+	__raw_writel((__force u32) cpu_to_be32(out_param >> 32),
+								hcr + 3);
+	__raw_writel((__force u32) cpu_to_be32(out_param & 0xfffffffful),
+								hcr + 4);
+	__raw_writel((__force u32) cpu_to_be32(token << 16),
+								hcr + 5);
+
+	/* __raw_writel may not order writes. */
+	wmb();
+
+	__raw_writel((__force u32) cpu_to_be32((1 << HCR_GO_BIT) |
+					(event ? (1 << HCR_E_BIT) : 0) |
+					(op_modifier << HCR_OPMOD_SHIFT)|
+					op), hcr + 6);
+
+	/*
+	 * Make sure that our HCR writes don't get mixed in with
+	 * writes from another CPU starting a FW command.
+	 */
+	mmiowb();
+	ret = 0;
+
+out:
+	mutex_unlock(&cmd->hcr_mutex);
+	return ret;
+}
+
+static inline struct sx_cmd_mailbox *sx_mailbox(dma_addr_t *dma_addr)
+{
+	return container_of(dma_addr, struct sx_cmd_mailbox, dma);
+}
+
+static int sx_cmd_post_i2c(struct sx_dev *dev, int sx_dev_id,
+		struct sx_cmd_mailbox *in_mb, struct sx_cmd_mailbox *out_mb,
+		u32 in_modifier, u8 op_modifier, u16 op, u16 token, int event,
+		int in_mb_size)
+{
+	struct sx_cmd *cmd = &sx_priv(dev)->cmd;
+	int ret = -EAGAIN;
+	u32 hcr2 = (u32)SX_HCR2_BASE;
+	u32 hcr_buf[7];
+	int err = 0;
+
+	if (op != SX_CMD_QUERY_FW &&
+		op != SX_CMD_QUERY_BOARDINFO &&
+		op != SX_CMD_CONFIG_PROFILE &&
+		op != SX_CMD_ACCESS_REG) {
+			sx_err(dev, "command (0x%x) not supported by I2C "
+					"ifc\n", op);
+			return -EINVAL;
+	}
+
+	mutex_lock(&cmd->hcr_mutex);
+
+	err = wait_for_cmd_pending(dev, sx_dev_id, DPT_PATH_I2C, op, I2C_GO_BIT_TIMEOUT_MSECS);
+	if ( -EUNATCH == err ) {
+		sx_err(dev, "client not ready yet for "
+				"sx_dev_id %d\n", sx_dev_id);
+		goto out;
+	} else if ( -ETIMEDOUT == err ) {
+		sx_warn(dev, "I2C go bit not cleared\n");
+		if (sx_glb.sx_i2c.set_go_bit_stuck) {
+			int i2c_dev_id;
+			err = sx_dpt_get_i2c_dev_by_id(
+					sx_dev_id,
+					&i2c_dev_id);
+			if (err) {
+				sx_err(dev, "sx_dpt_get_i2c_dev_by_id "
+						"for dev_id: %d "
+						"failed !\n",
+						sx_dev_id);
+				return -EINVAL;
+			}
+
+			sx_glb.sx_i2c.set_go_bit_stuck(i2c_dev_id);
+		}
+
+		goto out;
+	} else if (err) {
+		sx_err(dev, "client return error %d, "
+				"sx_dev_id %d\n", err, sx_dev_id);
+		goto out;
+	}
+
+	/*
+		Some of the commands use mailboxes. In order to use
+		mailboxes through the i2c, special area is reserved on
+		the i2c address space that can be used for input and
+		output mailboxes. Such mailboxes are called Local
+		Mailboxes. Copy the pci mailboxes to local mailboxes
+	*/
+	if (in_mb) {
+		ret = sx_dpt_i2c_write_buf(sx_dev_id,
+				sx_glb.sx_dpt.dpt_info[sx_dev_id].in_mb_offset,
+				in_mb->buf, in_mb_size);
+		if (ret) {
+			printk(KERN_DEBUG "sx_cmd_post_i2c: first write_buf "
+					"failed, err = %d\n", ret);
+			goto out;
+		}
+	}
+
+	/*
+		When using a local mailbox, software
+		should specify 0 as the Input/Output parameters.
+	*/
+	memset(hcr_buf, 0, 28);
+	hcr_buf[0] = 0;
+	hcr_buf[1] = 0;
+	hcr_buf[2] = cpu_to_be32(in_modifier);
+	hcr_buf[3] = 0;
+	hcr_buf[4] = 0;
+	hcr_buf[5] = cpu_to_be32(token << 16);
+	hcr_buf[6] = cpu_to_be32((op_modifier << HCR_OPMOD_SHIFT) | op);
+	ret = sx_dpt_i2c_write_buf(sx_dev_id, hcr2, (void *)hcr_buf,  28);
+	if (ret) {
+		printk(KERN_DEBUG "sx_cmd_post_i2c: second write_buf "
+				"failed, err = %d\n", ret);
+		goto out;
+	}
+	/* We write to go bit after writing all other HCR values */
+	hcr_buf[6] |= cpu_to_be32(1 << HCR_GO_BIT);
+	ret = sx_dpt_i2c_writel(
+			sx_dev_id,
+			hcr2 + 6 * sizeof(hcr2),
+			hcr_buf[6]);
+	if (ret) {
+		printk(KERN_DEBUG "sx_cmd_post_i2c: first writel "
+				"failed, err = %d\n", ret);
+		goto out;
+	}
+
+	ret = 0;
+
+out:
+	mutex_unlock(&cmd->hcr_mutex);
+	return ret;
+}
+
+static int sx_cmd_post_sgmii(struct sx_dev *dev, int sx_dev_id,
+		struct sx_cmd_mailbox *in_mb, struct sx_cmd_mailbox *out_mb,
+		u32 in_modifier, u8 op_modifier, u16 op, u16 token, int event,
+		int in_mb_size)
+{
+	struct sx_cmd *cmd = &sx_priv(dev)->cmd;
+	int ret = -EAGAIN;
+	u32 hcr2 = (u32)SX_HCR2_BASE;
+	u32 hcr_buf[7];
+	int err = 0;
+	u16 reg_id = 0;
+
+	if (op != SX_CMD_QUERY_FW &&
+		op != SX_CMD_CONFIG_PROFILE &&
+		op != SX_CMD_ACCESS_REG &&
+		op != SX_CMD_INIT_MAD_DEMUX &&
+		op != SX_CMD_MAD_IFC) {
+		printk(KERN_ERR PFX "Cannot post cmd %s 0x%x) through SGMII\n", cmd_str(op), op);
+		return 0;
+	}
+
+	if (op == SX_CMD_ACCESS_REG)
+		reg_id = be16_to_cpu((*(u16 *)(in_mb->buf + 4)));
+
+#ifdef SX_DEBUG
+	printk(KERN_DEBUG PFX "Going to post cmd %s (0x%x, reg_id 0x%x) through SGMII\n", cmd_str(op), op, reg_id);
+#endif
+
+	mutex_lock(&cmd->hcr_mutex);
+
+	err = wait_for_cmd_pending(dev, sx_dev_id, DPT_PATH_SGMII, op, GO_BIT_TIMEOUT_MSECS);
+	if ( -ETIMEDOUT == err ) {
+		sx_warn(dev, "SGMII go bit not cleared\n");
+		goto out;
+	} else if (err) {
+		sx_err(dev, "Got err %d when trying to read the GO bit"
+				" of device %d through SGMII\n", err, sx_dev_id);
+		goto out;
+	}
+
+	/*
+		Some of the commands use mailboxes. In order to use
+		mailboxes through the i2c, special area is reserved on
+		the i2c address space that can be used for input and
+		output mailboxes. Such mailboxes are called Local
+		Mailboxes. Copy the pci mailboxes to local mailboxes
+	*/
+	if (in_mb) {
+		ret = sx_glb.sx_sgmii.cr_space_write_buf(sx_dev_id,
+				sx_glb.sx_dpt.dpt_info[sx_dev_id].in_mb_offset,
+				in_mb->buf, in_mb_size);
+		if (ret) {
+			printk(KERN_DEBUG "sx_cmd_post_sgmii: first write_buf "
+					"failed, err = %d\n", ret);
+			goto out;
+		}
+
+	}
+
+	/*
+		When using a local mailbox, software
+		should specify 0 as the Input/Output parameters.
+	*/
+	memset(hcr_buf, 0, 28);
+	hcr_buf[0] = 0;
+	hcr_buf[1] = 0;
+	hcr_buf[2] = cpu_to_be32(in_modifier);
+	hcr_buf[3] = 0;
+	hcr_buf[4] = 0;
+	hcr_buf[5] = cpu_to_be32(token << 16);
+	hcr_buf[6] = cpu_to_be32((op_modifier << HCR_OPMOD_SHIFT) | op);
+	ret = sx_glb.sx_sgmii.cr_space_write_buf(sx_dev_id, hcr2, (void *)hcr_buf,  28);
+	if (ret) {
+		printk(KERN_DEBUG "sx_cmd_post_sgmii: second write_buf "
+				"failed, err = %d\n", ret);
+		goto out;
+	}
+	/* We write to go bit after writing all other HCR values */
+	hcr_buf[6] |= cpu_to_be32(1 << HCR_GO_BIT);
+	ret = sx_glb.sx_sgmii.cr_space_writel(sx_dev_id, hcr2 + 6 * sizeof(hcr2), hcr_buf[6]);
+	if (ret) {
+		printk(KERN_DEBUG "sx_cmd_post_sgmii: first writel "
+				"failed, err = %d\n", ret);
+		goto out;
+	}
+
+	ret = 0;
+
+out:
+	mutex_unlock(&cmd->hcr_mutex);
+	return ret;
+}
+
+static int sx_cmd_post(struct sx_dev *dev, int sx_dev_id,
+		struct sx_cmd_mailbox *in_mb, struct sx_cmd_mailbox *out_mb,
+		u32 in_modifier, u8 op_modifier, u16 op, u16 token, int event,
+		int cmd_path, int in_mb_size)
+{
+	int err = 0;
+	if (cmd_path == DPT_PATH_I2C) {
+		err =  sx_cmd_post_i2c(dev, sx_dev_id, in_mb, out_mb,
+					in_modifier, op_modifier,
+					op, token, event, in_mb_size);
+	} else if (cmd_path == DPT_PATH_SGMII) {
+		err =  sx_cmd_post_sgmii(dev, sx_dev_id, in_mb, out_mb,
+					in_modifier, op_modifier,
+					op, token, event, in_mb_size);
+	} else if (cmd_path == DPT_PATH_PCI_E) {
+		if (dev->pdev) {
+			err =  sx_cmd_post_pci(dev, in_mb, out_mb,
+					in_modifier, op_modifier,
+					op, token, event);
+		}
+	} else {
+		printk(KERN_WARNING "%s(): Error: sx_dev_id %d unsupported "
+				"cmd_path %d in_mod: 0x%x, op_mod: 0x%x "
+				"op: 0x%x\n",
+			   __func__, sx_dev_id, cmd_path, in_modifier,
+			   op_modifier, op);
+		err = -EINVAL;
+	}
+
+	return err;
+}
+
+static int sx_cmd_poll(struct sx_dev *dev, int sx_dev_id,
+		struct sx_cmd_mailbox *in_param,
+		struct sx_cmd_mailbox *out_param,
+		int out_is_imm, u32 in_modifier, u8 op_modifier,
+		u16 op, unsigned long timeout, int cmd_path, int in_mb_size)
+{
+	struct sx_priv *priv = sx_priv(dev);
+	int err = 0;
+	u32 status;
+	struct semaphore *poll_sem;
+	int i2c_dev_id = 0;
+
+	poll_sem = (cmd_path == DPT_PATH_I2C) ?
+			&priv->cmd.i2c_poll_sem : (cmd_path == DPT_PATH_SGMII) ?
+				&priv->cmd.sgmii_poll_sem : &priv->cmd.pci_poll_sem;
+	down(poll_sem);
+
+	if (cmd_path == DPT_PATH_I2C) {
+		err = sx_dpt_get_i2c_dev_by_id(sx_dev_id, &i2c_dev_id);
+		if (err)
+			goto out_sem;
+
+		if (!sx_glb.sx_i2c.enforce) {
+			sx_err(dev, "enforce is NULL!!!\n");
+			goto out_sem;
+		}
+
+		err = sx_glb.sx_i2c.enforce(i2c_dev_id);
+		if (err) {
+			sx_warn(dev, "I2C bus 0x%x of device %d is not ready. "
+					"command %s will not be performed. err = %d\n",
+					i2c_dev_id, sx_dev_id, cmd_str(op), err);
+			goto out_sem;
+		}
+	}
+
+	err = sx_cmd_post(dev, sx_dev_id, in_param, out_param, in_modifier,
+			op_modifier, op, CMD_POLL_TOKEN, 0, cmd_path, in_mb_size);
+	if (err) {
+		printk(KERN_WARNING "sx_cmd_poll: got err = %d "
+				"from sx_cmd_post\n", err);
+		goto out;
+	}
+
+//#ifdef NO_PCI
+//	goto out;
+//#endif
+
+	err = wait_for_cmd_pending(dev, sx_dev_id, cmd_path, op, timeout);
+	if (err) {
+		printk(KERN_WARNING "sx_cmd_poll: got err = %d from "
+				"cmd_pending\n", err);
+		goto out;
+	}
+
+	if (out_is_imm && out_param) {
+		out_param->imm_data =
+		(u64)cmd_get_hcr_reg(dev, sx_dev_id,
+				HCR_OUT_PARAM_OFFSET, cmd_path, &err) << 32 |
+				(u64)cmd_get_hcr_reg(dev, sx_dev_id,
+				HCR_OUT_PARAM_OFFSET + 4, cmd_path, &err);
+	}
+
+	status = cmd_get_hcr_reg(dev, sx_dev_id, HCR_STATUS_OFFSET, cmd_path, &err) >> 24;
+	if (err) {
+		printk(KERN_WARNING "Reading of HCR status after posting the "
+				"mailbox has failed for %s command\n",
+				cmd_str(op));
+		goto out;
+	}
+
+	err = sx_status_to_errno(status);
+	if (err) {
+		sx_warn(dev, "%s failed. FW status = 0x%x\n", cmd_str(op), status);
+	} else {
+		if (cmd_path == DPT_PATH_I2C) {
+			if (!err && out_param && out_param->buf) {
+				memset(out_param->buf, 0, sx_glb.sx_dpt.dpt_info[sx_dev_id].out_mb_size);
+				err = sx_dpt_i2c_read_buf(sx_dev_id, sx_glb.sx_dpt.dpt_info[sx_dev_id].out_mb_offset,
+						out_param->buf, 200);
+				if (err) {
+					sx_err(dev, "%s failed.error at sx_dpt_i2c_read_buf. err = %d\n", cmd_str(op), err);
+					goto out;
+				}
+			}
+		} else if (cmd_path == DPT_PATH_SGMII) {
+			if (!err && out_param && out_param->buf) {
+#ifdef SX_DEBUG
+				printk(KERN_DEBUG PFX "sx_cmd_poll: For device %u, out_mb_size = %u, out_mb_offset = 0x%x\n",
+						sx_dev_id,
+						sx_glb.sx_dpt.dpt_info[sx_dev_id].out_mb_size,
+						sx_glb.sx_dpt.dpt_info[sx_dev_id].out_mb_offset);
+#endif
+				memset(out_param->buf, 0, sx_glb.sx_dpt.dpt_info[sx_dev_id].out_mb_size);
+				sx_glb.sx_sgmii.cr_space_read_buf(sx_dev_id,
+						sx_glb.sx_dpt.dpt_info[sx_dev_id].out_mb_offset,
+						out_param->buf,
+						sx_glb.sx_dpt.dpt_info[sx_dev_id].out_mb_size);
+			}
+		}
+	}
+
+out:
+	if (cmd_path == DPT_PATH_I2C)
+		sx_glb.sx_i2c.release(i2c_dev_id);
+
+out_sem:
+	up(poll_sem);
+
+	return err;
+}
+
+void sx_cmd_set_op_tlv(struct ku_operation_tlv *op_tlv, u32 reg_id, u8 method)
+{
+	op_tlv->type = 1;
+	op_tlv->length = 4;
+	op_tlv->dr = 0;
+	op_tlv->status = 0;
+	op_tlv->register_id = reg_id;
+	op_tlv->r = 0;
+	op_tlv->method = method; /* 0x01 = Query, 0x02 Write */
+	op_tlv->op_class = 1;
+	op_tlv->tid = 0;
+}
+EXPORT_SYMBOL(sx_cmd_set_op_tlv);
+
+void sx_cmd_event(struct sx_dev *dev, u16 token, u8 status, u64 out_param)
+{
+	struct sx_priv *priv = sx_priv(dev);
+	struct sx_cmd_context *context =
+		&priv->cmd.context[token & priv->cmd.token_mask];
+
+	/* previously timed out command completing at long last */
+	if (token != context->token)
+		return;
+
+	context->result    = sx_status_to_errno(status);
+	if (context->result) {
+		sx_warn(dev, "command %s failed. FW status = %d, " \
+				"driver result = %d\n",
+				cmd_str(context->opcode), status, context->result);
+	}
+
+	context->out_param = out_param;
+	complete(&context->done);
+}
+EXPORT_SYMBOL(sx_cmd_event);
+
+static int sx_cmd_wait(struct sx_dev *dev, int sx_dev_id,
+		struct sx_cmd_mailbox *in_param,
+		struct sx_cmd_mailbox *out_param,
+		int out_is_imm, u32 in_modifier, u8 op_modifier,
+		u16 op, unsigned long timeout, int cmd_path)
+{
+	struct sx_cmd *cmd = &sx_priv(dev)->cmd;
+	struct sx_cmd_context *context;
+	int err = 0;
+
+	down(&cmd->event_sem);
+	spin_lock(&cmd->context_lock);
+	BUG_ON(cmd->free_head < 0);
+	context = &cmd->context[cmd->free_head];
+	context->token += cmd->token_mask + 1;
+	context->opcode = op;
+	cmd->free_head = context->next;
+	spin_unlock(&cmd->context_lock);
+	init_completion(&context->done);
+#ifdef NO_PCI
+	if (cmd_ifc_stub_func) {
+		void *in_buf = in_param ? in_param->buf : NULL;
+		void *out_buf = out_param ? out_param->buf : NULL;
+
+		printk(KERN_INFO "Sending command 0x%x (%s) to the "
+				"verification kernel module\n", op, cmd_str(op));
+		err = cmd_ifc_stub_func(in_buf, out_buf, SX_MAILBOX_SIZE,
+				op_modifier, op, in_modifier, context->token);
+		if (err) {
+			printk(KERN_DEBUG PFX "Got error %d from cmd_ifc_stub_func", err);
+		}
+	} else {
+		goto out;
+	}
+#else	
+	sx_cmd_post(dev, sx_dev_id, in_param, out_param, in_modifier,
+			op_modifier, op, context->token, 1, cmd_path, 0);
+#endif
+#ifdef INCREASED_TIMEOUT
+	msleep(10000);
+#endif
+	if (!wait_for_completion_timeout(&context->done, msecs_to_jiffies(timeout))) {
+		if (!context->done.done) {
+			sx_err(dev, "command 0x%x (%s) timeout for "
+					"device %d\n", op, cmd_str(op), sx_dev_id);
+			err = -EBUSY;
+			goto out;
+		}
+	}
+
+	err = context->result;
+	if (err)
+		goto out;
+
+	if (out_is_imm && out_param)
+		out_param->imm_data = context->out_param;
+
+out:
+	spin_lock(&cmd->context_lock);
+	context->next = cmd->free_head;
+	cmd->free_head = context - cmd->context;
+	spin_unlock(&cmd->context_lock);
+
+	up(&cmd->event_sem);
+	return err;
+}
+
+void __dump_cmd(struct sx_dev *dev, int sx_dev_id,
+		struct sx_cmd_mailbox *in_param,
+		struct sx_cmd_mailbox *out_param,
+		int out_is_imm, u32 in_modifier, u8 op_modifier,
+		u16 op, unsigned long timeout, int in_mb_size,
+		int cmd_path )
+{
+	int print_cmd = 0;
+	printk("%s(): in cmd_path: %d (1- i2c, 2 - sgmii, 3 - pci), op:0x%x \n",
+			   __func__,cmd_path, op);
+
+	if  (i2c_cmd_op == SX_DBG_CMD_OP_TYPE_ANY ||
+		(i2c_cmd_op == 0x40 &&  NULL != in_param &&
+		i2c_cmd_reg_id == be16_to_cpu( ((struct emad_operation *)
+					(in_param->buf))->register_id ) )) {
+		print_cmd = 1;
+	} else {
+		return;
+	}
+
+	if (print_cmd && NULL != in_param && NULL != in_param->buf ) {
+		  mem_blk_dump("CMD input dump", in_param->buf, 0x40);
+	}
+	
+	if (print_cmd && NULL != out_param && NULL != out_param->buf ) {
+		mem_blk_dump("CMD outparam dump", out_param->buf, 0x40);
+	}
+
+	if (i2c_cmd_dump_cnt != 0xFFFF && i2c_cmd_dump_cnt>0)
+			i2c_cmd_dump_cnt--;
+
+	if (i2c_cmd_dump_cnt == 0) {
+		i2c_cmd_dump = 0;
+		i2c_cmd_dump_cnt = 0;
+		i2c_cmd_op = 0xFFFF;
+	}
+}
+
+int __sx_cmd(struct sx_dev *dev, int sx_dev_id,
+		struct sx_cmd_mailbox *in_param,
+		struct sx_cmd_mailbox *out_param,
+		int out_is_imm, u32 in_modifier, u8 op_modifier,
+		u16 op, unsigned long timeout, int in_mb_size)
+{
+	int err  = 0;
+	int cmd_path = sx_dpt_get_cmd_path(sx_dev_id);
+
+#ifdef PD_BU_LIMITED_REG_ACCESS
+	switch (op) {
+	case SX_CMD_ACCESS_REG:
+	{
+		u16 reg_id = be16_to_cpu(((struct emad_operation *)
+				(in_param->buf))->register_id);
+
+		switch (reg_id) {
+		case PLD_REG_ID:
+		case PLIB_REG_ID:
+		case PMTU_REG_ID:
+		case PTYS_REG_ID:
+		case PSPA_REG_ID:
+		case PVLC_REG_ID:
+		case PAOS_REG_ID:
+		case MGIR_REG_ID:
+		case HCAP_REG_ID:
+		case HTGT_REG_ID:
+		case HPKT_REG_ID:
+		case SPZR_REG_ID:
+		case MFBE_REG_ID:
+		case MFBA_REG_ID:
+		case MFPA_REG_ID:
+        case MFSM_REG_ID:
+        case PMAOS_REG_ID:
+        case MFSC_REG_ID:
+        case MFM_REG_ID:
+        case MCIA_REG_ID:
+        case MJTAG_REG_ID:
+                case MFCR_REG_ID:
+                case PMPC_REG_ID:
+#ifdef SX_DEBUG
+			printk(KERN_INFO PFX "__sx_cmd: Running command %s with reg_id 0x%x\n", cmd_str(op), reg_id);
+#endif
+			break;
+		default:
+			printk(KERN_INFO PFX "__sx_cmd: command %s with reg_id 0x%x is not yet "
+					"supported. Not running it\n", cmd_str(op), reg_id);
+			return 0;
+		}
+
+		break;
+	}
+
+	default:
+#ifdef SX_DEBUG
+		printk(KERN_INFO PFX "__sx_cmd: Running command %s\n", cmd_str(op));
+#endif
+		break;
+	}
+#endif
+	if (cmd_path == DPT_PATH_INVALID) {
+		if (op == SX_CMD_ACCESS_REG) {
+			u16 reg_id = be16_to_cpu(((struct emad_operation *)
+					(in_param->buf))->register_id);
+
+			printk(KERN_ERR PFX "Command path in DPT for device %d is not valid. "
+					"Aborting command %s (register ID 0x%x)\n", sx_dev_id, cmd_str(op), reg_id);
+		} else {
+			printk(KERN_ERR PFX "Command path in DPT for device %d is not valid. "
+					"Aborting command %s\n", sx_dev_id, cmd_str(op));
+		}
+
+		return -EINVAL;
+	}
+
+	if (cmd_path == DPT_PATH_PCI_E && dev->dev_stuck == 1) {
+		if (printk_ratelimit())
+			printk(KERN_ERR PFX "Device %d is stuck from a previous command. "
+				"Aborting command %s\n", sx_dev_id, cmd_str(op));
+		return -EINVAL;
+	}
+
+	if (sx_priv(dev)->cmd.use_events && cmd_path != DPT_PATH_I2C
+			&& cmd_path != DPT_PATH_SGMII)
+		err = sx_cmd_wait(dev, sx_dev_id, in_param, out_param,
+				out_is_imm, in_modifier, op_modifier,
+				op, timeout, cmd_path);
+	else
+		err = sx_cmd_poll(dev, sx_dev_id, in_param, out_param,
+				out_is_imm, in_modifier, op_modifier,
+				op, timeout, cmd_path, in_mb_size);
+
+	if (i2c_cmd_dump)
+		__dump_cmd(dev, sx_dev_id, in_param, out_param,
+				out_is_imm, in_modifier, op_modifier,
+				op, timeout, in_mb_size, cmd_path);
+
+#ifdef PD_BU
+#ifdef SX_DEBUG
+	if (!err)
+		printk(KERN_INFO PFX "__sx_cmd: command %s finished successfully\n", cmd_str(op));
+#endif
+	if (err) 
+		printk(KERN_INFO PFX "__sx_cmd: command %s finished with err %d\n", cmd_str(op), err);
+#endif
+	return err;
+}
+EXPORT_SYMBOL(__sx_cmd);
+
+void sx_cmd_unmap(struct sx_dev *dev)
+{
+	struct sx_cmd *cmd = &sx_priv(dev)->cmd;
+
+	iounmap(cmd->hcr);
+	cmd->hcr = NULL;
+}
+
+int sx_cmd_pool_create(struct sx_dev *dev)
+{
+	struct sx_cmd *cmd = &sx_priv(dev)->cmd;
+#if 0
+	cmd->pool = pci_pool_create("sx_cmd", dev->pdev,
+					 SX_MAILBOX_SIZE,
+					 SX_MAILBOX_SIZE, 0);
+#endif
+	if (!cmd->pool)
+		return -ENOMEM;
+
+	return 0;
+}
+
+void sx_cmd_pool_destroy(struct sx_dev *dev)
+{
+	struct sx_cmd *cmd = &sx_priv(dev)->cmd;
+#if 0
+	pci_pool_destroy(cmd->pool);
+#endif
+	cmd->pool = NULL;
+}
+
+int sx_cmd_init(struct sx_dev *dev)
+{
+	struct sx_cmd *cmd = &sx_priv(dev)->cmd;
+
+	mutex_init(&cmd->hcr_mutex);
+	sema_init(&cmd->pci_poll_sem, 1);
+	sema_init(&cmd->i2c_poll_sem, 1);
+	sema_init(&cmd->sgmii_poll_sem, 1);
+	cmd->use_events = 0;
+	cmd->toggle     = 1;
+	cmd->max_cmds = 10;
+
+	return 0;
+}
+
+int sx_cmd_init_pci(struct sx_dev *dev)
+{
+	sx_priv(dev)->cmd.hcr = ioremap(pci_resource_start(dev->pdev, 0) +
+			SX_HCR_BASE, SX_HCR_SIZE);
+	if (!sx_priv(dev)->cmd.hcr) {
+		sx_err(dev, "Couldn't map command register.");
+		return -ENOMEM;
+	}
+
+	sx_info(dev, "map cmd: phys: 0x%llx , virt: 0x%p \n",
+			(u64)(pci_resource_start(dev->pdev, 0) + SX_HCR_BASE),
+			sx_priv(dev)->cmd.hcr);
+
+	return 0;
+}
+
+/*
+ * Switch to using events to issue FW commands (can only be called
+ * after event queue for command events has been initialized).
+ */
+int sx_cmd_use_events(struct sx_dev *dev)
+{
+	struct sx_priv *priv = sx_priv(dev);
+	int i;
+
+#ifdef PD_BU
+	return 0; /* No events yet... */
+#endif
+	if (priv->cmd.use_events == 1)
+		return 0;
+
+	priv->cmd.context = kmalloc(priv->cmd.max_cmds *
+				sizeof(struct sx_cmd_context), GFP_KERNEL);
+	if (!priv->cmd.context)
+		return -ENOMEM;
+
+	for (i = 0; i < priv->cmd.max_cmds; ++i) {
+		priv->cmd.context[i].token = i;
+		priv->cmd.context[i].next  = i + 1;
+	}
+
+	priv->cmd.context[priv->cmd.max_cmds - 1].next = -1;
+	priv->cmd.free_head = 0;
+	sema_init(&priv->cmd.event_sem, priv->cmd.max_cmds);
+	spin_lock_init(&priv->cmd.context_lock);
+	for (priv->cmd.token_mask = 1;
+	     priv->cmd.token_mask < priv->cmd.max_cmds;
+	     priv->cmd.token_mask <<= 1)
+		; /* nothing */
+	--priv->cmd.token_mask;
+
+	priv->cmd.use_events = 1;
+	down(&priv->cmd.pci_poll_sem);
+
+	return 0;
+}
+EXPORT_SYMBOL(sx_cmd_use_events);
+
+/*
+ * Switch back to polling (used when shutting down the device)
+ */
+void sx_cmd_use_polling(struct sx_dev *dev)
+{
+	struct sx_priv *priv = sx_priv(dev);
+	int i;
+
+	if (priv->cmd.use_events == 0)
+		return;
+
+	priv->cmd.use_events = 0;
+
+	for (i = 0; i < priv->cmd.max_cmds; ++i)
+		down(&priv->cmd.event_sem);
+
+	kfree(priv->cmd.context);
+
+	up(&priv->cmd.pci_poll_sem);
+}
+EXPORT_SYMBOL(sx_cmd_use_polling);
+
+struct sx_cmd_mailbox *sx_alloc_cmd_mailbox(struct sx_dev *dev, int sx_dev_id)
+{
+	struct sx_cmd_mailbox *mailbox;
+
+	mailbox = kmalloc(sizeof *mailbox, GFP_KERNEL);
+	if (!mailbox)
+		return ERR_PTR(-ENOMEM);
+
+	if (!dev->pdev) {
+		mailbox->buf = kzalloc(SX_MAILBOX_SIZE, GFP_KERNEL);
+	} else {
+		if (!sx_dpt_is_path_valid(dev->device_id, DPT_PATH_PCI_E)) {
+			kfree(mailbox);
+			return ERR_PTR(-ENOMEM);
+		}
+#if 0
+		mailbox->buf = pci_pool_alloc(sx_priv(dev)->cmd.pool,
+				GFP_KERNEL, &mailbox->dma);
+#endif
+	}
+
+	if (!mailbox->buf) {
+		kfree(mailbox);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	return mailbox;
+}
+EXPORT_SYMBOL(sx_alloc_cmd_mailbox);
+
+
+void sx_free_cmd_mailbox(struct sx_dev *dev, struct sx_cmd_mailbox *mailbox)
+{
+	if (!mailbox)
+		return;
+
+	if (!dev->pdev) {
+		kfree(mailbox->buf);
+	} else {
+#if 0
+		if (mailbox->dma)
+			pci_pool_free(sx_priv(dev)->cmd.pool, mailbox->buf,
+					mailbox->dma);
+#endif
+	}
+
+	kfree(mailbox);
+}
+EXPORT_SYMBOL(sx_free_cmd_mailbox);
+
+
+/************************************************
+ *                  EOF                         *
+ ***********************************************/
diff --git a/linux/drivers/hwmon/mellanox/cq.c b/linux/drivers/hwmon/mellanox/cq.c
new file mode 100644
index 0000000..15e5e29
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/cq.c
@@ -0,0 +1,1296 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2016 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+/************************************************
+ * Includes
+ ***********************************************/
+
+#include <linux/sched.h>
+#include <linux/skbuff.h>
+#include <linux/wait.h>
+#include <linux/sched.h>
+#include <linux/delay.h>
+#include <linux/hardirq.h>
+#include <linux/interrupt.h>
+#include <linux/kthread.h>
+#include <linux/mlx_sx/device.h>
+#include <linux/mlx_sx/driver.h>
+#include <linux/mlx_sx/cmd.h>
+#include <linux/mlx_sx/kernel_user.h>
+#include "sx.h"
+#include "cq.h"
+#include "dq.h"
+#include "ib.h"
+#include "alloc.h"
+#include "sx_proc.h"
+
+/************************************************
+ *  Definitions
+ ***********************************************/
+
+#define MAX_MATCHING_LISTENERS 100
+#define ETH_CRC_LENGTH 4
+#define IB_CRC_LENGTH 6
+
+/************************************************
+ * Globals
+ ***********************************************/
+
+extern struct sx_globals sx_glb;
+extern int rx_debug;
+extern int rx_debug_pkt_type;
+extern int rx_debug_emad_type;
+extern int rx_dump;
+extern int rx_dump_cnt;
+unsigned int credit_thread_vals[1001] = {0};
+unsigned int arr_count = 0;
+atomic_t cq_backup_polling_enabled=ATOMIC_INIT(1);
+int debug_cq_backup_poll_cqn=CQN_INVALID;
+struct handler_entry {
+	cq_handler handler;
+	void *context;
+};
+
+/************************************************
+ *  Functions
+ ***********************************************/
+
+static u16 get_truncate_size_from_db(struct sx_dev *dev, int dqn)
+{
+	u16 ret = 0;
+    unsigned long flags;
+
+	spin_lock_irqsave(&sx_priv(dev)->db_lock,flags);
+	ret = sx_priv(dev)->truncate_size_db[dqn];
+	spin_unlock_irqrestore(&sx_priv(dev)->db_lock,flags);
+
+	return ret;
+}
+
+/* Returns 1 if the port/lag id is found in the trap filter DB and the packet should be dropped */
+static u8 check_trap_port_in_filter_db(struct sx_dev *dev, u16 hw_synd,
+		u8 is_lag, u16 sysport_lag_id)
+{
+	int i;
+	u8 ret = 0;
+    unsigned long flags;
+
+	if (is_lag) {
+		u16 lag_id = (sysport_lag_id >> 4) & 0xfff;
+		spin_lock_irqsave(&sx_priv(dev)->db_lock,flags);
+		for (i = 0; i < MAX_LAG_PORTS_IN_FILTER; i++) {
+			if (sx_priv(dev)->lag_filter_db[hw_synd][i] == lag_id) {
+				inc_filtered_lag_packets_counter(dev);
+				ret = 1;
+				break;
+			}
+		}
+
+		spin_unlock_irqrestore(&sx_priv(dev)->db_lock,flags);
+		return ret;
+	}
+
+	/* EMADs can be received with sysport==0 */
+	if (sysport_lag_id == 0)
+		return 0;
+
+	spin_lock_irqsave(&sx_priv(dev)->db_lock,flags);
+	for (i = 0; i < MAX_SYSTEM_PORTS_IN_FILTER; i++) {
+		if (sx_priv(dev)->sysport_filter_db[hw_synd][i] == sysport_lag_id) {
+			inc_filtered_port_packets_counter(dev);
+			ret = 1;
+			break;
+		}
+	}
+
+	spin_unlock_irqrestore(&sx_priv(dev)->db_lock,flags);
+	return ret;
+}
+
+static u16 get_vid_from_db(struct sx_dev *dev, u8 is_lag, u16 sysport_lag_id)
+{
+	u16 ret = 1;
+    unsigned long flags;
+
+	spin_lock_irqsave(&sx_priv(dev)->db_lock,flags);
+	if (is_lag) {
+		u16 lag_id = (sysport_lag_id >> 4) & 0xfff;
+		ret = sx_priv(dev)->pvid_lag_db[lag_id];
+	} else
+		ret = sx_priv(dev)->pvid_sysport_db[sysport_lag_id];
+
+	spin_unlock_irqrestore(&sx_priv(dev)->db_lock,flags);
+
+	return ret;
+}
+
+static u32 get_qpn(u16 hw_synd, struct sk_buff *skb)
+{
+	u8 lnh = 0;
+	u32 qpn;
+
+	lnh = ((struct ib_header_lrh *)skb->data)->sl_lnh & 0x3;
+	if (lnh == 3)
+		qpn = be32_to_cpu(((struct ib_header_multicast *)
+				skb->data)->bth.dest_qp) & 0xffffff;
+	else if (lnh == 2)
+		qpn = be32_to_cpu(((struct ib_header_unicast *)
+				skb->data)->bth.dest_qp) & 0xffffff;
+	else
+		qpn = 0xffffffff;
+
+	return qpn;
+}
+
+static int is_matching(struct completion_info *ci,
+		struct listener_entry *listener)
+{
+	if (listener->swid != ci->swid &&
+			listener->swid != SWID_NUM_DONT_CARE) {
+		return 0;
+	}
+
+	/* If the packet came from a user (loopback), don't return it to the same user */
+	if (ci->context != NULL && listener->context == ci->context) {
+		return 0;
+	}
+
+	switch (listener->listener_type) {
+	case L2_TYPE_DONT_CARE:
+		if (listener->critireas.dont_care.sysport ==
+				SYSPORT_DONT_CARE_VALUE) {
+			return 1;
+		}
+
+		/* LAGs will also work in the same way */
+		if (ci->sysport != listener->critireas.dont_care.sysport) {
+			break;
+		}
+
+		return 1;
+	case L2_TYPE_ETH:
+		if ((ci->pkt_type != PKT_TYPE_ETH) &&
+				(ci->pkt_type != PKT_TYPE_FCoETH)) {
+		    break;
+		}
+
+		if (ci->info.eth.ethtype != listener->critireas.eth.ethtype &&
+			listener->critireas.eth.ethtype !=
+					ETHTYPE_DONT_CARE_VALUE) {
+			break;
+		}
+
+		if (ci->info.eth.dmac != listener->critireas.eth.dmac
+			&& listener->critireas.eth.dmac !=
+					DMAC_DONT_CARE_VALUE) {
+			break;
+		}
+
+		if (listener->critireas.eth.emad_tid != TID_DONT_CARE_VALUE &&
+				ci->info.eth.emad_tid !=
+						listener->critireas.eth.emad_tid) {
+			break;
+		}
+
+        if (listener->critireas.eth.from_rp != IS_RP_DONT_CARE_E &&
+                ci->info.eth.from_rp != listener->critireas.eth.from_rp) {
+            break;
+        }
+
+        if (listener->critireas.eth.from_bridge != IS_BRIDGE_DONT_CARE_E &&
+                ci->info.eth.from_bridge != listener->critireas.eth.from_bridge) {
+            break;
+        }
+
+		return 1;
+	case L2_TYPE_IB:
+		/* TODO: IB Raw packets have no IB header at all so they
+		 * need a special handling */
+		if (ci->pkt_type == PKT_TYPE_IB_Raw ||
+				ci->pkt_type == PKT_TYPE_IB_non_Raw ||
+				ci->pkt_type == PKT_TYPE_FCoIB ||
+				ci->pkt_type == PKT_TYPE_ETHoIB) {
+			if (ci->info.ib.qpn == listener->critireas.ib.qpn ||
+				listener->critireas.ib.qpn ==
+						QPN_DONT_CARE_VALUE) {
+				return 1;
+			}
+		}
+
+		break;
+	default:
+		break;
+	}
+
+	return 0;
+}
+
+/*
+ * filter the listeners table, and call all relevant handlers
+ */
+void dispatch_pkt(struct sx_dev *dev, struct completion_info *ci, u16 entry, int dispatch_default) 
+{
+	struct listener_entry *listener;
+	struct list_head *pos;
+	unsigned long flags;
+	static struct handler_entry callbacks[MAX_MATCHING_LISTENERS];
+	int num_found = 0;
+	int i = 0;
+
+	/* validate the syndrome range */
+	if (entry > NUM_HW_SYNDROMES){
+		printk(KERN_ERR PFX "Error: arrived synd %d is out of range (1..%d) \n",
+				entry, NUM_HW_SYNDROMES);
+		return;
+	}
+
+	spin_lock_irqsave(&sx_glb.listeners_lock, flags);
+	/* Checking syndrome registration and NUM_HW_SYNDROMES callback iff dispatch_default set */
+	/* I don't like the syndrome dispatchers at all, but it's too late to change */
+	while (1) {
+		if (!list_empty(&sx_glb.listeners_db[entry].list)) {
+			list_for_each(pos, &sx_glb.listeners_db[entry].list) {
+				listener = list_entry(pos, struct listener_entry, list);
+
+				if ((listener->is_default && num_found == 0) || is_matching(ci, listener)) {
+					callbacks[num_found].handler =
+							listener->handler;
+					callbacks[num_found].context =
+							listener->context;
+					listener->rx_pkts++;
+					++num_found;
+				}
+				if (num_found == MAX_MATCHING_LISTENERS)
+					break;
+			}
+		}
+		if (!dispatch_default || (entry ==  NUM_HW_SYNDROMES) || (num_found == MAX_MATCHING_LISTENERS))
+			break;
+		entry = NUM_HW_SYNDROMES;
+	}
+	spin_unlock_irqrestore(&sx_glb.listeners_lock, flags);
+
+	for (i = 0; i < num_found; i++)
+		callbacks[i].handler(ci, callbacks[i].context);
+
+	if (num_found == 0){
+		inc_unconsumed_packets_counter(dev, ci->hw_synd, ci->pkt_type);
+	}
+}
+
+static int chk_completion_info(struct completion_info *ci)
+{
+	int err = 0;
+
+	if (ci->swid >= NUMBER_OF_SWIDS && ci->swid != SWID_NUM_DONT_CARE) {
+		err = -EINVAL;
+#ifdef SX_DEBUG
+		if (printk_ratelimit())
+			printk(KERN_DEBUG PFX "The given cqe is not valid: " \
+				" swid=[%d]\n", ci->swid);
+#endif
+	}
+
+	if (ci->hw_synd > NUM_HW_SYNDROMES) {
+		err = -EINVAL;
+#ifdef SX_DEBUG
+		if (printk_ratelimit())
+			printk(KERN_DEBUG PFX "The given cqe is not valid: " \
+				"hw_synd=[%d]\n", ci->hw_synd);
+#endif
+	}
+
+	return err;
+}
+
+/*
+ * extracts the needed data from the cqe and from the packet, calls
+ * the filter listeners with that info
+ */
+int rx_skb(void *context, struct sk_buff *skb, struct sx_cqe *cqe)
+{
+	struct completion_info *ci = NULL;
+	u16 hw_synd;
+	int err = 0;
+	int dqn;
+	u16 truncate_size;
+	u8 crc_present = 0;
+	struct sx_dev * sx_device = (struct sx_dev *)context;
+	u8 swid = 0;
+	u8 is_from_rp = IS_RP_DONT_CARE_E;
+	u16 fid = 0;
+
+#ifdef SX_DEBUG
+	printk(KERN_DEBUG PFX "rx_skb: Entered function\n");
+
+{
+	__be32 *buf = (void *)cqe;
+/*	int i; */
+
+	if (printk_ratelimit())
+		printk(KERN_DEBUG "CQE %p contents:\n%08x\n%08x\n%08x\n%08x\n",
+				cqe, be32_to_cpu(buf[0]), be32_to_cpu(buf[1]),
+				be32_to_cpu(buf[2]), be32_to_cpu(buf[3]));
+
+/*	buf = (void *)skb->data;
+	printk(KERN_DEBUG "packet contents:\n");
+	for (i = 0; i < be16_to_cpu(cqe->byte_count)/4; i++)
+		printk(KERN_DEBUG "%08x\n", be32_to_cpu(buf[i])); */
+}
+
+#endif
+
+	ci = kzalloc(sizeof(*ci), GFP_ATOMIC);
+	if (!ci) {
+			err = -ENOMEM;
+			goto out_free_skb;
+	}
+
+	hw_synd  = be16_to_cpu(cqe->trap_id) & 0x01FF;
+	/* TODO: WA because LP packets return with hw_synd = 0 */
+	if (!hw_synd) {
+		hw_synd = 1;
+#ifdef SX_DEBUG
+		printk(KERN_DEBUG PFX "Got a packet with trap_id==0, "
+				"probably a LP response\n");
+#endif
+	}
+
+	/* update skb->len to the real len,
+	 * instead of the max len we allocated */
+	skb->len = be16_to_cpu(cqe->dqn5_byte_count) & 0x3FFF;
+	/* TODO: remove this check in the future. ISX bit means ISX header is present */
+	if (be16_to_cpu(cqe->dqn5_byte_count) >> 15) {
+		skb->data += ISX_HDR_SIZE;
+		skb->len -= ISX_HDR_SIZE;
+#ifdef SX_DEBUG
+		printk(KERN_DEBUG PFX "Got a packet with ISX header\n");
+#endif
+	}
+
+	ci->skb = skb;
+	ci->swid = (cqe->type_swid >> 1) & 0x7;
+	ci->context = NULL;
+	ci->dev = sx_device;
+	/* We need to remove the CRC if present, or otherwise MTU packets
+	 * will be dropped in higher levels. */
+	crc_present = cqe->type_swid & 0x1;
+	ci->sysport = be16_to_cpu(cqe->system_port_lag_id);
+	ci->hw_synd = hw_synd;
+	ci->pkt_type = (cqe->type_swid >> 5) & 0x7;
+	ci->is_send = (cqe->e_sr_dqn_owner >> 6) & 0x1;
+	ci->is_lag = cqe->lag & 0x80 ? 1 : 0;
+	if (ci->is_lag) {
+		ci->lag_subport = cqe->vlan2_lag_subport & 0x1f;
+	}
+	else {
+		ci->lag_subport = 0;
+    }
+
+	/* If packet arrived from external port then ci->sysport != 0 or is_lag != 0 */
+	if (ci->sysport != 0 || ci->is_lag != 0) {
+		err = sx_core_get_swid(sx_device, ci, &swid);
+	    if (err) {
+	        err = -EINVAL ;
+	        goto out;
+	    }
+#ifdef SX_DEBUG
+        printk(KERN_DEBUG PFX "rx_skb() pkt_type:%d, hw_synd:%d is_lag:%d, sysport:0x%x, "
+                "old_swid:%d, new_swid: %d\n", ci->pkt_type, ci->hw_synd, ci->is_lag,
+                ci->sysport, ci->swid, swid);
+#endif
+        ci->swid = swid;
+	}
+	else if (ci->swid < NUMBER_OF_SWIDS &&
+	    sx_device->profile.swid_type[ci->swid] == SX_KU_L2_TYPE_ROUTER_PORT){
+		/* if event arrived from Router Port Swid forward it to swid 0 (Default ETH swid) */
+	    ci->swid = 0;
+	}
+
+	if (rx_debug &&
+		(rx_debug_pkt_type == SX_DBG_PACKET_TYPE_ANY ||
+		 rx_debug_pkt_type == ci->hw_synd) &&
+		(rx_debug_emad_type == SX_DBG_EMAD_TYPE_ANY ||
+		rx_debug_emad_type ==
+				be16_to_cpu(((struct sx_emad *)skb->data)->emad_op.register_id)) ) {
+
+		__be32 *cqebuf = (void *)cqe;
+
+		printk(KERN_DEBUG "CQE %p contents:\n%08x\n%08x\n%08x\n%08x\n",
+				cqe, be32_to_cpu(cqebuf[0]),
+				be32_to_cpu(cqebuf[1]),
+				be32_to_cpu(cqebuf[2]),
+				be32_to_cpu(cqebuf[3]));
+
+		printk(KERN_DEBUG PFX "rx_skb: swid = %d, "
+			"sysport = %d, hw_synd = %d (reg_id: 0x%x),"
+			"pkt_type = %d, byte_count = %d, is_lag: %d\n",
+			ci->swid, ci->sysport, ci->hw_synd,
+			be16_to_cpu(((struct sx_emad *)skb->data)->emad_op.register_id),
+			ci->pkt_type, skb->len, ci->is_lag);
+
+
+		if (rx_dump) {
+			int i;
+			u8 *buf = (void *)skb->data;
+			int cnt = skb->len;
+
+			for (i = 0; i < cnt; i++) {
+				if (i == 0 || (i%4 == 0))
+					printk("\n0x%04x : ", i);
+
+				printk(" 0x%02x", buf[i]);
+			}
+
+			printk("\n");
+		}
+
+		if (rx_dump_cnt != SX_DBG_COUNT_UNLIMITED && rx_dump_cnt>0)
+			rx_dump_cnt--;
+
+		if (rx_dump_cnt == 0){
+			rx_dump = 0;
+			rx_debug = 0;
+			rx_debug_pkt_type = 0xFF;
+		}
+	}
+
+	if (ci->swid < NUMBER_OF_SWIDS) {
+		sx_device->stats.rx_by_pkt_type[ci->swid][ci->pkt_type]++;
+		sx_device->stats.rx_by_synd[ci->swid][ci->hw_synd]++;
+	} else {
+		sx_device->stats.rx_by_pkt_type[NUMBER_OF_SWIDS][ci->pkt_type]++;
+		sx_device->stats.rx_by_synd[NUMBER_OF_SWIDS][ci->hw_synd]++;
+	}
+
+	/* Check if the port/lag is in the traps filter DB. If so we silently drop the packet */
+	if (check_trap_port_in_filter_db((struct sx_dev *)context, ci->hw_synd,
+			ci->is_lag, ci->sysport))
+		goto out;
+
+	dqn = (cqe->e_sr_dqn_owner >> 1) & SX_CQE_DQN_MASK;
+	if (be16_to_cpu(cqe->dqn5_byte_count) & SX_CQE_DQN_MSB_MASK)
+		dqn |= (1 << SX_CQE_DQN_MSB_SHIFT);
+
+	/*put the original packet size befor truncation in the complition info*/
+	ci->original_packet_size = skb->len;
+
+	truncate_size = get_truncate_size_from_db((struct sx_dev *)context, dqn);
+	if (truncate_size > 0 && truncate_size < skb->len)
+			skb->len = truncate_size;
+
+	switch (ci->pkt_type) {
+	case PKT_TYPE_ETH:
+	case PKT_TYPE_FCoETH:
+		ci->info.eth.ethtype = be16_to_cpu(((struct sx_eth_hdr *)
+				skb->data)->ethertype);
+		ci->info.eth.dmac = be64_to_cpu(((struct sx_eth_hdr *)
+				skb->data)->dmac_smac1) >> 16;
+		if (ci->info.eth.ethtype == ETHTYPE_EMAD)
+			ci->info.eth.emad_tid = be64_to_cpu(((struct sx_emad *)
+				skb->data)->emad_op.tid) >> 32;
+		else if (ci->info.eth.ethtype == ETHTYPE_VLAN) {
+			ci->is_tagged = 1;
+			/* Router Port is not supported in SwitchX A1.
+			 * Get vlan from cqe: vlan [0:9] bits from 16 bit field (ulp_crc_vlan_flow)
+			 * and vlan [10:11] bits from 8 bit field (vlan2_lag_subport) */
+			ci->vid = ((be16_to_cpu(cqe->ulp_crc_vlan_flow) & 0x3ff0) >> 4) |
+			        ((cqe->vlan2_lag_subport & 0x60) << 5);
+		}
+		else {
+			ci->is_tagged = 0;
+			ci->vid = get_vid_from_db((struct sx_dev *)context,
+					ci->is_lag, ci->sysport);
+		}
+		if(crc_present){
+		    ci->original_packet_size -= ETH_CRC_LENGTH;
+		}
+		if (crc_present && !truncate_size){
+		    skb->len -= ETH_CRC_LENGTH;
+		}
+
+        /* if sysport is 0 and is_lag is 0 that the packet is FW event,
+            and we don't need to check if from RP / bridge */
+		if (ci->sysport != 0 || ci->is_lag != 0) {
+            err = sx_core_get_rp_mode((struct sx_dev *)context,
+                                      ci->is_lag, ci->sysport, ci->vid,
+                                      &is_from_rp);
+            if (err) {
+                printk(KERN_ERR PFX "Failed sx_core_get_rp_mode(). err: %d \n",err);
+            }
+            ci->info.eth.from_rp = (is_from_rp) ? IS_RP_FROM_RP_E : IS_RP_NOT_FROM_RP_E;
+        
+            err = sx_core_get_fid_by_port_vid((struct sx_dev *)context, ci, &fid);
+            if (err) {
+                printk(KERN_ERR PFX "Failed sx_core_get_bridge(). err: %d \n",err);
+            }
+            ci->bridge_id = fid;
+            ci->info.eth.from_bridge = (fid) ?
+                    IS_BRIDGE_FROM_BRIDGE_E : IS_BRIDGE_NOT_FROM_BRIDGE_E;
+		}
+		break;
+	case PKT_TYPE_IB_Raw: /* TODO: Extract qpn from IB Raw pkts */
+	case PKT_TYPE_IB_non_Raw:
+	case PKT_TYPE_FCoIB:
+	case PKT_TYPE_ETHoIB:
+		ci->info.ib.qpn = get_qpn(hw_synd, skb);
+		if (ci->info.ib.qpn == 0xffffffff) {
+			if (printk_ratelimit())
+				printk(KERN_WARNING PFX "Received IB packet "
+					"is not valid. Dropping the packet\n");
+			err = -EINVAL;
+			goto out;
+		}
+
+		/* Extract the IB port from the sysport */
+		ci->sysport = (ci->sysport >> 4) & 0x7f;
+		if (crc_present && !truncate_size)
+			skb->len -= IB_CRC_LENGTH;
+		break;
+	default:
+		if (printk_ratelimit())
+			printk(KERN_WARNING PFX "Received packet type is FC, "
+				"and therefore unsupported right now\n");
+		err = -EINVAL;
+		goto out;
+	}
+
+	err = chk_completion_info(ci);
+	if (err) {
+		err = -EINVAL ;
+		goto out;
+	}
+
+#ifdef SX_DEBUG
+	if (printk_ratelimit())
+		printk(KERN_DEBUG PFX " rx_skb() received packet data: "
+			"skb->len=[%d] sysport=[%d] hw_synd(trap_id)=[%d] "
+			"swid=[%d] pkt_type=[%d]\n",
+			ci->skb->len, ci->sysport, ci->hw_synd,
+			ci->swid, ci->pkt_type);
+#endif
+
+	dispatch_pkt((struct sx_dev *)context, ci, hw_synd, 1);
+
+out:
+	kfree(ci);
+out_free_skb:
+	sx_skb_free(skb);
+
+	return err;
+}
+EXPORT_SYMBOL(rx_skb);
+
+static void *sx_get_cqe_from_buf(struct sx_buf *buf, int n)
+{
+	int offset = n * sizeof(struct sx_cqe);
+
+	if (buf->nbufs == 1)
+		return buf->u.direct.buf + offset;
+	else
+		return buf->u.page_list[offset >> PAGE_SHIFT].buf +
+			(offset & (PAGE_SIZE - 1));
+}
+
+static void *sx_get_cqe(struct sx_cq *cq, int n)
+{
+	return sx_get_cqe_from_buf(&cq->buf, n);
+}
+
+static void *sx_get_sw_cqe(struct sx_cq *cq, int n)
+{
+	struct sx_cqe *cqe = sx_get_cqe(cq, n & (cq->nent - 1));
+
+	return (cqe->e_sr_dqn_owner & 0x1) ^ !!(n & cq->nent) ? NULL : cqe;
+}
+
+static struct sx_cqe *sx_next_cqe_sw(struct sx_cq *cq)
+{
+	return sx_get_sw_cqe(cq, cq->cons_index);
+}
+
+void wqe_sync_for_cpu(struct sx_dq *dq, int idx)
+{
+	int dir = dq->is_send ? DMA_TO_DEVICE : DMA_FROM_DEVICE;
+
+	pci_dma_sync_single_for_cpu(dq->dev->pdev,
+			dq->sge[idx].hdr_pld_sg.dma_addr,
+			dq->sge[idx].hdr_pld_sg.len, dir);
+	pci_unmap_single(dq->dev->pdev,
+			dq->sge[idx].hdr_pld_sg.dma_addr,
+			dq->sge[idx].hdr_pld_sg.len, dir);
+	dq->sge[idx].hdr_pld_sg.vaddr = NULL;
+	dq->sge[idx].hdr_pld_sg.len = 0;
+	if (dq->is_send) {
+		if (dq->sge[idx].pld_sg_1.len) {
+			pci_dma_sync_single_for_cpu(dq->dev->pdev,
+					dq->sge[idx].pld_sg_1.dma_addr,
+					dq->sge[idx].pld_sg_1.len, dir);
+			pci_unmap_single(dq->dev->pdev,
+					dq->sge[idx].pld_sg_1.dma_addr,
+					dq->sge[idx].pld_sg_1.len, dir);
+			dq->sge[idx].pld_sg_1.vaddr = NULL;
+			dq->sge[idx].pld_sg_1.len = 0;
+		}
+
+		if (dq->sge[idx].pld_sg_2.len) {
+			pci_dma_sync_single_for_cpu(dq->dev->pdev,
+					dq->sge[idx].pld_sg_2.dma_addr,
+					dq->sge[idx].pld_sg_2.len, dir);
+			pci_unmap_single(dq->dev->pdev,
+					dq->sge[idx].pld_sg_2.dma_addr,
+					dq->sge[idx].pld_sg_2.len, dir);
+			dq->sge[idx].pld_sg_2.vaddr = NULL;
+			dq->sge[idx].pld_sg_2.len = 0;
+		}
+	}
+}
+
+static int post_skb(struct sx_dq *dq)
+{
+	u16 size = dq->dev->profile.rdq_properties[dq->dqn].entry_size;
+	int err = 0;
+	struct sk_buff *new_skb;
+
+	new_skb = alloc_skb(size, GFP_ATOMIC);
+	if (!new_skb) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	if (skb_put(new_skb, size) == NULL) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	sx_core_post_recv(dq, new_skb);
+
+out:
+	return err;
+}
+
+
+static int sx_poll_one(struct sx_cq *cq)
+{
+	struct sx_cqe *cqe;
+	int dqn;
+	int is_send;
+	int is_err;
+	struct sx_dq *dq;
+	int err = 0;
+	struct sx_priv *priv;
+	u16 wqe_ctr;
+	u16 idx;
+	unsigned long flags;
+
+	spin_lock_irqsave(&cq->lock, flags);
+	cqe = sx_next_cqe_sw(cq);
+	if (!cqe) {
+		spin_unlock_irqrestore(&cq->lock, flags);
+		return -EAGAIN;
+	}
+
+	++cq->cons_index;
+	spin_unlock_irqrestore(&cq->lock, flags);
+
+	/*
+	 * Make sure we read CQ entry contents after we've checked the
+	 * ownership bit.
+	 */
+	rmb();
+
+	dqn = (cqe->e_sr_dqn_owner >> 1) & SX_CQE_DQN_MASK;
+	if (be16_to_cpu(cqe->dqn5_byte_count) & SX_CQE_DQN_MSB_MASK)
+		dqn |= (1 << SX_CQE_DQN_MSB_SHIFT);
+
+	is_err = !!(cqe->e_sr_dqn_owner & SX_CQE_IS_ERR_MASK);
+	is_send  = !!(cqe->e_sr_dqn_owner & SX_CQE_IS_SEND_MASK);
+	priv  = sx_priv(cq->sx_dev);
+	if (is_send) {
+	    if (dqn >= NUMBER_OF_SDQS) {
+            sx_warn(cq->sx_dev, "dqn %d is larger than max SDQ %d.\n",
+                    dqn, NUMBER_OF_SDQS);
+            return 0;
+	    }
+	}
+	else {
+        if (dqn >= NUMBER_OF_RDQS) {
+            sx_warn(cq->sx_dev, "dqn %d is larger than max RDQ %d.\n",
+                    dqn, NUMBER_OF_RDQS);
+            return 0;
+        }
+	}
+	dq = is_send ? priv->sdq_table.dq[dqn] : priv->rdq_table.dq[dqn];
+
+#ifdef SX_DEBUG
+	printk(KERN_DEBUG PFX "sx_poll_one: dqn = %d, is_err = %d, "
+			"is_send = %d\n", dqn, is_err, is_send);
+#endif
+
+	if (!dq) {
+		if (printk_ratelimit()) {
+			sx_warn(cq->sx_dev, "could not find dq context for %s "
+					"dqn = %d\n",
+					is_send ? "send" : "recv", dqn);
+		}
+
+		return 0;
+	}
+
+	wqe_ctr = be16_to_cpu(cqe->wqe_counter) & (dq->wqe_cnt - 1);
+	if (is_err && !dq->is_flushing) {
+		sx_warn(cq->sx_dev, "got %s completion with error, "
+			"syndrom=0x%x, vendor=0x%x\n",
+			is_send ? "send" : "recv", cqe->trap_id & 0xFF,
+			(cqe->trap_id >> 8) & 0xFF);
+
+		if (!is_send) {
+			idx = dq->tail++ & (dq->wqe_cnt - 1);
+			wqe_sync_for_cpu(dq, idx);
+			sx_skb_free(dq->sge[idx].skb);
+			dq->sge[idx].skb = NULL;
+		}
+
+		goto skip;
+	}
+
+	if (is_send) {
+		/* find the wqe and unmap the DMA buffer.
+		 * sx_skb_free to call  the destructor which will signal
+		 * the write-semaphore for ETH packets, or call gen_completion
+		 * for IB packets. */
+		do {
+			idx = dq->tail++ & (dq->wqe_cnt - 1);
+			wqe_sync_for_cpu(dq, idx);
+			sx_skb_free(dq->sge[idx].skb);
+			dq->sge[idx].skb = NULL;
+		} while (idx != wqe_ctr);
+
+		spin_lock_irqsave(&dq->lock, flags);
+		sx_add_pkts_to_sdq(dq);
+		spin_unlock_irqrestore(&dq->lock, flags);
+	} else {
+		/* get the skb from the right rdq entry, unmap the buffers
+		 * and call rx_skb with the cqe and skb */
+		/* this while is temporary, we need it because FW doesn't
+		 * send us error CQes for wqe too short. */
+		idx = dq->tail & (dq->wqe_cnt - 1);
+		while (idx != wqe_ctr) {
+			if (printk_ratelimit())
+				printk(KERN_DEBUG PFX "sx_poll_one: Err "
+						"wqe_ctr=[%u] "
+						"!= dq->tail=[%u]\n",
+						wqe_ctr, dq->tail);
+/*			idx = dq->tail & (dq->wqe_cnt - 1); */
+			wqe_sync_for_cpu(dq, idx);
+			sx_skb_free(dq->sge[idx].skb);
+			dq->sge[idx].skb = NULL;
+			dq->tail++;
+			idx = dq->tail & (dq->wqe_cnt - 1);
+			post_skb(dq);
+		}
+
+		++dq->tail;
+		wqe_sync_for_cpu(dq, idx);
+#ifdef SX_DEBUG
+		printk(KERN_DEBUG PFX "sx_poll_one: This is a RDQ, idx = %d, "
+				"wqe_ctr = %d, dq->tail = %d. "
+				"Calling rx_skb\n",
+			idx, wqe_ctr, dq->tail - 1);
+#endif
+		if (!is_err)
+			rx_skb(cq->sx_dev, dq->sge[idx].skb, cqe);
+		else
+			sx_skb_free(dq->sge[idx].skb);
+
+		dq->sge[idx].skb = NULL;
+	}
+skip:
+	sx_cq_set_ci(cq);
+	if (!is_send && !dq->is_flushing)
+		err = post_skb(dq);
+
+	if (is_send)
+		wake_up_interruptible(&dq->tx_full_wait);
+
+	return err;
+}
+
+/* return errno on error, otherwise num of handled cqes */
+int sx_cq_completion(struct sx_dev *dev, u32 cqn, u16 weight)
+{
+	struct sx_cq *cq;
+	struct cq_rate_limiter_params *rl_params = &sx_priv(dev)->cq_table.cq_rl_params[cqn];
+	unsigned long flags;
+	int num_of_cqes = 0;
+	int err = 0;
+
+	spin_lock_irqsave(&sx_priv(dev)->cq_table.lock, flags);
+	cq = (sx_priv(dev)->cq_table.cq[cqn]);
+	spin_unlock_irqrestore(&sx_priv(dev)->cq_table.lock, flags);
+	if (!cq) {
+		if (printk_ratelimit()) {
+			sx_warn(dev, "Completion event for bogus CQ %08x\n", cqn);
+		}
+
+		return -EAGAIN;
+	}
+
+	do {
+		if (rl_params->use_limiter && rl_params->curr_cq_credit == 0) {
+			/* Print a message on the first time we drop a packet on each RDQ */
+			if (!rl_params->num_cq_stops) {
+				sx_warn(dev, "RDQ rate limiter was activated for RDQ %u\n",
+						cqn - NUMBER_OF_SDQS);
+			}
+
+			rl_params->num_cq_stops++;
+			goto out;
+		}
+
+		err = sx_poll_one(cq);
+		/* -EAGAIN is the only error where the consumer index is not increased */
+		if (rl_params->use_limiter && err != -EAGAIN) {
+			rl_params->curr_cq_credit--;
+		} else {
+	        atomic_inc(&cq->bkp_poll_data.curr_num_cq_polls);
+		}
+	} while (!err && ++num_of_cqes < weight);
+
+	if (num_of_cqes < weight) {
+	    if (rl_params->use_limiter && (rl_params->curr_cq_credit == 0)) {
+	        sx_info(dev,"CQ:%d tried to ARM from tasklet with zero credits\n",cqn);
+	        goto out;
+	    }
+		sx_cq_arm(cq);
+	}
+
+out:
+	if (!err || err == -EAGAIN)
+		return num_of_cqes;
+
+	return err;
+}
+
+
+static int sx_SW2HW_CQ(struct sx_dev *dev, struct sx_cmd_mailbox *mailbox,
+			 int cq_num)
+{
+	return sx_cmd(dev, dev->device_id, mailbox, cq_num, 0, SX_CMD_SW2HW_CQ,
+			SX_CMD_TIME_CLASS_A, sx_priv(dev)->fw.local_in_mb_size);
+}
+
+static int sx_HW2SW_CQ(struct sx_dev *dev, int cq_num)
+{
+	return sx_cmd_box(dev, dev->device_id, 0, 0, cq_num, 0,
+			SX_CMD_HW2SW_CQ, SX_CMD_TIME_CLASS_A,
+			sx_priv(dev)->fw.local_in_mb_size);
+}
+
+int sx_core_create_cq(struct sx_dev *dev, int nent, struct sx_cq **cq, u8 cqn)
+{
+	struct sx_priv *priv = sx_priv(dev);
+	struct sx_cq_table *cq_table = &priv->cq_table;
+	struct sx_cmd_mailbox *mailbox;
+	struct sx_cq_context *cq_context;
+	int err;
+	unsigned long flags;
+	struct sx_cq *tcq;
+
+	if (nent <= 0 || dev == NULL)
+		return -EINVAL;
+
+	tcq = kzalloc(sizeof *tcq, GFP_KERNEL);
+	if (!tcq)
+		return -ENOMEM;
+
+	tcq->cqn = sx_bitmap_set(&cq_table->bitmap, cqn);
+	if (tcq->cqn == -1) {
+		err = -ENOMEM;
+		goto out_free_cq;
+	}
+
+	spin_lock_irqsave(&cq_table->lock, flags);
+	cq_table->cq[tcq->cqn] = tcq;
+	spin_unlock_irqrestore(&cq_table->lock, flags);
+	mailbox = sx_alloc_cmd_mailbox(dev, dev->device_id);
+	if (IS_ERR(mailbox)) {
+		err = PTR_ERR(mailbox);
+		goto free_from_cq_table;
+	}
+
+	cq_context = mailbox->buf;
+	memset(cq_context, 0, sizeof *cq_context);
+	err = sx_buf_alloc(dev, nent * sizeof(struct sx_cqe), PAGE_SIZE,
+			   &tcq->buf, 1);
+	if (err)
+		goto err_mbox;
+
+	tcq->nent = nent;
+	cq_context->eq_num = priv->eq_table.eq[SX_EQ_COMP].eqn;
+	cq_context->log_cq_size = (u8)ilog2(nent);
+	if (dev->pdev)
+		sx_fill_page_list(cq_context->dma_addr, &tcq->buf);
+
+	err = sx_SW2HW_CQ(dev, mailbox, tcq->cqn);
+	if (err)
+		goto err_buf;
+
+	sx_free_cmd_mailbox(dev, mailbox);
+	tcq->set_ci_db  = dev->db_base + SX_DBELL_CQ_CI_OFFSET + 4 * tcq->cqn;
+	tcq->arm_db     = dev->db_base + SX_DBELL_CQ_ARM_OFFSET + 4 * tcq->cqn;
+	tcq->sx_dev = dev;
+	tcq->cons_index = 0;
+	atomic_set(&tcq->refcount, 1);
+    atomic_set(&tcq->bkp_poll_data.curr_num_cq_polls,0);
+    atomic_set(&tcq->bkp_poll_data.cq_bkp_poll_mode,0);
+	init_completion(&tcq->free);
+	spin_lock_init(&tcq->lock);
+	spin_lock_init(&tcq->rearm_lock);
+	sx_cq_set_ci(tcq);
+	sx_cq_arm(tcq);
+	*cq = tcq;
+
+	return 0;
+
+err_buf:
+	sx_buf_free(dev, tcq->nent * sizeof(struct sx_cqe), &tcq->buf);
+
+err_mbox:
+	sx_free_cmd_mailbox(dev, mailbox);
+
+free_from_cq_table:
+	spin_lock_irqsave(&cq_table->lock, flags);
+	cq_table->cq[tcq->cqn] = NULL;
+	spin_unlock_irqrestore(&cq_table->lock, flags);
+	sx_bitmap_free(&cq_table->bitmap, tcq->cqn);
+
+out_free_cq:
+	kfree(tcq);
+
+	return err;
+}
+
+
+void sx_core_destroy_cq(struct sx_dev *dev, struct sx_cq *cq)
+{
+	struct sx_priv *priv = sx_priv(dev);
+	struct sx_cq_table *cq_table = &priv->cq_table;
+	unsigned long flags;
+	int err;
+
+	if (!cq) {
+		sx_err(dev, "sx_core_destroy_cq  : cq(%p) == NULL\n", cq);
+		return;
+	}
+
+	spin_lock_irqsave(&cq_table->lock, flags);
+	cq_table->cq[cq->cqn] = NULL;
+	spin_unlock_irqrestore(&cq_table->lock, flags);
+	err = sx_HW2SW_CQ(dev, cq->cqn);
+	if (err) {
+		sx_warn(dev, "HW2SW_CQ failed (%d) "
+				"for CQN %06x\n", err, cq->cqn);
+	}
+
+	synchronize_irq(priv->eq_table.eq[SX_EQ_COMP].irq);
+	if (atomic_dec_and_test(&cq->refcount))
+		complete(&cq->free);
+	wait_for_completion(&cq->free);
+
+	sx_bitmap_free(&cq_table->bitmap, cq->cqn);
+	sx_buf_free(dev, cq->nent * sizeof(struct sx_cqe), &cq->buf);
+	kfree(cq);
+}
+
+int sx_core_init_cq_table(struct sx_dev *dev)
+{
+	struct sx_cq_table *cq_table = &sx_priv(dev)->cq_table;
+	struct sx_cq      **cq_array = NULL;
+	int		    err	     = 0;
+	int		    i	     = 0;
+	unsigned long flags;
+
+	spin_lock_init(&cq_table->lock);
+
+	cq_array = kmalloc(dev->dev_cap.max_num_cqs * sizeof(*cq_array),
+			   GFP_KERNEL);
+	if (!cq_array)
+		return -ENOMEM;
+
+	for (i = 0; i < dev->dev_cap.max_num_cqs; i++)
+		cq_array[i] = NULL;
+
+	spin_lock_irqsave(&cq_table->lock, flags);
+	cq_table->cq = cq_array;
+	spin_unlock_irqrestore(&cq_table->lock, flags);
+	err = sx_bitmap_init(&cq_table->bitmap, dev->dev_cap.max_num_cqs);
+	if (err)
+		return err;
+
+	cq_table->cq_credit_thread = NULL;
+	cq_table->credit_thread_active = 0;
+	cq_table->rl_time_interval = 50; /* This is the default value */
+	cq_table->cq_rl_params = kzalloc(dev->dev_cap.max_num_cqs * sizeof(*cq_table->cq_rl_params),
+			   GFP_KERNEL);
+	if (!cq_table->cq_rl_params) {
+		printk(KERN_ERR PFX "Not enough memory for the rate limiter parameters\n");
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+void sx_core_destroy_cq_table(struct sx_dev *dev)
+{
+	struct sx_cq_table *cq_table = &sx_priv(dev)->cq_table;
+	unsigned long flags;
+	int i;
+
+	if (cq_table->cq_credit_thread) {
+		kthread_stop(cq_table->cq_credit_thread);
+		cq_table->cq_credit_thread = NULL;
+		for (i = 0; i < dev->dev_cap.max_num_cqs; i++) {
+			cq_table->cq_rl_params[i].use_limiter = 0;
+		}
+	}
+
+	kfree(cq_table->cq_rl_params);
+	cq_table->cq_rl_params = NULL;
+	spin_lock_irqsave(&cq_table->lock, flags);
+	kfree(cq_table->cq);
+	cq_table->cq = NULL;
+	spin_unlock_irqrestore(&cq_table->lock, flags);
+}
+
+void sx_core_dump_synd_tbl(struct sx_dev *dev)
+{
+	struct listener_entry *listener;
+	struct list_head *pos;
+	unsigned long flags;
+	u16 entry = 0;
+
+	spin_lock_irqsave(&sx_glb.listeners_lock, flags);
+	for (entry = 0; entry < NUM_HW_SYNDROMES + 1; entry++) {
+		if (!list_empty(&sx_glb.listeners_db[entry].list)) {
+			list_for_each(pos, &sx_glb.listeners_db[entry].list) {
+				listener = list_entry(pos,
+						struct listener_entry, list);
+				printk(KERN_DEBUG
+					"=============================\n");
+				printk(KERN_DEBUG
+						"synd=%d, swid=%d, is_def:%d, "
+						"handler:%p, rx_pkt:%llu \n",
+						entry,
+						listener->swid,
+						listener->is_default,
+						listener->handler,
+						listener->rx_pkts);
+
+				switch (listener->listener_type) {
+				case L2_TYPE_DONT_CARE:
+					printk(KERN_DEBUG "list_type: "
+							"DONT_CARE, crit [port:0x%x] \n",
+							listener->critireas.dont_care.sysport);
+					break;
+				case L2_TYPE_IB:
+					printk(KERN_DEBUG "list_type: IB, crit"
+						" [qpn:0x%x (%d)] \n",
+						listener->critireas.ib.qpn,
+						listener->critireas.ib.qpn);
+					break;
+				case L2_TYPE_ETH:
+					printk(KERN_DEBUG "list_type: ETH, crit "
+						"[ethtype:0x%x, dmac:%llx, "
+						"emad_tid:0x%x, from_rp:%u, from_bridge:%u ] \n",
+					listener->critireas.eth.ethtype,
+					listener->critireas.eth.dmac,
+					listener->critireas.eth.emad_tid,
+					listener->critireas.eth.from_rp,
+					listener->critireas.eth.from_bridge);
+					break;
+				case L2_TYPE_FC:
+					printk(KERN_DEBUG "list_type: FC \n");
+					break;
+				default:
+					printk(KERN_DEBUG "list_type: UNKNOWN \n");
+					break;
+				}
+			}
+		}
+	}
+
+	spin_unlock_irqrestore(&sx_glb.listeners_lock, flags);
+}
+
+int sx_cq_credit_thread_handler(void *cq_ctx)
+{
+	struct sx_priv *sx_priv = (struct sx_priv *)cq_ctx;
+	struct sx_cq_table *cq_table = &sx_priv->cq_table;
+	struct sx_dq_table *rdq_table = &sx_priv->rdq_table;
+	int i;
+	struct sx_cqe *cqe;
+
+	while (!kthread_should_stop()) {
+		unsigned int actual_sleep_time_msecs;
+		init_completion(&cq_table->done);
+		actual_sleep_time_msecs = jiffies_to_msecs(jiffies);
+
+		/* Waiting for a completion that will never happen,
+		 * just so we wake up after the credit interval */
+		if (!wait_for_completion_timeout(&cq_table->done,
+				msecs_to_jiffies(cq_table->rl_time_interval))) {
+			actual_sleep_time_msecs = jiffies_to_msecs(jiffies) - actual_sleep_time_msecs;
+
+			credit_thread_vals[arr_count % 1001] = jiffies_to_msecs(jiffies);
+			for (i = NUMBER_OF_SDQS; i < sx_priv->dev.dev_cap.max_num_cqs; i++) {
+				/* If the array is not allocated,
+				 * it means that we are in deinit phase */
+				if (!cq_table->cq_rl_params)
+					break;
+
+				if (sx_bitmap_test(&rdq_table->bitmap, (i - NUMBER_OF_SDQS)) &&
+						cq_table->cq_rl_params[i].use_limiter) {
+					struct cq_rate_limiter_params *rl_params =
+							&cq_table->cq_rl_params[i];
+					int prev_credit = rl_params->curr_cq_credit;
+					int credits = rl_params->interval_credit;
+
+					while (actual_sleep_time_msecs >= cq_table->rl_time_interval) {
+						credits += (rl_params->interval_credit / 10) + 1;
+						actual_sleep_time_msecs -= cq_table->rl_time_interval / 10;
+					}
+
+					rl_params->curr_cq_credit += credits;
+					if (rl_params->curr_cq_credit > rl_params->max_cq_credit)
+						rl_params->curr_cq_credit = rl_params->max_cq_credit;
+
+					/* ARM the CQ if not armed */
+					if (prev_credit < rl_params->interval_credit)
+						sx_cq_arm(cq_table->cq[i]);
+
+	                if (atomic_read(&cq_backup_polling_enabled)) {
+	                    if ((debug_cq_backup_poll_cqn == i) ||
+	                        ((prev_credit > rl_params->interval_credit) &&
+                            (cq_table->cq[i]->bkp_poll_data.last_interval_num_cq_polls ==
+                             atomic_read(&cq_table->cq[i]->bkp_poll_data.curr_num_cq_polls)) &&
+                             (cq_table->cq[i]->bkp_poll_data.last_interval_cons_index
+                                     == cq_table->cq[i]->cons_index))) {
+
+                            cqe=sx_next_cqe_sw(cq_table->cq[i]);
+                            if ((debug_cq_backup_poll_cqn == i) ||
+                                 (cqe &&
+                                 !atomic_read(&cq_table->cq[i]->bkp_poll_data.cq_bkp_poll_mode))) {
+                                /* Trigger backup poll */
+                                int dqn;
+                                dqn = (cqe->e_sr_dqn_owner >> 1) & SX_CQE_DQN_MASK;
+                                if (be16_to_cpu(cqe->dqn5_byte_count) & SX_CQE_DQN_MSB_MASK)
+                                    dqn |= (1 << SX_CQE_DQN_MSB_SHIFT);
+
+                                if (net_ratelimit()) {
+                                    sx_info(cq_table->cq[i]->sx_dev,"triggering backup poll for CQ:%d "
+                                            "RDQ:%d cons_index:%d\n",
+                                        i,dqn,cq_table->cq[i]->cons_index);
+                                }
+                                debug_cq_backup_poll_cqn=CQN_INVALID;
+                                atomic_set(&cq_table->cq[i]->bkp_poll_data.cq_bkp_poll_mode,1);
+                                atomic_inc(&sx_priv->cq_backup_polling_refcnt);
+                                tasklet_schedule(&sx_priv->intr_tasklet);
+                            }
+	                    }
+	                }
+
+	                cq_table->cq[i]->bkp_poll_data.last_interval_num_cq_polls=
+	                        atomic_read(&cq_table->cq[i]->bkp_poll_data.curr_num_cq_polls);
+	                cq_table->cq[i]->bkp_poll_data.last_interval_cons_index = cq_table->cq[i]->cons_index;
+				}
+			}
+
+			arr_count++;
+		}
+	}
+
+	return 0;
+}
+
+
+void sx_cq_show_cq(struct sx_dev *dev, int cqn)
+{
+    struct sx_priv *priv = sx_priv(dev);
+    struct sx_cq_table * __maybe_unused cq_table=&priv->cq_table;
+    struct sx_cq *cq = priv->cq_table.cq[cqn];
+    int iii,jjj;
+    struct sx_cqe *cqe;
+    u8 cqe_owner[16];
+
+    if (!cq) {
+        printk("cq %d doesn't exist \n", cqn);
+        return;
+    }
+
+    printk("[cq %d]: cqn:%d, cons_index:%d, nent:%d cons_index&(nent-1):%d"
+            " ref_cnt:%d \n",
+            cqn,
+            cq->cqn,
+            cq->cons_index,
+            cq->nent,
+            (cq->cons_index & (cq->nent - 1)),
+            atomic_read(&cq->refcount)
+            );
+
+    printk("CQ %d owner:\n", cqn);
+    for (iii = 0; iii < cq->nent / 16; iii++) {
+        for (jjj=0; (jjj<16) && ((iii*16+jjj) < cq->nent) ; jjj++) {
+            cqe = sx_get_cqe(cq, (iii*16+jjj) & (cq->nent - 1));
+            if (cqe) {
+                cqe_owner[jjj]=cqe->e_sr_dqn_owner;
+            } else {
+                cqe_owner[jjj]=255;
+            }
+        }
+
+        printk("[%5.5d]: %2.2x %2.2x %2.2x %2.2x %2.2x %2.2x %2.2x %2.2x "
+                        "%2.2x %2.2x %2.2x %2.2x %2.2x %2.2x %2.2x %2.2x\n",
+               iii*16,
+               cqe_owner[0],cqe_owner[1],cqe_owner[2],cqe_owner[3],
+               cqe_owner[4],cqe_owner[5],cqe_owner[6],cqe_owner[7],
+               cqe_owner[8],cqe_owner[9],cqe_owner[10],cqe_owner[11],
+               cqe_owner[12],cqe_owner[13],cqe_owner[14],cqe_owner[15]);
+    }
+    printk("\n");
+}
+
+void sx_cq_flush_rdq(struct sx_dev *dev, int dqn)
+{
+    int iii;
+    int idx;
+    struct sx_priv *priv = sx_priv(dev);
+    struct sx_dq_table *rdq_table = &priv->rdq_table;
+    struct sx_dq *dq = rdq_table->dq[dqn];
+
+    idx = dq->tail & (dq->wqe_cnt - 1);
+    for (iii=0; iii<dq->wqe_cnt; iii++) {
+        printk("%s:%d flushing rdq %d sge %d \n",__func__,__LINE__,dqn,idx);
+        wqe_sync_for_cpu(dq, idx);
+        sx_skb_free(dq->sge[idx].skb);
+        dq->sge[idx].skb = NULL;
+        dq->tail++;
+        idx = dq->tail & (dq->wqe_cnt - 1);
+        post_skb(dq);
+    }
+
+}
+
+/************************************************
+ *                  EOF                         *
+ ***********************************************/
diff --git a/linux/drivers/hwmon/mellanox/cq.h b/linux/drivers/hwmon/mellanox/cq.h
new file mode 100644
index 0000000..4141704
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/cq.h
@@ -0,0 +1,133 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+#ifndef SX_CQ_H
+#define SX_CQ_H
+
+/************************************************
+ * Includes
+ ***********************************************/
+
+#include <linux/types.h>
+#include "sx.h"
+
+#define CQN_INVALID 255
+
+/************************************************
+ * Enums
+ ***********************************************/
+
+enum {
+	SX_CQE_OWNER_MASK		= 0x01,
+	SX_CQE_IS_SEND_MASK		= 0x40,
+	SX_CQE_IS_ERR_MASK		= 0x80,
+	SX_CQE_DQN_MASK			= 0x1f,
+	SX_CQE_DQN_MSB_MASK		= 0x4000,
+	SX_CQE_DQN_MSB_SHIFT	= 5,
+};
+
+/************************************************
+ * Structures
+ ***********************************************/
+
+struct sx_cq_context {
+	u8		eq_num;  /* 0 or 1 */
+	u8		reserved1;
+	u8		flags;
+	u8		log_cq_size;
+	u16		reserved2;
+	__be16	producer_counter;
+	u16		reserved3;
+	u16 	reserved4;
+	u32		reserved5;
+	__be64	dma_addr[8];  /* CQE buffer dma addresses */
+};
+
+struct sx_cqe {
+	u8		vlan2_lag_subport;
+	u8		lag;
+	__be16	system_port_lag_id;
+	__be16	wqe_counter;
+	__be16	dqn5_byte_count;
+	__be16  ulp_crc_vlan_flow;
+	__be16	trap_id;
+	__be16	reserved1;
+	u8		type_swid;
+	u8		e_sr_dqn_owner;
+};
+
+/************************************************
+ * Inline Functions
+ ***********************************************/
+
+static inline void sx_cq_arm(struct sx_cq *cq)
+{
+#ifndef NO_PCI
+	unsigned long flags;
+
+   	spin_lock_irqsave(&cq->rearm_lock, flags);
+
+	/*
+	 * Make sure that descriptors are written before
+	 * doorbell.
+	 */
+	wmb();
+
+	__raw_writel((__force u32) cpu_to_be32(cq->cons_index & 0xffff),
+		     cq->arm_db);
+
+	mmiowb();
+	spin_unlock_irqrestore(&cq->rearm_lock, flags);
+#endif
+}
+
+/* Update the Consumer Index */
+static inline void sx_cq_set_ci(struct sx_cq *cq)
+{
+#ifndef NO_PCI
+	/*
+	 * Make sure that descriptors are written before
+	 * doorbell.
+	 */
+	wmb();
+
+	__raw_writel((__force u32) cpu_to_be32((cq->cons_index + cq->nent) &
+			0xffff), cq->set_ci_db);
+
+	mmiowb();
+#endif
+}
+
+/************************************************
+ * Functions
+ ***********************************************/
+void dispatch_pkt(struct sx_dev *dev, struct completion_info *ci, u16 hw_synd, int dispatch_default);
+int sx_core_init_cq_table(struct sx_dev *dev);
+void sx_core_destroy_cq_table(struct sx_dev *dev);
+int sx_core_create_cq(struct sx_dev *dev, int nent, struct sx_cq **cq, u8 cqn);
+void sx_core_destroy_cq(struct sx_dev *dev, struct sx_cq *cq);
+int sx_cq_completion(struct sx_dev *dev, u32 cqn, u16 weight);
+void sx_core_dump_synd_tbl(struct sx_dev *dev);
+int rx_skb(void *context, struct sk_buff *skb, struct sx_cqe *cqe);
+int sx_cq_credit_thread_handler(void *cq_ctx);
+void wqe_sync_for_cpu(struct sx_dq *dq, int idx);
+void sx_cq_show_cq(struct sx_dev *dev, int cqn);
+void sx_cq_flush_rdq(struct sx_dev *my_dev, int idx);
+
+
+#endif /* SX_CQ_H */
+
+/************************************************
+ *                  EOF                         *
+ ***********************************************/
+
diff --git a/linux/drivers/hwmon/mellanox/dq.c b/linux/drivers/hwmon/mellanox/dq.c
new file mode 100644
index 0000000..90553fc
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/dq.c
@@ -0,0 +1,991 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2016 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+
+/************************************************
+ * Includes                                     *
+ ***********************************************/
+
+#include <linux/skbuff.h>
+#include <linux/pci.h>
+#include <linux/if_ether.h>
+#include "dq.h"
+#include "cq.h"
+#include "alloc.h"
+#include "sx.h"
+#include <linux/mlx_sx/cmd.h>
+#include <linux/mlx_sx/kernel_user.h>
+#include "sx_dpt.h"
+#include "sx_proc.h"
+
+extern struct sx_globals sx_glb;
+
+/************************************************
+ *  Globals
+ ***********************************************/
+static int (*tx_stub_func)(struct sk_buff *skb, u8 sdqn);
+extern int tx_debug;
+extern int tx_debug_pkt_type;
+extern int tx_debug_emad_type;
+extern int tx_dump;
+extern int tx_dump_cnt;
+
+/************************************************
+ * Functions                            *
+ ***********************************************/
+
+void register_ver_tx_stub(int (*func)(struct sk_buff *skb, u8 sdqn))
+{
+    tx_stub_func = func;
+}
+EXPORT_SYMBOL(register_ver_tx_stub);
+
+static int sx_nent_to_4k(int nent)
+{
+    return ALIGN(nent * SX_DESC_SIZE, 4096) / 4096;
+}
+
+static inline void * sx_buf_offset(struct sx_buf *buf, int offset)
+{
+    if (buf->nbufs == 1) {
+        return buf->u.direct.buf + offset;
+    } else {
+        return buf->u.page_list[offset >> PAGE_SHIFT].buf +
+               (offset & (PAGE_SIZE - 1));
+    }
+}
+
+static void * sx_get_wqe(struct sx_dq *dq, int offset)
+{
+    return sx_buf_offset(&dq->buf, offset);
+}
+
+static void * sx_get_send_wqe(struct sx_dq *dq, int n)
+{
+    return sx_get_wqe(dq, n << SX_WQE_SHIFT);
+}
+
+static __be16 sx_set_wqe_flags(u8 set_lp, enum ku_pkt_type type)
+{
+    u16 flags = 0;
+
+    flags |= (1 << 15);
+    if (set_lp) {
+        flags |= (1 << 14);
+    }
+    switch (type) {
+    case SX_PKT_TYPE_ETH_CTL_UC:
+    case SX_PKT_TYPE_ETH_CTL_MC:
+    case SX_PKT_TYPE_ETH_DATA:
+    case SX_PKT_TYPE_DROUTE_EMAD_CTL:
+    case SX_PKT_TYPE_EMAD_CTL:
+        flags |= (0xA << 7);
+        break;
+
+    case SX_PKT_TYPE_FCOE_CTL_UC:
+    case SX_PKT_TYPE_FCOE_CTL_MC:
+        flags |= (0xD << 7);
+        break;
+
+    case SX_PKT_TYPE_IB_RAW_CTL:
+    case SX_PKT_TYPE_IB_RAW_DATA:
+        flags |= (0x8 << 7);
+        break;
+
+    case SX_PKT_TYPE_IB_TRANSPORT_CTL:
+    case SX_PKT_TYPE_IB_TRANSPORT_DATA:
+        flags |= (0x9 << 7);
+        break;
+
+    case SX_PKT_TYPE_FCOIB_CTL:
+        flags |= (0xC << 7);
+        break;
+
+    case SX_PKT_TYPE_FC_CTL_UC:
+    case SX_PKT_TYPE_FC_CTL_MC:
+    case SX_PKT_TYPE_EOIB_CTL:
+    default:
+        break; /* unsupported */
+    }
+
+    return cpu_to_be16(flags);
+}
+
+static int sx_build_send_packet(struct sx_dq *sdq, struct sk_buff *skb, struct sx_wqe *wqe, int idx)
+{
+    int i, num_frags;
+
+    sdq->sge[idx].skb = skb;
+    sdq->sge[idx].hdr_pld_sg.vaddr = skb->data;
+    sdq->sge[idx].hdr_pld_sg.len = skb_headlen(skb);
+    sdq->sge[idx].hdr_pld_sg.dma_addr = pci_map_single(sdq->dev->pdev,
+                                                       sdq->sge[idx].hdr_pld_sg.vaddr,
+                                                       sdq->sge[idx].hdr_pld_sg.len, PCI_DMA_TODEVICE);
+    if (dma_mapping_error(&sdq->dev->pdev->dev,
+                          sdq->sge[idx].hdr_pld_sg.dma_addr)) {
+        return -ENOMEM;
+    }
+
+    wqe->dma_addr[0] = cpu_to_be64(sdq->sge[idx].hdr_pld_sg.dma_addr);
+    wqe->byte_count[0] = cpu_to_be16(sdq->sge[idx].hdr_pld_sg.len);
+
+    pci_dma_sync_single_for_device(sdq->dev->pdev,
+                                   sdq->sge[idx].hdr_pld_sg.dma_addr,
+                                   sdq->sge[idx].hdr_pld_sg.len, DMA_TO_DEVICE);
+    num_frags = skb_shinfo(skb)->nr_frags;
+    if (num_frags > 2) {
+        return -EINVAL;
+    }
+
+    for (i = 0; i < num_frags; i++) {
+        skb_frag_t         *frag = &skb_shinfo(skb)->frags[i];
+        struct sx_sge_data *sge_data;
+        if (i == 0) {
+            sge_data = &sdq->sge[idx].pld_sg_1;
+        } else {
+            sge_data = &sdq->sge[idx].pld_sg_2;
+        }
+
+        sge_data->vaddr = lowmem_page_address(frag->page.p) +
+                          frag->page_offset;
+        sge_data->len = frag->size;
+
+        sge_data->dma_addr = pci_map_single(sdq->dev->pdev,
+                                            sge_data->vaddr, sge_data->len, PCI_DMA_TODEVICE);
+        if (dma_mapping_error(&sdq->dev->pdev->dev, sge_data->dma_addr)) {
+            return -ENOMEM;
+        }
+
+        wqe->dma_addr[i + 1] = cpu_to_be64(sge_data->dma_addr);
+        wqe->byte_count[i + 1] = cpu_to_be16(sge_data->len);
+        pci_dma_sync_single_for_device(sdq->dev->pdev,
+                                       sge_data->dma_addr, sge_data->len, DMA_TO_DEVICE);
+    }
+
+    for (; i < 2; i++) {
+        wqe->byte_count[i + 1] = 0;
+    }
+
+    return 0;
+}
+
+static int sx_dq_overflow(struct sx_dq *sdq, struct sx_cq *cq)
+{
+    unsigned      cur;
+    unsigned long flags;
+
+    cur = sdq->head - sdq->tail;
+    if (likely(cur + 1 < sdq->wqe_cnt)) {
+        return 0;
+    }
+
+    spin_lock_irqsave(&cq->lock, flags);
+    cur = sdq->head - sdq->tail;
+    spin_unlock_irqrestore(&cq->lock, flags);
+
+    return cur + 1 >= sdq->wqe_cnt;
+}
+
+static int sx_is_overflow(unsigned head, unsigned tail, int n)
+{
+    if ((int)(head - tail) >= n) {
+        return -1;
+    } else {
+        return 0;
+    }
+}
+
+
+int sx_add_pkts_to_sdq(struct sx_dq *sdq)
+{
+    struct sx_wqe    *wqe;
+    int               err = 0;
+    int               wqe_idx;
+    struct sx_pkt    *curr_pkt;
+    struct list_head *pos, *q;
+    u8                arm = 0;
+
+    list_for_each_safe(pos, q, &sdq->pkts_list.list) {
+        curr_pkt = list_entry(pos, struct sx_pkt, list);
+        list_del(pos);
+        wqe_idx = sdq->head & (sdq->wqe_cnt - 1);
+        wqe = sx_get_send_wqe(sdq, wqe_idx);
+
+        wqe->flags = sx_set_wqe_flags(curr_pkt->set_lp, curr_pkt->type);
+        err = sx_build_send_packet(sdq, curr_pkt->skb, wqe, wqe_idx);
+        if (err) {
+            sx_skb_free(curr_pkt->skb);
+            break;
+        }
+
+        ++sdq->head;
+        kfree(curr_pkt);
+        arm = 1;
+        if (sx_dq_overflow(sdq, sdq->cq)) {
+            break; /* go to arm the db */
+        }
+    }
+
+    if (arm) {
+        /*
+         * Make sure that descriptors are written before
+         * doorbell.
+         */
+        wmb();
+
+        __raw_writel((__force u32)cpu_to_be32(sdq->head & 0xffff),
+                     sdq->db);
+        mmiowb();
+    }
+    return err;
+}
+
+int __sx_core_post_send(struct sx_dev *dev, struct sk_buff *skb, struct isx_meta *meta)
+{
+    unsigned long  flags;
+    int            err = 0;
+    struct sx_pkt *new_pkt;
+    struct sx_dq  *sdq = NULL;
+    u8             sdqn;
+    u8             stclass;
+
+    if (dev->global_flushing == 1) {
+        if (printk_ratelimit()) {
+            printk(KERN_WARNING "__sx_core_post_send: Cannot send packet "
+                   "while in global_flushing mode\n");
+        }
+        sx_skb_free(skb);
+        return -EFAULT;
+    }
+
+    if (NUMBER_OF_SWIDS <= meta->swid) {
+        printk(KERN_WARNING "__sx_core_post_send: Cannot send packet on swid %u\n",
+               meta->swid);
+        sx_skb_free(skb);
+        return -EFAULT;
+    }
+
+    if (!dev || !dev->profile_set) {
+        printk(KERN_WARNING PFX "__sx_core_post_send() cannot "
+               "execute because the profile is not "
+               "set\n");
+        sx_skb_free(skb);
+        return -EFAULT;
+    }
+
+    err = sx_get_sdq(meta, dev, meta->type, meta->swid,
+                     meta->etclass, &stclass, &sdqn);
+
+    if (err) {
+        if (printk_ratelimit()) {
+            printk(KERN_WARNING PFX "sx_core_post_send: error in callback structure sx_get_sdq for "
+                   "selecting SDQ \n");
+        }
+        sx_skb_free(skb);
+        return -EFAULT;
+    }
+
+    sdq = sx_priv(dev)->sdq_table.dq[sdqn];
+
+    if (!sdq) {
+        if (printk_ratelimit()) {
+            printk(KERN_WARNING PFX "sx_core_post_send: ERR: " \
+                   "SDQ was not allocated\n");
+        }
+        sx_skb_free(skb);
+        return -EFAULT;
+    }
+
+    err = sx_build_isx_header(meta, skb, stclass);
+    if (err) {
+        if (printk_ratelimit()) {
+            printk(KERN_WARNING PFX "sx_core_post_send: "
+                   "error in build header/stub\n");
+        }
+        sx_skb_free(skb);
+        return -EFAULT;
+    }
+
+    if ((skb->len > 2048) && (dev->profile.cpu_egress_tclass[sdqn] >= 2)) {
+        printk(KERN_ERR PFX "sx_core_post_send: cannot send packet of size %u "
+               "from SDQ %u since it's binded to cpu_tclass %u\n",
+               skb->len, sdqn, dev->profile.cpu_egress_tclass[sdqn]);
+        sx_skb_free(skb);
+        return -EFAULT;
+    }
+
+    /* send the packet also to the verification stub.
+     * Since the verification environment doesn't yet support ETH L3
+     * packets we don't pass them such packets. */
+    if (tx_stub_func && 
+            (meta->type != SX_PKT_TYPE_ETH_DATA && 
+             meta->type != SX_PKT_TYPE_ETH_CTL_UC)) {
+        tx_stub_func(skb, 0);
+    }
+
+#ifdef NO_PCI /* The IB tests needs us to generate IB completions */
+    if ((meta->type == SX_PKT_TYPE_IB_TRANSPORT_DATA) ||
+        (meta->type == SX_PKT_TYPE_IB_TRANSPORT_CTL) ||
+        (meta->type == SX_PKT_TYPE_ETH_DATA) ||
+        (meta->type == SX_PKT_TYPE_ETH_CTL_UC) ||
+        (tx_stub_func == NULL)) {
+        sx_skb_free(skb);
+    }
+
+    return 0;
+#endif
+
+    new_pkt = kmalloc(sizeof(*new_pkt), GFP_ATOMIC);
+    if (!new_pkt) {
+        if (printk_ratelimit()) {
+            printk(KERN_WARNING PFX "sx_core_post_send: "
+                   "error - cannot allocate packets memory\n");
+        }
+        sx_skb_free(skb);
+        return -ENOMEM;
+    }
+
+    new_pkt->skb = skb;
+    new_pkt->set_lp = meta->lp;
+    new_pkt->type = meta->type;
+    spin_lock_irqsave(&sdq->lock, flags);
+    list_add_tail(&new_pkt->list, &sdq->pkts_list.list);
+    if (sx_dq_overflow(sdq, sdq->cq)) {
+        goto out;
+    }
+
+    if (dev->pdev) {
+        err = sx_add_pkts_to_sdq(sdq);
+    }
+
+out:
+    spin_unlock_irqrestore(&sdq->lock, flags);
+    return err;
+}
+
+int sx_core_post_send(struct sx_dev *dev, struct sk_buff *skb, struct isx_meta *meta)
+{
+    int err = 0;
+
+    /* WA for pad TX packets with size less than ETH_ZLEN */
+    if (((meta->type == SX_PKT_TYPE_ETH_DATA) ||
+         (meta->type == SX_PKT_TYPE_ETH_CTL_UC)) &&
+         (skb->len < ETH_ZLEN)) {
+        err = skb_pad(skb, ETH_ZLEN - skb->len);
+        if (err) {
+            sx_skb_free(skb);
+            return err;
+        }
+        skb->len = ETH_ZLEN;
+        skb_set_tail_pointer(skb, ETH_ZLEN);
+    }
+
+    if (tx_debug &&
+        ((tx_debug_pkt_type == SX_DBG_PACKET_TYPE_ANY) ||
+         (tx_debug_pkt_type == meta->type)) &&
+        ((tx_debug_emad_type == SX_DBG_EMAD_TYPE_ANY) ||
+         (tx_debug_emad_type ==
+          be16_to_cpu(((struct sx_emad *)skb->data)->emad_op.register_id)))) {
+        printk(KERN_DEBUG PFX "sx_core_post_send: Sending "
+               "pkt with meta: "
+               "et: %d , swid: %d , sysport:0x%x, rdq: %d,"
+               "to_cpu: %d, lp:%d, type: %d, dev_id: %d rx_is_router: %d "
+               "fid_valid: %d fid :%d, skb_headlen(skb):%d, skb_shinfo(skb)->nr_frags:%d  \n",
+               meta->etclass, meta->swid,
+               meta->system_port_mid,
+               meta->rdq, meta->to_cpu, meta->lp, meta->type,
+               meta->dev_id, meta->rx_is_router, meta->fid_valid, meta->fid,
+               skb_headlen(skb), skb_shinfo(skb)->nr_frags);
+
+
+        if (tx_dump) {
+            int i;
+            u8 *buf = (void*)skb->data;
+            int cnt = skb_headlen(skb);
+
+            for (i = 0; i < cnt; i++) {
+                if ((i == 0) || (i % 4 == 0)) {
+                    printk("\n0x%04x : ", i);
+                }
+
+                printk(" 0x%02x", buf[i]);
+            }
+
+            if (skb_shinfo(skb)->nr_frags) {
+                skb_frag_t *frag = &skb_shinfo(skb)->frags[0];
+                int         prev_size = cnt;
+                buf = (void*)(lowmem_page_address(frag->page.p) + frag->page_offset);
+                cnt = frag->size;
+                for (i = 0; i < cnt; i++) {
+                    if ((i == 0) || (i % 4 == 0)) {
+                        printk("\n0x%04x : ", i + prev_size);
+                    }
+
+                    printk(" 0x%02x", buf[i]);
+                }
+
+                if (skb_shinfo(skb)->nr_frags > 1) {
+                    skb_frag_t *frag = &skb_shinfo(skb)->frags[1];
+
+                    prev_size += cnt;
+                    buf = (void*)(lowmem_page_address(frag->page.p) + frag->page_offset);
+                    cnt = frag->size;
+                    for (i = 0; i < cnt; i++) {
+                        if ((i == 0) || (i % 4 == 0)) {
+                            printk("\n0x%04x : ", i + prev_size);
+                        }
+
+                        printk(" 0x%02x", buf[i]);
+                    }
+                }
+            }
+
+            printk("\n");
+        }
+
+        if ((tx_dump_cnt != 0xFFFF) && (tx_dump_cnt > 0)) {
+            tx_dump_cnt--;
+        }
+
+        if (tx_dump_cnt == 0) {
+            tx_dump = 0;
+            tx_debug = 0;
+            tx_debug_pkt_type = 0xFF;
+        }
+    }
+
+    if (dev != NULL && meta->swid < NUMBER_OF_SWIDS) {
+    	sx_glb.stats.tx_by_pkt_type[meta->swid][meta->type]++;
+        dev->stats.tx_by_pkt_type[meta->swid][meta->type]++;
+    } else if (dev != NULL) {
+    	sx_glb.stats.tx_by_pkt_type[NUMBER_OF_SWIDS][meta->type]++;
+        dev->stats.tx_by_pkt_type[NUMBER_OF_SWIDS][meta->type]++;
+    } else if (sx_glb.tmp_dev_ptr ) {
+	    sx_glb.tmp_dev_ptr->stats.tx_by_pkt_type[NUMBER_OF_SWIDS][meta->type]++;
+	}
+
+    if ((meta->type == SX_PKT_TYPE_DROUTE_EMAD_CTL) || /* emad */
+        (meta->type == SX_PKT_TYPE_EMAD_CTL)) { /* emad */
+        err = sx_dpt_send_emad(meta->dev_id, skb, meta);
+    } else if ((meta->type == SX_PKT_TYPE_IB_TRANSPORT_CTL) ||
+               (meta->type == SX_PKT_TYPE_IB_TRANSPORT_DATA)) { /* mad */
+        err = sx_dpt_send_mad(meta->dev_id, skb, meta);
+    }
+#ifndef NO_PCI /* In real mode we should only call __sx_core_post_send when we have PCI device */
+    else if (dev && dev->pdev) {
+        err = __sx_core_post_send(dev, skb, meta);
+    } else {
+        err = -EFAULT;
+    }
+#else
+    else {
+        err = __sx_core_post_send(dev, skb, meta);
+    }
+#endif
+
+    return err;
+}
+EXPORT_SYMBOL(sx_core_post_send);
+
+/*
+ * Posts a buffer to the HW RDQ
+ * The skb contains the kernel buffer address and length of the buffer.
+ * Post recv maps the buffer to DMA memory and adds it to RDQ.
+ */
+void sx_core_post_recv(struct sx_dq *rdq, struct sk_buff *skb)
+{
+    unsigned long  flags;
+    int            idx;
+    struct sx_wqe *wqe;
+    int            length = rdq->dev->profile.rdq_properties[rdq->dqn].entry_size;
+
+#ifdef CONFIG_44x
+    int i;
+#endif
+
+    if (!rdq->dev->profile_set) {
+        printk(KERN_WARNING PFX "sx_core_post_recv() cannot "
+               "execute because the profile is not "
+               "set\n");
+        return;
+    }
+    spin_lock_irqsave(&rdq->lock, flags);
+    /* Sanity check - overflow should never happen here */
+    if (sx_is_overflow(rdq->head, rdq->tail, rdq->wqe_cnt)) {
+        if (printk_ratelimit()) {
+            printk(KERN_WARNING PFX "ERR: overflow in rdq, " \
+                   "rdq->tail = %u, "
+                   "rdq->head = %u\n", rdq->tail, rdq->head);
+        }
+        goto out;
+    }
+    idx = rdq->head & (rdq->wqe_cnt - 1);
+    wqe = sx_get_wqe(rdq, idx << SX_WQE_SHIFT);
+    memset(wqe, 0, sizeof *wqe);
+
+    rdq->sge[idx].skb = skb;
+    rdq->sge[idx].hdr_pld_sg.vaddr = skb->data;
+    rdq->sge[idx].hdr_pld_sg.len = length;
+#ifdef CONFIG_44x
+    /*
+     *   (L1_CACHE_BYTES = 32)
+     *   L2 corruption PPC460 Errata workaround :
+     *   1. Turn on bit 27 in L2_CFG reg (made in drv init)
+     *   2. Write to first word of each cache line of the
+     *   posted buffer. This will cause to invalidate
+     *   of L2 buffer
+     */
+    for (i = 0; i < rdq->sge[idx].hdr_pld_sg.len; i += L1_CACHE_BYTES) {
+        *(u32*)(rdq->sge[idx].hdr_pld_sg.vaddr + i) = 0x55555555;
+    }
+#else
+    *(u32*)(rdq->sge[idx].hdr_pld_sg.vaddr) = 0x55555555; /* Mark 1st 4 bytes */
+#endif
+
+    /* DMA_TODEVICE in order the marking will be written */
+    rdq->sge[idx].hdr_pld_sg.dma_addr = pci_map_single(rdq->dev->pdev,
+                                                       rdq->sge[idx].hdr_pld_sg.vaddr,
+                                                       length, PCI_DMA_TODEVICE);
+    if (dma_mapping_error(&rdq->dev->pdev->dev,
+                          rdq->sge[idx].hdr_pld_sg.dma_addr)) {
+        if (printk_ratelimit()) {
+            printk(KERN_WARNING PFX "%s:%d: Err: "
+                   "got dma_mapping error\n", __func__, __LINE__);
+        }
+        goto out;
+    }
+
+    rdq->sge[idx].hdr_pld_sg.dma_addr = pci_map_single(rdq->dev->pdev,
+                                                       rdq->sge[idx].hdr_pld_sg.vaddr,
+                                                       length, PCI_DMA_FROMDEVICE);
+    if (dma_mapping_error(&rdq->dev->pdev->dev,
+                          rdq->sge[idx].hdr_pld_sg.dma_addr)) {
+        if (printk_ratelimit()) {
+            printk(KERN_WARNING PFX "sx_core_post_recv: Err: "
+                   "got dma_mapping error\n");
+        }
+        goto out;
+    }
+
+    wqe->dma_addr[0] =
+        cpu_to_be64(rdq->sge[idx].hdr_pld_sg.dma_addr);
+    wqe->byte_count[0] = cpu_to_be16(rdq->sge[idx].hdr_pld_sg.len);
+
+    pci_dma_sync_single_for_device(rdq->dev->pdev,
+                                   rdq->sge[idx].hdr_pld_sg.dma_addr,
+                                   rdq->sge[idx].hdr_pld_sg.len, DMA_FROM_DEVICE);
+
+    rdq->head++;
+    /*
+     * Make sure that descriptors are written before
+     * doorbell.
+     */
+    wmb();
+
+    __raw_writel((__force u32)cpu_to_be32(rdq->head & 0xffff), rdq->db);
+
+    mmiowb();
+
+out:
+    spin_unlock_irqrestore(&rdq->lock, flags);
+}
+
+/*
+ * This function is used because we can't call kfree_skb on IB packets
+ * which hold info in the nonlinear part of the SKB which was not
+ * allocated by us, but was given to us by the user */
+void sx_skb_free(struct sk_buff *skb)
+{
+    int i;
+
+    if (!skb) {
+        return;
+    }
+
+    for (i = 0; i < skb_shinfo(skb)->nr_frags; i++) {
+        skb_frag_t *frag = &skb_shinfo(skb)->frags[i];
+        frag->page.p = NULL;
+        frag->page_offset = 0;
+        frag->size = 0;
+    }
+    if (i) {
+        skb_shinfo(skb)->nr_frags = 0;
+        skb_shinfo(skb)->frag_list = NULL;
+    }
+
+    kfree_skb(skb);
+}
+EXPORT_SYMBOL(sx_skb_free);
+
+/************************************************
+ * Create Functions                             *
+ ***********************************************/
+
+int sx_init_dq_table(struct sx_dev *dev, unsigned int ndqs, int send)
+{
+    struct sx_priv     *priv = sx_priv(dev);
+    struct sx_dq_table *dq_table = NULL;
+    struct sx_dq      **dq_array = NULL;
+    int                 err = 0;
+    int                 i = 0;
+    unsigned long       flags;
+
+    dq_table = ((send == true) ? &(priv->sdq_table) : &(priv->rdq_table));
+    spin_lock_init(&dq_table->lock);
+
+    dq_array = kzalloc(ndqs * sizeof(*dq_array), GFP_KERNEL);
+
+    if (!dq_array) {
+        return -ENOMEM;
+    }
+    for (i = 0; i < ndqs; i++) {
+        dq_array[i] = NULL;
+    }
+
+    spin_lock_irqsave(&dq_table->lock, flags);
+    dq_table->dq = dq_array;
+    spin_unlock_irqrestore(&dq_table->lock, flags);
+
+    err = sx_bitmap_init(&dq_table->bitmap, ndqs);
+
+    return err;
+}
+
+int sx_core_init_sdq_table(struct sx_dev *dev)
+{
+    return sx_init_dq_table(dev, dev->dev_cap.max_num_sdqs, true);
+}
+
+int sx_core_init_rdq_table(struct sx_dev *dev)
+{
+    return sx_init_dq_table(dev, dev->dev_cap.max_num_rdqs, false);
+}
+
+int sx_hw2sw_dq(struct sx_dev *dev, struct sx_dq *dq)
+{
+    int err;
+
+    err = sx_cmd(dev, dev->device_id, 0, dq->dqn & 0xffffff, !dq->is_send,
+                 SX_CMD_HW2SW_DQ, SX_CMD_TIME_CLASS_A,
+                 sx_priv(dev)->fw.local_in_mb_size);
+    if (!err) {
+        dq->state = DQ_STATE_ERROR;
+    }
+
+    return err;
+}
+
+int sx_dq_modify_2err(struct sx_dev *dev, struct sx_dq *dq)
+{
+    int err;
+
+    err = sx_cmd(dev, dev->device_id, 0, dq->dqn, !dq->is_send,
+                 SX_CMD_2ERR_DQ, SX_CMD_TIME_CLASS_A,
+                 sx_priv(dev)->fw.local_in_mb_size);
+    if (!err) {
+        dq->state = DQ_STATE_ERROR;
+    }
+
+    return err;
+}
+
+int sx_sw2hw_dq(struct sx_dev *dev, struct sx_dq *dq)
+{
+    struct sx_cmd_mailbox *mailbox;
+    struct sx_dq_context  *context;
+    int                    err = 0;
+    int                    i;
+    struct sk_buff        *skb = NULL;
+
+    mailbox = sx_alloc_cmd_mailbox(dev, dev->device_id);
+    if (IS_ERR(mailbox)) {
+        err = PTR_ERR(mailbox);
+        mailbox = NULL;
+        goto out;
+    }
+
+    memset(mailbox->buf, 0, SX_MAILBOX_SIZE);
+    context = mailbox->buf;
+    context->cq = dq->cq->cqn;
+    context->log2_dq_sz = ilog2(sx_nent_to_4k(dq->wqe_cnt));
+    if (dq->is_send) {
+        context->sdq_tclass = dev->profile.cpu_egress_tclass[dq->dqn];
+    }
+
+    if (dev->pdev) {
+        sx_fill_page_list(context->dma_addr, &dq->buf);
+    }
+
+    err = sx_cmd(dev, dev->device_id, mailbox, dq->dqn & 0xffffff,
+                 !dq->is_send, SX_CMD_SW2HW_DQ, SX_CMD_TIME_CLASS_A,
+                 sx_priv(dev)->fw.local_in_mb_size);
+    if (!err) {
+        if (dev->pdev && !dq->is_send) {
+            u16     size = dev->profile.rdq_properties[dq->dqn].entry_size;
+            uint8_t nent = dev->profile.rdq_properties[dq->dqn].number_of_entries;
+            for (i = 0; i < nent; i++) {
+                skb = alloc_skb(size, GFP_KERNEL);
+                if (!skb) {
+                    err = -ENOMEM;
+                    goto out;
+                }
+
+                if (skb_put(skb, size) == NULL) {
+                    err = -ENOMEM;
+                    goto out;
+                }
+
+                sx_core_post_recv(dq, skb);
+            }
+        }
+
+        dq->state = DQ_STATE_RTS;
+    }
+
+out:
+    if (mailbox) {
+        sx_free_cmd_mailbox(dev, mailbox);
+    }
+
+    return err;
+}
+
+static int sx_dq_alloc(struct sx_dev *dev, u8 send, struct sx_dq *dq, u8 dqn)
+{
+    int                 err;
+    int                 dq_base;
+    unsigned long       flags;
+    struct sx_priv     *priv = sx_priv(dev);
+    struct sx_dq_table *dq_table = send ?
+                                   &priv->sdq_table : &priv->rdq_table;
+
+    dq->is_send = send;
+    dq->dqn = sx_bitmap_set(&dq_table->bitmap, dqn);
+    if (dq->dqn == -1) {
+        return -ENOMEM;
+    }
+
+    dq->sge = kzalloc(dq->wqe_cnt * sizeof *dq->sge, GFP_KERNEL);
+    if (!dq->sge) {
+        err = -ENOMEM;
+        goto err_out;
+    }
+
+    spin_lock_irqsave(&dq_table->lock, flags);
+    dq_table->dq[dq->dqn] = dq;
+    spin_unlock_irqrestore(&dq_table->lock, flags);
+    atomic_set(&dq->refcount, 1);
+    init_completion(&dq->free);
+    init_waitqueue_head(&dq->tx_full_wait);
+    dq_base = send ? SX_SEND_DQ_DB_BASE : SX_RECV_DQ_DB_BASE;
+    dq->db = dev->db_base + dq_base + dq->dqn * 4;
+    spin_lock_init(&dq->lock);
+    dq->state = DQ_STATE_RESET;
+    dq->dev = dev;
+    if (send) {
+        INIT_LIST_HEAD(&dq->pkts_list.list);
+    }
+
+    return 0;
+
+err_out:
+    kfree(dq->sge);
+    sx_bitmap_free(&dq_table->bitmap, dq->dqn);
+
+    return err;
+}
+
+static int sx_create_dq(struct sx_dev *dev, int nent, u8 dqn, struct sx_dq **dq, u8 send)
+{
+    int           ret;
+    struct sx_dq *tdq;
+    struct sx_cq *cq;
+    int           cqn = send ? dqn : dqn + NUMBER_OF_SDQS;
+
+    tdq = kzalloc(sizeof *tdq, GFP_KERNEL);
+    if (!tdq) {
+        return -ENOMEM;
+    }
+
+    ret = sx_core_create_cq(dev, (1 << dev->dev_cap.log_max_cq_sz),
+                            &cq, cqn);
+    if (ret < 0) {
+        goto free_dq;
+    }
+
+    tdq->cq = cq;
+    ret = sx_buf_alloc(dev, nent * sizeof(struct sx_wqe),
+                       PAGE_SIZE, &tdq->buf, 0);
+    if (ret) {
+        ret = -ENOMEM;
+        goto free_cq;
+    }
+
+    tdq->wqe_cnt = nent;
+    ret = sx_dq_alloc(dev, send, tdq, dqn);
+    if (ret) {
+        goto free_buf;
+    }
+
+    tdq->state = DQ_STATE_RESET;
+    tdq->is_flushing = 0;
+    /* TODO: handle errors */
+    ret = sx_sw2hw_dq(dev, tdq);
+    *dq = tdq;
+
+    return 0;
+
+free_buf:
+    sx_buf_free(dev, nent * sizeof(struct sx_wqe), &tdq->buf);
+free_cq:
+    sx_core_destroy_cq(dev, cq);
+free_dq:
+    kfree(tdq);
+    return ret;
+}
+
+int sx_core_create_sdq(struct sx_dev *dev, int nent, u8 dqn, struct sx_dq **sdq_p)
+{
+    return sx_create_dq(dev, nent, dqn, sdq_p, 1);
+}
+
+int sx_core_create_rdq(struct sx_dev *dev, int nent, u8 dqn, struct sx_dq **rdq_p)
+{
+    return sx_create_dq(dev, nent, dqn, rdq_p, 0);
+}
+
+/************************************************
+ * Destroy Functions                            *
+ ***********************************************/
+static void sx_free_dq_sges(struct sx_dev *dev, struct sx_dq *dq)
+{
+    int i;
+
+    for (i = 0; i < dq->wqe_cnt; ++i) {
+        if (dq->sge[i].hdr_pld_sg.vaddr) {
+#ifndef NO_PCI
+            wqe_sync_for_cpu(dq, i);
+#endif
+            /* The destructor assumes the context of the user
+             *  who sent the packet still exists, and we might
+             *  get a kernel oops if the user already closed the FD */
+            dq->sge[i].skb->destructor = NULL;
+            sx_skb_free(dq->sge[i].skb);
+        }
+    }
+}
+
+static void sx_dq_free(struct sx_dev *dev, struct sx_dq *dq)
+{
+    struct list_head *pos, *q;
+    struct sx_pkt    *tmp_pkt;
+
+    if (atomic_dec_and_test(&dq->refcount)) {
+        complete(&dq->free);
+    }
+
+    wait_for_completion(&dq->free);
+
+    if (dq->is_send && !list_empty(&dq->pkts_list.list)) {
+        list_for_each_safe(pos, q, &dq->pkts_list.list) {
+            tmp_pkt = list_entry(pos, struct sx_pkt, list);
+            list_del(pos);
+            /* The destructor assumes the context of the user
+             *  who sent the packet still exists, and we might
+             *  get a kernel oops if the user already closed the FD */
+            tmp_pkt->skb->destructor = NULL;
+            sx_skb_free(tmp_pkt->skb);
+            kfree(tmp_pkt);
+        }
+    }
+
+    sx_free_dq_sges(dev, dq);
+    kfree(dq->sge);
+}
+
+static void sx_dq_remove(struct sx_dev *dev, struct sx_dq *dq)
+{
+    struct sx_dq_table *dq_table = dq->is_send ?
+                                   &sx_priv(dev)->sdq_table : &sx_priv(dev)->rdq_table;
+    unsigned long flags;
+
+    sx_bitmap_free(&dq_table->bitmap, dq->dqn);
+    spin_lock_irqsave(&dq_table->lock, flags);
+    dq_table->dq[dq->dqn] = NULL;
+    spin_unlock_irqrestore(&dq_table->lock, flags);
+}
+
+static void sx_destroy_dq(struct sx_dev *dev, struct sx_dq *dq)
+{
+    sx_core_destroy_cq(dev, dq->cq);
+    sx_dq_remove(dev, dq);
+    sx_dq_free(dev, dq);
+    sx_buf_free(dev, dq->wqe_cnt * sizeof(struct sx_wqe), &dq->buf);
+    kfree(dq);
+}
+
+void sx_core_destroy_sdq(struct sx_dev *dev, struct sx_dq *dq)
+{
+    sx_destroy_dq(dev, dq);
+}
+
+void sx_core_destroy_rdq(struct sx_dev *dev, struct sx_dq *dq)
+{
+    sx_destroy_dq(dev, dq);
+}
+
+void sx_core_destroy_sdq_table(struct sx_dev *dev, u8 free_table)
+{
+    struct sx_priv     *priv = sx_priv(dev);
+    struct sx_dq_table *sdq_table = &priv->sdq_table;
+    int                 i;
+
+    for (i = 0; i < dev->dev_cap.max_num_sdqs; i++) {
+        if (sdq_table->dq[i]) {
+            if (atomic_read(&sdq_table->dq[i]->refcount) == 1) {
+                sx_core_destroy_sdq(dev, sdq_table->dq[i]);
+            } else {
+                atomic_dec(&sdq_table->dq[i]->refcount);
+            }
+        }
+    }
+
+    if (free_table) {
+        kfree(sdq_table->dq);
+    }
+}
+
+void sx_core_destroy_rdq_table(struct sx_dev *dev, u8 free_table)
+{
+    struct sx_priv     *priv = sx_priv(dev);
+    struct sx_dq_table *rdq_table = &priv->rdq_table;
+    int                 i;
+
+    for (i = 0; i < dev->dev_cap.max_num_rdqs; i++) {
+        if (rdq_table->dq[i]) {
+            if (atomic_read(&rdq_table->dq[i]->refcount) == 1) {
+                sx_core_destroy_rdq(dev, rdq_table->dq[i]);
+            } else {
+                atomic_dec(&rdq_table->dq[i]->refcount);
+            }
+        }
+    }
+
+    if (free_table) {
+        kfree(rdq_table->dq);
+    }
+}
+
+/************************************************
+ *                  EOF                         *
+ ***********************************************/
diff --git a/linux/drivers/hwmon/mellanox/dq.h b/linux/drivers/hwmon/mellanox/dq.h
new file mode 100644
index 0000000..24321fe
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/dq.h
@@ -0,0 +1,65 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+#ifndef DQ_H_
+#define DQ_H_
+
+#define SX_WQE_SHIFT 5
+#define SX_DESC_SIZE 32
+/************************************************
+ * Includes
+ ***********************************************/
+
+#include <linux/mlx_sx/device.h>
+#include "sx.h"
+
+
+/************************************************
+ * Structs
+ ***********************************************/
+
+/* DQ (SDQ/RDQ context) */
+struct sx_dq_context {
+	u8      cq;
+	u8      sdq_tclass;
+	u8      reserved1;
+	u8      log2_dq_sz;
+	u32     reserved2;
+	u32     reserved3;
+	u32     reserved4;
+	__be64	dma_addr[8];  /* DQE buffer dma addresses */
+	/* Physical Address of Descriptor Queue page <i> (i=0,1,...,7) */
+};
+
+/************************************************
+ * Functions
+ ***********************************************/
+void sx_core_post_recv(struct sx_dq *rdq, struct sk_buff *skb);
+int sx_core_init_sdq_table(struct sx_dev *dev);
+int sx_core_init_rdq_table(struct sx_dev *dev);
+int sx_core_create_sdq(struct sx_dev *dev, int nent,
+		u8 dqn, struct sx_dq **sdq_p);
+int sx_core_create_rdq(struct sx_dev *dev, int nent,
+		u8 dqn, struct sx_dq **rdq_p);
+
+void sx_core_destroy_sdq_table(struct sx_dev *dev, u8 free_table);
+void sx_core_destroy_rdq_table(struct sx_dev *dev, u8 free_table);
+void sx_core_destroy_sdq(struct sx_dev *dev, struct sx_dq *dq);
+void sx_core_destroy_rdq(struct sx_dev *dev, struct sx_dq *dq);
+int sx_add_pkts_to_sdq(struct sx_dq *sdq);
+int sx_hw2sw_dq(struct sx_dev *dev, struct sx_dq *dq);
+int sx_dq_modify_2err(struct sx_dev *dev, struct sx_dq *dq);
+#endif /* DQ_H_ */
+/************************************************
+ *                  EOF                         *
+ ***********************************************/
diff --git a/linux/drivers/hwmon/mellanox/eq.c b/linux/drivers/hwmon/mellanox/eq.c
new file mode 100644
index 0000000..f1a9c4c
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/eq.c
@@ -0,0 +1,495 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+/************************************************
+ * Includes                                     *
+ ***********************************************/
+
+#include <linux/workqueue.h>
+#include <linux/dma-mapping.h>
+#include <linux/interrupt.h>
+#include <linux/mlx_sx/device.h>
+#include <linux/mlx_sx/cmd.h>
+#include "sx.h"
+#include "eq.h"
+#include "cq.h"
+#include "alloc.h"
+
+#define SX_DBELL_EQ_CI_OFFSET	0x600
+#define SX_DBELL_EQ_ARM_OFFSET	0xa00
+#define SX_CLR_INT_SIZE			0x00004
+
+extern atomic_t cq_backup_polling_enabled;
+
+static void sx_intr_tasklet_handler(unsigned long data);
+
+/************************************************
+ * Functions                                    *
+ ***********************************************/
+
+static void sx_eq_set_ci(struct sx_eq *eq, int req_not)
+{
+	__raw_writel((__force u32)
+			cpu_to_be32((eq->cons_index + eq->nent) & 0xffff),
+			eq->ci_db);
+	/* We want to make sure the first one is written before the second one */
+	mb();
+
+	if (req_not)
+		__raw_writel((__force u32) cpu_to_be32(eq->cons_index & 0xffff),
+						       eq->arm_db);
+	/* We still want ordering, just not swabbing, so add a barrier */
+	mb();
+}
+
+static struct sx_eqe *sx_get_eqe(struct sx_eq *eq, u32 entry)
+{
+	unsigned long off = (entry & (eq->nent - 1)) * sizeof(struct sx_eqe);
+	return eq->page_list[off / PAGE_SIZE].buf + off % PAGE_SIZE;
+}
+
+static struct sx_eqe *sx_next_eqe_sw(struct sx_eq *eq)
+{
+	struct sx_eqe *eqe = sx_get_eqe(eq, eq->cons_index);
+	return (eqe->owner & 0x1) ^ !!(eq->cons_index & eq->nent) ? NULL : eqe;
+}
+
+static u16 sx_iterate_eq(struct sx_dev *dev, struct sx_eq *eq,
+		struct sx_bitmap *cq_bitmap, int *set_ci)
+{
+	struct sx_eqe *eqe;
+	u16 num_current_cqs = 0;
+
+	/* In Pelican it's possible that we get an interrupt
+	 * before the EQe is written. So we will ignore it and
+	 * just update the ci, so another interrupt will happen
+	 * when the EQe is written */
+	while ((eqe = sx_next_eqe_sw(eq))) {
+		/*
+		 * Make sure we read EQ entry contents after we've
+		 * checked the ownership bit.
+		 */
+		rmb();
+
+		switch (eqe->type) {
+		case SX_EVENT_TYPE_COMP:
+			if (!cq_bitmap) /* This should never happen */
+				break;
+			if (sx_bitmap_test(cq_bitmap, eqe->cqn))
+				break; /* This CQ's bit is already set */
+			sx_bitmap_set(cq_bitmap, eqe->cqn);
+			++num_current_cqs;
+			break;
+
+		case SX_EVENT_TYPE_CMD:
+			sx_cmd_event(dev,
+				     be16_to_cpu(eqe->event.cmd.token),
+				     eqe->event.cmd.status,
+				     be64_to_cpu(eqe->event.cmd.out_param));
+			break;
+		default:
+			sx_warn(dev, "Unhandled event %02x(%02x) on EQ %d "
+			"at index %u\n", eqe->type, eqe->subtype, eq->eqn,
+			eq->cons_index);
+			break;
+		};
+
+		++eq->cons_index;
+		++(*set_ci);
+
+		/*
+		 * The HCA will think the queue has overflowed if we
+		 * don't tell it we've been processing events.  We
+		 * create our EQs with SX_NUM_SPARE_EQE extra
+		 * entries, so we must update our consumer index at
+		 * least that often.
+		 */
+		if (unlikely((*set_ci) >= SX_NUM_SPARE_EQE)) {
+			/*
+			 * Conditional on hca_type is OK here because
+			 * this is a rare case, not the fast path.
+			 */
+			sx_eq_set_ci(eq, 0);
+			*set_ci = 0;
+		}
+	}
+
+	return num_current_cqs;
+}
+
+static void sx_eq_int(struct sx_dev *dev, struct sx_eq *eq)
+{
+	struct sx_bitmap cq_bitmap;
+	int i;
+	int err = 0;
+	u16 weight;
+	u16 num_current_cqs = 0;
+	int set_ci = 0;
+	int cq_bkp_poll_mode;
+    struct sx_priv *priv = sx_priv(dev);
+
+	if (eq->eqn == SX_EQ_ASYNC) {
+		sx_iterate_eq(dev, eq, NULL, &set_ci);
+		goto out;
+	}
+
+	err = sx_bitmap_init(&cq_bitmap, dev->dev_cap.max_num_cqs);
+	if (err)
+		goto out;
+
+	num_current_cqs = sx_iterate_eq(dev, eq, &cq_bitmap, &set_ci);
+
+	while (num_current_cqs) {
+		weight = 1 << dev->dev_cap.log_max_sdq_sz;
+		for (i = 0; i < dev->dev_cap.max_num_cqs; i++) {
+			if (test_bit(i, cq_bitmap.table)) {
+				if (i >= NUMBER_OF_SDQS)
+					weight = dev->profile.rdq_properties[i - NUMBER_OF_SDQS].rdq_weight;
+				err = sx_cq_completion(dev, i, weight);
+				if (err < 0)
+					printk(KERN_WARNING PFX "sx_eq_int: "
+						"cq_completion failed\n");
+				else if (err < weight) {
+					sx_bitmap_free(&cq_bitmap, i);
+					--num_current_cqs;
+				}
+			}
+		}
+
+		num_current_cqs += sx_iterate_eq(dev, eq, &cq_bitmap, &set_ci);
+	}
+
+out:
+    sx_eq_set_ci(eq, 1);
+
+    /* Backup poll */
+    if (priv && priv->cq_table.cq_rl_params &&
+        atomic_read(&cq_backup_polling_enabled) &&
+        atomic_read(&priv->cq_backup_polling_refcnt)) {
+        int found=0;
+        for (i = NUMBER_OF_SDQS; i < dev->dev_cap.max_num_cqs; i++) {
+            if (!priv->cq_table.cq[i])
+                continue;
+            cq_bkp_poll_mode=atomic_read(&priv->cq_table.cq[i]->bkp_poll_data.cq_bkp_poll_mode);
+            if (cq_bkp_poll_mode) {
+                found=1;
+                atomic_set(&priv->cq_table.cq[i]->bkp_poll_data.cq_bkp_poll_mode,0);
+                atomic_dec(&priv->cq_backup_polling_refcnt);
+                if (net_ratelimit()) {
+                printk(KERN_INFO PFX "sx_eq_int: doing backup poll for QN:%d\n",i);
+                }
+                weight = dev->profile.rdq_properties[i - NUMBER_OF_SDQS].rdq_weight;
+                err = sx_cq_completion(dev, i, weight);
+                if (err < 0)
+                    printk(KERN_WARNING PFX "sx_eq_int: "
+                        "backup poll cq_completion failed\n");
+            }
+        }
+        if (!found && atomic_read(&priv->cq_backup_polling_refcnt)) {
+            if (net_ratelimit()) {
+                printk(KERN_INFO PFX "sx_eq_int: vain backup polling\n");
+            }
+            atomic_set(&priv->cq_backup_polling_refcnt,0);
+        }
+    }    
+
+}
+
+static void sx_intr_tasklet_handler(unsigned long data)
+{
+	struct sx_dev *dev = (struct sx_dev *)data;
+	struct sx_priv *priv = sx_priv(dev);
+	int i;
+
+	for (i = 0; i < SX_NUM_EQ; ++i) {
+		if (!sx_bitmap_test(&priv->eq_table.bitmap, priv->eq_table.eq[i].eqn)) {
+			/* This is for avoiding cases of receiving interrupts during
+			 * deinit process, which happened on multi-core CPUs */
+			printk(KERN_DEBUG PFX "sx_intr_tasklet_handler: Skipping EQ %d "
+					"which was already freed\n",priv->eq_table.eq[i].eqn);
+			continue;
+		}
+
+		sx_eq_int(dev, &priv->eq_table.eq[i]);
+	}
+}
+
+static irqreturn_t sx_interrupt(int irq, void *dev_ptr)
+{
+	struct sx_dev *dev = dev_ptr;
+	struct sx_priv *priv = sx_priv(dev);
+
+	writel(priv->eq_table.clr_mask, priv->eq_table.clr_int);
+
+	tasklet_schedule(&priv->intr_tasklet);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t sx_msi_x_interrupt(int irq, void *dev_ptr)
+{
+    struct sx_dev  *dev = dev_ptr;
+    struct sx_priv *priv = sx_priv(dev);
+    
+	tasklet_schedule(&priv->intr_tasklet);
+
+	return IRQ_HANDLED;
+}
+
+
+static int sx_SW2HW_EQ(struct sx_dev *dev, struct sx_cmd_mailbox *mailbox,
+			 int eq_num)
+{
+	return sx_cmd(dev, dev->device_id, mailbox, eq_num, 0, SX_CMD_SW2HW_EQ,
+			SX_CMD_TIME_CLASS_A, sx_priv(dev)->fw.local_in_mb_size);
+}
+
+static int sx_HW2SW_EQ(struct sx_dev *dev, int eq_num)
+{
+	return sx_cmd(dev, dev->device_id, 0, eq_num, 0, SX_CMD_HW2SW_EQ,
+			SX_CMD_TIME_CLASS_A,
+			sx_priv(dev)->fw.local_in_mb_size);
+}
+
+
+static void sx_free_eq(struct sx_dev *dev, struct sx_eq *eq)
+{
+	struct sx_priv *priv = sx_priv(dev);
+	int err;
+	int npages = PAGE_ALIGN(sizeof(struct sx_eqe) * eq->nent) / PAGE_SIZE;
+	int i;
+
+	sx_bitmap_free(&priv->eq_table.bitmap, eq->eqn);
+	err = sx_HW2SW_EQ(dev, eq->eqn);
+	if (err) {
+		sx_warn(dev, "HW2SW_EQ failed (%d)\n", err);
+	}
+
+	for (i = 0; i < npages; ++i)
+		pci_free_consistent(dev->pdev, PAGE_SIZE,
+					eq->page_list[i].buf,
+					eq->page_list[i].map);
+
+	kfree(eq->page_list);
+}
+
+static int sx_create_eq(struct sx_dev *dev, int nent,
+		struct sx_eq *eq)
+{
+	struct sx_priv *priv = sx_priv(dev);
+	struct sx_cmd_mailbox *mailbox;
+	struct sx_eq_context *eq_context;
+	int npages;
+	int err = -ENOMEM;
+	int i;
+
+	eq->dev   = dev;
+	eq->nent  = roundup_pow_of_two(max(nent, 2));
+	npages = PAGE_ALIGN(eq->nent * sizeof(struct sx_eqe)) / PAGE_SIZE;
+
+	eq->page_list = kmalloc(npages * sizeof(*eq->page_list), GFP_KERNEL);
+	if (!eq->page_list)
+		goto err_out;
+
+	for (i = 0; i < npages; ++i)
+		eq->page_list[i].buf = NULL;
+
+	mailbox = sx_alloc_cmd_mailbox(dev, dev->device_id);
+	if (IS_ERR(mailbox))
+		goto err_out_free;
+
+	eq_context = mailbox->buf;
+
+	for (i = 0; i < npages; ++i) {
+		eq->page_list[i].buf = dma_alloc_coherent(&dev->pdev->dev,
+							  PAGE_SIZE,
+							  &eq->page_list[i].map,
+							  GFP_KERNEL);
+		if (!eq->page_list[i].buf)
+			goto err_out_free_pages;
+		memset(eq->page_list[i].buf, 0x1, PAGE_SIZE);
+	}
+
+	eq->eqn = sx_bitmap_alloc(&priv->eq_table.bitmap);
+	if (eq->eqn == -1)
+		goto err_out_free_pages;
+
+	eq->ci_db = dev->db_base + SX_DBELL_EQ_CI_OFFSET + 4 * eq->eqn;
+	eq->arm_db = dev->db_base + SX_DBELL_EQ_ARM_OFFSET + 4 * eq->eqn;
+	memset(eq_context, 0, sizeof *eq_context);
+	eq_context->flags	  = SX_EQ_STATE_ARMED;
+	eq_context->log_eq_size	  = ilog2(eq->nent) & 0xf;
+	eq_context->int_msi_x	  = !!(dev->flags & SX_FLAG_MSI_X);
+
+	for (i = 0; i < npages; ++i)
+		eq_context->dma_addr[i] = cpu_to_be64(eq->page_list[i].map);
+
+	err = sx_SW2HW_EQ(dev, mailbox, eq->eqn);
+	if (err) {
+		sx_warn(dev, "SW2HW_EQ failed (%d)\n", err);
+		goto err_out_free_eq;
+	}
+
+	sx_free_cmd_mailbox(dev, mailbox);
+	eq->cons_index = 0;
+
+	return err;
+
+err_out_free_eq:
+	sx_bitmap_free(&priv->eq_table.bitmap, eq->eqn);
+
+err_out_free_pages:
+	for (i = 0; i < npages; ++i) {
+		if (eq->page_list[i].buf) {
+			dma_free_coherent(&dev->pdev->dev, PAGE_SIZE,
+					  eq->page_list[i].buf,
+					  eq->page_list[i].map);
+		}
+	}
+
+	sx_free_cmd_mailbox(dev, mailbox);
+
+err_out_free:
+	kfree(eq->page_list);
+
+err_out:
+	return err;
+}
+
+static void sx_free_irqs(struct sx_dev *dev)
+{
+	struct sx_eq_table *eq_table = &sx_priv(dev)->eq_table;
+
+	if (eq_table->have_irq)
+		free_irq(dev->pdev->irq, dev);
+	if (eq_table->eq[0].have_irq) {
+		free_irq(eq_table->eq[0].irq, dev);
+		eq_table->eq[0].have_irq = 0;
+	}
+}
+
+static int sx_map_clr_int(struct sx_dev *dev)
+{
+	struct sx_priv *priv = sx_priv(dev);
+
+	priv->clr_base = ioremap(pci_resource_start(dev->pdev,
+	priv->fw.clr_int_bar) + priv->fw.clr_int_base, SX_CLR_INT_SIZE);
+	if (!priv->clr_base) {
+		sx_err(dev, "Couldn't map interrupt clear" \
+		"register, aborting.\n");
+		return -ENOMEM;
+	}
+
+	return 0;
+}
+
+static void sx_unmap_clr_int(struct sx_dev *dev)
+{
+	struct sx_priv *priv = sx_priv(dev);
+
+	iounmap(priv->clr_base);
+}
+
+int sx_init_eq_table(struct sx_dev *dev)
+{
+	struct sx_priv *priv = sx_priv(dev);
+	int i, err;
+
+#ifdef NO_PCI
+	/* No need to create EQs in simulator mode */
+	return 0;
+#endif
+	
+	tasklet_init(&priv->intr_tasklet, sx_intr_tasklet_handler,
+			(unsigned long)dev);
+	err = sx_bitmap_init(&priv->eq_table.bitmap, SX_NUM_EQ);
+	if (err)
+		return err;
+
+	err = sx_map_clr_int(dev);
+	if (err)
+		goto err_out;
+
+	priv->eq_table.clr_mask = swab32(1 << (priv->eq_table.inta_pin & 31));
+	priv->eq_table.clr_int  = priv->clr_base;
+	err = sx_create_eq(dev, SX_NUM_ASYNC_EQE, &priv->eq_table.eq[SX_EQ_ASYNC]);
+	if (err)
+		goto err_out_unmap;
+
+	err = sx_create_eq(dev, SX_NUM_COMP_EQE, &priv->eq_table.eq[SX_EQ_COMP]);
+	if (err)
+		goto err_out_async;
+
+	if (dev->flags & SX_FLAG_MSI_X) {
+		err = request_irq(priv->eq_table.eq[0].irq,
+				  sx_msi_x_interrupt,
+				  0, DRV_NAME "_msix", dev);
+		if (err)
+			goto err_out_async;
+
+		priv->eq_table.eq[SX_EQ_COMP].have_irq = 1;
+		priv->eq_table.eq[SX_EQ_ASYNC].have_irq = 1;
+
+	} else {
+		err = request_irq(dev->pdev->irq, sx_interrupt,
+				  IRQF_SHARED, DRV_NAME, dev);
+		if (err)
+			goto err_out_comp;
+
+		priv->eq_table.have_irq = 1;
+	}
+
+	for (i = 0; i < SX_NUM_EQ; ++i)
+		sx_eq_set_ci(&priv->eq_table.eq[i], 1);
+
+	return 0;
+
+err_out_comp:
+	sx_free_eq(dev, &priv->eq_table.eq[SX_EQ_COMP]);
+
+err_out_async:
+	sx_free_eq(dev, &priv->eq_table.eq[SX_EQ_ASYNC]);
+
+err_out_unmap:
+	sx_unmap_clr_int(dev);
+	sx_free_irqs(dev);
+
+err_out:
+
+	return err;
+}
+
+
+void sx_cleanup_eq_table(struct sx_dev *dev)
+{
+	struct sx_priv *priv = sx_priv(dev);
+	int i;
+
+#ifdef NO_PCI
+	return;
+#endif
+
+	sx_free_irqs(dev);
+	for (i = 0; i < SX_NUM_EQ; ++i)
+		sx_free_eq(dev, &priv->eq_table.eq[i]);
+
+	sx_unmap_clr_int(dev);
+
+	tasklet_kill(&priv->intr_tasklet);
+}
+
+
+/************************************************
+ *                  EOF                         *
+ ***********************************************/
+
diff --git a/linux/drivers/hwmon/mellanox/eq.h b/linux/drivers/hwmon/mellanox/eq.h
new file mode 100644
index 0000000..5f06cf5
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/eq.h
@@ -0,0 +1,97 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+
+#ifndef EQ_H_
+#define EQ_H_
+
+/************************************************
+ * Enum
+ ***********************************************/
+
+enum SX_NUM_EQE {
+	SX_NUM_COMP_EQE		= 0x100,
+	SX_NUM_ASYNC_EQE	= 0x100,
+	SX_NUM_SPARE_EQE	= 0x80
+};
+
+enum SX_EQ_STATE {
+	SX_EQ_STATE_ARMED	 = 0x01,
+	SX_EQ_STATE_FIRED	 = 0x00,
+	SX_EQ_STATE_ALWAYS_ARMED = 0x03
+};
+
+enum SX_EQ_PKT_TYPE {
+	SX_EQ_PKT_TYPE_RAW_IB   = 0x000,
+	SX_EQ_PKT_TYPE_IB_TRANS = 0x001, /* IB transport  */
+	SX_EQ_PKT_TYPE_ETH      = 0x010,
+	SX_EQ_PKT_TYPE_FC_IB    = 0x100, /* Fibre Channel over IB */
+	SX_EQ_PKT_TYPE_FC_ETH   = 0x101 /* Fibre Channel over Ethernet */
+};
+
+enum SX_EQ_TYPE {
+	SX_EQ_ASYNC,
+	SX_EQ_COMP,
+	SX_NUM_EQ
+};
+
+/************************************************
+ * Structs
+ ***********************************************/
+
+/* Event Que */
+struct sx_eq_context {
+	u8      int_msi_x;
+	u8      reserved1;
+	u8		flags;
+	u8      log_eq_size;
+	u16     reserved2;
+	__be16  producer_counter;
+	u64     reserved3;
+	__be64	dma_addr[8];
+};
+
+/* Event Que Entry (Descriptor)*/
+struct sx_eqe {
+	union {
+		u32		raw[3];
+		struct {
+			__be16	token;
+			u8	reserved1;
+			u8	status;
+			__be64	out_param;
+		} __attribute__((packed)) cmd;
+	}	event;
+	u8 	type;
+	u8	subtype;
+	u8	cqn;
+	u8	owner;  /* SW(consumer)/HW(producer) owner*/
+} __attribute__((packed));;
+
+/************************************************
+ * Externals
+ ***********************************************/
+extern struct tasklet_struct intr_tasklet;
+
+/************************************************
+ * Functions prototype
+ ***********************************************/
+
+int sx_init_eq_table(struct sx_dev *dev);
+void sx_cleanup_eq_table(struct sx_dev *dev);
+
+#endif /* EQ_H_ */
+
+/************************************************
+ *                  EOF                         *
+ ***********************************************/
diff --git a/linux/drivers/hwmon/mellanox/fw.c b/linux/drivers/hwmon/mellanox/fw.c
new file mode 100644
index 0000000..61350ad
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/fw.c
@@ -0,0 +1,5330 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+#include "fw.h"
+#include "sx.h"
+#include <linux/mlx_sx/cmd.h>
+#include <linux/mlx_sx/driver.h>
+
+extern struct sx_globals sx_glb;
+#define MCIA_REG_ID 0x9014
+
+enum {
+	MCIA_INVALID_PORT 		= 0x17,
+	MCIA_PORT_NOT_SUPP 		= 0x27,
+	MCIA_NOT_CONNECTED 		= 0x37,
+	MCIA_NO_EEPROM 			= 0x47,
+	MCIA_INVALID_PAGE 		= 0x57,
+	MCIA_INVALID_DEVICE_ADDR 	= 0x67,
+	MCIA_INVALID_I2C_DEV_ADDR	= 0x77,
+	MCIA_CABLE_NOT_SUPP 		= 0x87,
+	MCIA_I2C_ERROR 			= 0x97,
+};
+
+static const char *mcia_err_str(u8 status)
+{
+	switch (status) {
+	case MCIA_INVALID_PORT:
+		return "Invalid port";
+	case MCIA_PORT_NOT_SUPP:
+		return "Port not supported";
+	case MCIA_NOT_CONNECTED:
+		return "Not connected";
+	case MCIA_NO_EEPROM:
+		return "No EEPROM";
+	case MCIA_INVALID_PAGE:
+		return "Invalid page";
+	case MCIA_INVALID_DEVICE_ADDR:
+		return "Invalid device address";
+	case MCIA_INVALID_I2C_DEV_ADDR:
+		return "Invalid I2C device address";
+	case MCIA_CABLE_NOT_SUPP:
+		return "Cable not supported";
+	case MCIA_I2C_ERROR:
+		return "I2C";
+	default:
+		return "Unknown";
+	}
+}
+
+#define SX_GET(dest, source, offset)					\
+	do {								\
+		void *__p = (char *) (source) + (offset);		\
+		switch (sizeof(dest)) {				\
+		case 1:							\
+			(dest) = *(u8 *) __p;				\
+			break;	      					\
+		case 2: 						\
+			(dest) = be16_to_cpup(__p);			\
+			break;	      					\
+		case 4: 						\
+			(dest) = be32_to_cpup(__p); 			\
+			break;	      					\
+		case 8: 						\
+			(dest) = be64_to_cpup(__p); 			\
+			break;	      					\
+		default: 						\
+			break;						\
+		}							\
+	} while (0)
+
+#define SX_PUT(dest, source, offset)					\
+	do {								\
+		void *__d = ((char *) (dest) + (offset));		\
+		switch (sizeof(source)) {				\
+		case 1:							\
+			*(u8 *) __d = (source);				\
+			break; 						\
+		case 2:							\
+			*(__be16 *) __d = cpu_to_be16(source);		\
+			break; 						\
+		case 4:							\
+			*(__be32 *) __d = cpu_to_be32(source);		\
+			break; 						\
+		case 8:							\
+			*(__be64 *) __d = cpu_to_be64(source);		\
+			break; 						\
+		default: 						\
+			break;						\
+		}							\
+	} while (0)
+
+#ifdef SX_DEBUG
+void dump_mailbox(void *mbox, int size, char *title)
+{
+	u8 *buf = mbox;
+	int i;
+
+	printk(KERN_DEBUG "%s: MBOX contents (%p)\n", title, mbox);
+	for (i = 0; i < size ; i += 32)
+		printk(KERN_DEBUG "%02x%02x%02x%02x\n%02x%02x%02x%02x\n"
+		       "%02x%02x%02x%02x\n%02x%02x%02x%02x\n"
+		       "%02x%02x%02x%02x\n%02x%02x%02x%02x\n"
+		       "%02x%02x%02x%02x\n%02x%02x%02x%02x\n",
+		       buf[i+0], buf[i+1], buf[i+2], buf[i+3],
+		       buf[i+4], buf[i+5], buf[i+6], buf[i+7],
+		       buf[i+8], buf[i+9], buf[i+10], buf[i+11],
+		       buf[i+12], buf[i+13], buf[i+14], buf[i+15],
+		       buf[i+16], buf[i+17], buf[i+18], buf[i+19],
+		       buf[i+20], buf[i+21], buf[i+22], buf[i+23],
+		       buf[i+24], buf[i+25], buf[i+26], buf[i+27],
+		       buf[i+28], buf[i+29], buf[i+30], buf[i+31]);
+}
+#endif
+
+static void get_board_id(void *vsd, char *board_id)
+{
+#define VSD_OFFSET_SX_BOARD_ID	0xd0
+#define SX_PSID_SIZE		16
+
+	memset(board_id, 0, SX_BOARD_ID_LEN);
+	memcpy(board_id, vsd + VSD_OFFSET_SX_BOARD_ID, SX_PSID_SIZE);
+}
+
+#define QUERY_FW_IN_MB_SIZE	0x4c
+int sx_QUERY_FW(struct sx_dev *dev, struct ku_query_fw* query_fw)
+{
+	struct sx_fw  fw;
+	struct sx_cmd *cmd = &sx_priv(dev)->cmd;
+	struct sx_cmd_mailbox *mailbox;
+	u32 *outbox;
+	int err = 0;
+	int	 target_dev_id;
+
+	fw.local_in_mb_size = 0;
+	fw.local_out_mb_offset = 0;
+	fw.fw_icm = 0;
+	fw.local_in_mb_offset = 0;
+	fw.local_out_mb_size = 0;
+
+	if (NULL == query_fw) {
+		target_dev_id = DEFAULT_DEVICE_ID;
+	} else {
+		target_dev_id = query_fw->dev_id;
+	}
+
+	if (DEFAULT_DEVICE_ID < target_dev_id) {
+		printk(KERN_NOTICE "dev_id %d exceeded range : 1 - %d",
+					target_dev_id, target_dev_id);
+		return -EINVAL;
+	}
+
+#define QUERY_FW_VER_OFFSET		0x00
+#define QUERY_FW_CORE_CLOCK_OFFSET	0x08
+#define QUERY_FW_DEBUG_TRACE_OFFSET	0x0c
+#define QUERY_FW_FW_HOUR_OFFSET		0x10
+#define QUERY_FW_FW_MINUTES_OFFSET	0x11
+#define QUERY_FW_FW_SECONDS_OFFSET	0x12
+#define QUERY_FW_FW_YEAR_OFFSET		0x14
+#define QUERY_FW_FW_MONTH_OFFSET	0x16
+#define QUERY_FW_FW_DAY_OFFSET		0x17
+#define QUERY_FW_ERR_START_OFFSET	0x30
+#define QUERY_FW_ERR_SIZE_OFFSET	0x38
+#define QUERY_FW_ERR_BAR_OFFSET		0x3c
+#define QUERY_FW_CLR_INT_BASE_OFFSET	0x20
+#define QUERY_FW_CLR_INT_BAR_OFFSET	0x28
+#define QUERY_FW_DB_PAGE_OFFSET_OFFSET	0x40
+#define QUERY_FW_DB_PAGE_BAR_OFFSET	0x48
+#define QUERY_FW_SIZE_OFFSET           	0x00
+
+	mailbox = sx_alloc_cmd_mailbox(dev, target_dev_id);
+	if (IS_ERR(mailbox))
+		return PTR_ERR(mailbox);
+	outbox = mailbox->buf;
+	err = sx_cmd_box(dev, target_dev_id, 0, mailbox, 0, 0,
+			SX_CMD_QUERY_FW, SX_CMD_TIME_CLASS_A,
+			QUERY_FW_IN_MB_SIZE);
+	if (err)
+		goto out;
+
+	SX_GET(fw.fw_pages, outbox, QUERY_FW_SIZE_OFFSET);
+	SX_GET(fw.fw_ver,   outbox, QUERY_FW_VER_OFFSET);
+	/*
+	 * FW subminor version is at more significant bits than minor
+	 * version, so swap here.
+	 */
+	fw.fw_ver = (fw.fw_ver & 0xffff00000000ull) |
+		((fw.fw_ver & 0xffff0000ull) >> 16) |
+		((fw.fw_ver & 0x0000ffffull) << 16);
+	dev->fw_ver = fw.fw_ver;
+	SX_GET(fw.core_clock,  outbox, QUERY_FW_CORE_CLOCK_OFFSET);
+	SX_GET(fw.debug_trace, outbox, QUERY_FW_DEBUG_TRACE_OFFSET);
+	fw.debug_trace = fw.debug_trace >> 7;
+	cmd->max_cmds = 1;
+	sx_dbg(dev, "FW version %012llx, max commands %d\n",
+		  (unsigned long long) fw.fw_ver, cmd->max_cmds);
+	SX_GET(fw.catas_offset, outbox, QUERY_FW_ERR_START_OFFSET);
+	SX_GET(fw.catas_size,   outbox, QUERY_FW_ERR_SIZE_OFFSET);
+	SX_GET(fw.catas_bar,    outbox, QUERY_FW_ERR_BAR_OFFSET);
+	fw.catas_bar = (fw.catas_bar >> 6) * 2;
+	sx_dbg(dev, "Error buffer offset at 0x%llx, size 0x%x in BAR %u\n",
+		 (unsigned long long) fw.catas_offset, fw.catas_size,
+		 fw.catas_bar);
+	SX_GET(fw.clr_int_base, outbox, QUERY_FW_CLR_INT_BASE_OFFSET);
+	SX_GET(fw.clr_int_bar,  outbox, QUERY_FW_CLR_INT_BAR_OFFSET);
+	fw.clr_int_bar = (fw.clr_int_bar >> 6) * 2;
+	sx_dbg(dev, "FW size %d KB\n", fw.fw_pages << 2);
+	sx_dbg(dev, "Clear int base at 0x%llx, in BAR %u\n",
+		 (unsigned long long) fw.clr_int_base, fw.clr_int_bar);
+	SX_GET(fw.doorbell_page_offset, outbox,
+			QUERY_FW_DB_PAGE_OFFSET_OFFSET);
+	SX_GET(fw.doorbell_page_bar,    outbox, QUERY_FW_DB_PAGE_BAR_OFFSET);
+	SX_GET(fw.fw_hour,    outbox, QUERY_FW_FW_HOUR_OFFSET);
+	SX_GET(fw.fw_minutes, outbox, QUERY_FW_FW_MINUTES_OFFSET);
+	SX_GET(fw.fw_seconds, outbox, QUERY_FW_FW_SECONDS_OFFSET);
+	SX_GET(fw.fw_year,    outbox, QUERY_FW_FW_YEAR_OFFSET);
+	SX_GET(fw.fw_month,   outbox, QUERY_FW_FW_MONTH_OFFSET);
+	SX_GET(fw.fw_day,     outbox, QUERY_FW_FW_DAY_OFFSET);
+
+	if (NULL != query_fw) {
+		query_fw->core_clk = fw.core_clock;
+		query_fw->dt = fw.debug_trace;
+		query_fw->fw_year = fw.fw_year;
+		query_fw->fw_month = fw.fw_month;
+		query_fw->fw_day = fw.fw_day;
+		query_fw->fw_hour = fw.fw_hour;
+		query_fw->fw_minutes = fw.fw_minutes;
+		query_fw->fw_seconds = fw.fw_seconds;
+		query_fw->fw_rev = fw.fw_ver;
+	}
+
+	if (0 == sx_priv(dev)->is_fw_initialized &&
+		DPT_PATH_PCI_E == sx_glb.sx_dpt.dpt_info[target_dev_id].cmd_path) {
+		memcpy(&sx_priv(dev)->fw, &fw, sizeof(fw));
+		sx_priv(dev)->is_fw_initialized = 1;
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, mailbox);
+	return err;
+}
+EXPORT_SYMBOL(sx_QUERY_FW);
+
+int sx_QUERY_FW_2(struct sx_dev *dev, int sx_dev_id)
+{
+	struct sx_fw  *fw  = &sx_priv(dev)->fw;
+	struct sx_cmd_mailbox *mailbox;
+	int err = 0;
+	u32 out_mb_info, in_mb_info;
+
+#define QUERY_FW_OUT_MB_INFO_OFFSET	0x00
+#define QUERY_FW_IN_MB_INFO_OFFSET	0x04
+	mailbox = sx_alloc_cmd_mailbox(dev, dev->device_id);
+	if (IS_ERR(mailbox))
+		return PTR_ERR(mailbox);
+	mailbox->imm_data = 0ULL;
+	err = sx_cmd_imm(dev, sx_dev_id, 0, mailbox, 0, 1, SX_CMD_QUERY_FW,
+			    SX_CMD_TIME_CLASS_A, QUERY_FW_IN_MB_SIZE);
+	if (err)
+		goto out;
+
+	in_mb_info = mailbox->imm_data >> 32;
+	out_mb_info = mailbox->imm_data & 0xFFFFFFFFUL;
+
+	/* TODO: what about endianess?? */
+	if (dev->device_id == sx_dev_id) {
+		fw->local_in_mb_offset	= (in_mb_info & 0x000FFFFF);
+		fw->local_in_mb_size	= in_mb_info >> 20;
+		fw->local_out_mb_offset	= (out_mb_info & 0x000FFFFF);
+		fw->local_out_mb_size	= out_mb_info >> 20;
+	}
+#ifdef SX_DEBUG
+	printk(KERN_INFO PFX "sx_QUERY_FW_2 for dev_id %u before:\n"
+			"in_mb_offset=0x%x\n"
+			"in_mb_size=%u\n"
+			"out_mb_offset=0x%x\n"
+			"out_mb_size=%u\n",
+			sx_dev_id,
+			sx_glb.sx_dpt.dpt_info[sx_dev_id].in_mb_offset,
+			sx_glb.sx_dpt.dpt_info[sx_dev_id].in_mb_size,
+			sx_glb.sx_dpt.dpt_info[sx_dev_id].out_mb_offset,
+			sx_glb.sx_dpt.dpt_info[sx_dev_id].out_mb_size);
+#endif
+	sx_glb.sx_dpt.dpt_info[sx_dev_id].in_mb_offset = (in_mb_info & 0x000FFFFF);
+	sx_glb.sx_dpt.dpt_info[sx_dev_id].in_mb_size = in_mb_info >> 20;
+	sx_glb.sx_dpt.dpt_info[sx_dev_id].out_mb_offset = (out_mb_info & 0x000FFFFF);
+	sx_glb.sx_dpt.dpt_info[sx_dev_id].out_mb_size = out_mb_info >> 20;
+#ifdef SX_DEBUG
+	printk(KERN_INFO PFX "sx_QUERY_FW_2 for dev_id %u results:\n"
+			"in_mb_offset=0x%x\n"
+			"in_mb_size=%u\n"
+			"out_mb_offset=0x%x\n"
+			"out_mb_size=%u\n",
+			sx_dev_id,
+			sx_glb.sx_dpt.dpt_info[sx_dev_id].in_mb_offset,
+			sx_glb.sx_dpt.dpt_info[sx_dev_id].in_mb_size,
+			sx_glb.sx_dpt.dpt_info[sx_dev_id].out_mb_offset,
+			sx_glb.sx_dpt.dpt_info[sx_dev_id].out_mb_size);
+#endif
+out:
+	sx_free_cmd_mailbox(dev, mailbox);
+	return err;
+}
+
+int sx_map_cmd(struct sx_dev *dev, u16 op, struct sx_icm *icm)
+{
+	struct sx_cmd_mailbox *mailbox;
+	struct sx_icm_iter iter;
+	__be64 *pages;
+	int lg;
+	int nent = 0;
+	int i;
+	int err = 0;
+	int ts = 0, tc = 0;
+
+	mailbox = sx_alloc_cmd_mailbox(dev, dev->device_id);
+	if (IS_ERR(mailbox))
+		return PTR_ERR(mailbox);
+	memset(mailbox->buf, 0, SX_MAILBOX_SIZE);
+	pages = mailbox->buf;
+
+	for (sx_icm_first(icm, &iter);
+	     !sx_icm_last(&iter);
+	     sx_icm_next(&iter)) {
+		/*
+		 * We have to pass pages that are aligned to their
+		 * size, so find the least significant 1 in the
+		 * address or size and use that as our log2 size.
+		 */
+		lg = ffs(sx_icm_addr(&iter) | sx_icm_size(&iter)) - 1;
+		if (lg < SX_ICM_PAGE_SHIFT) {
+			sx_warn(dev, "Got FW area not aligned to "
+					"%d (%llx/%lx).\n", SX_ICM_PAGE_SIZE,
+				   (unsigned long long) sx_icm_addr(&iter),
+				   sx_icm_size(&iter));
+			err = -EINVAL;
+			goto out;
+		}
+
+		for (i = 0; i < sx_icm_size(&iter) >> lg; ++i) {
+			pages[nent] =
+				cpu_to_be64((sx_icm_addr(&iter) + (i << lg)) |
+					    (lg - SX_ICM_PAGE_SHIFT));
+			ts += 1 << (lg - 10);
+			++tc;
+
+			if (++nent == SX_MAILBOX_SIZE / 16) {
+				err = sx_cmd(dev, dev->device_id, mailbox,
+					nent, 0, op,
+					SX_CMD_TIME_CLASS_B,
+					sx_priv(dev)->fw.local_in_mb_size);
+				if (err)
+					goto out;
+				nent = 0;
+			}
+		}
+	}
+
+	if (nent)
+		err = sx_cmd(dev, dev->device_id, mailbox, nent, 0, op,
+				SX_CMD_TIME_CLASS_B, 0);
+	if (err)
+		goto out;
+
+	switch (op) {
+	case SX_CMD_MAP_FA:
+		sx_dbg(dev, "Mapped %d chunks/%d KB for FW.\n", tc, ts);
+		break;
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, mailbox);
+	return err;
+}
+
+
+int sx_MAP_FA(struct sx_dev *dev, struct sx_icm *icm)
+{
+	return sx_map_cmd(dev, SX_CMD_MAP_FA, icm);
+}
+
+int sx_UNMAP_FA(struct sx_dev *dev)
+{
+	return sx_cmd(dev, dev->device_id, 0, 0, 0, SX_CMD_UNMAP_FA,
+			SX_CMD_TIME_CLASS_B,
+			sx_priv(dev)->fw.local_in_mb_size);
+}
+
+int sx_QUERY_AQ_CAP(struct sx_dev *dev)
+{
+	struct sx_cmd_mailbox *mailbox;
+	u32 *outbox;
+	u8 field;
+	int err;
+	struct sx_dev_cap *dev_cap = &dev->dev_cap;
+
+#define QUERY_DEV_CAP_MAX_SDQ_SZ_OFFSET		0x0
+#define QUERY_DEV_CAP_MAX_SDQ_OFFSET		0x3
+#define QUERY_DEV_CAP_MAX_RDQ_SZ_OFFSET		0x4
+#define QUERY_DEV_CAP_MAX_RDQ_OFFSET		0x7
+#define QUERY_DEV_CAP_MAX_CQ_SZ_OFFSET		0x8
+#define QUERY_DEV_CAP_MAX_CQ_OFFSET		0xb
+#define QUERY_DEV_CAP_MAX_EQ_SZ_OFFSET		0xc
+#define QUERY_DEV_CAP_MAX_EQ_OFFSET		0xf
+#define QUERY_DEV_CAP_MAX_SG_SQ_OFFSET		0x12
+#define QUERY_DEV_CAP_MAX_SG_RQ_OFFSET		0x13
+
+	mailbox = sx_alloc_cmd_mailbox(dev, dev->device_id);
+	if (IS_ERR(mailbox))
+		return PTR_ERR(mailbox);
+	outbox = mailbox->buf;
+
+	err = sx_cmd_box(dev, dev->device_id, 0, mailbox, 0, 0,
+			SX_CMD_QUERY_AQ_CAP, SX_CMD_TIME_CLASS_A, 0);
+
+	if (err)
+		goto out;
+
+	SX_GET(field, outbox, QUERY_DEV_CAP_MAX_SDQ_SZ_OFFSET);
+	dev_cap->log_max_sdq_sz = min((int)field, SX_MAX_LOG_DQ_SIZE);
+	SX_GET(field, outbox, QUERY_DEV_CAP_MAX_SDQ_OFFSET);
+	dev_cap->max_num_sdqs = field;
+	SX_GET(field, outbox, QUERY_DEV_CAP_MAX_RDQ_SZ_OFFSET);
+	dev_cap->log_max_rdq_sz = min((int)field, SX_MAX_LOG_DQ_SIZE);
+	SX_GET(field, outbox, QUERY_DEV_CAP_MAX_RDQ_OFFSET);
+	dev_cap->max_num_rdqs = field;
+	SX_GET(field, outbox, QUERY_DEV_CAP_MAX_CQ_SZ_OFFSET);
+	dev_cap->log_max_cq_sz = field;
+	SX_GET(field, outbox, QUERY_DEV_CAP_MAX_CQ_OFFSET);
+	dev_cap->max_num_cqs = field;
+	SX_GET(field, outbox, QUERY_DEV_CAP_MAX_EQ_SZ_OFFSET);
+	dev_cap->log_max_eq_sz = field;
+	SX_GET(field, outbox, QUERY_DEV_CAP_MAX_EQ_OFFSET);
+	dev_cap->max_num_eqs = field;
+	SX_GET(field, outbox, QUERY_DEV_CAP_MAX_SG_SQ_OFFSET);
+	dev_cap->max_sg_sq = field;
+	SX_GET(field, outbox, QUERY_DEV_CAP_MAX_SG_RQ_OFFSET);
+	dev_cap->max_sg_rq = field;
+
+	sx_dbg(dev, "Log Max SDQ sz: %d, num SDQs: %d\n",
+		  dev_cap->log_max_sdq_sz, dev_cap->max_num_sdqs);
+	sx_dbg(dev, "Log Max RDQ sz: %d, num RDQs: %d\n",
+		  dev_cap->log_max_rdq_sz, dev_cap->max_num_rdqs);
+	sx_dbg(dev, "Log Max CQ sz: %d, num CQs: %d\n",
+		  dev_cap->log_max_cq_sz, dev_cap->max_num_cqs);
+	sx_dbg(dev, "Log Max EQ sz: %d, num EQs: %d\n",
+		  dev_cap->log_max_eq_sz, dev_cap->max_num_eqs);
+
+out:
+	sx_free_cmd_mailbox(dev, mailbox);
+	return err;
+}
+EXPORT_SYMBOL(sx_QUERY_AQ_CAP);
+
+int sx_QUERY_BOARDINFO(struct sx_dev *dev, struct sx_board *board)
+{
+	struct sx_cmd_mailbox *mailbox;
+	u32 *outbox;
+	int err;
+
+#define QUERY_ADAPTER_INTA_PIN_OFFSET      0x10
+#define QUERY_ADAPTER_VSD_VENDOR_ID_OFFSET 0x1e
+#define QUERY_ADAPTER_VSD_OFFSET           0x20
+
+	memset(board, 0, sizeof(*board));
+	mailbox = sx_alloc_cmd_mailbox(dev, dev->device_id);
+	if (IS_ERR(mailbox))
+		return PTR_ERR(mailbox);
+
+	outbox = mailbox->buf;
+	err = sx_cmd_box(dev, dev->device_id, 0, mailbox, 0, 0,
+			SX_CMD_QUERY_BOARDINFO, SX_CMD_TIME_CLASS_A, 0);
+	if (err)
+		goto out;
+
+	SX_GET(board->vsd_vendor_id, outbox,
+			QUERY_ADAPTER_VSD_VENDOR_ID_OFFSET);
+	SX_GET(board->inta_pin, outbox, QUERY_ADAPTER_INTA_PIN_OFFSET);
+	sx_dbg(dev, "sx_QUERY_ADAPTER: inta_pin = 0x%x\n", board->inta_pin);
+
+	if (board->vsd_vendor_id == PCI_VENDOR_ID_MELLANOX)
+		get_board_id(outbox + QUERY_ADAPTER_VSD_OFFSET / 4,
+				board->board_id);
+
+out:
+	sx_free_cmd_mailbox(dev, mailbox);
+	return err;
+}
+EXPORT_SYMBOL(sx_QUERY_BOARDINFO);
+
+static void set_opoeration_tlv(void *inbox, struct ku_operation_tlv *op_tlv)
+{
+	u16 type_len = 0;
+	u8 dr_status = 0;
+	u8 r_method = 0;
+
+#define TYPE_LEN_OFFSET		0x00
+#define DR_STATUS_OFFSET	0x02
+#define REGISTER_ID_OFFSET	0x04
+#define R_METHOD_OFFSET		0x06
+#define CLASS_OFFSET		0x07
+#define TID_OFFSET		0x08
+
+	type_len = op_tlv->length | (op_tlv->type << 11);
+	SX_PUT(inbox, type_len, TYPE_LEN_OFFSET);
+	dr_status = op_tlv->status | (op_tlv->dr << 7);
+	SX_PUT(inbox, dr_status, DR_STATUS_OFFSET);
+	SX_PUT(inbox, op_tlv->register_id, REGISTER_ID_OFFSET);
+	r_method = op_tlv->method | (op_tlv->r << 7);
+	SX_PUT(inbox, r_method, R_METHOD_OFFSET);
+	SX_PUT(inbox, op_tlv->op_class, CLASS_OFFSET);
+	SX_PUT(inbox, op_tlv->tid, TID_OFFSET);
+}
+
+#define OPERATION_TLV_SIZE	0x10
+#define IN_MB_SIZE(reg_dword_size) \
+	(((reg_dword_size) * 4) + OPERATION_TLV_SIZE)
+static void get_operation_tlv(void *outbox, struct ku_operation_tlv *op_tlv)
+{
+	u16 type_len = 0;
+	u8 dr_status = 0;
+	u8 r_method = 0;
+
+#define TYPE_LEN_OFFSET		0x00
+#define DR_STATUS_OFFSET	0x02
+#define REGISTER_ID_OFFSET	0x04
+#define R_METHOD_OFFSET		0x06
+#define CLASS_OFFSET		0x07
+#define TID_OFFSET		0x08
+
+	SX_GET(type_len, outbox, TYPE_LEN_OFFSET);
+	op_tlv->length = type_len & 0x7ff;
+	op_tlv->type = type_len >> 11;
+	SX_GET(dr_status, outbox, DR_STATUS_OFFSET);
+	op_tlv->status = dr_status & 0x7f;
+	op_tlv->dr = dr_status >> 7;
+	SX_GET(op_tlv->register_id, outbox, REGISTER_ID_OFFSET);
+	SX_GET(r_method, outbox, R_METHOD_OFFSET);
+	op_tlv->method = r_method & 0x7f;
+	op_tlv->r = r_method >> 7;
+	SX_GET(op_tlv->op_class, outbox, CLASS_OFFSET);
+	SX_GET(op_tlv->tid, outbox, TID_OFFSET);
+	if (op_tlv->status) {
+		if (op_tlv->register_id != MCIA_REG_ID)
+			printk(KERN_WARNING PFX "get_operation_tlv: Err: Got status "
+				"0x%x for register 0x%x\n",
+				op_tlv->status, op_tlv->register_id);
+		else
+			printk(KERN_WARNING PFX "MCIA register reported "
+					"%s error\n", mcia_err_str(op_tlv->status));
+	}
+}
+
+#define REG_TLV_OFFSET		0x10
+#define REG_TLV_TYPE		0x03
+int sx_ACCESS_REG_MGIR(struct sx_dev *dev, struct ku_access_mgir_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_HW_INFO_OFFSET			0x14
+#define REG_HW_INFO_DEVICE_HW_REVISION_OFFSET	0x0
+#define REG_HW_INFO_DEVICE_ID_OFFSET		0x2
+#define REG_HW_INFO_DVFS_OFFSET			0x7
+#define REG_HW_INFO_UPTIME_OFFSET		0x1c
+
+#define REG_FW_INFO_OFFSET			0x34
+#define REG_FW_INFO_MAJOR_OFFSET		0x01
+#define REG_FW_INFO_MINOR_OFFSET		0x02
+#define REG_FW_INFO_SUB_MINOR_OFFSET		0x03
+#define REG_FW_INFO_BUILD_ID_OFFSET		0x04
+#define REG_FW_INFO_MONTH_OFFSET		0x08
+#define REG_FW_INFO_DAY_OFFSET			0x09
+#define REG_FW_INFO_YEAR_OFFSET			0x0a
+#define REG_FW_INFO_HOUR_OFFSET			0x0e
+#define REG_FW_INFO_PSID_OFFSET			0x10
+#define REG_FW_INFO_INI_FILE_VERSION_OFFSET	0x20
+#define REG_FW_INFO_EXTENDED_MAJOR_OFFSET	0x24
+#define REG_FW_INFO_EXTENDED_MINOR_OFFSET	0x28
+#define REG_FW_INFO_EXTENDED_SUB_MINOR_OFFSET	0x2c
+
+#define REG_SW_INFO_OFFSET			0x74
+#define REG_SW_INFO_MAJOR_OFFSET		0x01
+#define REG_SW_INFO_MINOR_OFFSET		0x02
+#define REG_SW_INFO_SUB_MINOR_OFFSET		0x03
+
+#define MGIR_REG_LEN				0x21
+
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= MGIR_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+#if 0 /* This is a RO register */
+	SX_PUT(inbox, reg_data->mgir_reg.hw_info.device_hw_revision,
+			REG_HW_INFO_OFFSET + REG_HW_INFO_DEVICE_HW_REVISION_OFFSET);
+	SX_PUT(inbox, reg_data->mgir_reg.hw_info.device_id,
+			REG_HW_INFO_OFFSET + REG_HW_INFO_DEVICE_ID_OFFSET);
+	SX_PUT(inbox, reg_data->mgir_reg.hw_info.dvfs,
+			REG_HW_INFO_OFFSET + REG_HW_INFO_DVFS_OFFSET);
+	SX_PUT(inbox, reg_data->mgir_reg.hw_info.uptime,
+			REG_HW_INFO_OFFSET + REG_HW_INFO_UPTIME_OFFSET);
+	SX_PUT(inbox, reg_data->mgir_reg.fw_info.major,
+			REG_FW_INFO_OFFSET + REG_FW_INFO_MAJOR_OFFSET);
+	SX_PUT(inbox, reg_data->mgir_reg.fw_info.minor,
+			REG_FW_INFO_OFFSET + REG_FW_INFO_MINOR_OFFSET);
+	SX_PUT(inbox, reg_data->mgir_reg.fw_info.sub_minor,
+			REG_FW_INFO_OFFSET + REG_FW_INFO_SUB_MINOR_OFFSET);
+	SX_PUT(inbox, reg_data->mgir_reg.fw_info.build_id,
+			REG_FW_INFO_OFFSET + REG_FW_INFO_BUILD_ID_OFFSET);
+	SX_PUT(inbox, reg_data->mgir_reg.fw_info.month,
+			REG_FW_INFO_OFFSET + REG_FW_INFO_MONTH_OFFSET);
+	SX_PUT(inbox, reg_data->mgir_reg.fw_info.day,
+			REG_FW_INFO_OFFSET + REG_FW_INFO_DAY_OFFSET);
+	SX_PUT(inbox, reg_data->mgir_reg.fw_info.year,
+			REG_FW_INFO_OFFSET + REG_FW_INFO_YEAR_OFFSET);
+	SX_PUT(inbox, reg_data->mgir_reg.fw_info.hour,
+			REG_FW_INFO_OFFSET + REG_FW_INFO_HOUR_OFFSET);
+	memcpy(inbox + REG_FW_INFO_OFFSET + REG_FW_INFO_PSID_OFFSET,
+			reg_data->mgir_reg.fw_info.psid, SX_PSID_SIZE);
+	SX_PUT(inbox, reg_data->mgir_reg.fw_info.ini_file_version,
+			REG_FW_INFO_OFFSET + REG_FW_INFO_INI_FILE_VERSION_OFFSET);
+	SX_PUT(inbox, reg_data->mgir_reg.fw_info.extended_major,
+			REG_FW_INFO_OFFSET + REG_FW_INFO_EXTENDED_MAJOR_OFFSET);
+	SX_PUT(inbox, reg_data->mgir_reg.fw_info.extended_minor,
+			REG_FW_INFO_OFFSET + REG_FW_INFO_EXTENDED_MINOR_OFFSET);
+	SX_PUT(inbox, reg_data->mgir_reg.fw_info.extended_sub_minor,
+			REG_FW_INFO_OFFSET + REG_FW_INFO_EXTENDED_SUB_MINOR_OFFSET);
+
+	SX_PUT(inbox, reg_data->mgir_reg.sw_info.major,
+			REG_SW_INFO_OFFSET + REG_SW_INFO_MAJOR_OFFSET);
+	SX_PUT(inbox, reg_data->mgir_reg.sw_info.minor,
+			REG_FW_INFO_OFFSET + REG_SW_INFO_MINOR_OFFSET);
+	SX_PUT(inbox, reg_data->mgir_reg.sw_info.sub_minor,
+			REG_SW_INFO_OFFSET + REG_SW_INFO_SUB_MINOR_OFFSET);
+#endif
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(MGIR_REG_LEN));
+printk("%s err=%d\n", __func__, err);
+	if (err)
+		goto out;
+printk("%s err=%d oper=%d\n", __func__, err, reg_data->op_tlv.method);
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->mgir_reg.hw_info.device_hw_revision, outbox,
+				REG_HW_INFO_OFFSET + REG_HW_INFO_DEVICE_HW_REVISION_OFFSET);
+		SX_GET(reg_data->mgir_reg.hw_info.device_id, outbox,
+				REG_HW_INFO_OFFSET + REG_HW_INFO_DEVICE_ID_OFFSET);
+		SX_GET(reg_data->mgir_reg.hw_info.dvfs, outbox,
+				REG_HW_INFO_OFFSET + REG_HW_INFO_DVFS_OFFSET);
+		reg_data->mgir_reg.hw_info.dvfs &= 0x1f;
+		SX_GET(reg_data->mgir_reg.hw_info.uptime, outbox,
+				REG_HW_INFO_OFFSET + REG_HW_INFO_UPTIME_OFFSET);
+		SX_GET(reg_data->mgir_reg.fw_info.major, outbox,
+				REG_FW_INFO_OFFSET + REG_FW_INFO_MAJOR_OFFSET);
+		SX_GET(reg_data->mgir_reg.fw_info.minor, outbox,
+				REG_FW_INFO_OFFSET + REG_FW_INFO_MINOR_OFFSET);
+		SX_GET(reg_data->mgir_reg.fw_info.sub_minor, outbox,
+				REG_FW_INFO_OFFSET + REG_FW_INFO_SUB_MINOR_OFFSET);
+		SX_GET(reg_data->mgir_reg.fw_info.build_id, outbox,
+				REG_FW_INFO_OFFSET + REG_FW_INFO_BUILD_ID_OFFSET);
+		SX_GET(reg_data->mgir_reg.fw_info.month, outbox,
+				REG_FW_INFO_OFFSET + REG_FW_INFO_MONTH_OFFSET);
+		SX_GET(reg_data->mgir_reg.fw_info.day, outbox,
+				REG_FW_INFO_OFFSET + REG_FW_INFO_DAY_OFFSET);
+		SX_GET(reg_data->mgir_reg.fw_info.year, outbox,
+				REG_FW_INFO_OFFSET + REG_FW_INFO_YEAR_OFFSET);
+		SX_GET(reg_data->mgir_reg.fw_info.hour, outbox,
+				REG_FW_INFO_OFFSET + REG_FW_INFO_HOUR_OFFSET);
+		memcpy(reg_data->mgir_reg.fw_info.psid,
+			(u8 *)outbox + REG_FW_INFO_OFFSET + REG_FW_INFO_PSID_OFFSET,
+			SX_PSID_SIZE);
+		SX_GET(reg_data->mgir_reg.fw_info.ini_file_version, outbox,
+				REG_FW_INFO_OFFSET + REG_FW_INFO_INI_FILE_VERSION_OFFSET);
+		SX_GET(reg_data->mgir_reg.fw_info.extended_major, outbox,
+				REG_FW_INFO_OFFSET + REG_FW_INFO_EXTENDED_MAJOR_OFFSET);
+		SX_GET(reg_data->mgir_reg.fw_info.extended_minor, outbox,
+				REG_FW_INFO_OFFSET + REG_FW_INFO_EXTENDED_MINOR_OFFSET);
+		SX_GET(reg_data->mgir_reg.fw_info.extended_sub_minor, outbox,
+				REG_FW_INFO_OFFSET + REG_FW_INFO_EXTENDED_SUB_MINOR_OFFSET);
+		SX_GET(reg_data->mgir_reg.sw_info.major, outbox,
+				REG_SW_INFO_OFFSET + REG_SW_INFO_MAJOR_OFFSET);
+		SX_GET(reg_data->mgir_reg.sw_info.minor, outbox,
+				REG_FW_INFO_OFFSET + REG_SW_INFO_MINOR_OFFSET);
+		SX_GET(reg_data->mgir_reg.sw_info.sub_minor, outbox,
+				REG_SW_INFO_OFFSET + REG_SW_INFO_SUB_MINOR_OFFSET);
+	}
+
+#ifdef NO_PCI_XX
+	/* simulate for VM systems */
+	reg_data->mgir_reg.hw_info.device_id = SXD_MGIR_HW_DEV_ID_SX;
+	reg_data->mgir_reg.hw_info.device_hw_revision =	SXD_MGIR_HW_REV_ID_SX_A2;
+#endif
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_MGIR);
+
+int sx_ACCESS_REG_PLIB(struct sx_dev *dev, struct ku_access_plib_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_LOCAL_PORT_OFFSET	0x15
+#define REG_IB_PORT_OFFSET	0x17
+#define PLIB_REG_LEN		0x05
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= PLIB_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->plib_reg.local_port, REG_LOCAL_PORT_OFFSET);
+	SX_PUT(inbox, reg_data->plib_reg.ib_port, REG_IB_PORT_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(PLIB_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->plib_reg.local_port, outbox,
+				REG_LOCAL_PORT_OFFSET);
+		SX_GET(reg_data->plib_reg.ib_port, outbox,
+				REG_IB_PORT_OFFSET);
+
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_PLIB);
+
+int sx_ACCESS_REG_PMLP(struct sx_dev *dev, struct ku_access_pmlp_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+	int i;
+	u8 tmp_u8;
+
+#define REG_LOCAL_PORT_OFFSET	0x15
+#define REG_DIFF_RX_TX_OFFSET	0x14
+#define REG_WIDTH_OFFSET	0x17
+#define REG_RX_LANE_0_OFFSET	0x18
+#define REG_LANE_0_OFFSET	0x19
+#define REG_MODULE_0_OFFSET	0x1b
+#define PMLP_REG_LEN		0x11
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= PMLP_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	tmp_u8 = reg_data->pmlp_reg.use_different_rx_tx << 7;
+	SX_PUT(inbox, tmp_u8, REG_DIFF_RX_TX_OFFSET);
+	SX_PUT(inbox, reg_data->pmlp_reg.local_port, REG_LOCAL_PORT_OFFSET);
+	SX_PUT(inbox, reg_data->pmlp_reg.width, REG_WIDTH_OFFSET);
+	for (i = 0; i < NUMBER_OF_SERDESES; i++) {
+		SX_PUT(inbox, reg_data->pmlp_reg.rx_lane[i],
+				REG_RX_LANE_0_OFFSET + (4 * i));
+		SX_PUT(inbox, reg_data->pmlp_reg.lane[i],
+				REG_LANE_0_OFFSET + (4 * i));
+		SX_PUT(inbox, reg_data->pmlp_reg.module[i],
+				REG_MODULE_0_OFFSET + (4 * i));
+	}
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(PMLP_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(tmp_u8, outbox, REG_DIFF_RX_TX_OFFSET);
+		reg_data->pmlp_reg.use_different_rx_tx = tmp_u8 >> 7;
+		SX_GET(reg_data->pmlp_reg.local_port, outbox,
+				REG_LOCAL_PORT_OFFSET);
+		SX_GET(reg_data->pmlp_reg.width, outbox,
+				REG_WIDTH_OFFSET);
+
+		for (i = 0; i < NUMBER_OF_SERDESES; i++) {
+			SX_GET(reg_data->pmlp_reg.rx_lane[i], outbox,
+					REG_RX_LANE_0_OFFSET + (4 * i));
+			SX_GET(reg_data->pmlp_reg.lane[i], outbox,
+					REG_LANE_0_OFFSET + (4 * i));
+			SX_GET(reg_data->pmlp_reg.module[i], outbox,
+					REG_MODULE_0_OFFSET + (4 * i));
+		}
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_PMLP);
+
+int sx_ACCESS_REG_MHSR(struct sx_dev *dev,struct ku_access_mhsr_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_HEALTH_OFFSET	0x17
+#define MHSR_REG_LEN		0x2
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= MHSR_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->mhsr_reg.health, REG_HEALTH_OFFSET);
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(MHSR_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->mhsr_reg.health, outbox,
+				REG_HEALTH_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_MHSR);
+
+int sx_ACCESS_REG_PTYS(struct sx_dev *dev, struct ku_access_ptys_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_LOCAL_PORT_OFFSET	0x15
+#define REG_PROTO_MASK_OFFSET	0x17
+#define REG_FC_CAP_OFFSET	0x1c
+#define REG_ETH_CAP_OFFSET	0x20
+#define REG_IB_CAP_OFFSET	0x24
+#define REG_FC_ADMIN_OFFSET	0x28
+#define REG_ETH_ADMIN_OFFSET	0x2c
+#define REG_IB_ADMIN_OFFSET	0x30
+#define REG_FC_OPER_OFFSET	0x34
+#define REG_ETH_OPER_OFFSET	0x38
+#define REG_IB_OPER_OFFSET	0x3c
+#define PTYS_REG_LEN		0x11
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= PTYS_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->ptys_reg.local_port, REG_LOCAL_PORT_OFFSET);
+	SX_PUT(inbox, reg_data->ptys_reg.proto_mask, REG_PROTO_MASK_OFFSET);
+	SX_PUT(inbox, reg_data->ptys_reg.fc_proto_capability,
+			REG_FC_CAP_OFFSET);
+	SX_PUT(inbox, reg_data->ptys_reg.eth_proto_capability,
+			REG_ETH_CAP_OFFSET);
+	SX_PUT(inbox, reg_data->ptys_reg.ib_proto_capability,
+			REG_IB_CAP_OFFSET);
+	SX_PUT(inbox, reg_data->ptys_reg.fc_proto_admin, REG_FC_ADMIN_OFFSET);
+	SX_PUT(inbox, reg_data->ptys_reg.eth_proto_admin,
+			REG_ETH_ADMIN_OFFSET);
+	SX_PUT(inbox, reg_data->ptys_reg.ib_proto_admin, REG_IB_ADMIN_OFFSET);
+	SX_PUT(inbox, reg_data->ptys_reg.fc_proto_oper, REG_FC_OPER_OFFSET);
+	SX_PUT(inbox, reg_data->ptys_reg.eth_proto_oper, REG_ETH_OPER_OFFSET);
+	SX_PUT(inbox, reg_data->ptys_reg.ib_proto_oper, REG_IB_OPER_OFFSET);
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(PTYS_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->ptys_reg.local_port, outbox,
+				REG_LOCAL_PORT_OFFSET);
+		SX_GET(reg_data->ptys_reg.proto_mask, outbox,
+				REG_PROTO_MASK_OFFSET);
+		SX_GET(reg_data->ptys_reg.fc_proto_capability, outbox,
+				REG_FC_CAP_OFFSET);
+		SX_GET(reg_data->ptys_reg.eth_proto_capability, outbox,
+				REG_ETH_CAP_OFFSET);
+		SX_GET(reg_data->ptys_reg.ib_proto_capability, outbox,
+				REG_IB_CAP_OFFSET);
+		SX_GET(reg_data->ptys_reg.fc_proto_admin, outbox,
+				REG_FC_ADMIN_OFFSET);
+		SX_GET(reg_data->ptys_reg.eth_proto_admin, outbox,
+				REG_ETH_ADMIN_OFFSET);
+		SX_GET(reg_data->ptys_reg.ib_proto_admin, outbox,
+				REG_IB_ADMIN_OFFSET);
+		SX_GET(reg_data->ptys_reg.fc_proto_oper, outbox,
+				REG_FC_OPER_OFFSET);
+		SX_GET(reg_data->ptys_reg.eth_proto_oper, outbox,
+				REG_ETH_OPER_OFFSET);
+		SX_GET(reg_data->ptys_reg.ib_proto_oper, outbox,
+				REG_IB_OPER_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_PTYS);
+
+int sx_ACCESS_REG_QSTCT(struct sx_dev *dev,
+		struct ku_access_qstct_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_SWID_OFFSET		0x14
+#define REG_PRIO_OFFSET		0x16
+#define REG_UTCLASS_OFFSET	0x1b
+#define REG_MTCLASS_OFFSET	0x1f
+#define QSTCT_REG_LEN		0x05
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= QSTCT_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->qstct_reg.swid, REG_SWID_OFFSET);
+	SX_PUT(inbox, reg_data->qstct_reg.prio, REG_PRIO_OFFSET);
+	SX_PUT(inbox, reg_data->qstct_reg.utclass, REG_UTCLASS_OFFSET);
+	SX_PUT(inbox, reg_data->qstct_reg.mtclass, REG_MTCLASS_OFFSET);
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(QSTCT_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->qstct_reg.swid, outbox,
+				REG_SWID_OFFSET);
+		SX_GET(reg_data->qstct_reg.prio, outbox, REG_PRIO_OFFSET);
+		SX_GET(reg_data->qstct_reg.utclass, outbox,
+				REG_UTCLASS_OFFSET);
+		SX_GET(reg_data->qstct_reg.mtclass, outbox,
+				REG_MTCLASS_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_QSTCT);
+
+int sx_ACCESS_REG_QSPTC(struct sx_dev *dev,
+		struct ku_access_qsptc_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_LOCAL_IPORT_OFFSET	0x14
+#define REG_LOCAL_EPORT_OFFSET	0x15
+#define REG_ITCLASS_OFFSET	0x16
+#define REG_TCLASS_OFFSET	0x1b
+#define QSPTC_REG_LEN		0x03
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= QSPTC_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->qsptc_reg.local_iport, REG_LOCAL_IPORT_OFFSET);
+	SX_PUT(inbox, reg_data->qsptc_reg.local_eport, REG_LOCAL_EPORT_OFFSET);
+	SX_PUT(inbox, reg_data->qsptc_reg.itclass, REG_ITCLASS_OFFSET);
+	SX_PUT(inbox, reg_data->qsptc_reg.tclass, REG_TCLASS_OFFSET);
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(QSPTC_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->qsptc_reg.local_iport, outbox,
+				REG_LOCAL_IPORT_OFFSET);
+		SX_GET(reg_data->qsptc_reg.local_eport, outbox,
+				REG_LOCAL_EPORT_OFFSET);
+		SX_GET(reg_data->qsptc_reg.itclass, outbox,
+				REG_ITCLASS_OFFSET);
+		SX_GET(reg_data->qsptc_reg.tclass, outbox, REG_TCLASS_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_QSPTC);
+
+int sx_ACCESS_REG_PSPA(struct sx_dev *dev, struct ku_access_pspa_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_SWID_OFFSET		0x14
+#define REG_LOCAL_PORT_OFFSET	0x15
+#define REG_SUB_PORT_OFFSET	0x16
+#define PSPA_REG_LEN		0x03
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= PSPA_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->pspa_reg.swid, REG_SWID_OFFSET);
+	SX_PUT(inbox, reg_data->pspa_reg.local_port, REG_LOCAL_PORT_OFFSET);
+	SX_PUT(inbox, reg_data->pspa_reg.sub_port, REG_SUB_PORT_OFFSET);
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(PSPA_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->pspa_reg.swid, outbox, REG_SWID_OFFSET);
+		SX_GET(reg_data->pspa_reg.local_port, outbox,
+				REG_LOCAL_PORT_OFFSET);
+		SX_GET(reg_data->pspa_reg.sub_port, outbox,
+				REG_SUB_PORT_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_PSPA);
+
+int sx_ACCESS_REG_SPZR(struct sx_dev *dev, struct ku_access_spzr_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+	u8 tmp_val_u8 = 0;
+	int i;
+
+#define REG_SWID_OFFSET			0x14
+#define REG_NDM_OFFSET			0x16
+#define REG_NDM_BIT_N			7
+#define REG_ENH_SW_P0_MASK_OFFSET	0x16
+#define REG_ENH_SW_P0_MASK_BIT_N	6
+#define REG_CM_OFFSET			0x16
+#define REG_CM_BIT_N			5
+#define REG_VK_OFFSET			0x16
+#define REG_VK_BIT_N			4
+#define REG_MP_OFFSET			0x16
+#define REG_MP_BIT_N			3
+#define REG_SIG_OFFSET			0x16
+#define REG_SIG_BIT_N			2
+#define REG_NG_OFFSET			0x16
+#define REG_NG_BIT_N			1
+#define REG_G0_OFFSET			0x16
+#define REG_G0_BIT_N			0
+#define REG_ENH_SW_P0_OFFSET		0x17
+#define REG_ENH_SW_P0_BIT_N		1
+#define REG_CAP_MASK_OFFSET		0x18
+#define REG_SYS_IMG_GUID_H_OFFSET	0x1c
+#define REG_SYS_IMG_GUID_L_OFFSET	0x20
+#define REG_GUID0_H_OFFSET		0x24
+#define REG_GUID0_L_OFFSET		0x28
+#define REG_NODE_GUID_H_OFFSET		0x2c
+#define REG_NODE_GUID_L_OFFSET		0x30
+#define REG_V_KEY_H_OFFSET		0x34
+#define REG_V_KEY_L_OFFSET		0x38
+#define REG_MAX_PKEY_OFFSET		0x3c
+#define REG_NODE_DESC_OFFSET		0x44
+#define SPZR_REG_LEN			0x41
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= SPZR_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	/* SWitch partition ID */
+	SX_PUT(inbox, reg_data->spzr_reg.swid, REG_SWID_OFFSET);
+	tmp_val_u8 = 0;
+	/* Node description mask.Set to 1 to write the NodeDescription field */
+	tmp_val_u8 |= (reg_data->spzr_reg.ndm ? (1<<REG_NDM_BIT_N) : 0);
+	/* Enhanced Switch Port 0 mask. When 1, the EnhSwP0 field is valid
+	 * (see below). When 0, the EnhSwP0 field is ignored */
+	tmp_val_u8 |= (reg_data->spzr_reg.EnhSwP0_mask ?
+			(1<<REG_ENH_SW_P0_MASK_BIT_N) : 0);
+	/* Set PortInfo:CapabilityMask to PortInfo:CapabilityMask specified */
+	tmp_val_u8 |= (reg_data->spzr_reg.cm ? (1<<REG_CM_BIT_N) : 0);
+	/* Set the internal GSA V_Key. Incoming VendorSpecific MADs must have
+	 * matching V_Key to the one set by this command */
+	tmp_val_u8 |= (reg_data->spzr_reg.vk ? (1<<REG_VK_BIT_N) : 0);
+	/* Change PKey table size to max_pkey */
+	tmp_val_u8 |= (reg_data->spzr_reg.mp ? (1<<REG_MP_BIT_N) : 0);
+	/* Set System Image GUID to system_image_guid specified.
+	System_image_guid and sig must be the same for all ports.*/
+	tmp_val_u8 |= (reg_data->spzr_reg.sig ? (1<<REG_SIG_BIT_N) : 0);
+	/* Set node GUID to node_guid specified.
+	node_guid and ng must be the same for all ports.*/
+	tmp_val_u8 |= (reg_data->spzr_reg.ng ? (1<<REG_NG_BIT_N) : 0);
+	/* Set port GUID0 to GUID0 specified */
+	tmp_val_u8 |= (reg_data->spzr_reg.g0 ? (1<<REG_G0_BIT_N) : 0);
+	SX_PUT(inbox, tmp_val_u8, REG_NDM_OFFSET);
+	tmp_val_u8 = 0;
+	/* When set, it enables Enhanced Switch
+	 * Port 0. Reported in NodeInfo. Otherwise,
+	 * Enhanced Switch Port 0 is disabled. */
+	tmp_val_u8 |= (reg_data->spzr_reg.EnhSwP0 ?
+			(1<<REG_ENH_SW_P0_BIT_N) : 0);
+	SX_PUT(inbox, tmp_val_u8, REG_ENH_SW_P0_OFFSET);
+	if (reg_data->spzr_reg.cm)
+		/* Sets the PortInfoCapabilityMask:
+		Specifies the supported capabilities of this node.
+		A bit set to 1 for affirmation of supported capability.*/
+		SX_PUT(inbox, reg_data->spzr_reg.capability_mask,
+				REG_CAP_MASK_OFFSET);
+	if (reg_data->spzr_reg.sig)
+		/* System Image GUID, takes effect only if
+		 * the sig bit is set. Must be the same for
+		 * both ports.*/
+		SX_PUT(inbox, reg_data->spzr_reg.system_image_guid_h_l,
+				REG_SYS_IMG_GUID_H_OFFSET);
+	if (reg_data->spzr_reg.g0)
+		/*EUI-64 GUID assigned by the manufacturer,
+		 * takes effect only if the g0 bit is set */
+		SX_PUT(inbox, reg_data->spzr_reg.guid0_h_l, REG_GUID0_H_OFFSET);
+	if (reg_data->spzr_reg.ng)
+		/* Node GUID, takes effect only if the ng bit is set
+		 * Must be the same for both ports.*/
+		SX_PUT(inbox, reg_data->spzr_reg.node_guid_h_l,
+				REG_NODE_GUID_H_OFFSET);
+	if (reg_data->spzr_reg.vk) {
+		/* The internal GSA V_Key. Incoming VendorSpecific MADs must
+		 * have a V_Key matching the one set by this command. If not
+		 * specified, then must be set to 0. 32 V_Key_l Takes effect
+		 * only if the VK bit is set
+		 */
+		SX_PUT(inbox, reg_data->spzr_reg.v_key_h, REG_V_KEY_H_OFFSET);
+		SX_PUT(inbox, reg_data->spzr_reg.v_key_l, REG_V_KEY_L_OFFSET);
+	}
+	if (reg_data->spzr_reg.mp)
+		/* max_pkey is derived from the profile - no set.
+		 * Maximum pkeys for the port.
+		 * Must be the same for both ports.
+		 * Takes effect if the mp bit is set */
+		SX_PUT(inbox, reg_data->spzr_reg.max_pkey,
+				REG_MAX_PKEY_OFFSET);
+	if (reg_data->spzr_reg.ndm)
+		/* Text string that describes the node */
+		for (i = 0; i < 64; i++)
+			SX_PUT(inbox, reg_data->spzr_reg.NodeDescription[i],
+					REG_NODE_DESC_OFFSET + i);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(SPZR_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->spzr_reg.swid, outbox, REG_SWID_OFFSET);
+
+		SX_GET(tmp_val_u8, outbox, REG_NDM_OFFSET);
+		reg_data->spzr_reg.ndm =
+				tmp_val_u8 & (1<<REG_NDM_BIT_N) ? 1 : 0 ;
+		reg_data->spzr_reg.EnhSwP0_mask =
+			tmp_val_u8 & (1<<REG_ENH_SW_P0_MASK_BIT_N) ? 1 : 0 ;
+		reg_data->spzr_reg.cm =
+				tmp_val_u8 & (1<<REG_CM_BIT_N)  ? 1 : 0 ;
+		reg_data->spzr_reg.vk =
+				tmp_val_u8 & (1<<REG_VK_BIT_N)  ? 1 : 0 ;
+		reg_data->spzr_reg.mp =
+				tmp_val_u8 & (1<<REG_MP_BIT_N)  ? 1 : 0 ;
+		reg_data->spzr_reg.sig =
+				tmp_val_u8 & (1<<REG_SIG_BIT_N) ? 1 : 0 ;
+		reg_data->spzr_reg.ng =
+				tmp_val_u8 & (1<<REG_NG_BIT_N)  ? 1 : 0 ;
+		reg_data->spzr_reg.g0 =
+				tmp_val_u8 & (1<<REG_G0_BIT_N)  ? 1 : 0 ;
+
+		SX_GET(tmp_val_u8, outbox, REG_ENH_SW_P0_OFFSET);
+		reg_data->spzr_reg.EnhSwP0 =
+				tmp_val_u8 & (1<<REG_ENH_SW_P0_BIT_N) ? 1 : 0 ;
+
+		SX_GET(reg_data->spzr_reg.capability_mask, outbox,
+				REG_CAP_MASK_OFFSET);
+		SX_GET(reg_data->spzr_reg.system_image_guid_h_l, outbox,
+				REG_SYS_IMG_GUID_H_OFFSET);
+		SX_GET(reg_data->spzr_reg.guid0_h_l, outbox,
+				REG_GUID0_H_OFFSET);
+		SX_GET(reg_data->spzr_reg.node_guid_h_l, outbox,
+				REG_NODE_GUID_H_OFFSET);
+		SX_GET(reg_data->spzr_reg.v_key_h, outbox,
+				REG_V_KEY_H_OFFSET);
+		SX_GET(reg_data->spzr_reg.v_key_l, outbox,
+				REG_V_KEY_L_OFFSET);
+		SX_GET(reg_data->spzr_reg.max_pkey, outbox,
+				REG_MAX_PKEY_OFFSET);
+		for (i = 0; i < 64; i++)
+			SX_GET(reg_data->spzr_reg.NodeDescription[i], outbox,
+					REG_NODE_DESC_OFFSET + i);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_SPZR);
+
+int sx_ACCESS_REG_MJTAG(struct sx_dev *dev, struct ku_access_mjtag_reg *reg_data)
+{
+    struct sx_cmd_mailbox *in_mailbox;
+    struct sx_cmd_mailbox *out_mailbox;
+    u32 *inbox;
+    u32 *outbox;
+    int err, counter=0;
+    u16 type_len = 0;
+    u8 tmp_val_u8;
+
+#define MJTAG_REG_CMD_OFFSET                  0x14
+#define MJTAG_REG_CMD_BIT_N                   6
+#define MJTAG_REG_SEQ_NUM_BIT_N               0x0f
+#define MJTAG_REG_SEQ_NUM_OFFSET              0x14
+#define MJTAG_REG_TRANSACTIONS_SIZE_OFFSET    0x17
+#define MJTAG_REG_JTAG_TRANSACTION_OFFSET     0x18
+#define MJTAG_REG_TRANSACTION_TDO_BIT_N       0x03
+#define MJTAG_REG_TRANSACTION_TDI_BIT_N       0x01
+#define MJTAG_REG_LEN                   0x0c
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= MJTAG_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	tmp_val_u8 = 0;
+	tmp_val_u8 |= reg_data->mjtag_reg.cmd << MJTAG_REG_CMD_BIT_N;
+	tmp_val_u8 |=reg_data->mjtag_reg.seq_num;
+	SX_PUT(inbox, tmp_val_u8, MJTAG_REG_CMD_OFFSET);
+	SX_PUT(inbox, reg_data->mjtag_reg.size, MJTAG_REG_TRANSACTIONS_SIZE_OFFSET);
+	for(;counter<reg_data->mjtag_reg.size;counter++) {
+		tmp_val_u8=0;
+		tmp_val_u8 |= (reg_data->mjtag_reg.jtag_transaction_sets[counter].tdo & 0x01) << MJTAG_REG_TRANSACTION_TDO_BIT_N;
+		tmp_val_u8 |= (reg_data->mjtag_reg.jtag_transaction_sets[counter].tdi & 0x01) << MJTAG_REG_TRANSACTION_TDI_BIT_N;
+		tmp_val_u8 |= (reg_data->mjtag_reg.jtag_transaction_sets[counter].tms & 0x01);
+		SX_PUT(inbox,tmp_val_u8, MJTAG_REG_JTAG_TRANSACTION_OFFSET+counter);
+	}
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(MJTAG_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	SX_GET(tmp_val_u8, outbox, MJTAG_REG_CMD_OFFSET);
+	reg_data->mjtag_reg.cmd = tmp_val_u8 & (1 << MJTAG_REG_CMD_BIT_N) ? 1 : 0;
+	SX_GET(tmp_val_u8, outbox, MJTAG_REG_SEQ_NUM_OFFSET);
+	reg_data->mjtag_reg.seq_num = tmp_val_u8 & MJTAG_REG_SEQ_NUM_BIT_N ? 1 : 0;
+	SX_GET(reg_data->mjtag_reg.size, outbox,
+	        MJTAG_REG_TRANSACTIONS_SIZE_OFFSET);
+
+	for (counter = 0; counter < reg_data->mjtag_reg.size; counter++) {
+		SX_GET(tmp_val_u8, outbox, MJTAG_REG_JTAG_TRANSACTION_OFFSET);
+		reg_data->mjtag_reg.jtag_transaction_sets[counter].tdi =
+		        tmp_val_u8 & (1 << MJTAG_REG_TRANSACTION_TDI_BIT_N) ? 1 : 0;
+		reg_data->mjtag_reg.jtag_transaction_sets[counter].tdo =
+		        tmp_val_u8 & (1 << MJTAG_REG_TRANSACTION_TDO_BIT_N) ? 1 : 0;
+		reg_data->mjtag_reg.jtag_transaction_sets[counter].tms =
+		        tmp_val_u8 & 1 ? 1 : 0;
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_MJTAG);
+
+int sx_ACCESS_REG_PPSC(struct sx_dev *dev, struct ku_access_ppsc_reg *reg_data)
+{
+    struct sx_cmd_mailbox *in_mailbox;
+    struct sx_cmd_mailbox *out_mailbox;
+    u32 *inbox;
+    u32 *outbox;
+    int err;
+    u16 type_len = 0;
+
+#define PPSC_REG_LOCAL_PORT_OFFSET                      0x15
+#define PPSC_REG_LOCAL_PORT_N                           0x08
+#define PPSC_REG_WRPS_ADMIN_OFFSET                      0x27
+#define PPSC_REG_WRPS_ADMIN_N                           0x04
+#define PPSC_REG_WRPS_STATUS_OFFSET                     0x2B
+#define PPSC_REG_WRPS_STATUS_N                          0x04
+#define PPSC_REG_UP_THRESHOLD_OFFSET                    0x2D
+#define PPSC_REG_UP_THRESHOLD_N                         0x08
+#define PPSC_REG_DOWN_THRESHOLD_OFFSET                  0x2D
+#define PPSC_REG_DOWN_THRESHOLD_N                       0x08
+#define PPSC_REG_SRPS_ADMIN_OFFSET                      0x35
+#define PPSC_REG_SRPS_ADMIN_N                           0x04
+#define PPSC_REG_SRPS_STATUS_OFFSET                     0x39
+#define PPSC_REG_SRPS_STATUS_N                          0x04
+
+#define PPSC_REG_LEN                                    0x0D
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= PPSC_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+    SX_PUT(inbox, reg_data->ppsc_reg.local_port, PPSC_REG_LOCAL_PORT_OFFSET);
+    SX_PUT(inbox, reg_data->ppsc_reg.wrps_admin, PPSC_REG_WRPS_ADMIN_OFFSET);
+    SX_PUT(inbox, reg_data->ppsc_reg.wrps_status, PPSC_REG_WRPS_STATUS_OFFSET);
+    SX_PUT(inbox, reg_data->ppsc_reg.up_threshold, PPSC_REG_UP_THRESHOLD_OFFSET);
+    SX_PUT(inbox, reg_data->ppsc_reg.down_threshold, PPSC_REG_DOWN_THRESHOLD_OFFSET);
+    SX_PUT(inbox, reg_data->ppsc_reg.srps_admin, PPSC_REG_SRPS_ADMIN_OFFSET);
+    SX_PUT(inbox, reg_data->ppsc_reg.srps_status, PPSC_REG_SRPS_STATUS_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(PPSC_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+        SX_GET(reg_data->ppsc_reg.local_port, outbox, PPSC_REG_LOCAL_PORT_OFFSET);
+        SX_GET(reg_data->ppsc_reg.wrps_admin, outbox, PPSC_REG_WRPS_ADMIN_OFFSET);
+        SX_GET(reg_data->ppsc_reg.wrps_status, outbox, PPSC_REG_WRPS_STATUS_OFFSET);
+        SX_GET(reg_data->ppsc_reg.up_threshold, outbox,PPSC_REG_UP_THRESHOLD_OFFSET);
+        SX_GET(reg_data->ppsc_reg.down_threshold, outbox,PPSC_REG_DOWN_THRESHOLD_OFFSET);
+        SX_GET(reg_data->ppsc_reg.srps_admin, outbox, PPSC_REG_SRPS_ADMIN_OFFSET);
+        SX_GET(reg_data->ppsc_reg.srps_status, outbox, PPSC_REG_SRPS_STATUS_OFFSET);
+
+
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_PPSC);
+
+int sx_ACCESS_REG_PAOS(struct sx_dev *dev, struct ku_access_paos_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+	u8 tmp_val_u8;
+
+#define REG_SWID_OFFSET			0x14
+#define REG_LOCAL_PORT_OFFSET		0x15
+#define REG_ADMIN_STATUS_OFFSET		0x16
+#define REG_OPER_STATUS_OFFSET		0x17
+#define REG_ASE_OFFSET			0x18
+#define REG_ASE_BIT_N			7
+#define REG_EE_BIT_N			6
+#define REG_E_OFFSET			0x1B
+#define PAOS_REG_LEN			0x05
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= PAOS_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->paos_reg.swid, REG_SWID_OFFSET);
+	SX_PUT(inbox, reg_data->paos_reg.local_port, REG_LOCAL_PORT_OFFSET);
+	SX_PUT(inbox, reg_data->paos_reg.admin_status,
+			REG_ADMIN_STATUS_OFFSET);
+	tmp_val_u8 = 0;
+	tmp_val_u8 |= reg_data->paos_reg.ase ? (1 <<  REG_ASE_BIT_N) : 0;
+	tmp_val_u8 |= reg_data->paos_reg.ee ? (1 <<  REG_EE_BIT_N) : 0;
+	SX_PUT(inbox, tmp_val_u8, REG_ASE_OFFSET);
+	SX_PUT(inbox, reg_data->paos_reg.e, REG_E_OFFSET);
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(PAOS_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->paos_reg.swid, outbox,
+				REG_SWID_OFFSET);
+		SX_GET(reg_data->paos_reg.local_port, outbox,
+				REG_LOCAL_PORT_OFFSET);
+		SX_GET(reg_data->paos_reg.admin_status, outbox,
+			   REG_ADMIN_STATUS_OFFSET);
+		SX_GET(reg_data->paos_reg.oper_status, outbox,
+				REG_OPER_STATUS_OFFSET);
+		SX_GET(tmp_val_u8, outbox, REG_ASE_OFFSET);
+		reg_data->paos_reg.ase =
+				tmp_val_u8 & (1 <<  REG_ASE_BIT_N) ? 1 : 0;
+		reg_data->paos_reg.ee =
+				tmp_val_u8 & (1 <<  REG_EE_BIT_N) ? 1 : 0;
+		SX_GET(reg_data->paos_reg.e, outbox, REG_E_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_PAOS);
+
+int sx_ACCESS_REG_PPLM(struct sx_dev *dev, struct ku_access_pplm_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+	u32 retransmission_active_and_fec_mode_active;
+
+#define REG_LOCAL_PORT_OFFSET				0x15
+#define REG_PORT_PROFILE_MODE_OFFSET		0x1C
+#define REG_STATIC_PORT_PROFILE_OFFSET		0x1D
+#define REG_ACTIVE_PORT_PROFILE_OFFSET		0x1E
+#define REG_RETRANSMISSION_ACTIVE_OFFSET	0x20
+#define PPLM_REG_LEN						0x06
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= PPLM_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->pplm_reg.local_port, REG_LOCAL_PORT_OFFSET);
+	SX_PUT(inbox, reg_data->pplm_reg.port_profile_mode, REG_PORT_PROFILE_MODE_OFFSET);
+	SX_PUT(inbox, reg_data->pplm_reg.static_port_profile,REG_STATIC_PORT_PROFILE_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(PPLM_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->pplm_reg.local_port, outbox,
+				REG_LOCAL_PORT_OFFSET);
+		SX_GET(reg_data->pplm_reg.port_profile_mode, outbox,
+				REG_PORT_PROFILE_MODE_OFFSET);
+		SX_GET(reg_data->pplm_reg.static_port_profile, outbox,
+				REG_STATIC_PORT_PROFILE_OFFSET);
+		SX_GET(reg_data->pplm_reg.active_port_profile, outbox,
+				REG_ACTIVE_PORT_PROFILE_OFFSET);
+
+		SX_GET(retransmission_active_and_fec_mode_active, outbox,
+						REG_RETRANSMISSION_ACTIVE_OFFSET);
+		reg_data->pplm_reg.retransmission_active =
+				(retransmission_active_and_fec_mode_active >> 24) & 0xFF;
+		reg_data->pplm_reg.fec_mode_active =
+						retransmission_active_and_fec_mode_active & 0xFFFFFF;
+
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_PPLM);
+
+int sx_ACCESS_REG_PLPC(struct sx_dev *dev, struct ku_access_plpc_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_PROFILE_ID_OFFSET						0x14
+#define REG_PLPC_PROTO_MASK_OFFSET					0x16
+#define REG_LANE_SPEED_OFFSET						0x1A
+#define REG_LPBF_OFFSET								0x1E
+#define REG_FEC_MODE_POLICY_OFFSET					0x1F
+#define REG_RETRANSMISSION_CAPABILITY_OFFSET		0x20
+#define REG_FEC_MODE_CAPABILITY_OFFSET				0x21
+#define REG_RETRANSMISSION_SUPPORT_ADMIN_OFFSET		0x24
+#define REG_FEC_MODE_SUPPORT_ADMIN_OFFSET			0x25
+#define REG_RETRANSMISSION_REQUEST_ADMIN_OFFSET		0x28
+#define REG_FEC_MODE_REQUEST_ADMIN_OFFSET			0x29
+#define PLPC_REG_LEN								0xB
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= PLPC_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->plpc_reg.profile_id, REG_PROFILE_ID_OFFSET);
+	SX_PUT(inbox, reg_data->plpc_reg.proto_mask, REG_PLPC_PROTO_MASK_OFFSET);
+	SX_PUT(inbox, reg_data->plpc_reg.lane_speed,REG_LANE_SPEED_OFFSET);
+	SX_PUT(inbox, reg_data->plpc_reg.lpbf,REG_LPBF_OFFSET);
+	SX_PUT(inbox, reg_data->plpc_reg.fec_mode_policy,REG_FEC_MODE_POLICY_OFFSET);
+	SX_PUT(inbox, reg_data->plpc_reg.retransmission_capability,REG_RETRANSMISSION_CAPABILITY_OFFSET);
+	SX_PUT(inbox, reg_data->plpc_reg.fec_mode_capability,REG_FEC_MODE_CAPABILITY_OFFSET);
+	SX_PUT(inbox, reg_data->plpc_reg.retransmission_support_admin,REG_RETRANSMISSION_SUPPORT_ADMIN_OFFSET);
+	SX_PUT(inbox, reg_data->plpc_reg.fec_mode_support_admin,REG_FEC_MODE_SUPPORT_ADMIN_OFFSET);
+	SX_PUT(inbox, reg_data->plpc_reg.retransmission_request_admin,REG_RETRANSMISSION_REQUEST_ADMIN_OFFSET);
+	SX_PUT(inbox, reg_data->plpc_reg.fec_mode_request_admin,REG_FEC_MODE_REQUEST_ADMIN_OFFSET);
+
+
+
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(PLPC_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->plpc_reg.profile_id, outbox,
+				REG_PROFILE_ID_OFFSET);
+		SX_GET(reg_data->plpc_reg.proto_mask, outbox,
+				REG_PROTO_MASK_OFFSET);
+		SX_GET(reg_data->plpc_reg.lane_speed, outbox,
+				REG_LANE_SPEED_OFFSET);
+		SX_GET(reg_data->plpc_reg.lpbf, outbox,
+				REG_LPBF_OFFSET);
+		SX_GET(reg_data->plpc_reg.fec_mode_policy, outbox,
+				REG_FEC_MODE_POLICY_OFFSET);
+		SX_GET(reg_data->plpc_reg.retransmission_capability, outbox,
+				REG_RETRANSMISSION_CAPABILITY_OFFSET);
+		SX_GET(reg_data->plpc_reg.fec_mode_capability, outbox,
+				REG_FEC_MODE_CAPABILITY_OFFSET);
+		SX_GET(reg_data->plpc_reg.retransmission_support_admin, outbox,
+				REG_RETRANSMISSION_SUPPORT_ADMIN_OFFSET);
+		SX_GET(reg_data->plpc_reg.fec_mode_support_admin, outbox,
+				REG_FEC_MODE_SUPPORT_ADMIN_OFFSET);
+		SX_GET(reg_data->plpc_reg.retransmission_request_admin, outbox,
+				REG_RETRANSMISSION_REQUEST_ADMIN_OFFSET);
+		SX_GET(reg_data->plpc_reg.fec_mode_request_admin, outbox,
+				REG_FEC_MODE_REQUEST_ADMIN_OFFSET);
+
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_PLPC);
+
+int sx_ACCESS_REG_PMPC(struct sx_dev *dev, struct ku_access_pmpc_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+	u32 i = 0;
+
+#define REG_MODULE_STATE_UPDATED_OFFSET			0x14
+#define PMPC_REG_LEN			0x09
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= PMPC_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	for(i = 0 ; i < 8; i++){
+		SX_PUT(inbox, reg_data->pmpc_reg.module_state_updated_bitmap[i], REG_MODULE_STATE_UPDATED_OFFSET+(i*4));
+	}
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(PMPC_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		for(i = 0 ; i < 8; i++){
+			SX_GET(reg_data->pmpc_reg.module_state_updated_bitmap[i], outbox,
+					REG_MODULE_STATE_UPDATED_OFFSET + (i*4));
+		}
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_PMPC);
+
+int sx_ACCESS_REG_PMPR(struct sx_dev *dev, struct ku_access_pmpr_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define PMPR_REG_MODULE_OFFSET		0x15
+#define PMPR_REG_ATTENUATION_5G_OFFSET		0x1B
+#define PMPR_REG_ATTENUATION_7G_OFFSET      0x1F
+#define PMPR_REG_ATTENUATION_12G_OFFSET     0x23
+#define PMPR_REG_LEN			0x05
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= PMPR_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->pmpr_reg.module, PMPR_REG_MODULE_OFFSET);
+	SX_PUT(inbox, reg_data->pmpr_reg.attenuation5g,	PMPR_REG_ATTENUATION_5G_OFFSET);
+	SX_PUT(inbox, reg_data->pmpr_reg.attenuation7g, PMPR_REG_ATTENUATION_7G_OFFSET);
+	SX_PUT(inbox, reg_data->pmpr_reg.attenuation12g, PMPR_REG_ATTENUATION_12G_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(PMPR_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->pmpr_reg.module, outbox,
+				PMPR_REG_MODULE_OFFSET);
+		SX_GET(reg_data->pmpr_reg.attenuation5g, outbox,
+			   PMPR_REG_ATTENUATION_5G_OFFSET);
+		SX_GET(reg_data->pmpr_reg.attenuation7g, outbox,
+		       PMPR_REG_ATTENUATION_7G_OFFSET);
+		SX_GET(reg_data->pmpr_reg.attenuation12g, outbox,
+		       PMPR_REG_ATTENUATION_12G_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_PMPR);
+
+int sx_ACCESS_REG_PMAOS(struct sx_dev *dev,
+		struct ku_access_pmaos_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+	u8 tmp_val_u8;
+
+#define REG_MODULE_OFFSET		0x15
+#define PMAOS_REG_LEN			0x05
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= PMAOS_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->pmaos_reg.module, REG_LOCAL_PORT_OFFSET);
+	SX_PUT(inbox, reg_data->pmaos_reg.admin_status,
+			REG_ADMIN_STATUS_OFFSET);
+	tmp_val_u8 = 0;
+	tmp_val_u8 |= reg_data->pmaos_reg.ase ? (1 <<  REG_ASE_BIT_N) : 0;
+	tmp_val_u8 |= reg_data->pmaos_reg.ee ? (1 <<  REG_EE_BIT_N) : 0;
+	SX_PUT(inbox, tmp_val_u8, REG_ASE_OFFSET);
+	SX_PUT(inbox, reg_data->pmaos_reg.e, REG_E_OFFSET);
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(PMAOS_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->pmaos_reg.module, outbox,
+				REG_LOCAL_PORT_OFFSET);
+		SX_GET(reg_data->pmaos_reg.admin_status, outbox,
+			   REG_ADMIN_STATUS_OFFSET);
+		SX_GET(reg_data->pmaos_reg.oper_status, outbox,
+				REG_OPER_STATUS_OFFSET);
+		SX_GET(tmp_val_u8, outbox, REG_ASE_OFFSET);
+		reg_data->pmaos_reg.ase =
+				tmp_val_u8 & (1 <<  REG_ASE_BIT_N) ? 1 : 0;
+		reg_data->pmaos_reg.ee =
+				tmp_val_u8 & (1 <<  REG_EE_BIT_N) ? 1 : 0;
+		SX_GET(reg_data->pmaos_reg.e, outbox, REG_E_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_PMAOS);
+
+int sx_ACCESS_REG_PMTU(struct sx_dev *dev, struct ku_access_pmtu_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_LOCAL_PORT_OFFSET	0x15
+#define REG_MAX_MTU_OFFSET	0x18
+#define REG_ADMIN_MTU_OFFSET	0x1c
+#define REG_OPER_MTU_OFFSET	0x20
+#define PMTU_REG_LEN		0x05
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= PMTU_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->pmtu_reg.local_port, REG_LOCAL_PORT_OFFSET);
+	SX_PUT(inbox, reg_data->pmtu_reg.max_mtu, REG_MAX_MTU_OFFSET);
+	SX_PUT(inbox, reg_data->pmtu_reg.admin_mtu, REG_ADMIN_MTU_OFFSET);
+	SX_PUT(inbox, reg_data->pmtu_reg.oper_mtu, REG_OPER_MTU_OFFSET);
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(PMTU_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->pmtu_reg.local_port, outbox,
+				REG_LOCAL_PORT_OFFSET);
+		SX_GET(reg_data->pmtu_reg.max_mtu, outbox,
+				REG_MAX_MTU_OFFSET);
+		SX_GET(reg_data->pmtu_reg.admin_mtu, outbox,
+				REG_ADMIN_MTU_OFFSET);
+		SX_GET(reg_data->pmtu_reg.oper_mtu, outbox,
+				REG_OPER_MTU_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_PMTU);
+
+int sx_ACCESS_REG_PELC(struct sx_dev *dev, struct ku_access_pelc_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_PELC_OP_OFFSET   0x14
+#define REG_PELC_LOCAL_PORT_OFFSET   0x15
+
+// 0x16, 0x17 reserved not in use
+
+#define REG_PELC_OP_ADMIN_OFFSET    0x18
+#define REG_PELC_OP_CAPABILITY_OFFSET   0x19
+#define REG_PELC_OP_REQUEST_OFFSET  0x1a
+#define REG_PELC_OP_ACTIVE_OFFSET  0x1b
+
+#define REG_PELC_ADMIN_OFFSET    0x1c
+#define REG_PELC_CAPABILITY_OFFSET   0x24
+#define REG_PELC_REQUEST_OFFSET  0x2c
+#define REG_PELC_ACTIVE_OFFSET	0x34
+#define PELC_REG_LEN    0x0b
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= PELC_REG_LEN;
+
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+    reg_data->pelc_reg.op = reg_data->pelc_reg.op << 4;
+	SX_PUT(inbox, reg_data->pelc_reg.op, REG_PELC_OP_OFFSET);
+    SX_PUT(inbox, reg_data->pelc_reg.local_port, REG_PELC_LOCAL_PORT_OFFSET);
+    SX_PUT(inbox, reg_data->pelc_reg.op_admin, REG_PELC_OP_ADMIN_OFFSET);
+    SX_PUT(inbox, reg_data->pelc_reg.op_capability, REG_PELC_OP_CAPABILITY_OFFSET);
+    SX_PUT(inbox, reg_data->pelc_reg.op_request, REG_PELC_OP_REQUEST_OFFSET);
+    SX_PUT(inbox, reg_data->pelc_reg.op_active, REG_PELC_OP_ACTIVE_OFFSET);
+    SX_PUT(inbox, reg_data->pelc_reg.admin, REG_PELC_ADMIN_OFFSET);
+	SX_PUT(inbox, reg_data->pelc_reg.capability, REG_PELC_CAPABILITY_OFFSET);
+    SX_PUT(inbox, reg_data->pelc_reg.request, REG_PELC_REQUEST_OFFSET);
+	SX_PUT(inbox, reg_data->pelc_reg.active, REG_PELC_ACTIVE_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(PELC_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->pelc_reg.op, outbox,
+				REG_PELC_OP_OFFSET);
+        reg_data->pelc_reg.op = reg_data->pelc_reg.op >> 4;
+        SX_GET(reg_data->pelc_reg.local_port, outbox,
+				REG_PELC_LOCAL_PORT_OFFSET);
+        SX_GET(reg_data->pelc_reg.op_admin, outbox,
+                REG_PELC_OP_ADMIN_OFFSET);
+        SX_GET(reg_data->pelc_reg.op_capability, outbox,
+                REG_PELC_OP_CAPABILITY_OFFSET);
+        SX_GET(reg_data->pelc_reg.op_request, outbox,
+                REG_PELC_OP_REQUEST_OFFSET);
+        SX_GET(reg_data->pelc_reg.op_active, outbox,
+                REG_PELC_OP_ACTIVE_OFFSET);
+        SX_GET(reg_data->pelc_reg.admin, outbox,
+				REG_PELC_ADMIN_OFFSET);
+		SX_GET(reg_data->pelc_reg.capability, outbox,
+				REG_PELC_CAPABILITY_OFFSET);
+		SX_GET(reg_data->pelc_reg.request, outbox,
+				REG_PELC_REQUEST_OFFSET);
+        SX_GET(reg_data->pelc_reg.active, outbox,
+				REG_PELC_ACTIVE_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_PELC);
+
+int sx_ACCESS_REG_PVLC(struct sx_dev *dev, struct ku_access_pvlc_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_LOCAL_PORT_OFFSET		0x15
+#define REG_VL_CAP_OFFSET		0x1b
+#define REG_VL_ADMIN_OFFSET		0x1f
+#define REG_VL_OPERATIONAL_OFFSET	0x23
+#define PVLC_REG_LEN			0x05
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= PVLC_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->pvlc_reg.local_port, REG_LOCAL_PORT_OFFSET);
+	SX_PUT(inbox, reg_data->pvlc_reg.vl_cap, REG_VL_CAP_OFFSET);
+	SX_PUT(inbox, reg_data->pvlc_reg.vl_admin, REG_VL_ADMIN_OFFSET);
+	SX_PUT(inbox, reg_data->pvlc_reg.vl_operational,
+			REG_VL_OPERATIONAL_OFFSET);
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(PVLC_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->pvlc_reg.local_port, outbox,
+				REG_LOCAL_PORT_OFFSET);
+		SX_GET(reg_data->pvlc_reg.vl_cap, outbox,
+				REG_VL_CAP_OFFSET);
+		SX_GET(reg_data->pvlc_reg.vl_admin, outbox,
+				REG_VL_ADMIN_OFFSET);
+		SX_GET(reg_data->pvlc_reg.vl_operational, outbox,
+				REG_VL_OPERATIONAL_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_PVLC);
+
+int sx_ACCESS_REG_MCIA(struct sx_dev *dev, struct ku_access_mcia_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_L_OFFSET			0x14
+#define REG_MODULE_OFFSET		0x15
+#define REG_STATUS_OFFSET		0x17
+#define REG_I2C_DEVICE_ADDRESS_OFFSET	0x18
+#define REG_PAGE_NUMBER_OFFSET		0x19
+#define REG_DEVICE_ADDRESS_OFFSET	0x1a
+#define REG_SIZE_OFFSET			0x1e
+#define REG_DWORD_0_OFFSET		0x24
+#define REG_DWORD_1_OFFSET		0x28
+#define REG_DWORD_2_OFFSET		0x2c
+#define REG_DWORD_3_OFFSET		0x30
+#define REG_DWORD_4_OFFSET		0x34
+#define REG_DWORD_5_OFFSET		0x38
+#define REG_DWORD_6_OFFSET		0x3c
+#define REG_DWORD_7_OFFSET		0x40
+#define REG_DWORD_8_OFFSET		0x44
+#define REG_DWORD_9_OFFSET		0x48
+#define REG_DWORD_10_OFFSET		0x4c
+#define REG_DWORD_11_OFFSET		0x50
+#define MCIA_REG_LEN			0x11
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= MCIA_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->mcia_reg.l, REG_L_OFFSET);
+	SX_PUT(inbox, reg_data->mcia_reg.module, REG_MODULE_OFFSET);
+	SX_PUT(inbox, reg_data->mcia_reg.i2c_device_address,
+			REG_I2C_DEVICE_ADDRESS_OFFSET);
+	SX_PUT(inbox, reg_data->mcia_reg.page_number, REG_PAGE_NUMBER_OFFSET);
+	SX_PUT(inbox, reg_data->mcia_reg.device_address,
+			REG_DEVICE_ADDRESS_OFFSET);
+	SX_PUT(inbox, reg_data->mcia_reg.size,
+			REG_SIZE_OFFSET);
+	SX_PUT(inbox, reg_data->mcia_reg.dword_0,
+			REG_DWORD_0_OFFSET);
+	SX_PUT(inbox, reg_data->mcia_reg.dword_1,
+			REG_DWORD_1_OFFSET);
+	SX_PUT(inbox, reg_data->mcia_reg.dword_2,
+			REG_DWORD_2_OFFSET);
+	SX_PUT(inbox, reg_data->mcia_reg.dword_3,
+			REG_DWORD_3_OFFSET);
+	SX_PUT(inbox, reg_data->mcia_reg.dword_4,
+			REG_DWORD_4_OFFSET);
+	SX_PUT(inbox, reg_data->mcia_reg.dword_5,
+			REG_DWORD_5_OFFSET);
+	SX_PUT(inbox, reg_data->mcia_reg.dword_6,
+			REG_DWORD_6_OFFSET);
+	SX_PUT(inbox, reg_data->mcia_reg.dword_7,
+			REG_DWORD_7_OFFSET);
+	SX_PUT(inbox, reg_data->mcia_reg.dword_8,
+			REG_DWORD_8_OFFSET);
+	SX_PUT(inbox, reg_data->mcia_reg.dword_9,
+			REG_DWORD_9_OFFSET);
+	SX_PUT(inbox, reg_data->mcia_reg.dword_10,
+			REG_DWORD_10_OFFSET);
+	SX_PUT(inbox, reg_data->mcia_reg.dword_11,
+			REG_DWORD_11_OFFSET);
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(MCIA_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->mcia_reg.l, outbox,
+				REG_L_OFFSET);
+		SX_GET(reg_data->mcia_reg.module, outbox,
+				REG_MODULE_OFFSET);
+		SX_GET(reg_data->mcia_reg.status, outbox,
+				REG_STATUS_OFFSET);
+		SX_GET(reg_data->mcia_reg.i2c_device_address, outbox,
+				REG_I2C_DEVICE_ADDRESS_OFFSET);
+		SX_GET(reg_data->mcia_reg.page_number, outbox,
+				REG_PAGE_NUMBER_OFFSET);
+		SX_GET(reg_data->mcia_reg.device_address, outbox,
+				REG_DEVICE_ADDRESS_OFFSET);
+		SX_GET(reg_data->mcia_reg.size, outbox, REG_SIZE_OFFSET);
+		SX_GET(reg_data->mcia_reg.dword_0, outbox,
+				REG_DWORD_0_OFFSET);
+		SX_GET(reg_data->mcia_reg.dword_1, outbox,
+				REG_DWORD_1_OFFSET);
+		SX_GET(reg_data->mcia_reg.dword_2, outbox,
+				REG_DWORD_2_OFFSET);
+		SX_GET(reg_data->mcia_reg.dword_3, outbox,
+				REG_DWORD_3_OFFSET);
+		SX_GET(reg_data->mcia_reg.dword_4, outbox,
+				REG_DWORD_4_OFFSET);
+		SX_GET(reg_data->mcia_reg.dword_5, outbox,
+				REG_DWORD_5_OFFSET);
+		SX_GET(reg_data->mcia_reg.dword_6, outbox,
+				REG_DWORD_6_OFFSET);
+		SX_GET(reg_data->mcia_reg.dword_7, outbox,
+				REG_DWORD_7_OFFSET);
+		SX_GET(reg_data->mcia_reg.dword_8, outbox,
+				REG_DWORD_8_OFFSET);
+		SX_GET(reg_data->mcia_reg.dword_9, outbox,
+				REG_DWORD_9_OFFSET);
+		SX_GET(reg_data->mcia_reg.dword_10, outbox,
+				REG_DWORD_10_OFFSET);
+		SX_GET(reg_data->mcia_reg.dword_11, outbox,
+				REG_DWORD_11_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_MCIA);
+
+int sx_ACCESS_REG_HPKT(struct sx_dev *dev, struct ku_access_hpkt_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+	u32 trap_info = 0;
+
+#define REG_ACK_BITN			24
+#define REG_ACTION_BITN			20
+#define REG_TRAP_GROUP_BITN		12
+#define REG_TRAP_INFO_OFFSET	0x14
+#define REG_HPKT_CTRL_OFFSET    0x19
+#define HPKT_REG_LEN			0x05
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= HPKT_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+
+	trap_info = ((u32)(reg_data->hpkt_reg.ack) & 0x1) << REG_ACK_BITN;
+	trap_info |= ((u32)(reg_data->hpkt_reg.action) & 0x7 )<< REG_ACTION_BITN;
+	trap_info |= ((u32)(reg_data->hpkt_reg.trap_group) & 0x3f ) << REG_TRAP_GROUP_BITN;
+	trap_info |= (u32)(reg_data->hpkt_reg.trap_id) & 0x1FF;
+	SX_PUT(inbox, trap_info, REG_TRAP_INFO_OFFSET);
+	SX_PUT(inbox, reg_data->hpkt_reg.control, REG_HPKT_CTRL_OFFSET);
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(HPKT_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(trap_info, outbox, REG_TRAP_INFO_OFFSET);
+		reg_data->hpkt_reg.ack = (trap_info >> REG_ACK_BITN) & 0x1;
+		reg_data->hpkt_reg.action = (trap_info >> REG_ACTION_BITN) & 0x7;
+		reg_data->hpkt_reg.trap_group = (trap_info >> REG_TRAP_GROUP_BITN) & 0x3F;
+		reg_data->hpkt_reg.trap_id = trap_info & 0x1FF;
+		SX_GET(reg_data->hpkt_reg.control, outbox, REG_HPKT_CTRL_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_HPKT);
+
+int sx_ACCESS_REG_HCAP(struct sx_dev *dev, struct ku_access_hcap_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_MAX_CPU_EGRESS_TCS_OFFSET		0x1b
+#define REG_MAX_CPU_INGRESS_TCS_OFFSET		0x1f
+#define REG_MAX_TRAP_GROUPS_OFFSET			0x23
+#define REG_DR_PATHS_OFFSET					0x27
+#define HCAP_REG_LEN						0x09
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= HCAP_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->hcap_reg.max_cpu_egress_tclass,
+			REG_MAX_CPU_EGRESS_TCS_OFFSET);
+	SX_PUT(inbox, reg_data->hcap_reg.max_cpu_ingress_tclass,
+			REG_MAX_CPU_INGRESS_TCS_OFFSET);
+	SX_PUT(inbox, reg_data->hcap_reg.max_num_trap_groups,
+			REG_MAX_TRAP_GROUPS_OFFSET);
+	SX_PUT(inbox, reg_data->hcap_reg.max_num_dr_paths,
+			REG_DR_PATHS_OFFSET);
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(HCAP_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->hcap_reg.max_cpu_egress_tclass, outbox,
+				REG_MAX_CPU_EGRESS_TCS_OFFSET);
+		SX_GET(reg_data->hcap_reg.max_cpu_ingress_tclass, outbox,
+				REG_MAX_CPU_INGRESS_TCS_OFFSET);
+		SX_GET(reg_data->hcap_reg.max_num_trap_groups, outbox,
+				REG_MAX_TRAP_GROUPS_OFFSET);
+		SX_GET(reg_data->hcap_reg.max_num_dr_paths, outbox,
+				REG_DR_PATHS_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_HCAP);
+
+int sx_ACCESS_REG_MFSC(struct sx_dev *dev, struct ku_access_mfsc_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_FAN_OFFSET	0x14
+#define REG_PWM_OFFSET	0x1b
+#define MFSC_REG_LEN	0x3
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= MFSC_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->mfsc_reg.pwm, REG_FAN_OFFSET);
+	SX_PUT(inbox, reg_data->mfsc_reg.pwm_duty_cycle, REG_PWM_OFFSET);
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(MFSC_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->mfsc_reg.pwm, outbox,
+				REG_FAN_OFFSET);
+		SX_GET(reg_data->mfsc_reg.pwm_duty_cycle, outbox,
+				REG_PWM_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_MFSC);
+
+int sx_ACCESS_REG_MFSM(struct sx_dev *dev, struct ku_access_mfsm_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_TACHO_OFFSET	0x14
+#define REG_N_OFFSET		0x17
+#define REG_RPM_OFFSET		0x1a
+#define MFSM_REG_LEN		0x3
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= MFSM_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->mfsm_reg.tacho, REG_TACHO_OFFSET);
+	SX_PUT(inbox, reg_data->mfsm_reg.n, REG_N_OFFSET);
+	SX_PUT(inbox, reg_data->mfsm_reg.rpm, REG_RPM_OFFSET);
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(MFSM_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->mfsm_reg.tacho, outbox,
+				REG_TACHO_OFFSET);
+		SX_GET(reg_data->mfsm_reg.n, outbox, REG_N_OFFSET);
+		SX_GET(reg_data->mfsm_reg.rpm, outbox, REG_RPM_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_MFSM);
+
+int sx_ACCESS_REG_MFSL(struct sx_dev *dev, struct ku_access_mfsl_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+	u8 flags;
+
+#define REG_FAN_OFFSET		0x14
+#define REG_EE_SHIFT		2
+#define REG_FLAGS_OFFSET	0x17
+#define REG_TACH_MIN_OFFSET	0x1a
+#define REG_TACH_MAX_OFFSET	0x1e
+#define MFSL_REG_LEN		0x3
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= MFSL_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->mfsl_reg.fan, REG_FAN_OFFSET);
+	flags = (reg_data->mfsl_reg.ee << REG_EE_SHIFT);
+	flags |= reg_data->mfsl_reg.ie;
+	SX_PUT(inbox, flags, REG_FLAGS_OFFSET);
+	SX_PUT(inbox, reg_data->mfsl_reg.tach_min, REG_TACH_MIN_OFFSET);
+	SX_PUT(inbox, reg_data->mfsl_reg.tach_max, REG_TACH_MAX_OFFSET);
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(MFSL_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->mfsl_reg.fan, outbox, REG_FAN_OFFSET);
+		SX_GET(flags, outbox, REG_FLAGS_OFFSET);
+		reg_data->mfsl_reg.ee = (flags >> REG_EE_SHIFT) & 0x3;
+		reg_data->mfsl_reg.ie = flags & 0x1;
+		SX_GET(reg_data->mfsl_reg.tach_min, outbox,
+				REG_TACH_MIN_OFFSET);
+		SX_GET(reg_data->mfsl_reg.tach_max, outbox,
+				REG_TACH_MAX_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_MFSL);
+
+int sx_ACCESS_REG_HDRT(struct sx_dev *dev, struct ku_access_hdrt_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	int i;
+	u16 type_len = 0;
+
+#define REG_DR_INDEX_OFFSET	0x14
+#define REG_HOP_CNT_OFFSET	0x16
+#define REG_FIRST_PATH_OFFSET	0x18
+#define REG_NUM_OF_PATHS	0x40
+#define REG_FIRST_RPATH_OFFSET	(REG_FIRST_PATH_OFFSET + REG_NUM_OF_PATHS)
+#define HDRT_REG_LEN		0x17
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= HDRT_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->hdrt_reg.dr_index, REG_DR_INDEX_OFFSET);
+	SX_PUT(inbox, reg_data->hdrt_reg.hop_cnt, REG_HOP_CNT_OFFSET);
+	for (i = 0; i < REG_NUM_OF_PATHS; i++)
+		SX_PUT(inbox, reg_data->hdrt_reg.path[i],
+				REG_FIRST_PATH_OFFSET + i);
+
+	for (i = 0; i < REG_NUM_OF_PATHS; i++)
+		SX_PUT(inbox, reg_data->hdrt_reg.rpath[i],
+				REG_FIRST_RPATH_OFFSET + i);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(HDRT_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->hdrt_reg.dr_index, outbox,
+				REG_DR_INDEX_OFFSET);
+		SX_GET(reg_data->hdrt_reg.hop_cnt, outbox, REG_HOP_CNT_OFFSET);
+		for (i = 0; i < REG_NUM_OF_PATHS; i++)
+			SX_GET(reg_data->hdrt_reg.path[i], outbox,
+					REG_FIRST_PATH_OFFSET + i);
+
+		for (i = 0; i < REG_NUM_OF_PATHS; i++)
+			SX_GET(reg_data->hdrt_reg.rpath[i], outbox,
+					REG_FIRST_RPATH_OFFSET + i);
+
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_HDRT);
+
+int sx_ACCESS_REG_MTMP(struct sx_dev *dev, struct ku_access_mtmp_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+	u8 flags = 0;
+
+#define REG_SENSOR_INDEX_OFFSET	0x17
+#define REG_TEMP_OFFSET		0x1a
+#define REG_MTE_SHIFT		0x7
+#define REG_MTR_SHIFT		0x6
+#define REG_MTE_MTR_OFFSET	0x1c
+#define REG_MAX_TEMP_OFFSET	0x1e
+#define REG_TEE_OFFSET		0x20
+#define REG_TEE_SHIFT		0x6
+#define REG_TEMP_THRSHLD_OFFSET	0x22
+#define MTMP_REG_LEN		0x6
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= MTMP_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->mtmp_reg.sensor_index,
+			REG_SENSOR_INDEX_OFFSET);
+	SX_PUT(inbox, reg_data->mtmp_reg.temperature, REG_TEMP_OFFSET);
+	flags |= reg_data->mtmp_reg.mte << REG_MTE_SHIFT;
+	flags |= reg_data->mtmp_reg.mtr << REG_MTR_SHIFT;
+	SX_PUT(inbox, flags, REG_MTE_MTR_OFFSET);
+	SX_PUT(inbox, reg_data->mtmp_reg.max_temperature, REG_MAX_TEMP_OFFSET);
+	flags = reg_data->mtmp_reg.tee << REG_TEE_SHIFT;
+	SX_PUT(inbox, flags, REG_TEE_OFFSET);
+	SX_PUT(inbox, reg_data->mtmp_reg.temperature_threshold,
+			REG_TEMP_THRSHLD_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(MTMP_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->mtmp_reg.sensor_index, outbox,
+				REG_SENSOR_INDEX_OFFSET);
+		SX_GET(reg_data->mtmp_reg.temperature, outbox,
+				REG_TEMP_OFFSET);
+		SX_GET(flags, outbox, REG_MTE_MTR_OFFSET);
+		reg_data->mtmp_reg.mte = (flags >> REG_MTE_SHIFT) & 0x1;
+		reg_data->mtmp_reg.mtr = (flags >> REG_MTR_SHIFT) & 0x1;
+		SX_GET(reg_data->mtmp_reg.max_temperature, outbox,
+				REG_MAX_TEMP_OFFSET);
+		SX_GET(flags, outbox, REG_TEE_OFFSET);
+		reg_data->mtmp_reg.tee = (flags >> REG_TEE_SHIFT) & 0x3;
+		SX_GET(reg_data->mtmp_reg.temperature_threshold, outbox,
+				REG_TEMP_THRSHLD_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_MTMP);
+
+int sx_ACCESS_REG_MMDIO(struct sx_dev *dev,
+		struct ku_access_mmdio_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_MDIO_INDEX_OFFSET	0x15
+#define REG_OPERATION_OFFSET	0x17
+#define REG_ADDRESS_OFFSET	0x18
+#define REG_DATA_OFFSET		0x1c
+#define MMDIO_REG_LEN		0x4
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= MMDIO_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->mmdio_reg.mdio_index, REG_MDIO_INDEX_OFFSET);
+	SX_PUT(inbox, reg_data->mmdio_reg.operation, REG_OPERATION_OFFSET);
+	SX_PUT(inbox, reg_data->mmdio_reg.address, REG_ADDRESS_OFFSET);
+	SX_PUT(inbox, reg_data->mmdio_reg.data, REG_DATA_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(MMDIO_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->mmdio_reg.mdio_index, outbox,
+				REG_MDIO_INDEX_OFFSET);
+		SX_GET(reg_data->mmdio_reg.operation, outbox,
+				REG_OPERATION_OFFSET);
+		SX_GET(reg_data->mmdio_reg.address, outbox, REG_ADDRESS_OFFSET);
+		SX_GET(reg_data->mmdio_reg.data, outbox, REG_DATA_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_MMDIO);
+
+int sx_ACCESS_REG_MMIA(struct sx_dev *dev, struct ku_access_mmia_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_OPERATION_OFFSET	0x17
+#define REG_DATA_OFFSET		0x1c
+#define MMIA_REG_LEN		0x4
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= MMIA_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->mmia_reg.operation, REG_OPERATION_OFFSET);
+	SX_PUT(inbox, reg_data->mmia_reg.data, REG_DATA_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(MMIA_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->mmia_reg.operation, outbox,
+				REG_OPERATION_OFFSET);
+		SX_GET(reg_data->mmia_reg.data, outbox, REG_DATA_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_MMIA);
+
+int sx_ACCESS_REG_MFPA(struct sx_dev *dev, struct ku_access_mfpa_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+	u8 flag;
+
+#define REG_P_OFFSET			0x16
+#define REG_FS_OFFSET			0x17
+#define REG_FS_BITS_SHIFT		4
+#define REG_BOOT_ADDRESS_OFFSET		0x18
+#define REG_FLASH_NUM_OFFSET		0x27
+#define REG_JEDEC_ID_OFFSET		0x28
+#define REG_BLOCK_ALLIGNMENT_OFFSET	0x2d
+#define REG_SECTOR_SIZE_OFFSET		0x2f
+#define REG_CAPABILITY_MASK_OFFSET	0x30
+#define REG_CAPABILITY_MASK_BIT_N	7
+#define MFPA_REG_LEN			0x40
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= MFPA_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->mfpa_reg.p, REG_P_OFFSET);
+	flag = (u8)(reg_data->mfpa_reg.fs << REG_FS_BITS_SHIFT);
+	SX_PUT(inbox, flag, REG_FS_OFFSET);
+	SX_PUT(inbox, reg_data->mfpa_reg.boot_address, REG_BOOT_ADDRESS_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(MFPA_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->mfpa_reg.p, outbox, REG_P_OFFSET);
+		SX_GET(flag, outbox, REG_FS_OFFSET);
+		reg_data->mfpa_reg.fs = (u8)(flag >> REG_FS_BITS_SHIFT);
+		SX_GET(reg_data->mfpa_reg.boot_address, outbox,
+				REG_BOOT_ADDRESS_OFFSET);
+		SX_GET(reg_data->mfpa_reg.flash_num, outbox,
+				REG_FLASH_NUM_OFFSET);
+		SX_GET(reg_data->mfpa_reg.jedec_id, outbox,
+				REG_JEDEC_ID_OFFSET);
+		SX_GET(reg_data->mfpa_reg.block_allignment, outbox,
+				REG_BLOCK_ALLIGNMENT_OFFSET);
+		SX_GET(reg_data->mfpa_reg.sector_size, outbox,
+				REG_SECTOR_SIZE_OFFSET);
+		SX_GET(flag, outbox, REG_CAPABILITY_MASK_OFFSET);
+		reg_data->mfpa_reg.capability_mask =
+				(u8)(flag >> REG_CAPABILITY_MASK_BIT_N);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_MFPA);
+
+int sx_ACCESS_REG_MFBE(struct sx_dev *dev, struct ku_access_mfbe_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+	u8 flag;
+
+#define REG_P_OFFSET		0x16
+#define REG_FS_OFFSET		0x17
+#define REG_FS_BITS_SHIFT	4
+#define REG_MFBE_ADDRESS_OFFSET	0x1c
+#define MFBE_REG_LEN		0x4
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= MFBE_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->mfbe_reg.p, REG_P_OFFSET);
+	flag = (u8)(reg_data->mfbe_reg.fs << REG_FS_BITS_SHIFT);
+	SX_PUT(inbox, flag, REG_FS_OFFSET);
+	SX_PUT(inbox, reg_data->mfbe_reg.address, REG_MFBE_ADDRESS_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(MFBE_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->mfbe_reg.p, outbox, REG_P_OFFSET);
+		SX_GET(flag, outbox, REG_FS_OFFSET);
+		reg_data->mfbe_reg.fs = (u8)(flag >> REG_FS_BITS_SHIFT);
+		SX_GET(reg_data->mfbe_reg.address, outbox,
+				REG_MFBE_ADDRESS_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_MFBE);
+
+int sx_ACCESS_REG_MFBA(struct sx_dev *dev, struct ku_access_mfba_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+	u8 flag;
+	int i;
+
+#define REG_P_OFFSET		0x16
+#define REG_FS_OFFSET		0x17
+#define REG_FS_BITS_SHIFT	4
+#define REG_MFBA_SIZE_OFFSET	0x1a
+#define REG_MFBA_ADDRESS_OFFSET	    0x1c
+#define REG_MFBA_DATA_OFFSET	    0x20
+#define MFBA_REG_LEN		0x40
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= MFBA_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->mfba_reg.p, REG_P_OFFSET);
+	flag = (u8)(reg_data->mfba_reg.fs << REG_FS_BITS_SHIFT);
+	SX_PUT(inbox, flag, REG_FS_OFFSET);
+	SX_PUT(inbox, reg_data->mfba_reg.size, REG_MFBA_SIZE_OFFSET);
+	SX_PUT(inbox, reg_data->mfba_reg.address, REG_MFBA_ADDRESS_OFFSET);
+	for (i = 0; i < 192; i++)
+		SX_PUT(inbox, reg_data->mfba_reg.data[i],
+				REG_MFBA_DATA_OFFSET + i);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(MFBA_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->mfba_reg.p, outbox, REG_P_OFFSET);
+		SX_GET(flag, outbox, REG_FS_OFFSET);
+		reg_data->mfba_reg.fs = (u8)(flag >> REG_FS_BITS_SHIFT);
+		SX_GET(reg_data->mfba_reg.size, outbox, REG_MFBA_SIZE_OFFSET);
+		SX_GET(reg_data->mfba_reg.address, outbox,
+				REG_MFBA_ADDRESS_OFFSET);
+		for (i = 0; i < 192; i++)
+			SX_GET(reg_data->mfba_reg.data[i], outbox,
+					REG_MFBA_DATA_OFFSET + i);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_MFBA);
+
+int sx_ACCESS_REG_QCAP(struct sx_dev *dev, struct ku_access_qcap_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_MAX_POLICER_PER_PORT_OFFSET		0x1f
+#define REG_MAX_POLICER_GLOBAL_OFFSET		0x23
+#define QCAP_REG_LEN				0x09
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= QCAP_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->qcap_reg.max_policers_per_port,
+			REG_MAX_POLICER_PER_PORT_OFFSET);
+	SX_PUT(inbox, reg_data->qcap_reg.max_policers_global,
+			REG_MAX_POLICER_GLOBAL_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(QCAP_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->qcap_reg.max_policers_per_port, outbox,
+				REG_MAX_POLICER_PER_PORT_OFFSET);
+		SX_GET(reg_data->qcap_reg.max_policers_global, outbox,
+				REG_MAX_POLICER_GLOBAL_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_QCAP);
+
+int sx_ACCESS_REG_RAW(struct sx_dev *dev, struct ku_access_raw_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define OP_TLV_SIZE	0x10
+
+	if (SX_MAILBOX_SIZE < OP_TLV_SIZE + reg_data->raw_reg.size) {
+		sx_warn(dev, "Cannot send the raw register access request "
+				"since the mailbox size is too small\n");
+		return -ENOMEM;
+	}
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= (reg_data->raw_reg.size / 4) + 1;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	err = copy_from_user(((u8 *)inbox) + OP_TLV_SIZE + 4,
+			reg_data->raw_reg.buff, reg_data->raw_reg.size);
+	if (err)
+		goto out;
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			20 + reg_data->raw_reg.size);
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	err = copy_to_user(reg_data->raw_reg.buff,
+			((u8 *)outbox) + OP_TLV_SIZE + 4,
+			reg_data->raw_reg.size);
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+}
+
+int sx_ACCESS_REG_RAW_BUFF(struct sx_dev *dev,
+		struct ku_access_reg_raw_buff *raw_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+
+	if (SX_MAILBOX_SIZE < raw_data->raw_buff.size) {
+		sx_warn(dev, "Cannot send the raw register access request "
+				"since the mailbox size is too small\n");
+		return -ENOMEM;
+	}
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, raw_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, raw_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	err = copy_from_user(((u8 *)inbox), raw_data->raw_buff.buff,
+			raw_data->raw_buff.size);
+	if (err)
+		goto out;
+
+	err = sx_cmd_box(dev, raw_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			raw_data->raw_buff.size);
+	if (err)
+		goto out;
+
+	err = copy_to_user(raw_data->raw_buff.buff, outbox,
+			raw_data->raw_buff.size);
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+}
+
+int sx_ACCESS_REG_MTWE(struct sx_dev *dev, struct ku_access_mtwe_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_SENSOR_WARNING_OFFSET	0x17
+#define MTWE_REG_LEN			0x3
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= MTWE_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->mtwe_reg.sensor_warning,
+			REG_SENSOR_WARNING_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(MTWE_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->mtwe_reg.sensor_warning, outbox,
+				REG_SENSOR_WARNING_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_MTWE);
+
+int sx_ACCESS_REG_MTCAP(struct sx_dev *dev,
+		struct ku_access_mtcap_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_SENSOR_COUNT_OFFSET	0x17
+#define MTCAP_REG_LEN		0x3
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= MTCAP_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->mtcap_reg.sensor_count,
+			REG_SENSOR_COUNT_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(MTCAP_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->mtcap_reg.sensor_count, outbox,
+				REG_SENSOR_COUNT_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_MTCAP);
+
+int sx_ACCESS_REG_FORE(struct sx_dev *dev, struct ku_access_fore_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_FAN_UNDER_LIMIT_OFFSET	0x14
+#define REG_FAN_OVER_LIMIT_OFFSET	0x18
+#define FORE_REG_LEN			0x4
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= FORE_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->fore_reg.fan_under_limit,
+			REG_FAN_UNDER_LIMIT_OFFSET);
+	SX_PUT(inbox, reg_data->fore_reg.fan_over_limit,
+			REG_FAN_OVER_LIMIT_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(FORE_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->fore_reg.fan_under_limit, outbox,
+				REG_FAN_UNDER_LIMIT_OFFSET);
+		SX_GET(reg_data->fore_reg.fan_over_limit, outbox,
+				REG_FAN_OVER_LIMIT_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_FORE);
+
+int sx_ACCESS_REG_MFCR(struct sx_dev *dev, struct ku_access_mfcr_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_PWM_FREQ_OFFSET	0x17
+#define REG_TACHO_ACTIVE_OFFSET	0x18
+#define REG_PWM_ACTIVE_OFFSET	0x1b
+#define MFCR_REG_LEN		0x3
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= MFCR_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->mfcr_reg.pwm_frequency, REG_PWM_FREQ_OFFSET);
+	SX_PUT(inbox, reg_data->mfcr_reg.tacho_active,
+			REG_TACHO_ACTIVE_OFFSET);
+	SX_PUT(inbox, reg_data->mfcr_reg.pwm_active, REG_PWM_ACTIVE_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(MFCR_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->mfcr_reg.pwm_frequency, outbox,
+				REG_PWM_FREQ_OFFSET);
+		SX_GET(reg_data->mfcr_reg.tacho_active, outbox,
+				REG_TACHO_ACTIVE_OFFSET);
+		SX_GET(reg_data->mfcr_reg.pwm_active, outbox,
+				REG_PWM_ACTIVE_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_MFCR);
+
+int sx_ACCESS_REG_MFM(struct sx_dev *dev, struct ku_access_mfm_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_MFM_INDEX_OFFSET	0x17
+#define REG_MEMORY_OFFSET	0x1c
+#define REG_MEMORY_MASK_OFFSET	0x24
+#define MFM_REG_LEN		0x7
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= MFM_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->mfm_reg.index, REG_MFM_INDEX_OFFSET);
+	SX_PUT(inbox, reg_data->mfm_reg.memory, REG_MEMORY_OFFSET);
+	SX_PUT(inbox, reg_data->mfm_reg.memory_mask, REG_MEMORY_MASK_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(MFM_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->mfm_reg.index, outbox,
+				REG_MFM_INDEX_OFFSET);
+		SX_GET(reg_data->mfm_reg.memory, outbox,
+				REG_MEMORY_OFFSET);
+		SX_GET(reg_data->mfm_reg.memory_mask, outbox,
+				REG_MEMORY_MASK_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_MFM);
+
+int sx_ACCESS_REG_QPRT(struct sx_dev *dev, struct ku_access_qprt_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_LOCAL_PORT_OFFSET	0x15
+#define REG_PRIO_OFFSET		0x16
+#define REG_RPRIO_OFFSET	0x1b
+#define QPRT_REG_LEN		0x3
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= QPRT_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->qprt_reg.local_port, REG_LOCAL_PORT_OFFSET);
+	SX_PUT(inbox, reg_data->qprt_reg.prio, REG_PRIO_OFFSET);
+	SX_PUT(inbox, reg_data->qprt_reg.rprio, REG_RPRIO_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(QPRT_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->qprt_reg.local_port, outbox,
+				REG_LOCAL_PORT_OFFSET);
+		SX_GET(reg_data->qprt_reg.prio, outbox, REG_PRIO_OFFSET);
+		SX_GET(reg_data->qprt_reg.rprio, outbox, REG_RPRIO_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_QPRT);
+
+int sx_ACCESS_REG_HTGT(struct sx_dev *dev, struct ku_access_htgt_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+	u16 mac_47_32;
+	u32 mac_31_0;
+	u8 tmp;
+
+#define REG_HTGT_SWID_OFFSET		0x14
+#define REG_HTGT_TYPE_OFFSET		0x16
+#define REG_HTGT_GRP_OFFSET			0x17
+#define REG_HTGT_PIDE_OFFSET		0x1A
+#define REG_HTGT_PID_OFFSET			0x1B
+#define REG_HTGT_MRR_ACTION_OFFSET  0x1E
+#define REG_HTGT_MRR_AGENT_OFFSET   0x1F
+#define REG_HTGT_PRIO_OFFSET        0x23
+#define REG_HTGT_PATH_OFFSET		0x24
+#define REG_HTGT_LP_CPU_TC_OFFSET	(REG_HTGT_PATH_OFFSET + 1)
+#define REG_HTGT_LP_RDQ_OFFSET		(REG_HTGT_PATH_OFFSET + 3)
+#define REG_HTGT_SP_STK_TC_OFFSET	(REG_HTGT_PATH_OFFSET + 0)
+#define REG_HTGT_SP_CPU_TC_OFFSET	(REG_HTGT_PATH_OFFSET + 1)
+#define REG_HTGT_SP_RDQ_OFFSET		(REG_HTGT_PATH_OFFSET + 3)
+#define REG_HTGT_SP_SYS_PORT_OFFSET	(REG_HTGT_PATH_OFFSET + 7)
+#define REG_HTGT_DRP_DR_PTR_OFFSET	(REG_HTGT_PATH_OFFSET + 3)
+#define REG_HTGT_EP_MAC_47_32_OFFSET	(REG_HTGT_PATH_OFFSET + 3)
+#define REG_HTGT_EP_MAC_31_0_OFFSET	(REG_HTGT_PATH_OFFSET + 7)
+#define REG_HTGT_EP_VID_OFFSET		(REG_HTGT_PATH_OFFSET + 0xb)
+#define HTGT_REG_LEN				0x41
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= HTGT_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->htgt_reg.swid, REG_HTGT_SWID_OFFSET);
+	SX_PUT(inbox, reg_data->htgt_reg.type, REG_HTGT_TYPE_OFFSET);
+	SX_PUT(inbox, reg_data->htgt_reg.trap_group, REG_HTGT_GRP_OFFSET);
+	tmp = (reg_data->htgt_reg.pide & 0x1) << 7;
+	SX_PUT(inbox, tmp, REG_HTGT_PIDE_OFFSET);
+	SX_PUT(inbox, reg_data->htgt_reg.pid, REG_HTGT_PID_OFFSET);
+	SX_PUT(inbox, reg_data->htgt_reg.mirror_action, REG_HTGT_MRR_ACTION_OFFSET);
+	SX_PUT(inbox, reg_data->htgt_reg.mirror_agent, REG_HTGT_MRR_AGENT_OFFSET);
+	SX_PUT(inbox, reg_data->htgt_reg.priority, REG_HTGT_PRIO_OFFSET);
+
+	switch (reg_data->htgt_reg.type) {
+	case HTGT_LOCAL_PATH:
+		SX_PUT(inbox, reg_data->htgt_reg.path.local_path.cpu_tclass,
+				REG_HTGT_LP_CPU_TC_OFFSET);
+		SX_PUT(inbox, reg_data->htgt_reg.path.local_path.rdq,
+				REG_HTGT_LP_RDQ_OFFSET);
+		break;
+
+	case HTGT_STACKING_PATH:
+		SX_PUT(inbox, reg_data->
+				htgt_reg.path.stacking_path.stacking_tclass,
+			   REG_HTGT_SP_STK_TC_OFFSET);
+		SX_PUT(inbox, reg_data->htgt_reg.path.stacking_path.cpu_tclass,
+			   REG_HTGT_SP_CPU_TC_OFFSET);
+		SX_PUT(inbox, reg_data->htgt_reg.path.stacking_path.rdq,
+			   REG_HTGT_SP_RDQ_OFFSET);
+		SX_PUT(inbox,
+			reg_data->htgt_reg.path.stacking_path.cpu_sys_port,
+			   REG_HTGT_SP_SYS_PORT_OFFSET);
+		break;
+
+	case HTGT_DR_PATH:
+		SX_PUT(inbox, reg_data->htgt_reg.path.dr_path.dr_ptr,
+				REG_HTGT_DRP_DR_PTR_OFFSET);
+		break;
+
+	case HTGT_ETH_PATH:
+		mac_47_32 = (reg_data->htgt_reg.path.eth_path.mac >> 32) &
+				0xFFFF;
+		mac_31_0 = reg_data->htgt_reg.path.eth_path.mac & 0xFFFFFFFF;
+		SX_PUT(inbox, mac_47_32, REG_HTGT_EP_MAC_47_32_OFFSET);
+		SX_PUT(inbox, mac_31_0, REG_HTGT_EP_MAC_31_0_OFFSET);
+		SX_PUT(inbox, reg_data->htgt_reg.path.eth_path.vid,
+				REG_HTGT_EP_VID_OFFSET);
+		break;
+
+	default:
+		printk(KERN_ERR "%s(): Incorrect HTGT path type: %d \n",
+					__func__, reg_data->htgt_reg.type);
+		return -EINVAL;
+	}
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(HTGT_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->htgt_reg.swid, outbox,
+				REG_HTGT_SWID_OFFSET);
+		SX_GET(reg_data->htgt_reg.type, outbox,
+				REG_HTGT_TYPE_OFFSET);
+		SX_GET(reg_data->htgt_reg.trap_group, outbox,
+				REG_HTGT_GRP_OFFSET);
+		SX_GET(tmp, outbox, REG_HTGT_PIDE_OFFSET);
+		reg_data->htgt_reg.pide = (tmp >> 0x7) & 0x1;
+		SX_GET(reg_data->htgt_reg.pid, outbox,
+				REG_HTGT_PID_OFFSET);
+		SX_GET(reg_data->htgt_reg.mirror_action, outbox,
+		       REG_HTGT_MRR_ACTION_OFFSET);
+		SX_GET(reg_data->htgt_reg.mirror_agent, outbox,
+		       REG_HTGT_MRR_AGENT_OFFSET);
+		SX_GET(reg_data->htgt_reg.priority, outbox,
+		       REG_HTGT_PRIO_OFFSET);
+
+		switch (reg_data->htgt_reg.type) {
+		case HTGT_LOCAL_PATH:
+			SX_GET(reg_data->htgt_reg.path.local_path.cpu_tclass,
+					outbox, REG_HTGT_LP_CPU_TC_OFFSET);
+			SX_GET(reg_data->htgt_reg.path.local_path.rdq, outbox,
+				REG_HTGT_LP_RDQ_OFFSET);
+			break;
+
+		case HTGT_STACKING_PATH:
+			SX_GET(reg_data->
+				htgt_reg.path.stacking_path.stacking_tclass,
+				    outbox, REG_HTGT_SP_STK_TC_OFFSET);
+			SX_GET(reg_data->
+				htgt_reg.path.stacking_path.cpu_tclass,
+				    outbox, REG_HTGT_SP_CPU_TC_OFFSET);
+			SX_GET(reg_data->htgt_reg.path.stacking_path.rdq,
+				    outbox, REG_HTGT_SP_RDQ_OFFSET);
+			SX_GET(reg_data->
+				htgt_reg.path.stacking_path.cpu_sys_port,
+				    outbox, REG_HTGT_SP_SYS_PORT_OFFSET);
+			break;
+
+		case HTGT_DR_PATH:
+			SX_GET(reg_data->htgt_reg.path.dr_path.dr_ptr,
+				    outbox, REG_HTGT_DRP_DR_PTR_OFFSET);
+			break;
+
+		case HTGT_ETH_PATH:
+			SX_GET(mac_47_32, outbox,
+					REG_HTGT_EP_MAC_47_32_OFFSET);
+			SX_GET(mac_31_0, outbox, REG_HTGT_EP_MAC_31_0_OFFSET);
+			reg_data->htgt_reg.path.eth_path.mac = mac_47_32;
+			reg_data->htgt_reg.path.eth_path.mac =
+				(reg_data->htgt_reg.path.eth_path.mac << 32) |
+				mac_31_0;
+			SX_GET(reg_data->htgt_reg.path.eth_path.vid, outbox,
+				   REG_HTGT_EP_VID_OFFSET);
+			break;
+
+		default:
+			printk(KERN_ERR "%s(): Incorrect HTGT path type: "
+					"%d on query\n", __func__,
+					reg_data->htgt_reg.type);
+			return -EINVAL;
+		}
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_HTGT);
+
+int sx_ACCESS_REG_SPAD(struct sx_dev *dev, struct ku_access_spad_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define REG_BASE_MAC_OFFSET	0x14
+#define SPAD_REG_LEN		0x05
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= SPAD_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->spad_reg.base_mac, REG_BASE_MAC_OFFSET);
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(SPAD_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->spad_reg.base_mac, outbox,
+				REG_BASE_MAC_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_SPAD);
+
+#define PROFILE_SET_MASK_H_OFFSET		0x00
+#define PROFILE_SET_MASK_L_OFFSET		0x08
+#define PROFILE_MAX_VEPA_OFFSET			0x13
+#define PROFILE_MAX_LAG_OFFSET			0x16
+#define PROFILE_MAX_PORT_OFFSET			0x1a
+#define PROFILE_MAX_MID_OFFSET			0x1e
+#define PROFILE_MAX_PGT_OFFSET			0x22
+#define PROFILE_MAX_SYSPORT_OFFSET		0x26
+#define PROFILE_MAX_VLANS_OFFSET		0x2a
+#define PROFILE_MAX_REGIONS_OFFSET		0x2e
+#define PROFILE_MAX_FLOOD_TABLES_OFFSET		0x31
+#define PROFILE_MAX_PER_VID_FLOOD_TABLES_OFFSET	0x32
+#define PROFILE_FLOOD_MODE_OFFSET		0x33
+#define PROFILE_MAX_FID_OFFSET_FLOOD_TABLES_OFFSET 0x34
+#define PROFILE_FID_OFFSET_TABLE_SIZE_OFFSET	   0x36
+#define PROFILE_MAX_PER_FID_FLOOD_TABLE_OFFSET	 0x38
+#define PROFILE_PER_FID_TABLE_SIZE_OFFSET		  0x3A
+#define PROFILE_MAX_IB_MC_OFFSET		0x42
+#define PROFILE_MAX_PKEY_OFFSET			0x46
+#define PROFILE_AR_SEC_OFFSET			0x4c
+#define PROFILE_AR_GRP_CAP_OFFSET		0x4e
+#define PROFILE_ARN_OFFSET			0x50
+#define PROFILE_ARN_BIT_N			7
+#define PROFILE_KVD_LINEAR_SIZE_OFFSET  0x54
+#define PROFILE_KVD_HASH_SINGLE_SIZE_OFFSET   0x58
+#define PROFILE_KVD_HASH_DOUBLE_SIZE_OFFSET   0x5C
+#define PROFILE_SWID_0_CONF_OFFSET		0x60
+#define PROFILE_SWID_1_CONF_OFFSET		0x68
+#define PROFILE_SWID_2_CONF_OFFSET		0x70
+#define PROFILE_SWID_3_CONF_OFFSET		0x78
+#define PROFILE_SWID_4_CONF_OFFSET		0x80
+#define PROFILE_SWID_5_CONF_OFFSET		0x88
+#define PROFILE_SWID_6_CONF_OFFSET		0x90
+#define PROFILE_SWID_7_CONF_OFFSET		0x98
+#define PROFILE_RESERVED1_OFFSET		0xb4
+#define CONFIG_PROFILE_MB_SIZE			0xb8
+int sx_GET_PROFILE(struct sx_dev *dev, struct ku_profile *profile)
+{
+	struct sx_cmd_mailbox *mailbox;
+	u32 *outbox;
+	int err;
+	u8 tmp;
+
+	memset(profile, 0, sizeof(*profile));
+	mailbox = sx_alloc_cmd_mailbox(dev, profile->dev_id);
+	if (IS_ERR(mailbox))
+		return PTR_ERR(mailbox);
+
+	outbox = mailbox->buf;
+	err = sx_cmd_box(dev, dev->device_id, 0, mailbox, 0, 0x2,
+			SX_CMD_CONFIG_PROFILE, SX_CMD_TIME_CLASS_A,
+			CONFIG_PROFILE_MB_SIZE);
+	if (err)
+		goto out;
+
+	SX_GET(profile->max_vepa_channels, outbox, PROFILE_MAX_VEPA_OFFSET);
+	SX_GET(profile->max_lag, outbox, PROFILE_MAX_LAG_OFFSET);
+	SX_GET(profile->max_port_per_lag, outbox, PROFILE_MAX_PORT_OFFSET);
+	SX_GET(profile->max_mid, outbox, PROFILE_MAX_MID_OFFSET);
+	SX_GET(profile->max_pgt, outbox, PROFILE_MAX_PGT_OFFSET);
+	SX_GET(profile->max_system_port, outbox, PROFILE_MAX_SYSPORT_OFFSET);
+	SX_GET(profile->max_active_vlans, outbox, PROFILE_MAX_VLANS_OFFSET);
+	SX_GET(profile->max_regions, outbox, PROFILE_MAX_REGIONS_OFFSET);
+	SX_GET(profile->max_flood_tables, outbox, PROFILE_MAX_FLOOD_TABLES_OFFSET);
+	SX_GET(profile->max_per_vid_flood_tables, outbox,
+			PROFILE_MAX_PER_VID_FLOOD_TABLES_OFFSET);
+	SX_GET(profile->flood_mode, outbox, PROFILE_FLOOD_MODE_OFFSET);
+	SX_GET(profile->max_fid_offset_flood_tables, outbox, PROFILE_MAX_FID_OFFSET_FLOOD_TABLES_OFFSET);
+	SX_GET(profile->fid_offset_table_size, outbox, PROFILE_FID_OFFSET_TABLE_SIZE_OFFSET);
+	SX_GET(profile->max_per_fid_flood_table, outbox, PROFILE_MAX_PER_FID_FLOOD_TABLE_OFFSET);
+	SX_GET(profile->per_fid_table_size, outbox, PROFILE_PER_FID_TABLE_SIZE_OFFSET);
+	SX_GET(profile->max_ib_mc, outbox, PROFILE_MAX_IB_MC_OFFSET);
+	SX_GET(profile->max_pkey, outbox, PROFILE_MAX_PKEY_OFFSET);
+	SX_GET(profile->ar_sec, outbox, PROFILE_AR_SEC_OFFSET);
+	SX_GET(profile->adaptive_routing_group_cap, outbox, PROFILE_AR_GRP_CAP_OFFSET);
+	SX_GET(profile->arn, outbox, PROFILE_ARN_OFFSET);
+	profile->arn = profile->arn >> PROFILE_ARN_BIT_N;
+	SX_GET(profile->kvd_linear_size, outbox, PROFILE_KVD_LINEAR_SIZE_OFFSET);
+	SX_GET(profile->kvd_hash_single_size, outbox, PROFILE_KVD_HASH_SINGLE_SIZE_OFFSET);
+	SX_GET(profile->kvd_hash_double_size, outbox, PROFILE_KVD_HASH_DOUBLE_SIZE_OFFSET);
+	SX_GET(tmp, outbox, PROFILE_SWID_0_CONF_OFFSET + 1);
+	profile->swid0_config_type.type = tmp >> 4;
+	SX_GET(tmp, outbox, PROFILE_SWID_1_CONF_OFFSET + 1);
+	profile->swid1_config_type.type = tmp >> 4;
+	SX_GET(tmp, outbox, PROFILE_SWID_2_CONF_OFFSET + 1);
+	profile->swid2_config_type.type = tmp >> 4;
+	SX_GET(tmp, outbox, PROFILE_SWID_3_CONF_OFFSET + 1);
+	profile->swid3_config_type.type =  tmp >> 4;
+	SX_GET(tmp, outbox, PROFILE_SWID_4_CONF_OFFSET + 1);
+	profile->swid4_config_type.type = tmp >> 4;
+	SX_GET(tmp, outbox, PROFILE_SWID_5_CONF_OFFSET + 1);
+	profile->swid5_config_type.type = tmp >> 4;
+	SX_GET(tmp, outbox, PROFILE_SWID_6_CONF_OFFSET + 1);
+	profile->swid6_config_type.type = tmp >> 4;
+	SX_GET(tmp, outbox, PROFILE_SWID_7_CONF_OFFSET + 1);
+	profile->swid7_config_type.type = tmp >> 4;
+	SX_GET(profile->swid0_config_type.properties, outbox,
+			PROFILE_SWID_0_CONF_OFFSET + 3);
+	SX_GET(profile->swid1_config_type.properties, outbox,
+			PROFILE_SWID_1_CONF_OFFSET + 3);
+	SX_GET(profile->swid2_config_type.properties, outbox,
+			PROFILE_SWID_2_CONF_OFFSET + 3);
+	SX_GET(profile->swid3_config_type.properties, outbox,
+			PROFILE_SWID_3_CONF_OFFSET + 3);
+	SX_GET(profile->swid4_config_type.properties, outbox,
+			PROFILE_SWID_4_CONF_OFFSET + 3);
+	SX_GET(profile->swid5_config_type.properties, outbox,
+			PROFILE_SWID_5_CONF_OFFSET + 3);
+	SX_GET(profile->swid6_config_type.properties, outbox,
+			PROFILE_SWID_6_CONF_OFFSET + 3);
+	SX_GET(profile->swid7_config_type.properties, outbox,
+			PROFILE_SWID_7_CONF_OFFSET + 3);
+	SX_GET(profile->reserved1, outbox, PROFILE_RESERVED1_OFFSET);
+
+out:
+	sx_free_cmd_mailbox(dev, mailbox);
+	return err;
+}
+EXPORT_SYMBOL(sx_GET_PROFILE);
+
+int sx_SET_PROFILE(struct sx_dev *dev, struct ku_profile *profile)
+{
+	struct sx_cmd_mailbox *mailbox;
+	u32 *inbox;
+	int err;
+	u8 temp_u8 = 0;
+
+	mailbox = sx_alloc_cmd_mailbox(dev, profile->dev_id);
+	if (IS_ERR(mailbox))
+		return PTR_ERR(mailbox);
+
+	inbox = mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	SX_PUT(inbox, profile->set_mask_0_63, PROFILE_SET_MASK_L_OFFSET);
+	SX_PUT(inbox, profile->set_mask_64_127, PROFILE_SET_MASK_H_OFFSET);
+	SX_PUT(inbox, profile->max_vepa_channels, PROFILE_MAX_VEPA_OFFSET);
+	SX_PUT(inbox, profile->max_lag, PROFILE_MAX_LAG_OFFSET);
+	SX_PUT(inbox, profile->max_port_per_lag, PROFILE_MAX_PORT_OFFSET);
+	SX_PUT(inbox, profile->max_mid, PROFILE_MAX_MID_OFFSET);
+	SX_PUT(inbox, profile->max_pgt, PROFILE_MAX_PGT_OFFSET);
+	SX_PUT(inbox, profile->max_system_port,
+			PROFILE_MAX_SYSPORT_OFFSET);
+	SX_PUT(inbox, profile->max_active_vlans, PROFILE_MAX_VLANS_OFFSET);
+	SX_PUT(inbox, profile->max_regions, PROFILE_MAX_REGIONS_OFFSET);
+	SX_PUT(inbox, profile->max_flood_tables, PROFILE_MAX_FLOOD_TABLES_OFFSET);
+	SX_PUT(inbox, profile->max_per_vid_flood_tables,
+			PROFILE_MAX_PER_VID_FLOOD_TABLES_OFFSET);
+	SX_PUT(inbox, profile->flood_mode, PROFILE_FLOOD_MODE_OFFSET);
+	SX_PUT(inbox, profile->max_fid_offset_flood_tables, PROFILE_MAX_FID_OFFSET_FLOOD_TABLES_OFFSET);
+	SX_PUT(inbox, profile->fid_offset_table_size, PROFILE_FID_OFFSET_TABLE_SIZE_OFFSET);
+	SX_PUT(inbox, profile->max_per_fid_flood_table, PROFILE_MAX_PER_FID_FLOOD_TABLE_OFFSET);
+	SX_PUT(inbox, profile->per_fid_table_size, PROFILE_PER_FID_TABLE_SIZE_OFFSET);
+	SX_PUT(inbox, profile->max_ib_mc, PROFILE_MAX_IB_MC_OFFSET);
+	SX_PUT(inbox, profile->max_pkey, PROFILE_MAX_PKEY_OFFSET);
+	SX_PUT(inbox, profile->ar_sec, PROFILE_AR_SEC_OFFSET);
+	SX_PUT(inbox, profile->adaptive_routing_group_cap, PROFILE_AR_GRP_CAP_OFFSET);
+	SX_PUT(inbox, (u8)(profile->arn << PROFILE_ARN_BIT_N), PROFILE_ARN_OFFSET);
+	SX_PUT(inbox, profile->kvd_linear_size, PROFILE_KVD_LINEAR_SIZE_OFFSET);
+	SX_PUT(inbox, profile->kvd_hash_single_size, PROFILE_KVD_HASH_SINGLE_SIZE_OFFSET);
+	SX_PUT(inbox, profile->kvd_hash_double_size, PROFILE_KVD_HASH_DOUBLE_SIZE_OFFSET);
+	SX_PUT(inbox, profile->swid0_config_type.mask,
+			PROFILE_SWID_0_CONF_OFFSET);
+	SX_PUT(inbox, profile->swid0_config_type.properties,
+			PROFILE_SWID_0_CONF_OFFSET + 3);
+	temp_u8 = (u8)(profile->swid0_config_type.type << 4);
+	SX_PUT(inbox, temp_u8, PROFILE_SWID_0_CONF_OFFSET + 1);
+	SX_PUT(inbox, profile->swid1_config_type.mask,
+			PROFILE_SWID_1_CONF_OFFSET);
+	SX_PUT(inbox, profile->swid1_config_type.properties,
+			PROFILE_SWID_1_CONF_OFFSET + 3);
+	temp_u8 = (u8)(profile->swid1_config_type.type << 4);
+	SX_PUT(inbox, temp_u8, PROFILE_SWID_1_CONF_OFFSET + 1);
+	SX_PUT(inbox, profile->swid2_config_type.mask,
+			PROFILE_SWID_2_CONF_OFFSET);
+	SX_PUT(inbox, profile->swid2_config_type.properties,
+			PROFILE_SWID_2_CONF_OFFSET + 3);
+	temp_u8 = (u8)(profile->swid2_config_type.type << 4);
+	SX_PUT(inbox, temp_u8, PROFILE_SWID_2_CONF_OFFSET + 1);
+	SX_PUT(inbox, profile->swid3_config_type.mask,
+			PROFILE_SWID_3_CONF_OFFSET);
+	SX_PUT(inbox, profile->swid3_config_type.properties,
+			PROFILE_SWID_3_CONF_OFFSET + 3);
+	temp_u8 = (u8)(profile->swid3_config_type.type << 4);
+	SX_PUT(inbox, temp_u8, PROFILE_SWID_3_CONF_OFFSET + 1);
+	SX_PUT(inbox, profile->swid4_config_type.mask,
+			PROFILE_SWID_4_CONF_OFFSET);
+	SX_PUT(inbox, profile->swid4_config_type.properties,
+			PROFILE_SWID_4_CONF_OFFSET + 3);
+	temp_u8 = (u8)(profile->swid4_config_type.type << 4);
+	SX_PUT(inbox, temp_u8, PROFILE_SWID_4_CONF_OFFSET + 1);
+	SX_PUT(inbox, profile->swid5_config_type.mask,
+			PROFILE_SWID_5_CONF_OFFSET);
+	SX_PUT(inbox, profile->swid5_config_type.properties,
+			PROFILE_SWID_5_CONF_OFFSET + 3);
+	temp_u8 = (u8)(profile->swid5_config_type.type << 4);
+	SX_PUT(inbox, temp_u8, PROFILE_SWID_5_CONF_OFFSET + 1);
+	SX_PUT(inbox, profile->swid6_config_type.mask,
+			PROFILE_SWID_6_CONF_OFFSET);
+	SX_PUT(inbox, profile->swid6_config_type.properties,
+			PROFILE_SWID_6_CONF_OFFSET + 3);
+	temp_u8 = (u8)(profile->swid6_config_type.type << 4);
+	SX_PUT(inbox, temp_u8, PROFILE_SWID_6_CONF_OFFSET + 1);
+	SX_PUT(inbox, profile->swid7_config_type.mask,
+			PROFILE_SWID_7_CONF_OFFSET);
+	SX_PUT(inbox, profile->swid7_config_type.properties,
+			PROFILE_SWID_7_CONF_OFFSET + 3);
+	temp_u8 = (u8)(profile->swid7_config_type.type << 4);
+	SX_PUT(inbox, temp_u8, PROFILE_SWID_7_CONF_OFFSET + 1);
+	SX_PUT(inbox, profile->reserved1, PROFILE_RESERVED1_OFFSET);
+
+	err = sx_cmd(dev, profile->dev_id, mailbox, 0, 1, SX_CMD_CONFIG_PROFILE,
+			SX_CMD_TIME_CLASS_A, CONFIG_PROFILE_MB_SIZE);
+
+	sx_free_cmd_mailbox(dev, mailbox);
+	return err;
+}
+
+int sx_ACCESS_REG_SSPR(struct sx_dev *dev, struct ku_access_sspr_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+	u8 tmp_val_u8 = 0;
+
+#define SSPR_REG_MASTER_BIT_OFFSET	0x14
+#define SSPR_REG_MASTER_BIT_N	7
+#define SSPR_REG_LOCAL_PORT_OFFSET	0x15
+#define SSPR_REG_SUB_PORT	0x16
+#define SSPR_REG_SYSTEM_PORT	0x1a
+#define SSPR_REG_LEN		0x03
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= SSPR_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+
+	tmp_val_u8 = 0;
+	tmp_val_u8 |= reg_data->sspr_reg.is_master ? (1 <<  SSPR_REG_MASTER_BIT_N) : 0;
+	SX_PUT(inbox, tmp_val_u8, SSPR_REG_MASTER_BIT_OFFSET);
+	SX_PUT(inbox, reg_data->sspr_reg.local_port, SSPR_REG_LOCAL_PORT_OFFSET);
+	SX_PUT(inbox, reg_data->sspr_reg.sub_port, SSPR_REG_SUB_PORT);
+	SX_PUT(inbox, reg_data->sspr_reg.system_port, SSPR_REG_SYSTEM_PORT);
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(SSPR_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(tmp_val_u8, outbox,
+			   SSPR_REG_MASTER_BIT_OFFSET);
+		if (tmp_val_u8 & (1 <<  SSPR_REG_MASTER_BIT_N)) {
+			reg_data->sspr_reg.is_master = 1;
+		}
+		SX_GET(reg_data->sspr_reg.local_port, outbox,
+				SSPR_REG_LOCAL_PORT_OFFSET);
+		SX_GET(reg_data->sspr_reg.sub_port, outbox,
+				SSPR_REG_SUB_PORT);
+		SX_GET(reg_data->sspr_reg.system_port, outbox,
+				SSPR_REG_SYSTEM_PORT);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_SSPR);
+
+int sx_ACCESS_REG_PPAD(struct sx_dev *dev, struct ku_access_ppad_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+	u16 mac_47_32 = 0;
+	u32	mac_31_8 = 0;
+
+#define PPAD_REG_BASE_MAC_47_32	0x16
+#define PPAD_REG_BASE_MAC_31_8	0x18
+#define PPAD_REG_LEN		0x05
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= PPAD_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+
+	mac_47_32 = (reg_data->ppad_reg.mac[0] << 8) | reg_data->ppad_reg.mac[1];
+	mac_31_8 = (reg_data->ppad_reg.mac[2] << 24) |
+		   (reg_data->ppad_reg.mac[3] << 16) |
+		   (reg_data->ppad_reg.mac[4] << 8); /* Last byte resevred (0) */
+	SX_PUT(inbox, mac_47_32, PPAD_REG_BASE_MAC_47_32);
+	SX_PUT(inbox, mac_47_32, PPAD_REG_BASE_MAC_31_8);
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(PPAD_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(mac_47_32, outbox,
+				PPAD_REG_BASE_MAC_47_32);
+		SX_GET(mac_31_8, outbox,
+				PPAD_REG_BASE_MAC_31_8);
+		reg_data->ppad_reg.mac[5] = (mac_31_8) & 0xff;
+		reg_data->ppad_reg.mac[4] = (mac_31_8 >> 8) & 0xff;
+		reg_data->ppad_reg.mac[3] = (mac_31_8 >> 16) & 0xff;
+		reg_data->ppad_reg.mac[2] = (mac_31_8 >> 24)& 0xff;
+		reg_data->ppad_reg.mac[1] = mac_47_32 & 0xff;
+		reg_data->ppad_reg.mac[0] = (mac_47_32 >> 8) & 0xff;
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_PPAD);
+
+
+int sx_ACCESS_REG_SPMCR(struct sx_dev *dev, struct ku_access_spmcr_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define SPMCR_REG_SWID_OFFSET	0x14
+#define SPMCR_REG_LOCAL_PORT_OFFSET	0x15
+#define SPMCR_REG_MAX_SUB_PORT_OFFSET	0x16
+#define SPMCR_REG_BASE_STAG_VID_OFFSET	0x1a
+#define SPMCR_REG_LEN		0x03
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= SPMCR_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->spmcr_reg.swid, SPMCR_REG_SWID_OFFSET);
+	SX_PUT(inbox, reg_data->spmcr_reg.local_port, SPMCR_REG_LOCAL_PORT_OFFSET);
+	SX_PUT(inbox, reg_data->spmcr_reg.max_sub_port, SPMCR_REG_MAX_SUB_PORT_OFFSET);
+	SX_PUT(inbox, reg_data->spmcr_reg.base_stag_vid, SPMCR_REG_BASE_STAG_VID_OFFSET);
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(SPMCR_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->spmcr_reg.swid, outbox,
+				SPMCR_REG_SWID_OFFSET);
+		SX_GET(reg_data->spmcr_reg.local_port, outbox,
+				SPMCR_REG_LOCAL_PORT_OFFSET);
+		SX_GET(reg_data->spmcr_reg.max_sub_port, outbox,
+				SPMCR_REG_MAX_SUB_PORT_OFFSET);
+		SX_GET(reg_data->spmcr_reg.swid, outbox,
+				SPMCR_REG_BASE_STAG_VID_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_SPMCR);
+
+int sx_ACCESS_REG_PBMC(struct sx_dev *dev, struct ku_access_pbmc_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err, i;
+	u16 type_len = 0;
+
+#define PBMC_REG_LOCAL_PORT_OFFSET	0x15
+#define PBMC_REG_XOF_TIMER_VAL_OFFSET	0x19
+#define PBMC_REG_XOF_REFRESH_OFFSET	0x1b
+#define PBMC_REG_PORT_BUFF_SIZE_OFFSET	0x1e
+#define PBMC_REG_BUFF_0_OFFSET	0x20
+#define PBMC_REG_LEN		24
+
+#define BUFF_SIZE_OFFSET	2
+#define BUFF_XOFF_OFFSET	4
+#define BUFF_XON_OFFSET		6
+
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= PBMC_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->pbmc_reg.local_port, PBMC_REG_LOCAL_PORT_OFFSET);
+	SX_PUT(inbox, reg_data->pbmc_reg.xof_timer_value, PBMC_REG_XOF_TIMER_VAL_OFFSET);
+	SX_PUT(inbox, reg_data->pbmc_reg.xof_refresh, PBMC_REG_XOF_REFRESH_OFFSET);
+	for (i=0;i<10;i++) {
+		int buff_offset = PBMC_REG_BUFF_0_OFFSET + 8*i;
+		SX_PUT(inbox, reg_data->pbmc_reg.buffer[i].size, buff_offset + BUFF_SIZE_OFFSET);
+		SX_PUT(inbox, reg_data->pbmc_reg.buffer[i].xof_threshold, buff_offset + BUFF_XOFF_OFFSET);
+		SX_PUT(inbox, reg_data->pbmc_reg.buffer[i].xon_threshold, buff_offset + BUFF_XON_OFFSET);
+	}
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(PBMC_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->pbmc_reg.local_port, outbox,
+				PBMC_REG_LOCAL_PORT_OFFSET);
+		SX_GET(reg_data->pbmc_reg.xof_timer_value, outbox,
+				PBMC_REG_XOF_TIMER_VAL_OFFSET);
+		SX_GET(reg_data->pbmc_reg.xof_refresh, outbox,
+				PBMC_REG_XOF_REFRESH_OFFSET);
+			for (i=0;i<10;i++) {
+				int buff_offset = PBMC_REG_BUFF_0_OFFSET + 8*i;
+				SX_GET(reg_data->pbmc_reg.buffer[i].size, outbox,buff_offset + BUFF_SIZE_OFFSET);
+				SX_GET(reg_data->pbmc_reg.buffer[i].xof_threshold, outbox,buff_offset + BUFF_XOFF_OFFSET);
+				SX_GET(reg_data->pbmc_reg.buffer[i].xon_threshold, outbox,buff_offset + BUFF_XON_OFFSET);
+			}
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_PBMC);
+
+int sx_ACCESS_REG_PPTB(struct sx_dev *dev, struct ku_access_pptb_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+	u8 tmp_val_u8 = 0;
+
+#define PPTB_REG_LOCAL_PORT_OFFSET	0x15
+#define PPTB_REG_CM_UM_BITS_OFFSET	0x16
+#define PPTB_REG_UM_BIT_N	0
+#define PPTB_REG_CM_BIT_N	1
+#define PPTB_REG_PM_OFFSET	0x17
+#define PPTB_REG_PRIO_7_6_BUFF_OFFSET	0x18
+#define PPTB_REG_PRIO_5_4_BUFF_OFFSET	0x19
+#define PPTB_REG_PRIO_3_2_BUFF_OFFSET	0x1a
+#define PPTB_REG_PRIO_1_0_BUFF_OFFSET	0x1b
+#define PPTB_REG_CTRL_UNTAG_BUFF_OFFSET	0x1f
+#define PPTB_REG_LEN		0x05
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= PPTB_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->pptb_reg.local_port, PPTB_REG_LOCAL_PORT_OFFSET);
+	tmp_val_u8 = (reg_data->pptb_reg.cm << PPTB_REG_CM_BIT_N) | reg_data->pptb_reg.um;
+	SX_PUT(inbox, tmp_val_u8, PPTB_REG_CM_UM_BITS_OFFSET);
+	SX_PUT(inbox, reg_data->pptb_reg.pm, PPTB_REG_PM_OFFSET);
+	tmp_val_u8 = (reg_data->pptb_reg.prio_7_buff << 4) | (reg_data->pptb_reg.prio_6_buff & 0xf);
+	SX_PUT(inbox, tmp_val_u8, PPTB_REG_PRIO_7_6_BUFF_OFFSET);
+	tmp_val_u8 = (reg_data->pptb_reg.prio_5_buff << 4) | (reg_data->pptb_reg.prio_4_buff & 0xf);
+	SX_PUT(inbox, tmp_val_u8, PPTB_REG_PRIO_5_4_BUFF_OFFSET);
+	tmp_val_u8 = (reg_data->pptb_reg.prio_3_buff << 4) | (reg_data->pptb_reg.prio_2_buff & 0xf);
+	SX_PUT(inbox, tmp_val_u8, PPTB_REG_PRIO_3_2_BUFF_OFFSET);
+	tmp_val_u8 = (reg_data->pptb_reg.prio_1_buff << 4) | (reg_data->pptb_reg.prio_0_buff & 0xf);
+	SX_PUT(inbox, tmp_val_u8, PPTB_REG_PRIO_1_0_BUFF_OFFSET);
+	tmp_val_u8 = (reg_data->pptb_reg.ctrl_buff << 4) | (reg_data->pptb_reg.untagged_buff & 0xf);
+	SX_PUT(inbox, tmp_val_u8, PPTB_REG_CTRL_UNTAG_BUFF_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(PPTB_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->pptb_reg.local_port, outbox,
+				PPTB_REG_LOCAL_PORT_OFFSET);
+		SX_GET(tmp_val_u8, outbox,
+				PPTB_REG_CM_UM_BITS_OFFSET);
+		reg_data->pptb_reg.um = (tmp_val_u8 >> PPTB_REG_UM_BIT_N) & 1;
+		reg_data->pptb_reg.cm =	(tmp_val_u8 >> PPTB_REG_CM_BIT_N) & 1;
+		SX_GET(reg_data->pptb_reg.pm, outbox,
+				PPTB_REG_PM_OFFSET);
+		SX_GET(tmp_val_u8, outbox,
+				PPTB_REG_PRIO_7_6_BUFF_OFFSET);
+		reg_data->pptb_reg.prio_6_buff = tmp_val_u8 & 0xf;
+		reg_data->pptb_reg.prio_7_buff = (tmp_val_u8 >> 4)& 0xf;
+		SX_GET(tmp_val_u8, outbox,
+				PPTB_REG_PRIO_5_4_BUFF_OFFSET);
+		reg_data->pptb_reg.prio_4_buff = tmp_val_u8 & 0xf;
+		reg_data->pptb_reg.prio_5_buff = (tmp_val_u8 >> 4)& 0xf;
+		SX_GET(tmp_val_u8, outbox,
+				PPTB_REG_PRIO_3_2_BUFF_OFFSET);
+		reg_data->pptb_reg.prio_2_buff = tmp_val_u8 & 0xf;
+		reg_data->pptb_reg.prio_3_buff = (tmp_val_u8 >> 4)& 0xf;
+		SX_GET(tmp_val_u8, outbox,
+				PPTB_REG_PRIO_1_0_BUFF_OFFSET);
+		reg_data->pptb_reg.prio_0_buff = tmp_val_u8 & 0xf;
+		reg_data->pptb_reg.prio_1_buff = (tmp_val_u8 >> 4)& 0xf;
+		SX_GET(tmp_val_u8, outbox,
+				PPTB_REG_CTRL_UNTAG_BUFF_OFFSET);
+		reg_data->pptb_reg.untagged_buff = tmp_val_u8 & 0xf;
+		reg_data->pptb_reg.ctrl_buff = (tmp_val_u8 >> 4)& 0xf;
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_PPTB);
+
+int sx_ACCESS_REG_SMID(struct sx_dev *dev, struct ku_access_smid_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err,i,j;
+	u16 type_len = 0;
+	u32 tmp_val_u32;
+	u32 tmp_mask_u32;
+
+#define SMID_REG_SWID_OFFSET	0x14
+#define SMID_REG_MID_OFFSET		0x16
+
+#define SMID_REG_PORTS_255_224_OFFSET		0x34
+#define SMID_REG_PORTS_223_192_OFFSET		0x38
+#define SMID_REG_PORTS_191_160_OFFSET		0x3c
+#define SMID_REG_PORTS_159_128_OFFSET		0x40
+#define SMID_REG_PORTS_127_96_OFFSET		0x44
+#define SMID_REG_PORTS_95_64_OFFSET			0x48
+#define SMID_REG_PORTS_63_32_OFFSET			0x4c
+#define SMID_REG_PORTS_31_0_OFFSET			 0x50
+
+#define SMID_REG_PORTS_255_224_MASK_OFFSET		0x54
+#define SMID_REG_PORTS_223_192_MASK_OFFSET		0x58
+#define SMID_REG_PORTS_191_160_MASK_OFFSET		0x5c
+#define SMID_REG_PORTS_159_128_MASK_OFFSET		0x60
+#define SMID_REG_PORTS_127_96_MASK_OFFSET		0x64
+#define SMID_REG_PORTS_95_64_MASK_OFFSET			0x68
+#define SMID_REG_PORTS_63_32_MASK_OFFSET			0x6c
+#define SMID_REG_PORTS_31_0_MASK_OFFSET			 0x70
+
+#define SMID_REG_LEN		0x19
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= SMID_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->smid_reg.swid, SMID_REG_SWID_OFFSET);
+	SX_PUT(inbox, reg_data->smid_reg.mid, SMID_REG_MID_OFFSET);
+
+	for (j=0;j<256;) {
+		tmp_val_u32 = 0;
+		tmp_mask_u32 = 0;
+		for (i=0;i<32;i++) {
+			tmp_val_u32 |= (reg_data->smid_reg.ports_bitmap[j+i]? (1<<i):0);
+			tmp_mask_u32 |= (reg_data->smid_reg.mask_bitmap[j+i]? (1<<i):0);
+		}
+
+		if (j==0) {
+			SX_PUT(inbox, tmp_val_u32, SMID_REG_PORTS_31_0_OFFSET);
+			SX_PUT(inbox, tmp_mask_u32, SMID_REG_PORTS_31_0_MASK_OFFSET);
+		}
+		else if (j==32) {
+			SX_PUT(inbox, tmp_val_u32, SMID_REG_PORTS_63_32_OFFSET);
+			SX_PUT(inbox, tmp_mask_u32, SMID_REG_PORTS_63_32_MASK_OFFSET);
+		}
+		else if (j==64) {
+			SX_PUT(inbox, tmp_val_u32, SMID_REG_PORTS_95_64_OFFSET);
+			SX_PUT(inbox, tmp_mask_u32, SMID_REG_PORTS_95_64_MASK_OFFSET);
+		}
+		else if (j==96) {
+			SX_PUT(inbox, tmp_val_u32, SMID_REG_PORTS_127_96_OFFSET);
+			SX_PUT(inbox, tmp_mask_u32, SMID_REG_PORTS_127_96_MASK_OFFSET);
+		}
+		else if (j==128) {
+			SX_PUT(inbox, tmp_val_u32, SMID_REG_PORTS_159_128_OFFSET);
+			SX_PUT(inbox, tmp_mask_u32, SMID_REG_PORTS_159_128_MASK_OFFSET);
+		}
+		else if (j==160) {
+			SX_PUT(inbox, tmp_val_u32, SMID_REG_PORTS_191_160_OFFSET);
+			SX_PUT(inbox, tmp_mask_u32, SMID_REG_PORTS_191_160_MASK_OFFSET);
+		}
+		else if (j==192) {
+			SX_PUT(inbox, tmp_val_u32, SMID_REG_PORTS_223_192_OFFSET);
+			SX_PUT(inbox, tmp_mask_u32, SMID_REG_PORTS_223_192_MASK_OFFSET);
+		}
+		else if (j==224) {
+			SX_PUT(inbox, tmp_val_u32, SMID_REG_PORTS_255_224_OFFSET);
+			SX_PUT(inbox, tmp_mask_u32, SMID_REG_PORTS_255_224_MASK_OFFSET);
+		}
+
+		j+=32;
+	}
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(SMID_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->smid_reg.swid, outbox,
+				SMID_REG_SWID_OFFSET);
+		SX_GET(reg_data->smid_reg.mid, outbox,
+				SMID_REG_MID_OFFSET);
+		/*
+		SX_GET(tmp_val_u32, outbox,SMID_REG_PORTS_255_224_OFFSET);
+		SX_GET(tmp_val_u32, outbox,SMID_REG_PORTS_223_192_OFFSET);
+		SX_GET(tmp_val_u32, outbox,SMID_REG_PORTS_191_160_OFFSET);
+		SX_GET(tmp_val_u32, outbox,SMID_REG_PORTS_159_128_OFFSET);
+		SX_GET(tmp_val_u32, outbox,SMID_REG_PORTS_127_96_OFFSET);
+		SX_GET(tmp_val_u32, outbox,SMID_REG_PORTS_95_64_OFFSET);
+		*/
+		SX_GET(tmp_val_u32, outbox,SMID_REG_PORTS_63_32_OFFSET);
+		for (i=0;i<32;i++) {
+			if (tmp_val_u32 & (1<<i)) {
+				reg_data->smid_reg.ports_bitmap[32 + i] = 1;
+			}
+		}
+		SX_GET(tmp_val_u32, outbox,SMID_REG_PORTS_31_0_OFFSET);
+		for (i=0;i<32;i++) {
+			if (tmp_val_u32 & (1<<i)) {
+				reg_data->smid_reg.ports_bitmap[0 + i] = 1;
+			}
+		}
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_SMID);
+
+int sx_ACCESS_REG_SPMS(struct sx_dev *dev, struct ku_access_spms_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+	u32	tmp_val_u32;
+
+	printk("%s() ENTER : dev_id: %d, 	tlv: op_class: %d, r: %d, reg_id: %d, method: %d, len: %d \n"
+				"			spms_reg: lport: 0x%x, state[0: 0x%x,1: 0x%x,14: 0x%x,15: 0x%x]\n",
+		   __func__,
+		   reg_data->dev_id,
+		   reg_data->op_tlv.op_class,
+		   reg_data->op_tlv.r,
+		   reg_data->op_tlv.register_id,
+		   reg_data->op_tlv.method,
+		   reg_data->op_tlv.length,
+
+		   reg_data->spms_reg.local_port,
+		   reg_data->spms_reg.state[0],
+		   reg_data->spms_reg.state[1],
+		   reg_data->spms_reg.state[14],
+		   reg_data->spms_reg.state[15]
+		   );
+
+#define SPMS_REG_LOCAL_PORT_OFFSET	0x15
+#define SPMS_REG_VLAN_63_48_STP_STATE_OFFSET	0x18
+#define SPMS_REG_VLAN_47_32_STP_STATE_OFFSET	0x1c
+#define SPMS_REG_VLAN_31_16_STP_STATE_OFFSET	0x20
+#define SPMS_REG_VLAN_15_0_STP_STATE_OFFSET		0x24
+#define SPMS_REG_LEN		0x06
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= SPMS_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->spms_reg.local_port, SPMS_REG_LOCAL_PORT_OFFSET);
+		tmp_val_u32 = 0x0C;
+		SX_PUT(inbox, tmp_val_u32, SPMS_REG_VLAN_15_0_STP_STATE_OFFSET);
+		tmp_val_u32 = 0;
+		SX_PUT(inbox, tmp_val_u32, SPMS_REG_VLAN_31_16_STP_STATE_OFFSET);
+		tmp_val_u32 = 0;
+		SX_PUT(inbox, tmp_val_u32, SPMS_REG_VLAN_47_32_STP_STATE_OFFSET);
+		tmp_val_u32 = 0;
+		SX_PUT(inbox, tmp_val_u32, SPMS_REG_VLAN_63_48_STP_STATE_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(SPMS_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->spms_reg.local_port, outbox,
+				SPMS_REG_LOCAL_PORT_OFFSET);
+		SX_GET(tmp_val_u32, outbox, SPMS_REG_VLAN_15_0_STP_STATE_OFFSET);
+		if (tmp_val_u32 & 0xc) {
+			reg_data->spms_reg.state[1]  = tmp_val_u32 & 0x3;
+		}
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_SPMS);
+
+int sx_ACCESS_REG_SPVID(struct sx_dev *dev, struct ku_access_spvid_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define SPVID_REG_LOCAL_PORT_OFFSET		0x15
+#define SPVID_REG_SUB_PORT_OFFSET			0x16
+#define SPVID_REG_PORT_VID_OFFSET			0x1a
+#define SPVID_REG_LEN		0x03
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= SPVID_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->spvid_reg.local_port, SPVID_REG_LOCAL_PORT_OFFSET);
+	SX_PUT(inbox, reg_data->spvid_reg.sub_port, SPVID_REG_SUB_PORT_OFFSET);
+	SX_PUT(inbox, reg_data->spvid_reg.port_default_vid, SPVID_REG_PORT_VID_OFFSET);
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(SPVID_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->spvid_reg.local_port, outbox,
+				SPVID_REG_LOCAL_PORT_OFFSET);
+		SX_GET(reg_data->spvid_reg.sub_port, outbox,
+				SPVID_REG_SUB_PORT_OFFSET);
+		SX_GET(reg_data->spvid_reg.port_default_vid, outbox,
+				SPVID_REG_PORT_VID_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_SPVID);
+
+int sx_ACCESS_REG_SFGC(struct sx_dev *dev, struct ku_access_sfgc_reg *reg_data)
+{
+	return -EINVAL; /* The register had changed and not sure we need CMD IFC for it */
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_SFGC);
+
+int sx_ACCESS_REG_SFD(struct sx_dev *dev, struct ku_access_sfd_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err,i;
+	u16 type_len = 0;
+	u8	tmp_val_u8;
+	u16	tmp_val_u16;
+	u32	tmp_val_u32;
+	u8 sfd_reg_len;
+
+
+#define SFD_REG_SWID_OFFSET							  0x14
+#define SFD_REG_OP_NEXT_LOCATOR_OFFSET	0x18
+#define SFD_REG_NUM_RECORDS_OFFSET			0x1f
+#define SFD_REG_FDB_RECORD_0_OFFSET			 0x24
+#define SFD_REG_WITH_SINGLE_RECORD_LEN	  0x9
+
+	sfd_reg_len = SFD_REG_WITH_SINGLE_RECORD_LEN +
+		(reg_data->sfd_reg.num_records - 1)*4;
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= sfd_reg_len;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->sfd_reg.swid, SFD_REG_SWID_OFFSET);
+	tmp_val_u32 = ((reg_data->sfd_reg.operation & 0x3) << 30) |
+							 (reg_data->sfd_reg.record_locator & ~(0x3 << 30));
+	SX_PUT(inbox, tmp_val_u32, SFD_REG_OP_NEXT_LOCATOR_OFFSET);
+	SX_PUT(inbox, reg_data->sfd_reg.num_records, SFD_REG_NUM_RECORDS_OFFSET);
+
+	for (i=0;i<reg_data->sfd_reg.num_records;i++) {
+		switch (reg_data->sfd_reg.sfd_type[i]) {
+		case SFD_TYPE_UNICAST:
+			{
+	#define SFD_REG_FDB_RECORD_UC_SWID_OFF	0
+	#define SFD_REG_FDB_RECORD_UC_TYPE_POLICY_OFF	1
+	#define SFD_REG_FDB_RECORD_UC_MAC_47_32_OFF	2
+	#define SFD_REG_FDB_RECORD_UC_MAC_31_0_OFF	4
+	#define SFD_REG_FDB_RECORD_UC_SUB_PORT_OFF	9
+	#define SFD_REG_FDB_RECORD_UC_FID_VID_OFF	10
+	#define SFD_REG_FDB_RECORD_UC_ACTION_OFF	12
+	#define SFD_REG_FDB_RECORD_UC_SYSTEM_PORT_OFF	14
+				int record_offset = SFD_REG_FDB_RECORD_0_OFFSET + 16*i;
+				SX_PUT(inbox, reg_data->sfd_reg.swid, record_offset +SFD_REG_FDB_RECORD_UC_SWID_OFF );
+				tmp_val_u8	= ((reg_data->sfd_reg.sfd_type[i] & 0xF)<<4) |
+										(reg_data->sfd_reg.sfd_data_type[i].uc.policy << 2);
+				SX_PUT(inbox, tmp_val_u8, record_offset +SFD_REG_FDB_RECORD_UC_TYPE_POLICY_OFF );
+				tmp_val_u16	= (reg_data->sfd_reg.sfd_data_type[i].uc.mac.ether_addr_octet[5] << 8) |
+										 reg_data->sfd_reg.sfd_data_type[i].uc.mac.ether_addr_octet[4];
+				SX_PUT(inbox, tmp_val_u16	, record_offset +SFD_REG_FDB_RECORD_UC_MAC_47_32_OFF );
+				tmp_val_u32	= (reg_data->sfd_reg.sfd_data_type[i].uc.mac.ether_addr_octet[3] << 24) |
+										 (reg_data->sfd_reg.sfd_data_type[i].uc.mac.ether_addr_octet[2] << 16) |
+										(reg_data->sfd_reg.sfd_data_type[i].uc.mac.ether_addr_octet[1] << 8) |
+										reg_data->sfd_reg.sfd_data_type[i].uc.mac.ether_addr_octet[0];
+				SX_PUT(inbox, tmp_val_u32, record_offset +SFD_REG_FDB_RECORD_UC_MAC_31_0_OFF );
+				SX_PUT(inbox, reg_data->sfd_reg.sfd_data_type[i].uc.sub_port, record_offset +SFD_REG_FDB_RECORD_UC_SUB_PORT_OFF );
+				SX_PUT(inbox, reg_data->sfd_reg.sfd_data_type[i].uc.fid_vid_type.vid, record_offset +SFD_REG_FDB_RECORD_UC_FID_VID_OFF );
+				tmp_val_u8	= reg_data->sfd_reg.sfd_data_type[i].uc.action << 4;
+				SX_PUT(inbox, tmp_val_u8, record_offset +SFD_REG_FDB_RECORD_UC_ACTION_OFF );
+				SX_PUT(inbox, reg_data->sfd_reg.sfd_data_type[i].uc.system_port, record_offset +SFD_REG_FDB_RECORD_UC_SYSTEM_PORT_OFF );
+				break;
+			}
+		case SFD_TYPE_UNICAST_LAG:
+			printk("DEMO func: skipping FDB LAG record add over i2c\n");
+			break;
+		case SFD_TYPE_MULTICAST:
+			printk("DEMO func: skipping FDB MC record add over i2c\n");
+			break;
+		}
+	}
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(sfd_reg_len /*SFD_REG_LEN*/));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		int record_offset = SFD_REG_FDB_RECORD_0_OFFSET;
+
+		printk("DEMO func: FDB record get over i2c will get only 1 record \n");
+
+		SX_GET(reg_data->sfd_reg.swid, outbox, SFD_REG_SWID_OFFSET);
+		SX_GET(tmp_val_u32, outbox,SFD_REG_OP_NEXT_LOCATOR_OFFSET);
+		reg_data->sfd_reg.operation = (tmp_val_u32 >> 30) & 0x3;
+		reg_data->sfd_reg.record_locator = tmp_val_u32 & ~(0x3 << 30);
+		SX_GET(reg_data->sfd_reg.num_records, outbox, SFD_REG_NUM_RECORDS_OFFSET);
+
+		SX_GET(reg_data->sfd_reg.swid, outbox, record_offset +SFD_REG_FDB_RECORD_UC_SWID_OFF );
+
+		SX_GET(tmp_val_u8, outbox, record_offset +SFD_REG_FDB_RECORD_UC_TYPE_POLICY_OFF );
+		reg_data->sfd_reg.sfd_type[0] = (tmp_val_u8	>> 4) & 0xF;
+		reg_data->sfd_reg.sfd_data_type[0].uc.policy = (tmp_val_u8 >> 2) & 0x3;
+
+		if (reg_data->sfd_reg.sfd_type[0] != SFD_TYPE_UNICAST) {
+			printk("DEMO func: FDB record get for sfd_type = %d ( != SFD_TYPE_UNICAST) not supported !!! \n",
+				   reg_data->sfd_reg.sfd_type[0] );
+			goto out;
+		}
+
+		SX_GET(tmp_val_u16, outbox, record_offset +SFD_REG_FDB_RECORD_UC_MAC_47_32_OFF );
+		reg_data->sfd_reg.sfd_data_type[i].uc.mac.ether_addr_octet[5] = (tmp_val_u16 >> 8) & 0xff;
+		reg_data->sfd_reg.sfd_data_type[i].uc.mac.ether_addr_octet[4] = tmp_val_u16 & 0xff;
+
+		SX_GET(tmp_val_u32, outbox, record_offset +SFD_REG_FDB_RECORD_UC_MAC_31_0_OFF );
+		reg_data->sfd_reg.sfd_data_type[i].uc.mac.ether_addr_octet[3] = (tmp_val_u32>> 24) & 0xff;
+		reg_data->sfd_reg.sfd_data_type[i].uc.mac.ether_addr_octet[2] = (tmp_val_u32>> 16) & 0xff;
+		reg_data->sfd_reg.sfd_data_type[i].uc.mac.ether_addr_octet[1] = (tmp_val_u32>> 8) & 0xff;
+		reg_data->sfd_reg.sfd_data_type[i].uc.mac.ether_addr_octet[0] = tmp_val_u32 & 0xff;
+
+		SX_GET(reg_data->sfd_reg.sfd_data_type[i].uc.sub_port, outbox, record_offset +SFD_REG_FDB_RECORD_UC_SUB_PORT_OFF );
+		SX_GET(reg_data->sfd_reg.sfd_data_type[i].uc.fid_vid_type.vid, outbox, record_offset +SFD_REG_FDB_RECORD_UC_FID_VID_OFF );
+
+		SX_GET(tmp_val_u8, outbox, record_offset +SFD_REG_FDB_RECORD_UC_ACTION_OFF );
+		reg_data->sfd_reg.sfd_data_type[i].uc.action = (tmp_val_u8 >> 4) & 0xf;
+
+		SX_GET(reg_data->sfd_reg.sfd_data_type[i].uc.system_port, outbox, record_offset +SFD_REG_FDB_RECORD_UC_SYSTEM_PORT_OFF );
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_SFD);
+
+int sx_ACCESS_REG_OEPFT(struct sx_dev *dev, struct ku_access_oepft_reg *reg_data)
+{
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+	u32 sr_flow_number = 0;
+
+#define OEPFT_REG_SR_FLOW_NUMBER_OFFSET	0x14
+#define OEPFT_REG_CPU_TCLASS_OFFSET	0x19
+#define OEPFT_REG_IF_OFFSET		0x1b
+#define OEPFT_REG_MAC_OFFSET		0x1c
+#define OEPFT_REG_LEN			0x05
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= OEPFT_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	sr_flow_number = reg_data->oepft_reg.flow_number & 0xffffff;
+	if (reg_data->oepft_reg.sr)
+		sr_flow_number |= 0x80000000;
+	SX_PUT(inbox, sr_flow_number, OEPFT_REG_SR_FLOW_NUMBER_OFFSET);
+	SX_PUT(inbox, reg_data->oepft_reg.cpu_tclass,
+			OEPFT_REG_CPU_TCLASS_OFFSET);
+	SX_PUT(inbox, reg_data->oepft_reg.interface, OEPFT_REG_IF_OFFSET);
+	SX_PUT(inbox, reg_data->oepft_reg.mac, OEPFT_REG_MAC_OFFSET);
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(OEPFT_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(sr_flow_number, outbox, OEPFT_REG_SR_FLOW_NUMBER_OFFSET);
+		reg_data->oepft_reg.flow_number = sr_flow_number & 0xffffff;
+		reg_data->oepft_reg.sr = ((sr_flow_number >> 31) & 0x1);
+		SX_GET(reg_data->oepft_reg.cpu_tclass, outbox,
+				OEPFT_REG_CPU_TCLASS_OFFSET);
+		SX_GET(reg_data->oepft_reg.interface, outbox,
+				OEPFT_REG_IF_OFFSET);
+		SX_GET(reg_data->oepft_reg.mac, outbox,
+				OEPFT_REG_MAC_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_OEPFT);
+
+int sx_ACCESS_REG_PLBF(struct sx_dev *dev, struct ku_access_plbf_reg *reg_data)
+{
+
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define PLBF_LOCAL_PORT_OFFSET		0x15
+#define PLBF_LBF_MODE_OFFSET		0x17
+#define PLBF_REG_LEN			0x03
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= PLBF_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->plbf_reg.port, PLBF_LOCAL_PORT_OFFSET);
+	SX_PUT(inbox, reg_data->plbf_reg.lbf_mode, PLBF_LBF_MODE_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(PAOS_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { // 0x01 = Query
+		SX_GET(reg_data->plbf_reg.port, outbox,PLBF_LOCAL_PORT_OFFSET);
+		SX_GET(reg_data->plbf_reg.lbf_mode, outbox, PLBF_LBF_MODE_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+        return 0;
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_PLBF);
+
+int sx_ACCESS_REG_SGCR(struct sx_dev *dev, struct ku_access_sgcr_reg *reg_data)
+{
+
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define SGCR_LLB_OFFSET		0x1b
+#define SGCR_REG_LEN			0x15
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= SGCR_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->sgcr_reg.llb, SGCR_LLB_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(SGCR_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { // 0x01 = Query
+		SX_GET(reg_data->sgcr_reg.llb, outbox,SGCR_LLB_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+        return 0;
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_SGCR);
+
+int sx_ACCESS_REG_MSCI(struct sx_dev *dev, struct ku_access_msci_reg *reg_data)
+{
+
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+
+#define MSCI_INDEX_OFFSET		0x17
+#define MSCI_VERSION_OFFSET		0x18
+#define MSCI_REG_LEN			0x5
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= MSCI_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->msci_reg.index, MSCI_INDEX_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(MSCI_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { // 0x01 = Query
+		SX_GET(reg_data->msci_reg.index, outbox, MSCI_INDEX_OFFSET);
+		SX_GET(reg_data->msci_reg.version, outbox, MSCI_VERSION_OFFSET);
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+        return 0;
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_MSCI);
+
+int sx_ACCESS_REG_MRSR(struct sx_dev *dev, struct ku_access_mrsr_reg *reg_data)
+{
+
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err;
+	u16 type_len = 0;
+	u8 tmp = 0;
+
+#define MRSR_COMMAND_OFFSET		0x17
+#define MRSR_REG_LEN			0x3
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= MRSR_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	tmp = (u8)reg_data->mrsr_reg.command;
+	SX_PUT(inbox, tmp, MRSR_COMMAND_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(MRSR_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+
+        return 0;
+}
+
+EXPORT_SYMBOL(sx_ACCESS_REG_MRSR);
+
+int sx_ACCESS_REG_MPSC(struct sx_dev *dev, struct ku_access_mpsc_reg *reg_data)
+{
+
+	struct sx_cmd_mailbox *in_mailbox;
+	struct sx_cmd_mailbox *out_mailbox;
+	u32 *inbox;
+	u32 *outbox;
+	int err = 0;
+	u16 type_len = 0;
+	u8 tmp = 0;
+
+#define REG_LOCAL_PORT_OFFSET		0x15
+#define REG_C_E_OFFSET			REG_LOCAL_PORT_OFFSET + 3
+#define REG_C_BIT_N			7
+#define REG_E_BIT_N			6
+#define REG_RATE_OFFSET                 REG_C_E_OFFSET + 4
+#define REG_COUNT_DROP_OFFSET           REG_RATE_OFFSET + 4
+#define MPSC_REG_LEN			0x6
+
+	in_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(in_mailbox))
+		return PTR_ERR(in_mailbox);
+
+	out_mailbox = sx_alloc_cmd_mailbox(dev, reg_data->dev_id);
+	if (IS_ERR(out_mailbox)) {
+		err = PTR_ERR(out_mailbox);
+		goto out_free;
+	}
+
+	inbox = in_mailbox->buf;
+	memset(inbox, 0, SX_MAILBOX_SIZE);
+	outbox = out_mailbox->buf;
+
+	set_opoeration_tlv(inbox, &reg_data->op_tlv);
+	type_len = REG_TLV_TYPE << 11;
+	type_len |= MPSC_REG_LEN;
+	SX_PUT(inbox, type_len, REG_TLV_OFFSET);
+	SX_PUT(inbox, reg_data->mpsc_reg.local_port, REG_LOCAL_PORT_OFFSET);
+	tmp = 0;
+	tmp |= reg_data->mpsc_reg.clear_count ? (1 << REG_C_BIT_N) : 0;
+	tmp |= reg_data->mpsc_reg.enable ? (1 << REG_E_BIT_N) : 0;
+	SX_PUT(inbox, tmp, REG_C_E_OFFSET);
+	SX_PUT(inbox, reg_data->mpsc_reg.rate, REG_RATE_OFFSET);
+
+	err = sx_cmd_box(dev, reg_data->dev_id, in_mailbox, out_mailbox, 0, 0,
+			SX_CMD_ACCESS_REG, SX_CMD_TIME_CLASS_A,
+			IN_MB_SIZE(MPSC_REG_LEN));
+	if (err)
+		goto out;
+
+	get_operation_tlv(outbox, &reg_data->op_tlv);
+	if (reg_data->op_tlv.method == 0x01) { /* 0x01 = Query */
+		SX_GET(reg_data->mpsc_reg.local_port, outbox,
+				REG_LOCAL_PORT_OFFSET);
+		SX_GET(reg_data->mpsc_reg.rate, outbox,
+				REG_RATE_OFFSET);
+		SX_GET(reg_data->mpsc_reg.count_drops, outbox,
+				REG_COUNT_DROP_OFFSET);
+
+	}
+
+out:
+	sx_free_cmd_mailbox(dev, out_mailbox);
+out_free:
+	sx_free_cmd_mailbox(dev, in_mailbox);
+	return err;
+}
+EXPORT_SYMBOL(sx_ACCESS_REG_MPSC);
diff --git a/linux/drivers/hwmon/mellanox/fw.h b/linux/drivers/hwmon/mellanox/fw.h
new file mode 100644
index 0000000..67624f3
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/fw.h
@@ -0,0 +1,25 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+#ifndef SX_FW_H
+#define SX_FW_H
+
+#include <linux/mlx_sx/device.h>
+#include "icm.h"
+
+int sx_SET_PROFILE(struct sx_dev *dev, struct ku_profile *profile);
+int sx_QUERY_FW_2(struct sx_dev *dev, int sx_dev_id);
+int sx_MAP_FA(struct sx_dev *dev, struct sx_icm *icm);
+int sx_UNMAP_FA(struct sx_dev *dev);
+
+#endif /* SX_FW_H */
diff --git a/linux/drivers/hwmon/mellanox/ib.h b/linux/drivers/hwmon/mellanox/ib.h
new file mode 100644
index 0000000..91a9c67
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/ib.h
@@ -0,0 +1,62 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+#ifndef SX_IB_H
+#define SX_IB_H
+
+
+struct ib_header_lrh {
+	u8  	vl_lver;
+	u8  	sl_lnh;
+	__be16 	dlid;
+	__be16 	packet_length;
+	__be16 	slid;
+};
+
+struct ib_header_bth {
+	u8  	opcode;
+	u8  	se_m_pad_tver;
+	__be16 	pkey;
+	__be32 	dest_qp;
+	__be32 	a_psn;
+};
+
+struct ib_header_deth {
+	__be32 	qkey;
+	__be32 	src_qp;
+};
+
+struct ib_header_grh {
+	__be32 	ipver_tclass_flowLabel;
+	__be16 	pay_len;
+	u8  	nxt_hdr;
+	u8  	hop_lmt;
+	u8  	sgid[16];
+	u8  	dgid[16];
+};
+
+struct ib_header_unicast {
+	struct ib_header_lrh 	lrh;
+	struct ib_header_bth 	bth;
+	struct ib_header_deth 	deth;
+};
+
+struct ib_header_multicast {
+	struct ib_header_lrh 	lrh;
+	struct ib_header_grh 	grh;
+	struct ib_header_bth 	bth;
+	struct ib_header_deth 	deth;
+};
+
+
+#endif /* SX_IB_H */
diff --git a/linux/drivers/hwmon/mellanox/icm.c b/linux/drivers/hwmon/mellanox/icm.c
new file mode 100644
index 0000000..f0d8b40
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/icm.c
@@ -0,0 +1,185 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+#include <linux/init.h>
+#include <linux/errno.h>
+#include <linux/mm.h>
+#include <linux/scatterlist.h>
+#include <linux/mlx_sx/cmd.h>
+#include "sx.h"
+#include "icm.h"
+#include "fw.h"
+
+/*
+ * We allocate in as big chunks as we can, up to a maximum of 256 KB
+ * per chunk.
+ */
+enum {
+	SX_ICM_ALLOC_SIZE	= 1 << 18,
+	SX_TABLE_CHUNK_SIZE	= 1 << 18
+};
+
+static void sx_free_icm_pages(struct sx_dev *dev, struct sx_icm_chunk *chunk)
+{
+	int i;
+
+	if (chunk->nsg > 0)
+		pci_unmap_sg(dev->pdev, chunk->mem, chunk->npages,
+			     PCI_DMA_BIDIRECTIONAL);
+
+	for (i = 0; i < chunk->npages; ++i)
+		__free_pages(sg_page(&chunk->mem[i]),
+			     get_order(chunk->mem[i].length));
+}
+
+static void sx_free_icm_coherent(struct sx_dev *dev, struct sx_icm_chunk *chunk)
+{
+	int i;
+
+	for (i = 0; i < chunk->npages; ++i)
+		dma_free_coherent(&dev->pdev->dev, chunk->mem[i].length,
+				  lowmem_page_address(sg_page(&chunk->mem[i])),
+				  sg_dma_address(&chunk->mem[i]));
+}
+
+void sx_free_icm(struct sx_dev *dev, struct sx_icm *icm, int coherent)
+{
+	struct sx_icm_chunk *chunk, *tmp;
+
+	if (!icm)
+		return;
+
+	list_for_each_entry_safe(chunk, tmp, &icm->chunk_list, list) {
+		if (coherent)
+			sx_free_icm_coherent(dev, chunk);
+		else
+			sx_free_icm_pages(dev, chunk);
+
+		kfree(chunk);
+	}
+
+	kfree(icm);
+}
+
+static int sx_alloc_icm_pages(struct scatterlist *mem, int order,
+				gfp_t gfp_mask)
+{
+	struct page *page;
+
+	page = alloc_pages(gfp_mask, order);
+	if (!page)
+		return -ENOMEM;
+
+	sg_set_page(mem, page, PAGE_SIZE << order, 0);
+	return 0;
+}
+
+static int sx_alloc_icm_coherent(struct device *dev, struct scatterlist *mem,
+				    int order, gfp_t gfp_mask)
+{
+	void *buf = dma_alloc_coherent(dev, PAGE_SIZE << order,
+				       &sg_dma_address(mem), gfp_mask);
+	if (!buf)
+		return -ENOMEM;
+
+	sg_set_buf(mem, buf, PAGE_SIZE << order);
+	BUG_ON(mem->offset);
+	sg_dma_len(mem) = PAGE_SIZE << order;
+	return 0;
+}
+
+struct sx_icm *sx_alloc_icm(struct sx_dev *dev, int npages,
+				gfp_t gfp_mask, int coherent)
+{
+	struct sx_icm *icm;
+	struct sx_icm_chunk *chunk = NULL;
+	int cur_order;
+	int ret;
+
+	/* We use sg_set_buf for coherent allocs, which assumes low memory */
+	BUG_ON(coherent && (gfp_mask & __GFP_HIGHMEM));
+
+	icm = kmalloc(sizeof *icm, gfp_mask & ~(__GFP_HIGHMEM | __GFP_NOWARN));
+	if (!icm)
+		return NULL;
+
+	icm->refcount = 0;
+	INIT_LIST_HEAD(&icm->chunk_list);
+
+	cur_order = get_order(SX_ICM_ALLOC_SIZE);
+
+	while (npages > 0) {
+		if (!chunk) {
+			chunk = kmalloc(sizeof *chunk,
+					gfp_mask &
+					~(__GFP_HIGHMEM | __GFP_NOWARN));
+			if (!chunk)
+				goto fail;
+
+			sg_init_table(chunk->mem, SX_ICM_CHUNK_LEN);
+			chunk->npages = 0;
+			chunk->nsg    = 0;
+			list_add_tail(&chunk->list, &icm->chunk_list);
+		}
+
+		while (1 << cur_order > npages)
+			--cur_order;
+
+		if (coherent)
+			ret = sx_alloc_icm_coherent(&dev->pdev->dev,
+					&chunk->mem[chunk->npages],
+					cur_order, gfp_mask);
+		else
+			ret = sx_alloc_icm_pages(&chunk->mem[chunk->npages],
+						   cur_order, gfp_mask);
+
+		if (!ret) {
+			++chunk->npages;
+
+			if (coherent)
+				++chunk->nsg;
+			else if (chunk->npages == SX_ICM_CHUNK_LEN) {
+				chunk->nsg = pci_map_sg(dev->pdev, chunk->mem,
+							chunk->npages,
+							PCI_DMA_BIDIRECTIONAL);
+
+				if (chunk->nsg <= 0)
+					goto fail;
+
+				chunk = NULL;
+			}
+
+			npages -= 1 << cur_order;
+		} else {
+			--cur_order;
+			if (cur_order < 0)
+				goto fail;
+		}
+	}
+
+	if (!coherent && chunk) {
+		chunk->nsg = pci_map_sg(dev->pdev, chunk->mem,
+					chunk->npages,
+					PCI_DMA_BIDIRECTIONAL);
+
+		if (chunk->nsg <= 0)
+			goto fail;
+	}
+
+	return icm;
+
+fail:
+	sx_free_icm(dev, icm, coherent);
+	return NULL;
+}
+
diff --git a/linux/drivers/hwmon/mellanox/icm.h b/linux/drivers/hwmon/mellanox/icm.h
new file mode 100644
index 0000000..7449d08
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/icm.h
@@ -0,0 +1,93 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+#ifndef SX_ICM_H
+#define SX_ICM_H
+
+#include <linux/list.h>
+#include <linux/pci.h>
+#include <linux/mutex.h>
+
+#define SX_ICM_CHUNK_LEN						\
+	((256 - sizeof(struct list_head) - 2 * sizeof(int)) /		\
+	 (sizeof(struct scatterlist)))
+
+enum {
+	SX_ICM_PAGE_SHIFT	= 12,
+	SX_ICM_PAGE_SIZE	= 1 << SX_ICM_PAGE_SHIFT,
+};
+
+struct sx_icm_chunk {
+	struct list_head	list;
+	int			npages;
+	int			nsg;
+	struct scatterlist	mem[SX_ICM_CHUNK_LEN];
+};
+
+struct sx_icm {
+	struct list_head	chunk_list;
+	int			refcount;
+};
+
+struct sx_icm_iter {
+	struct sx_icm	       *icm;
+	struct sx_icm_chunk  *chunk;
+	int			page_idx;
+};
+
+struct sx_dev;
+
+struct sx_icm *sx_alloc_icm(struct sx_dev *dev, int npages,
+				gfp_t gfp_mask, int coherent);
+void sx_free_icm(struct sx_dev *dev, struct sx_icm *icm, int coherent);
+
+static inline void sx_icm_first(struct sx_icm *icm,
+				  struct sx_icm_iter *iter)
+{
+	iter->icm      = icm;
+	iter->chunk    = list_empty(&icm->chunk_list) ?
+		NULL : list_entry(icm->chunk_list.next,
+				  struct sx_icm_chunk, list);
+	iter->page_idx = 0;
+}
+
+static inline int sx_icm_last(struct sx_icm_iter *iter)
+{
+	return !iter->chunk;
+}
+
+static inline void sx_icm_next(struct sx_icm_iter *iter)
+{
+	if (++iter->page_idx >= iter->chunk->nsg) {
+		if (iter->chunk->list.next == &iter->icm->chunk_list) {
+			iter->chunk = NULL;
+			return;
+		}
+
+		iter->chunk = list_entry(iter->chunk->list.next,
+					 struct sx_icm_chunk, list);
+		iter->page_idx = 0;
+	}
+}
+
+static inline dma_addr_t sx_icm_addr(struct sx_icm_iter *iter)
+{
+	return sg_dma_address(&iter->chunk->mem[iter->page_idx]);
+}
+
+static inline unsigned long sx_icm_size(struct sx_icm_iter *iter)
+{
+	return sg_dma_len(&iter->chunk->mem[iter->page_idx]);
+}
+
+#endif /* SX_ICM_H */
diff --git a/linux/drivers/hwmon/mellanox/intf.c b/linux/drivers/hwmon/mellanox/intf.c
new file mode 100644
index 0000000..42f3134
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/intf.c
@@ -0,0 +1,200 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+
+#include <linux/mlx_sx/driver.h>
+#include <linux/mlx_sx/device.h>
+#include "sx.h"
+#include "alloc.h"
+
+struct sx_device_context {
+	struct list_head	list;
+	struct sx_interface  	*intf;
+	void		       	*context;
+};
+
+static LIST_HEAD(intf_list);
+static LIST_HEAD(dev_list);
+static DEFINE_MUTEX(intf_mutex);
+
+static void sx_core_add_device(struct sx_interface *intf, struct sx_priv *priv)
+{
+	struct sx_device_context *dev_ctx;
+	union sx_event_data event_data, tca_init_event_data;
+	int i;
+	unsigned long flags;
+	u8 ib_swid_present = 0;
+
+	dev_ctx = kmalloc(sizeof *dev_ctx, GFP_KERNEL);
+	if (!dev_ctx)
+		return;
+
+	dev_ctx->intf    = intf;
+	dev_ctx->context = intf->add(&priv->dev);
+
+	if (dev_ctx->context) {
+		memset(&tca_init_event_data, 0, sizeof(tca_init_event_data));
+		for (i = 0; i < NUMBER_OF_SWIDS; i++) {
+			memset(&event_data, 0, sizeof(event_data));
+			if (sx_bitmap_test(&priv->swid_bitmap, i)) {
+				if (priv->dev.profile.swid_type[i] ==
+						SX_KU_L2_TYPE_IB) {
+					ib_swid_present = 1;
+					printk(KERN_DEBUG PFX "sx_core_add_device: "
+						"dispatching IB swid up event\n");
+					event_data.ib_swid_change.swid = i;
+					event_data.ib_swid_change.dev_id = 1;
+					dev_ctx->intf->event(&priv->dev,
+						dev_ctx->context,
+						SX_DEV_EVENT_IB_SWID_UP, &event_data);
+					if (priv->dev.profile.ipoib_router_port_enable[i])
+						tca_init_event_data.tca_init.swid[tca_init_event_data.tca_init.num_of_ib_swids++] = i;
+					} else if (priv->dev.profile.swid_type[i] ==
+							SX_KU_L2_TYPE_ETH) {
+						printk(KERN_DEBUG PFX "sx_core_add_device: "
+								"dispatching ETH swid up event\n");
+						event_data.eth_swid_up.swid = i;
+						event_data.eth_swid_up.synd = priv->swid_data[i].eth_swid_data.synd;
+						event_data.eth_swid_up.mac = priv->swid_data[i].eth_swid_data.mac;
+						dev_ctx->intf->event(&priv->dev, dev_ctx->context,
+						SX_DEV_EVENT_ETH_SWID_UP, &event_data); /* TODO: send the right trap id and mac */
+				}
+			}
+		}
+
+		if (ib_swid_present) {
+			if (tca_init_event_data.tca_init.num_of_ib_swids != 0) {
+				tca_init_event_data.tca_init.max_pkey =
+						priv->dev.profile.max_pkey;
+				dev_ctx->intf->event(	&priv->dev,
+							dev_ctx->context,
+							SX_DEV_EVENT_TYPE_TCA_INIT,
+							&tca_init_event_data);
+			}
+		}
+
+		spin_lock_irqsave(&priv->ctx_lock, flags);
+		list_add_tail(&dev_ctx->list, &priv->ctx_list);
+		spin_unlock_irqrestore(&priv->ctx_lock, flags);
+	} else
+		kfree(dev_ctx);
+}
+
+static void sx_core_remove_device(struct sx_interface *intf,
+					struct sx_priv *priv)
+{
+	struct sx_device_context *dev_ctx, *tmp;
+	unsigned long flags;
+
+	list_for_each_entry_safe(dev_ctx, tmp, &priv->ctx_list, list) {
+		if (dev_ctx->intf == intf) {
+			spin_lock_irqsave(&priv->ctx_lock, flags);
+			list_del(&dev_ctx->list);
+			spin_unlock_irqrestore(&priv->ctx_lock, flags);
+
+			intf->remove(&priv->dev, dev_ctx->context);
+			kfree(dev_ctx);
+			return;
+		}
+	}
+}
+
+int sx_register_interface(struct sx_interface *intf)
+{
+	struct sx_priv *priv;
+
+	if (!intf->add || !intf->remove)
+		return -EINVAL;
+
+	mutex_lock(&intf_mutex);
+	list_add_tail(&intf->list, &intf_list);
+	list_for_each_entry(priv, &dev_list, dev_list)
+		sx_core_add_device(intf, priv);
+
+	mutex_unlock(&intf_mutex);
+
+	return 0;
+}
+EXPORT_SYMBOL(sx_register_interface);
+
+void sx_unregister_interface(struct sx_interface *intf)
+{
+	struct sx_priv *priv;
+
+	mutex_lock(&intf_mutex);
+	list_for_each_entry(priv, &dev_list, dev_list)
+		sx_core_remove_device(intf, priv);
+
+	list_del(&intf->list);
+
+	mutex_unlock(&intf_mutex);
+}
+EXPORT_SYMBOL(sx_unregister_interface);
+
+void sx_core_dispatch_event(struct sx_dev *dev, enum sx_dev_event type,
+		union sx_event_data *event_data)
+{
+	struct sx_priv *priv = sx_priv(dev);
+	struct sx_device_context *dev_ctx;
+
+	/* We can't hold a spinlock when dispatching a swid up/down event
+	 *  because sx_ib will not be able to register the
+	 * IB device on PPC. We might have to solve this in the
+	 * future for scalability. */
+	spin_lock(&priv->ctx_lock);
+	list_for_each_entry(dev_ctx, &priv->ctx_list, list) {
+		if (dev_ctx->intf->event) {
+			printk(KERN_DEBUG PFX "sx_core_dispatch_event: "
+					"dispatching the event\n");
+			spin_unlock(&priv->ctx_lock);
+			mutex_lock(&intf_mutex);
+			dev_ctx->intf->event(dev, dev_ctx->context, type, event_data);
+			mutex_unlock(&intf_mutex);
+			spin_lock(&priv->ctx_lock);
+		}
+	}
+
+	spin_unlock(&priv->ctx_lock);
+}
+
+int sx_core_register_device(struct sx_dev *dev)
+{
+	struct sx_priv *priv = sx_priv(dev);
+	struct sx_interface *intf;
+
+	mutex_lock(&intf_mutex);
+	list_add_tail(&priv->dev_list, &dev_list);
+	list_for_each_entry(intf, &intf_list, list)
+		sx_core_add_device(intf, priv);
+
+	mutex_unlock(&intf_mutex);
+
+	return 0;
+}
+
+void sx_core_unregister_device(struct sx_dev *dev)
+{
+	struct sx_priv *priv = sx_priv(dev);
+	struct sx_interface *intf;
+#if 0
+	if (dev->pdev)
+		sx_core_stop_catas_poll(dev);
+#endif
+	mutex_lock(&intf_mutex);
+	list_for_each_entry(intf, &intf_list, list)
+		sx_core_remove_device(intf, priv);
+
+	list_del(&priv->dev_list);
+
+	mutex_unlock(&intf_mutex);
+}
diff --git a/linux/drivers/hwmon/mellanox/mlnx-asic-drv.c b/linux/drivers/hwmon/mellanox/mlnx-asic-drv.c
new file mode 100644
index 0000000..b78be03
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/mlnx-asic-drv.c
@@ -0,0 +1,2322 @@
+/**
+ *
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2015.  ALL RIGHTS RESERVED.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/version.h>
+#include <linux/types.h>
+#include <linux/acpi.h>
+#include <linux/slab.h>
+#include <linux/init.h>
+#include <linux/proc_fs.h>
+#include <linux/seq_file.h>
+#include <linux/dmi.h>
+#include <linux/capability.h>
+#include <linux/mutex.h>
+#include <linux/hwmon.h>
+#include <linux/hwmon-sysfs.h>
+#include <linux/i2c.h>
+#include <asm/uaccess.h>
+#include <asm/io.h>
+#include <linux/thermal.h>
+#include <linux/mlx_sx/kernel_user.h>
+#include <linux/mlx_sx/device.h>
+#include "mlnx-common-drv.h"
+
+#define TEMP_POLLING_INTERVAL (30 * 1000) /* interval between two temperature checks is 30 seconds */
+#define TEMP_PASSIVE_INTERVAL (30 * 1000) /* interval to wait between polls when performing passive cooling is 30 seconds */
+#define MAX_PWM_DUTY_CYCLE    255
+#define PWM_DUTY_CYCLE_STEP    10
+
+#define FAN_NUM                   12
+#define TACH_PWM_MAP              16
+#define FAN_TACH_NUM               4
+#define QSFP_MODULE_NUM           64
+#define FAN_ATTR_NUM               5
+#define TEMP_ATTR_NUM              4
+#define CPLD_ATTR_NUM              1
+#define QSFP_ATTR_NUM              6
+#define QSFP_DATA_VALID_TIME       (120 * 1000) /* 120 seconds */
+#define ENTRY_DATA_VALID_TIME      (3 * 1000)   /* 3 seconds */
+#define QSFP_PAGE_NUM              5
+#define QSFP_SUB_PAGE_NUM          3
+#define QSFP_PAGE_SIZE           128
+#define QSFP_SUB_PAGE_SIZE        48
+#define QSFP_LAST_SUB_PAGE_SIZE   32
+
+enum fan_contol_mode {
+	FAN_CTRL_KERNEL,
+	FAN_CTRL_US_LEGACY,
+	FAN_CTRL_US_PRIVATE,
+};
+
+typedef enum qsfp_module_status {
+    qsfp_good            = 0x00,
+    qsfp_no_eeprrom      = 0x01, /* No response from module's EEPROM. */
+    qsfp_not_supported   = 0x02, /* Module type not supported by the device. */
+    qsfp_not_connected   = 0x03, /* No module present indication.*/
+    qsfp_type_invalid    = 0x04, /* if the module is not qsfp or sfp bus.*/
+    qsfp_not_accessiable = 0x05, /* Not accessable module */
+    qsfp_i2c_error       = 0x09, /* Error occurred while trying to access the module's EEPROM using i2c */
+    qsfp_disable         = 0x10, /*  module is disabled by disable command */
+} qsfp_module_status_t;
+
+static inline const char *qsfp_status_2string(qsfp_module_status_t status)
+{
+    switch (status) {
+        case qsfp_good:
+            return "good";
+        case qsfp_no_eeprrom:
+            return "no_eeprrom";
+        case qsfp_not_connected:
+            return "not_connected";
+        case qsfp_type_invalid:
+            return "type_invalid";
+        case qsfp_not_accessiable:
+            return "not_accessiable";
+        case qsfp_i2c_error:
+            return "i2c_error";
+        case qsfp_disable:
+            return "disable";
+        default:
+            return "not exist";
+    }
+}
+
+typedef enum temp_module_attr {
+        temp_input,
+        temp_min,
+        temp_max,
+        temp_crit,
+        temp_conf,
+} temp_module_attr_t;
+
+typedef enum fan_module_attr {
+        fan_power,
+        fan_speed_tacho0,
+        fan_speed_tacho1,
+        fan_speed_tacho2,
+        fan_speed_tacho3,
+        fan_speed_min,
+        fan_speed_max,
+        fan_enable,
+        fan_conf,
+} fan_module_attr_t;
+
+typedef enum qsfp_module_attr {
+        qsfp_status,
+        qsfp_event,
+        qsfp_temp_input,
+        qsfp_temp_min,
+        qsfp_temp_max,
+        qsfp_temp_crit,
+} qsfp_module_attr_t;
+
+typedef enum cpld_attr {
+        cpld_version,
+} cpld_attr_t;
+
+struct fan_config {
+	struct mlnx_bsp_entry entry;  /* Entry id */
+        u8 num_tachos;                /* Tachometers' number */
+	u8 tacho_id[FAN_TACH_NUM];    /* Fan tachometer index */
+	u8 pwm_id;                    /* PWM tachometer index */
+	u16 speed[FAN_TACH_NUM];      /* Fan speed (Round Per Minute) calculated based on the time measurement between n */
+                                      /* fan pulses. Note that in order for the RPM to be correct, the n value should */
+                                      /* correspond to the number of tachometer pulses per rotation measured by the tachometer */
+	u16 speed_min[FAN_TACH_NUM];  /* Fan speed minimum (Round Per Minute) */
+	u16 speed_max[FAN_TACH_NUM];  /* Fan speed maximum (Round Per Minute) */
+	u8 enable[FAN_TACH_NUM];      /* Software enable state */
+	u8 pwm_duty_cycle;            /* Controls the duty cycle of the PWM. Value range from 0..255 */
+};
+
+struct temp_config {
+	struct mlnx_bsp_entry entry; /* Entry id */
+        u8 sensor_index;             /* Sensors index to access */
+        u32 temperature;             /* Temperature reading from the sensor. Reading in 0.125 Celsius degrees */
+        u8 mte;                      /* Enables measuring the max temperature on a sensor */
+        u8 mtr;                      /* Clears the value of the max temperature register */
+        u32 max_temperature;         /* The highest measured temperature from the sensor */
+        u8 tee;                      /* Temperature Event Enable */
+        u32 temperature_threshold;   /* Generate event if sensor temperature measurement is above the threshold and events enabled */
+};
+
+struct qsfp_config {
+	struct mlnx_bsp_entry entry; /* Entry id */
+        u8 module_index;             /* QSFP modules index to access */
+        u8 lock;                     /* Lock bit. Setting this bit will lock the access to the specific cable */
+        u8 status;                   /* module status (GOOD, NO_EEPROM_MODULES, MODULE_NOT_CONNECTED, I2C_ERROR, MODULE_DISABLED) */
+};
+
+struct cpld_config {
+	struct mlnx_bsp_entry entry; /* Entry id */
+        u8 index;                    /* CPLD index to access */
+        u32 version;                 /* CPLD version */
+};
+
+struct temp_config_params {
+        u8 num_sensors;
+	u8 sensor_active;                /* Indicates number of connected temprature sensors */
+        struct temp_config *sensor;
+};
+
+struct fan_config_params {
+        struct mlnx_bsp_entry entry;   /* Entry id */
+        u8 num_fan;
+	u8 pwm_frequency;               /* Controls the frequency of the PWM signal */
+	u16 pwm_active;                 /* Indicates which of the PWM control is active (bit per PWM) */
+	u16 tacho_active;               /* Indicates which of the tachometer is active (bit per tachometer) */
+	u8 num_cooling_levels;          /* pwm trip levels number */
+	u16 *cooling_levels;            /* pwm trip levels */
+	s16 cooling_cur_level;          /* pwm current level */
+        struct fan_config *fan;
+};
+
+struct qsfp_config_params {
+        struct mlnx_bsp_entry entry;   /* Entry id */
+        u8 num_modules;
+        u32 presence_bitmap[8];
+        unsigned long presence_bitmap_valid;
+        struct qsfp_config *module;
+        struct bin_attribute *eeprom;
+        struct bin_attribute **eeprom_attr_list;
+};
+
+struct cpld_config_params {
+        u8 num_cpld;
+        struct cpld_config *cpld;
+};
+
+enum ports_capabilty {
+	none_drv,
+	asic_drv_32_ports,
+	asic_drv_64_ports,
+	asic_drv_54_ports,
+	asic_drv_36_ports,
+	asic_drv_16_ports,
+};
+
+enum chips {
+	any_chip,
+	switchx2,
+	spectrum,
+};
+
+struct switchdev_if {
+        struct mutex access_lock;
+        u8 dev_id;
+        int (*REG_MFSC)(struct sx_dev *dev, struct ku_access_mfsc_reg *reg_data);
+        int (*REG_MFSM)(struct sx_dev *dev, struct ku_access_mfsm_reg *reg_data);
+        int (*REG_MTMP)(struct sx_dev *dev, struct ku_access_mtmp_reg *reg_data);
+        int (*REG_MTCAP)(struct sx_dev *dev, struct ku_access_mtcap_reg *reg_data);
+        int (*REG_MCIA)(struct sx_dev *dev, struct ku_access_mcia_reg *reg_data);
+        int (*REG_PMPC)(struct sx_dev *dev, struct ku_access_pmpc_reg *reg_data);
+        int (*REG_MSCI)(struct sx_dev *dev, struct ku_access_msci_reg *reg_data);
+        int (*REG_MJTAG)(struct sx_dev *dev, struct ku_access_mjtag_reg *reg_data);
+        int (*REG_PMAOS)(struct sx_dev *dev, struct ku_access_pmaos_reg *reg_data);
+        int (*REG_MFCR)(struct sx_dev *dev, struct ku_access_mfcr_reg *reg_data);
+        int (*REG_MGIR)(struct sx_dev *dev, struct ku_access_mgir_reg *reg_data);
+        void *(*DEV_CONTEXT)(void);
+};
+
+struct asic_data {
+	struct list_head               list;
+	struct kref                    kref;
+        struct i2c_client             *client;
+        enum ports_capabilty           port_cap;
+        enum chips                     kind;
+        struct device                 *hwmon_dev;
+        const char                    *name;
+        struct mutex                   access_lock;
+        struct temp_config_params      temp_config;
+        struct fan_config_params       fan_config;
+        struct cpld_config_params      cpld_config;
+        struct qsfp_config_params      qsfp_config;
+        struct attribute_group         group;
+        const struct attribute_group  *groups[4];
+        u8                             asic_id;
+        struct switchdev_if            switchdevif;
+	struct thermal_cooling_device *tcdev;
+	struct thermal_zone_device    *tzdev;
+};
+
+/* Container structure */
+struct asic_container {
+	struct list_head  list;
+        struct device    *hwmon_dev;
+};
+static struct asic_container asic_instances;
+
+#define ASIC_DRV_VERSION "0.0.1 24/08/2015"
+#define ASIC_DRV_DESCRIPTION "Mellanox ASIC BSP driver"
+MODULE_AUTHOR("Vadim Pasternak (vadimp@mellanox.com)");
+MODULE_DESCRIPTION(ASIC_DRV_DESCRIPTION);
+MODULE_LICENSE("GPL v2");
+MODULE_ALIAS("mlnx-asic");
+ 
+static unsigned short num_cpld = 3;
+module_param(num_cpld, ushort, 0);
+MODULE_PARM_DESC(num_cpld, "Number of CPLD, default is 3");
+static unsigned short num_tachos = 2;
+module_param(num_tachos, ushort, 0);
+MODULE_PARM_DESC(num_tachos, "Number of tachometers per fan, default is 2");
+static unsigned short tacho_flat = 1;
+module_param(tacho_flat, ushort, 0);
+MODULE_PARM_DESC(tacho_flat, "Each tachometer is presented as fan, default is 1");
+static unsigned short speed_min = 10500;
+module_param(speed_min, ushort, 0);
+MODULE_PARM_DESC(speed, "fan minimum speed (round per minute), default is 10500 RPM");
+static unsigned short speed_max = 23000;
+module_param(speed_max, ushort, 0);
+MODULE_PARM_DESC(speed_max, "fan maximum speed (round per minute), default is 23000 (100 percent)");
+static unsigned short pwm_duty_cycle = 153;
+module_param(pwm_duty_cycle, ushort, 0);
+MODULE_PARM_DESC(pwm_duty_cycle, "Duty cycle of the PWM, value range from 0..255, default is 153");
+static unsigned short asic_dev_id = 255;
+module_param(asic_dev_id, ushort, 0);
+MODULE_PARM_DESC(asic_dev_id, "ASIC device Id, default is 255");
+static unsigned short mte = 1;
+module_param(mte, ushort, 0);
+MODULE_PARM_DESC(mte, "Enable measuring the max temperature, default is enable (1)");
+static unsigned short mtr = 0;
+module_param(mtr, ushort, 0);
+MODULE_PARM_DESC(mte, "Clear the value of the max temperature, default is not clear (0)");
+static unsigned short tee = 0;
+module_param(tee, ushort, 0);
+MODULE_PARM_DESC(tee, "Enable temperature event, default is not disable (0)");
+static unsigned short temp_threshold = 80;
+module_param(temp_threshold, ushort, 0);
+MODULE_PARM_DESC(temp_threshold, "Temprature threshold, default is 80");
+static unsigned short qsfp_map[QSFP_MODULE_NUM] = { 64, 65, 66, 67, 68, 69, 70, 71,
+                                                    72, 73, 74, 75, 76, 77, 78, 79,
+                                                    80, 81, 82, 83, 84, 85, 86, 87,
+                                                    88, 89, 90, 91, 92, 93, 94, 95 };
+module_param_array(qsfp_map, ushort, NULL, 0644);
+MODULE_PARM_DESC(qsfp_map, "Module status offsets vector (default)");
+static unsigned short qsfp_eeprom_i2c_addr = 0x50;
+module_param(qsfp_eeprom_i2c_addr, ushort, 0);
+MODULE_PARM_DESC(qsfp_eeprom_i2c_addr, "I2C address of qsfp module eeprom, default is 0x50");
+static unsigned short auto_thermal_control = 1;
+module_param(auto_thermal_control, ushort, 0);
+MODULE_PARM_DESC(auto_thermal_control, "Automatic thermal control is enable, default is yes");
+
+#define REG_QUERY (1)
+#define REG_WRITE (2)
+#define SET_REG_TEMPLATE(reg_data, regid, method, devif) \
+        reg_data.op_tlv.type = 1;                        \
+        reg_data.op_tlv.length = 4;                      \
+        reg_data.op_tlv.dr = 0;                          \
+        reg_data.op_tlv.status = 0;                      \
+        reg_data.op_tlv.register_id = regid;             \
+        reg_data.op_tlv.r = 0;                           \
+        reg_data.op_tlv.method = method;                 \
+        reg_data.op_tlv.op_class = 1;                    \
+        reg_data.op_tlv.tid = 0;                         \
+	reg_data.dev_id = devif->dev_id;
+
+#define REG_ACCESS(devif, REGID, reg_data, err)                                     \
+	mutex_lock(&devif->access_lock);                                            \
+	err = devif->REG_##REGID((struct sx_dev *)devif->DEV_CONTEXT(), &reg_data); \
+	mutex_unlock(&devif->access_lock);                                          \
+	if (err)                                                                    \
+		return err;
+
+#define ENTRY_DATA_VALID(entry, refresh)                                       \
+	if (time_before(jiffies, entry.last_updated + refresh) && entry.valid) \
+		return 0;
+
+static int fan_get_power(struct switchdev_if *devif, struct fan_config *fan, u8 cache_drop)
+{
+	int err = 0;
+	u8 method = REG_QUERY;
+	struct ku_access_mfsc_reg reg_data;
+
+	if (!cache_drop)
+		ENTRY_DATA_VALID(fan->entry, ENTRY_DATA_VALID_TIME);
+
+	if (!devif->REG_MFSC || !devif->DEV_CONTEXT)
+		return err;
+
+	memset(&reg_data, 0, sizeof(struct ku_access_mfsc_reg));
+	SET_REG_TEMPLATE(reg_data, MFSC_REG_ID, method, devif);
+        reg_data.mfsc_reg.pwm = fan->pwm_id; /* Will affect all FANs */
+
+        REG_ACCESS(devif, MFSC, reg_data, err);
+
+        fan->entry.last_updated = jiffies;
+        fan->entry.valid = 1;
+	fan->pwm_duty_cycle = (reg_data.mfsc_reg.pwm_duty_cycle);
+
+	return err;
+}
+
+static int fan_set_power(struct switchdev_if *devif, struct fan_config *fan)
+{
+	int err = 0;
+	u8 method = REG_WRITE;
+	struct ku_access_mfsc_reg reg_data;
+
+	if (!devif->REG_MFSC || !devif->DEV_CONTEXT)
+		return err;
+
+	memset(&reg_data, 0, sizeof(struct ku_access_mfsc_reg));
+	SET_REG_TEMPLATE(reg_data, MFSC_REG_ID, method, devif);
+        reg_data.mfsc_reg.pwm = fan->pwm_id; /* Will affect all FANs */
+	reg_data.mfsc_reg.pwm_duty_cycle = fan->pwm_duty_cycle;
+
+        REG_ACCESS(devif, MFSC, reg_data, err);
+
+	return err;
+}
+
+static int fan_get_speed(struct switchdev_if *devif, struct fan_config *fan, char *buf, int tacho_id)
+{
+	int err = 0;
+	u8 method = REG_QUERY;
+	struct ku_access_mfsm_reg reg_data;
+
+	ENTRY_DATA_VALID(fan->entry, ENTRY_DATA_VALID_TIME);
+
+	if (!devif->REG_MFSM || !devif->DEV_CONTEXT)
+		return ENODEV;
+
+	memset(&reg_data, 0, sizeof(struct ku_access_mfsm_reg));
+	SET_REG_TEMPLATE(reg_data, MFSM_REG_ID, method, devif);
+        reg_data.mfsm_reg.tacho = fan->tacho_id[tacho_id];
+
+        REG_ACCESS(devif, MFSM, reg_data, err);
+        fan->entry.last_updated = jiffies;
+        fan->entry.valid = 1;
+
+	fan->speed[tacho_id] = reg_data.mfsm_reg.rpm;
+
+	return err;
+}
+
+static int fan_set_enable(struct asic_data *asicdata, int index, u8 enable)
+{
+	struct fan_config *fan;
+	int err = 0;
+
+	fan = &asicdata->fan_config.fan[index];
+
+	switch (enable) {
+		case FAN_CTRL_KERNEL:
+			fan->pwm_duty_cycle = pwm_duty_cycle;
+			err =  fan_set_power(&asicdata->switchdevif, fan);
+			if (err)
+				return err;
+
+			if (auto_thermal_control) {
+				asicdata->fan_config.cooling_cur_level = 0;
+				asicdata->tzdev->polling_delay = TEMP_POLLING_INTERVAL;
+				asicdata->tzdev->passive_delay = TEMP_PASSIVE_INTERVAL;
+				thermal_zone_device_update(asicdata->tzdev);
+				pr_notice("kernel mode fan control ON\n");
+			}
+			break;
+		case FAN_CTRL_US_LEGACY:
+		case FAN_CTRL_US_PRIVATE:
+			if (auto_thermal_control) {
+				asicdata->tzdev->polling_delay = 0;
+				asicdata->tzdev->passive_delay = 0;
+				thermal_zone_device_update(asicdata->tzdev);
+				pr_notice("kernel mode fan control OFF\n");
+			}
+			break;
+		default:
+			fan->pwm_duty_cycle = MAX_PWM_DUTY_CYCLE;
+			err =  fan_set_power(&asicdata->switchdevif, fan);
+			if (err)
+				return err;
+
+			if (auto_thermal_control) {
+				asicdata->tzdev->polling_delay = 0;
+				asicdata->tzdev->passive_delay = 0;
+				thermal_zone_device_update(asicdata->tzdev);
+				pr_notice("kernel mode fan control OFF\n");
+			}
+			break;
+
+	}
+	return err;
+}
+
+static int fan_get_config(struct switchdev_if *devif, struct fan_config_params *fan_config)
+{
+	int err = 0;
+	u8 method = REG_QUERY;
+	struct ku_access_mfcr_reg reg_data;
+
+	ENTRY_DATA_VALID(fan_config->entry, ENTRY_DATA_VALID_TIME);
+
+	if (!devif->REG_MFCR || !devif->DEV_CONTEXT)
+		return ENODEV;
+
+	memset(&reg_data, 0, sizeof(struct ku_access_mfcr_reg));
+	SET_REG_TEMPLATE(reg_data, MFCR_REG_ID, method, devif);
+
+        REG_ACCESS(devif, MFCR, reg_data, err);
+        fan_config->entry.last_updated = jiffies;
+        fan_config->entry.valid = 1;
+
+	fan_config->pwm_frequency = reg_data.mfcr_reg.pwm_frequency;
+	fan_config->pwm_active = reg_data.mfcr_reg.pwm_active;
+	fan_config->tacho_active = reg_data.mfcr_reg.tacho_active;
+
+	return err;
+}
+
+static int temp_get(struct switchdev_if *devif, struct temp_config *temp, u8 id, u8 cache_drop)
+{
+	int err = 0;
+	u8 method = REG_QUERY;
+	struct ku_access_mtmp_reg reg_data;
+
+	if (!cache_drop)
+		ENTRY_DATA_VALID(temp->entry, ENTRY_DATA_VALID_TIME);
+
+	if (!devif->REG_MTMP || !devif->DEV_CONTEXT)
+		return ENODEV;
+
+	memset(&reg_data, 0, sizeof(struct ku_access_mtmp_reg));
+	SET_REG_TEMPLATE(reg_data, MTMP_REG_ID, method, devif);
+	reg_data.mtmp_reg.sensor_index = temp->sensor_index + id; /* Sensors index to access */
+
+        REG_ACCESS(devif, MTMP, reg_data, err);
+        temp->entry.last_updated = jiffies;
+        temp->entry.valid = 1;
+        /* For temp->temperature < 0 consider to set:
+           temp->temperature = 0xffff + ((s16)temp->temperature) + 1;
+        */
+	temp->temperature = reg_data.mtmp_reg.temperature * 100; /* temp1_input */
+	temp->max_temperature = reg_data.mtmp_reg.max_temperature * 100; /* temp1_max */
+	temp->temperature_threshold = reg_data.mtmp_reg.temperature_threshold * 100; /* temp1_crit */
+	/* temp->temp1_min_hyst = reg_data.temperature_threshold_lo; Not implemented */
+	/* temp->temp1_max_hyst = reg_data.temperature_threshold_hi; Not implemented */
+
+        return err;
+}
+
+static int temp_get_config(struct switchdev_if *devif, struct temp_config_params *temp_config)
+{
+	int err = 0;
+	u8 method = REG_QUERY;
+	struct ku_access_mtcap_reg reg_data;
+
+	if (!devif->REG_MTCAP || !devif->DEV_CONTEXT)
+		return ENODEV;
+
+	memset(&reg_data, 0, sizeof(struct ku_access_mtcap_reg));
+	SET_REG_TEMPLATE(reg_data, MTCAP_REG_ID, method, devif);
+
+        REG_ACCESS(devif, MTCAP, reg_data, err);
+
+	temp_config->sensor_active = reg_data.mtcap_reg.sensor_count;
+
+	return err;
+}
+
+static int qsfp_get(struct switchdev_if *devif, struct qsfp_config *qsfp)
+{
+	int err = 0;
+	u8 method = REG_QUERY;
+	struct ku_access_mcia_reg reg_data;
+
+	ENTRY_DATA_VALID(qsfp->entry, ENTRY_DATA_VALID_TIME);
+
+	if (!devif->REG_MCIA || !devif->DEV_CONTEXT)
+		return ENODEV;
+
+	memset(&reg_data, 0, sizeof(struct ku_access_mcia_reg));
+	SET_REG_TEMPLATE(reg_data, MCIA_REG_ID, method, devif);
+        reg_data.mcia_reg.module = qsfp->module_index;
+        reg_data.mcia_reg.l = qsfp->lock;
+        reg_data.mcia_reg.page_number = 0;
+        reg_data.mcia_reg.size = QSFP_SUB_PAGE_SIZE;
+
+        REG_ACCESS(devif, MCIA, reg_data, err);
+        qsfp->entry.last_updated = jiffies;
+        qsfp->entry.valid = 1;
+
+	qsfp->status = reg_data.mcia_reg.status;
+
+	return err;
+}
+
+static int qsfp_get_eeprom(struct switchdev_if *devif, struct qsfp_config *qsfp,
+				char *buf, loff_t off, size_t count)
+{
+	int err = 0, res = 0, i, j, k, size, page = 0, subpage = 0, page_off = 0, subpage_off = 0;
+	u8 method = REG_QUERY;
+	struct ku_access_mcia_reg reg_data;
+	u32 tbuf[12];
+	u32 *rbuf;
+	u8  page_number[QSFP_PAGE_NUM] = { 0xa0, 0x00, 0x01, 0x02, 0x03 }; /* ftp://ftp.seagate.com/sff/SFF-8436.PDF */
+	u16  page_shift[QSFP_PAGE_NUM + 1] = { 0x00, 0x80, 0x80, 0x80, 0x80, 0x00 };
+	u16 sub_page_size[QSFP_SUB_PAGE_NUM] = { QSFP_SUB_PAGE_SIZE, QSFP_SUB_PAGE_SIZE, QSFP_LAST_SUB_PAGE_SIZE};
+
+	if (!devif->REG_MCIA || !devif->DEV_CONTEXT)
+		return ENODEV;
+
+	memset(&reg_data, 0, sizeof(struct ku_access_mcia_reg));
+	SET_REG_TEMPLATE(reg_data, MCIA_REG_ID, method, devif);
+        reg_data.mcia_reg.i2c_device_address = qsfp_eeprom_i2c_addr;
+        reg_data.mcia_reg.device_address = 0;
+        reg_data.mcia_reg.module = qsfp->module_index;
+        reg_data.mcia_reg.l = qsfp->lock;
+
+	/* Map offset to correct page number, subpage number and device internal offset */
+	page = off / QSFP_PAGE_SIZE;
+	page_off = off % QSFP_PAGE_SIZE;
+	subpage = page_off / QSFP_SUB_PAGE_SIZE;
+	subpage_off = page_off % QSFP_SUB_PAGE_SIZE;
+	reg_data.mcia_reg.device_address = subpage_off + page_shift[page];
+
+	for (i = page; i < QSFP_PAGE_NUM; i++) {
+		for (j = subpage; j < QSFP_SUB_PAGE_NUM; j++) {
+        		reg_data.mcia_reg.page_number = page_number[i];
+        		if (j == subpage)
+        			reg_data.mcia_reg.size = sub_page_size[j] - subpage_off;
+        		else
+        			reg_data.mcia_reg.size = sub_page_size[j];
+
+			REG_ACCESS(devif, MCIA, reg_data, err);
+
+			if (reg_data.mcia_reg.status)
+				return err;
+
+			rbuf = &reg_data.mcia_reg.dword_0;
+			size = ((reg_data.mcia_reg.size % 4) == 0) ? (reg_data.mcia_reg.size / 4) :
+				(reg_data.mcia_reg.size / 4) + 1;
+			for (k = 0; k < size; k++, rbuf++) {
+				tbuf[k] = ntohl(*rbuf);
+			}
+			memcpy(buf, tbuf, reg_data.mcia_reg.size);
+
+			if (count <= 0)
+				return res;
+			buf += reg_data.mcia_reg.size;
+			off += reg_data.mcia_reg.size;
+			count -= reg_data.mcia_reg.size;
+			res += reg_data.mcia_reg.size;
+			reg_data.mcia_reg.device_address += reg_data.mcia_reg.size;
+		}
+		reg_data.mcia_reg.device_address = page_shift[i + 1];
+	}
+
+	return res;
+}
+
+#define PMPC_REG_ID 0x501F /* Missed definition */
+static int qsfp_get_event(struct switchdev_if *devif, struct qsfp_config_params *qsfp_config)
+{
+	int err = 0, i;
+	u8 method = REG_QUERY;
+	struct ku_access_pmpc_reg reg_data;
+
+	ENTRY_DATA_VALID(qsfp_config->entry, ENTRY_DATA_VALID_TIME);
+
+	if (!devif->REG_PMPC || !devif->DEV_CONTEXT)
+		return ENODEV;
+
+	memset(&reg_data, 0, sizeof(struct ku_access_pmpc_reg));
+	SET_REG_TEMPLATE(reg_data, PMPC_REG_ID, method, devif);
+
+        REG_ACCESS(devif, PMPC, reg_data, err);
+        qsfp_config->entry.last_updated = jiffies;
+        qsfp_config->entry.valid = 1;
+
+	for (i = 0; i < 8; i++)
+		qsfp_config->presence_bitmap[7 - i] = reg_data.pmpc_reg.module_state_updated_bitmap[i];
+
+	return err;
+}
+
+static int qsfp_set_event(struct switchdev_if *devif, u32 *bitmap)
+{
+	int err = 0;
+	u8 method = REG_WRITE;
+	struct ku_access_pmpc_reg reg_data;
+
+	if (!devif->REG_PMPC || !devif->DEV_CONTEXT)
+		return ENODEV;
+
+	memset(&reg_data, 0, sizeof(struct ku_access_pmpc_reg));
+	SET_REG_TEMPLATE(reg_data, PMPC_REG_ID, method, devif);
+
+	memcpy(reg_data.pmpc_reg.module_state_updated_bitmap, bitmap,
+		sizeof(reg_data.pmpc_reg.module_state_updated_bitmap[0] * 8));
+
+        REG_ACCESS(devif, PMPC, reg_data, err);
+
+	return err;
+}
+
+#define MSCI_REG_ID 0x902A /* Missed defenition */
+static int cpld_get(struct switchdev_if *devif, struct cpld_config *cpld)
+{
+	int err = 0;
+	u8 method = REG_QUERY;
+	struct ku_access_msci_reg reg_data;
+
+	if (!devif->REG_MSCI || !devif->DEV_CONTEXT)
+		return err;
+
+	memset(&reg_data, 0, sizeof(struct ku_access_msci_reg));
+	SET_REG_TEMPLATE(reg_data, MSCI_REG_ID, method, devif);
+        reg_data.msci_reg.index = cpld->index;
+
+        REG_ACCESS(devif, MSCI, reg_data, err);
+        cpld->entry.last_updated = jiffies;
+        cpld->entry.valid = 1;
+
+	cpld->version = reg_data.msci_reg.version;
+
+	return err;
+}
+
+static int mgir_get(struct switchdev_if *devif, u16 *device_id)
+{
+	int err = 0;
+	u8 method = REG_QUERY;
+	struct ku_access_mgir_reg reg_data;
+
+	if (!devif->REG_MGIR || !devif->DEV_CONTEXT)
+		return err;
+
+	memset(&reg_data, 0, sizeof(struct ku_access_mgir_reg));
+	SET_REG_TEMPLATE(reg_data, MGIR_REG_ID, method, devif);
+
+        REG_ACCESS(devif, MGIR, reg_data, err);
+
+        *device_id = reg_data.mgir_reg.hw_info.device_id;
+
+	return err;
+}
+
+static ssize_t store_temp(struct device *dev,
+			  struct device_attribute *devattr,
+			  const char *buf,
+			  size_t count)
+{
+	struct asic_data *asicdata = i2c_get_clientdata(to_i2c_client(dev));
+	int index = to_sensor_dev_attr_2(devattr)->index;
+        int nr = to_sensor_dev_attr_2(devattr)->nr;
+	struct temp_config *temp = NULL;
+
+	temp = &asicdata->temp_config.sensor[index];
+	if (!temp)
+		return -EEXIST;
+
+        switch (nr) {
+        case temp_min:
+                break;
+        default:
+                return -EEXIST;
+        }
+
+	return count;
+}
+
+static ssize_t show_temp(struct device *dev,
+			 struct device_attribute *devattr,
+			 char *buf)
+{
+	struct asic_data *asicdata = i2c_get_clientdata(to_i2c_client(dev));
+	int index = to_sensor_dev_attr_2(devattr)->index;
+        int nr = to_sensor_dev_attr_2(devattr)->nr;
+	struct temp_config *temp = NULL;
+	int err = 0, res = 0;
+
+	temp = &asicdata->temp_config.sensor[index];
+	if (!temp)
+		return -EEXIST;
+
+        switch (nr) {
+        case temp_input:
+	        err = temp_get(&asicdata->switchdevif, temp, 0, 0);
+		if (err)
+			return -EEXIST;
+	        res = temp->temperature;
+                break;
+        case temp_min:
+                break;
+        case temp_max:
+	        err = temp_get(&asicdata->switchdevif, temp, 0, 0);
+		if (err)
+			return -EEXIST;
+	        res = temp->max_temperature;
+                break;
+        case temp_crit:
+	        err = temp_get(&asicdata->switchdevif, temp, 0, 0);
+		if (err)
+			return -EEXIST;
+	        res = temp->temperature_threshold;
+                break;
+        case temp_conf:
+		if (!temp_get_config(&asicdata->switchdevif, &asicdata->temp_config))
+		        res = asicdata->temp_config.sensor_active;
+		else
+		        goto req_err;
+		break;
+        default:
+                return -EEXIST;
+        }
+
+	return sprintf(buf, "%d\n", res);
+
+req_err:
+	if (err != ENODEV)
+		return err;
+	else
+		return sprintf(buf, "%d\n", res);
+}
+
+static ssize_t store_fan(struct device *dev,
+			 struct device_attribute *devattr,
+			 const char *buf,
+			 size_t count)
+{
+	struct asic_data *asicdata = i2c_get_clientdata(to_i2c_client(dev));
+	int index = to_sensor_dev_attr_2(devattr)->index;
+        int nr = to_sensor_dev_attr_2(devattr)->nr;
+	struct fan_config *fan = NULL;
+	int err;
+
+	fan = &asicdata->fan_config.fan[index];
+	if (!fan)
+		return -EEXIST;
+
+	switch (nr) {
+	case fan_power:
+		fan->pwm_duty_cycle = simple_strtoul(buf, NULL, 10);
+		if (fan->pwm_duty_cycle < pwm_duty_cycle)
+			fan->pwm_duty_cycle = pwm_duty_cycle;
+		err = fan_set_power(&asicdata->switchdevif, fan);
+		break;
+	case fan_speed_min:
+		fan->speed_min[0] = simple_strtoul(buf, NULL, 10);
+		break;
+	case fan_speed_max:
+		fan->speed_max[0] = simple_strtoul(buf, NULL, 10);
+		break;
+	case fan_enable:
+		fan->enable[0] = simple_strtoul(buf, NULL, 10);
+		err = fan_set_enable(asicdata, index, fan->enable[0]);
+		if (err)
+			return -EEXIST;
+		break;
+	default:
+		return -EEXIST;
+	}
+ 
+	return count;
+}
+
+static ssize_t show_fan(struct device *dev,
+			struct device_attribute *devattr,
+			char *buf)
+{
+	struct asic_data *asicdata = i2c_get_clientdata(to_i2c_client(dev));
+	int index = to_sensor_dev_attr_2(devattr)->index;
+        int nr = to_sensor_dev_attr_2(devattr)->nr;
+	struct fan_config *fan = NULL;
+	int err = 0, res = 0;
+
+	fan = &asicdata->fan_config.fan[index];
+	if (!fan)
+		return -EEXIST;
+
+	switch (nr) {
+        case fan_power:
+		err = fan_get_power(&asicdata->switchdevif, fan, 0);
+		if (err)
+			return -EEXIST;
+		res = fan->pwm_duty_cycle;
+		break;
+        case fan_speed_tacho0:
+		if (!fan_get_speed(&asicdata->switchdevif, fan, buf, 0))
+		        res = fan->speed[0];
+		else
+		        goto req_err;
+		break;
+        case fan_speed_tacho1:
+		err = fan_get_speed(&asicdata->switchdevif, fan, buf, 1);
+		if (err)
+			return -EEXIST;
+		res = fan->speed[1];
+		break;
+        case fan_speed_tacho2:
+		err = fan_get_speed(&asicdata->switchdevif, fan, buf, 2);
+		if (err)
+			return -EEXIST;
+		res = fan->speed[2];
+		break;
+        case fan_speed_tacho3:
+		err = fan_get_speed(&asicdata->switchdevif, fan, buf, 3);
+		if (err)
+			return -EEXIST;
+		res = fan->speed[3];
+		break;
+        case fan_speed_min:
+		res = fan->speed_min[0];
+		break;
+	case fan_speed_max:
+		res = fan->speed_max[0];
+		break;
+	case fan_enable:
+		res = fan->enable[0];
+		break;
+	case fan_conf:
+		if (!fan_get_config(&asicdata->switchdevif, &asicdata->fan_config))
+		        res = asicdata->fan_config.tacho_active;
+		else
+		        goto req_err;
+		break;
+	default:
+		return -EEXIST;
+	}
+ 
+	return sprintf(buf, "%d\n", res);
+
+req_err:
+	if (err != ENODEV)
+		return err;
+	else
+		return sprintf(buf, "%d\n", res);
+}
+
+
+static ssize_t show_qsfp(struct device *dev,
+			 struct device_attribute *devattr,
+			 char *buf)
+{
+	struct asic_data *asicdata = i2c_get_clientdata(to_i2c_client(dev));
+	int index = to_sensor_dev_attr_2(devattr)->index;
+        int nr = to_sensor_dev_attr_2(devattr)->nr;
+	struct qsfp_config *qsfp = NULL;
+	struct temp_config *temp = NULL;
+	u8 val;
+	int res = 0;
+
+	qsfp = &asicdata->qsfp_config.module[index];
+	if (!qsfp)
+		return -EEXIST;
+
+	temp = &asicdata->temp_config.sensor[0];
+	if (!temp)
+		return -EEXIST;
+
+	switch (nr) {
+	case qsfp_status:
+		res = qsfp_get(&asicdata->switchdevif, qsfp);
+		if (res)
+			return sprintf(buf, "%s\n", "request error");
+		else
+			return sprintf(buf, "%s\n", qsfp_status_2string(qsfp->status));
+
+	case qsfp_event:
+		res = qsfp_get_event(&asicdata->switchdevif, &asicdata->qsfp_config);
+		if (res) {
+			return sprintf(buf, "%s\n", "request error");
+		}
+		val = (asicdata->qsfp_config.presence_bitmap[(index / 32)] &
+			(BIT_MASK(index % 32))) >> (index % 32);
+		return sprintf(buf, "%d\n", val);
+
+	case qsfp_temp_input:
+	        res = temp_get(&asicdata->switchdevif, temp, qsfp_map[index], 0);
+		if (res) {
+			return sprintf(buf, "%s\n", "request error");
+		}
+	        res = temp->temperature;
+                break;
+
+        case qsfp_temp_min:
+                break;
+
+        case qsfp_temp_max:
+	        res = temp_get(&asicdata->switchdevif, temp, qsfp_map[index], 0);
+		if (res) {
+			return sprintf(buf, "%s\n", "request error");
+		}
+	        res = temp->max_temperature;
+                break;
+
+        case qsfp_temp_crit:
+	        res = temp_get(&asicdata->switchdevif, temp, qsfp_map[index], 0);
+		if (res) {
+			return sprintf(buf, "%s\n", "request error");
+		}
+	        res = temp->temperature_threshold;
+                break;
+	default:
+		return -EEXIST;
+	}
+
+	return sprintf(buf, "%d\n", res);
+}
+
+static ssize_t store_qsfp(struct device *dev,
+			  struct device_attribute *devattr,
+			  const char *buf,
+			  size_t count)
+{
+	struct asic_data *asicdata = i2c_get_clientdata(to_i2c_client(dev));
+	int index = to_sensor_dev_attr_2(devattr)->index;
+        int nr = to_sensor_dev_attr_2(devattr)->nr;
+	struct qsfp_config *qsfp = NULL;
+	struct temp_config *temp = NULL;
+	u32 presence_bitmap[8];
+	u32 setmask = ~BIT_MASK(index % 32);
+	u32 defmask = 0xffffffff;
+	int err, i;
+
+	qsfp = &asicdata->qsfp_config.module[index];
+	if (!qsfp)
+		return -EEXIST;
+
+	temp = &asicdata->temp_config.sensor[index];
+	if (!temp)
+		return -EEXIST;
+
+        switch (nr) {
+        case qsfp_temp_min:
+                break;
+	case qsfp_event:
+		for (i = 0; i < 8; i++) {
+			presence_bitmap[7 - i] = ((index / 32) == i) ? setmask : defmask;
+		}
+		err = qsfp_set_event(&asicdata->switchdevif, presence_bitmap);
+		if (err)
+			return -EEXIST;
+                break;
+        default:
+                return -EEXIST;
+        }
+
+	return count;
+}
+static ssize_t show_cpld(struct device *dev,
+			 struct device_attribute *devattr,
+			 char *buf)
+{
+	struct asic_data *asicdata = i2c_get_clientdata(to_i2c_client(dev));
+	int index = to_sensor_dev_attr_2(devattr)->index;
+        int nr = to_sensor_dev_attr_2(devattr)->nr;
+	struct cpld_config *cpld = NULL;
+	int res = 0;
+
+	cpld = &asicdata->cpld_config.cpld[index];
+	if (!cpld)
+		return -EEXIST;
+
+	switch (nr) {
+	case cpld_version:
+		res = cpld_get(&asicdata->switchdevif, cpld);
+		if (res)
+			return sprintf(buf, "%s\n", "request error");
+		else
+			return sprintf(buf, "%d\n", cpld->version);
+	default:
+		return -EEXIST;
+	}
+
+	return sprintf(buf, "%d\n", res);
+}
+
+static ssize_t qsfp_eeprom_bin_read(struct file *filp, struct kobject *kobj,
+                                    struct bin_attribute *attr,
+                                    char *buf, loff_t off, size_t count)
+{
+	struct asic_data *asicdata = dev_get_drvdata(container_of(kobj, struct device, kobj));
+	struct qsfp_config *module = (struct qsfp_config *)attr->private;
+
+	if (off > asicdata->qsfp_config.eeprom[module->module_index].size)
+		return -ESPIPE;
+	if (off == asicdata->qsfp_config.eeprom[module->module_index].size)
+		return 0;
+	if ((off + count) > asicdata->qsfp_config.eeprom[module->module_index].size)
+		count = asicdata->qsfp_config.eeprom[module->module_index].size - off;
+	if ((off + count) < asicdata->qsfp_config.eeprom[module->module_index].size)
+		return count;
+
+        return qsfp_get_eeprom(&asicdata->switchdevif, module, buf, off, count);
+}
+
+/* Attributes */
+#define SENSOR_DEVICE_ATTR_TEMP(id)                            \
+static SENSOR_DEVICE_ATTR_2(temp##id##_input, S_IRUGO,         \
+        show_temp, NULL, temp_input, id - 1);                  \
+static SENSOR_DEVICE_ATTR_2(temp##id##_min, S_IRUGO | S_IWUSR, \
+        show_temp, store_temp, temp_min, id - 1);              \
+static SENSOR_DEVICE_ATTR_2(temp##id##_max, S_IRUGO,           \
+        show_temp, NULL, temp_max, id - 1);                    \
+static SENSOR_DEVICE_ATTR_2(temp##id##_crit, S_IRUGO,          \
+        show_temp, NULL, temp_crit, id - 1)
+
+SENSOR_DEVICE_ATTR_TEMP(1);
+SENSOR_DEVICE_ATTR_TEMP(2);
+SENSOR_DEVICE_ATTR_TEMP(3);
+SENSOR_DEVICE_ATTR_TEMP(4);
+SENSOR_DEVICE_ATTR_TEMP(5);
+SENSOR_DEVICE_ATTR_TEMP(6);
+SENSOR_DEVICE_ATTR_TEMP(7);
+SENSOR_DEVICE_ATTR_TEMP(8);
+SENSOR_DEVICE_ATTR_TEMP(9);
+SENSOR_DEVICE_ATTR_TEMP(10);
+SENSOR_DEVICE_ATTR_TEMP(11);
+SENSOR_DEVICE_ATTR_TEMP(12);
+SENSOR_DEVICE_ATTR_TEMP(13);
+SENSOR_DEVICE_ATTR_TEMP(14);
+SENSOR_DEVICE_ATTR_TEMP(15);
+SENSOR_DEVICE_ATTR_TEMP(16);
+SENSOR_DEVICE_ATTR_TEMP(17);
+SENSOR_DEVICE_ATTR_TEMP(18);
+SENSOR_DEVICE_ATTR_TEMP(19);
+SENSOR_DEVICE_ATTR_TEMP(20);
+SENSOR_DEVICE_ATTR_TEMP(21);
+SENSOR_DEVICE_ATTR_TEMP(22);
+SENSOR_DEVICE_ATTR_TEMP(23);
+SENSOR_DEVICE_ATTR_TEMP(24);
+SENSOR_DEVICE_ATTR_TEMP(25);
+SENSOR_DEVICE_ATTR_TEMP(26);
+SENSOR_DEVICE_ATTR_TEMP(27);
+SENSOR_DEVICE_ATTR_TEMP(28);
+SENSOR_DEVICE_ATTR_TEMP(29);
+SENSOR_DEVICE_ATTR_TEMP(30);
+SENSOR_DEVICE_ATTR_TEMP(31);
+SENSOR_DEVICE_ATTR_TEMP(32);
+SENSOR_DEVICE_ATTR_TEMP(33);
+SENSOR_DEVICE_ATTR_TEMP(34);
+SENSOR_DEVICE_ATTR_TEMP(35);
+SENSOR_DEVICE_ATTR_TEMP(36);
+SENSOR_DEVICE_ATTR_TEMP(37);
+SENSOR_DEVICE_ATTR_TEMP(38);
+SENSOR_DEVICE_ATTR_TEMP(39);
+SENSOR_DEVICE_ATTR_TEMP(40);
+SENSOR_DEVICE_ATTR_TEMP(41);
+SENSOR_DEVICE_ATTR_TEMP(42);
+SENSOR_DEVICE_ATTR_TEMP(43);
+SENSOR_DEVICE_ATTR_TEMP(44);
+SENSOR_DEVICE_ATTR_TEMP(45);
+SENSOR_DEVICE_ATTR_TEMP(46);
+SENSOR_DEVICE_ATTR_TEMP(47);
+SENSOR_DEVICE_ATTR_TEMP(48);
+SENSOR_DEVICE_ATTR_TEMP(49);
+SENSOR_DEVICE_ATTR_TEMP(50);
+SENSOR_DEVICE_ATTR_TEMP(51);
+SENSOR_DEVICE_ATTR_TEMP(52);
+SENSOR_DEVICE_ATTR_TEMP(53);
+SENSOR_DEVICE_ATTR_TEMP(54);
+SENSOR_DEVICE_ATTR_TEMP(55);
+SENSOR_DEVICE_ATTR_TEMP(56);
+SENSOR_DEVICE_ATTR_TEMP(57);
+SENSOR_DEVICE_ATTR_TEMP(58);
+SENSOR_DEVICE_ATTR_TEMP(59);
+SENSOR_DEVICE_ATTR_TEMP(60);
+SENSOR_DEVICE_ATTR_TEMP(61);
+SENSOR_DEVICE_ATTR_TEMP(62);
+SENSOR_DEVICE_ATTR_TEMP(63);
+SENSOR_DEVICE_ATTR_TEMP(64);
+SENSOR_DEVICE_ATTR_TEMP(65);
+
+#define SENSOR_DEVICE_ATTR_FAN(id)                               \
+static SENSOR_DEVICE_ATTR_2(pwm##id, S_IRUGO | S_IWUSR,          \
+        show_fan, store_fan, fan_power, id - 1);                 \
+static SENSOR_DEVICE_ATTR_2(fan##id##_input, S_IRUGO,            \
+        show_fan, NULL, fan_speed_tacho0, id - 1);               \
+static SENSOR_DEVICE_ATTR_2(fan##id##_min, S_IRUGO | S_IWUSR,    \
+        show_fan, store_fan, fan_speed_min, id - 1);             \
+static SENSOR_DEVICE_ATTR_2(fan##id##_max, S_IRUGO | S_IWUSR,    \
+        show_fan, store_fan, fan_speed_max, id - 1);             \
+static SENSOR_DEVICE_ATTR_2(fan##id##_enable, S_IRUGO | S_IWUSR, \
+        show_fan, store_fan, fan_enable, id - 1);
+
+SENSOR_DEVICE_ATTR_FAN(1);
+SENSOR_DEVICE_ATTR_FAN(2);
+SENSOR_DEVICE_ATTR_FAN(3);
+SENSOR_DEVICE_ATTR_FAN(4);
+SENSOR_DEVICE_ATTR_FAN(5);
+SENSOR_DEVICE_ATTR_FAN(6);
+SENSOR_DEVICE_ATTR_FAN(7);
+SENSOR_DEVICE_ATTR_FAN(8);
+SENSOR_DEVICE_ATTR_FAN(9);
+SENSOR_DEVICE_ATTR_FAN(10);
+
+#define SENSOR_DEVICE_ATTR_CPLD(id)                      \
+static SENSOR_DEVICE_ATTR_2(cpld##id##_version, S_IRUGO, \
+        show_cpld, NULL, cpld_version, id - 1);
+
+SENSOR_DEVICE_ATTR_CPLD(1);
+SENSOR_DEVICE_ATTR_CPLD(2);
+SENSOR_DEVICE_ATTR_CPLD(3);
+
+#define SENSOR_DEVICE_ATTR_QSFP(id)                                 \
+static SENSOR_DEVICE_ATTR_2(qsfp##id##_status, S_IRUGO,             \
+        show_qsfp, NULL, qsfp_status, id - 1);                      \
+static SENSOR_DEVICE_ATTR_2(qsfp##id##_event, S_IRUGO | S_IWUSR,    \
+        show_qsfp, store_qsfp, qsfp_event, id - 1);                 \
+static SENSOR_DEVICE_ATTR_2(qsfp##id##_temp_input, S_IRUGO,         \
+        show_qsfp, NULL, qsfp_temp_input, id - 1);                  \
+static SENSOR_DEVICE_ATTR_2(qsfp##id##_temp_min, S_IRUGO | S_IWUSR, \
+        show_qsfp, store_qsfp, qsfp_temp_min, id - 1);              \
+static SENSOR_DEVICE_ATTR_2(qsfp##id##_temp_max, S_IRUGO,           \
+        show_qsfp, NULL, qsfp_temp_max, id - 1);                    \
+static SENSOR_DEVICE_ATTR_2(qsfp##id##_temp_crit, S_IRUGO,          \
+        show_qsfp, NULL, qsfp_temp_crit, id - 1)
+
+SENSOR_DEVICE_ATTR_QSFP(1);
+SENSOR_DEVICE_ATTR_QSFP(2);
+SENSOR_DEVICE_ATTR_QSFP(3);
+SENSOR_DEVICE_ATTR_QSFP(4);
+SENSOR_DEVICE_ATTR_QSFP(5);
+SENSOR_DEVICE_ATTR_QSFP(6);
+SENSOR_DEVICE_ATTR_QSFP(7);
+SENSOR_DEVICE_ATTR_QSFP(8);
+SENSOR_DEVICE_ATTR_QSFP(9);
+SENSOR_DEVICE_ATTR_QSFP(10);
+SENSOR_DEVICE_ATTR_QSFP(11);
+SENSOR_DEVICE_ATTR_QSFP(12);
+SENSOR_DEVICE_ATTR_QSFP(13);
+SENSOR_DEVICE_ATTR_QSFP(14);
+SENSOR_DEVICE_ATTR_QSFP(15);
+SENSOR_DEVICE_ATTR_QSFP(16);
+SENSOR_DEVICE_ATTR_QSFP(17);
+SENSOR_DEVICE_ATTR_QSFP(18);
+SENSOR_DEVICE_ATTR_QSFP(19);
+SENSOR_DEVICE_ATTR_QSFP(20);
+SENSOR_DEVICE_ATTR_QSFP(21);
+SENSOR_DEVICE_ATTR_QSFP(22);
+SENSOR_DEVICE_ATTR_QSFP(23);
+SENSOR_DEVICE_ATTR_QSFP(24);
+SENSOR_DEVICE_ATTR_QSFP(25);
+SENSOR_DEVICE_ATTR_QSFP(26);
+SENSOR_DEVICE_ATTR_QSFP(27);
+SENSOR_DEVICE_ATTR_QSFP(28);
+SENSOR_DEVICE_ATTR_QSFP(29);
+SENSOR_DEVICE_ATTR_QSFP(30);
+SENSOR_DEVICE_ATTR_QSFP(31);
+SENSOR_DEVICE_ATTR_QSFP(32);
+SENSOR_DEVICE_ATTR_QSFP(33);
+SENSOR_DEVICE_ATTR_QSFP(34);
+SENSOR_DEVICE_ATTR_QSFP(35);
+SENSOR_DEVICE_ATTR_QSFP(36);
+SENSOR_DEVICE_ATTR_QSFP(37);
+SENSOR_DEVICE_ATTR_QSFP(38);
+SENSOR_DEVICE_ATTR_QSFP(39);
+SENSOR_DEVICE_ATTR_QSFP(40);
+SENSOR_DEVICE_ATTR_QSFP(41);
+SENSOR_DEVICE_ATTR_QSFP(42);
+SENSOR_DEVICE_ATTR_QSFP(43);
+SENSOR_DEVICE_ATTR_QSFP(44);
+SENSOR_DEVICE_ATTR_QSFP(45);
+SENSOR_DEVICE_ATTR_QSFP(46);
+SENSOR_DEVICE_ATTR_QSFP(47);
+SENSOR_DEVICE_ATTR_QSFP(48);
+SENSOR_DEVICE_ATTR_QSFP(49);
+SENSOR_DEVICE_ATTR_QSFP(50);
+SENSOR_DEVICE_ATTR_QSFP(51);
+SENSOR_DEVICE_ATTR_QSFP(52);
+SENSOR_DEVICE_ATTR_QSFP(53);
+SENSOR_DEVICE_ATTR_QSFP(54);
+SENSOR_DEVICE_ATTR_QSFP(55);
+SENSOR_DEVICE_ATTR_QSFP(56);
+SENSOR_DEVICE_ATTR_QSFP(57);
+SENSOR_DEVICE_ATTR_QSFP(58);
+SENSOR_DEVICE_ATTR_QSFP(59);
+SENSOR_DEVICE_ATTR_QSFP(60);
+SENSOR_DEVICE_ATTR_QSFP(61);
+SENSOR_DEVICE_ATTR_QSFP(62);
+SENSOR_DEVICE_ATTR_QSFP(63);
+SENSOR_DEVICE_ATTR_QSFP(64);
+
+#define TEMP_ATTRS(id)                                   \
+        &sensor_dev_attr_temp##id##_input.dev_attr.attr, \
+        &sensor_dev_attr_temp##id##_min.dev_attr.attr,   \
+        &sensor_dev_attr_temp##id##_max.dev_attr.attr,   \
+        &sensor_dev_attr_temp##id##_crit.dev_attr.attr,
+
+#define FAN_ATTRS(id)                                   \
+        &sensor_dev_attr_pwm##id.dev_attr.attr,         \
+        &sensor_dev_attr_fan##id##_input.dev_attr.attr, \
+        &sensor_dev_attr_fan##id##_min.dev_attr.attr,   \
+        &sensor_dev_attr_fan##id##_max.dev_attr.attr,   \
+        &sensor_dev_attr_fan##id##_enable.dev_attr.attr,
+
+#define CPLD_ATTRS(id)                                     \
+        &sensor_dev_attr_cpld##id##_version.dev_attr.attr,
+
+#define QSFP_ATTRS(id)                                        \
+        &sensor_dev_attr_qsfp##id##_status.dev_attr.attr,     \
+        &sensor_dev_attr_qsfp##id##_event.dev_attr.attr,      \
+        &sensor_dev_attr_qsfp##id##_temp_input.dev_attr.attr, \
+        &sensor_dev_attr_qsfp##id##_temp_min.dev_attr.attr,   \
+        &sensor_dev_attr_qsfp##id##_temp_max.dev_attr.attr,   \
+        &sensor_dev_attr_qsfp##id##_temp_crit.dev_attr.attr,
+
+static struct attribute *temp_attributes[] = {
+        TEMP_ATTRS(1)
+        TEMP_ATTRS(2)
+        TEMP_ATTRS(3)
+        TEMP_ATTRS(4)
+        TEMP_ATTRS(5)
+        TEMP_ATTRS(6)
+        TEMP_ATTRS(7)
+        TEMP_ATTRS(8)
+        TEMP_ATTRS(9)
+        TEMP_ATTRS(10)
+        TEMP_ATTRS(11)
+        TEMP_ATTRS(12)
+        TEMP_ATTRS(13)
+        TEMP_ATTRS(14)
+        TEMP_ATTRS(15)
+        TEMP_ATTRS(16)
+        TEMP_ATTRS(17)
+        TEMP_ATTRS(18)
+        TEMP_ATTRS(19)
+        TEMP_ATTRS(20)
+        TEMP_ATTRS(21)
+        TEMP_ATTRS(22)
+        TEMP_ATTRS(23)
+        TEMP_ATTRS(24)
+        TEMP_ATTRS(25)
+        TEMP_ATTRS(26)
+        TEMP_ATTRS(27)
+        TEMP_ATTRS(28)
+        TEMP_ATTRS(29)
+        TEMP_ATTRS(30)
+        TEMP_ATTRS(31)
+        TEMP_ATTRS(32)
+        TEMP_ATTRS(33)
+        TEMP_ATTRS(34)
+        TEMP_ATTRS(35)
+        TEMP_ATTRS(36)
+        TEMP_ATTRS(37)
+        TEMP_ATTRS(38)
+        TEMP_ATTRS(39)
+        TEMP_ATTRS(40)
+        TEMP_ATTRS(41)
+        TEMP_ATTRS(42)
+        TEMP_ATTRS(43)
+        TEMP_ATTRS(44)
+        TEMP_ATTRS(45)
+        TEMP_ATTRS(46)
+        TEMP_ATTRS(47)
+        TEMP_ATTRS(48)
+        TEMP_ATTRS(49)
+        TEMP_ATTRS(50)
+        TEMP_ATTRS(51)
+        TEMP_ATTRS(52)
+        TEMP_ATTRS(53)
+        TEMP_ATTRS(54)
+        TEMP_ATTRS(55)
+        TEMP_ATTRS(56)
+        TEMP_ATTRS(57)
+        TEMP_ATTRS(58)
+        TEMP_ATTRS(59)
+        TEMP_ATTRS(60)
+        TEMP_ATTRS(61)
+        TEMP_ATTRS(62)
+        TEMP_ATTRS(63)
+        TEMP_ATTRS(64)
+        TEMP_ATTRS(65)
+        NULL
+};
+
+static struct attribute *fan_attributes[] = {
+        FAN_ATTRS(1)
+        FAN_ATTRS(2)
+        FAN_ATTRS(3)
+        FAN_ATTRS(4)
+        FAN_ATTRS(5)
+        FAN_ATTRS(6)
+        FAN_ATTRS(7)
+        FAN_ATTRS(8)
+        FAN_ATTRS(9)
+        FAN_ATTRS(10)
+        NULL
+};
+
+static struct attribute *cpld_attributes[] = {
+        CPLD_ATTRS(1)
+        CPLD_ATTRS(2)
+        CPLD_ATTRS(3)
+        NULL
+};
+
+static struct attribute *qsfp_attributes[] = {
+        QSFP_ATTRS(1)
+        QSFP_ATTRS(2)
+        QSFP_ATTRS(3)
+        QSFP_ATTRS(4)
+        QSFP_ATTRS(5)
+        QSFP_ATTRS(6)
+        QSFP_ATTRS(7)
+        QSFP_ATTRS(8)
+        QSFP_ATTRS(9)
+        QSFP_ATTRS(10)
+        QSFP_ATTRS(11)
+        QSFP_ATTRS(12)
+        QSFP_ATTRS(13)
+        QSFP_ATTRS(14)
+        QSFP_ATTRS(15)
+        QSFP_ATTRS(16)
+        QSFP_ATTRS(17)
+        QSFP_ATTRS(18)
+        QSFP_ATTRS(19)
+        QSFP_ATTRS(20)
+        QSFP_ATTRS(21)
+        QSFP_ATTRS(22)
+        QSFP_ATTRS(23)
+        QSFP_ATTRS(24)
+        QSFP_ATTRS(25)
+        QSFP_ATTRS(26)
+        QSFP_ATTRS(27)
+        QSFP_ATTRS(28)
+        QSFP_ATTRS(29)
+        QSFP_ATTRS(30)
+        QSFP_ATTRS(31)
+        QSFP_ATTRS(32)
+        QSFP_ATTRS(33)
+        QSFP_ATTRS(34)
+        QSFP_ATTRS(35)
+        QSFP_ATTRS(36)
+        QSFP_ATTRS(37)
+        QSFP_ATTRS(38)
+        QSFP_ATTRS(39)
+        QSFP_ATTRS(40)
+        QSFP_ATTRS(41)
+        QSFP_ATTRS(42)
+        QSFP_ATTRS(43)
+        QSFP_ATTRS(44)
+        QSFP_ATTRS(45)
+        QSFP_ATTRS(46)
+        QSFP_ATTRS(47)
+        QSFP_ATTRS(48)
+        QSFP_ATTRS(49)
+        QSFP_ATTRS(50)
+        QSFP_ATTRS(51)
+        QSFP_ATTRS(52)
+        QSFP_ATTRS(53)
+        QSFP_ATTRS(54)
+        QSFP_ATTRS(55)
+        QSFP_ATTRS(56)
+        QSFP_ATTRS(57)
+        QSFP_ATTRS(58)
+        QSFP_ATTRS(59)
+        QSFP_ATTRS(60)
+        QSFP_ATTRS(61)
+        QSFP_ATTRS(62)
+        QSFP_ATTRS(63)
+        QSFP_ATTRS(64)
+        NULL
+};
+
+/* Device attributes */
+#define TEMP_DEV_ATTRS(id)                          \
+        &sensor_dev_attr_temp##id##_input.dev_attr, \
+        &sensor_dev_attr_temp##id##_min.dev_attr,   \
+        &sensor_dev_attr_temp##id##_max.dev_attr,   \
+        &sensor_dev_attr_temp##id##_crit.dev_attr,
+
+#define FAN_DEV_ATTRS(id)                          \
+        &sensor_dev_attr_pwm##id.dev_attr,         \
+        &sensor_dev_attr_fan##id##_input.dev_attr, \
+        &sensor_dev_attr_fan##id##_min.dev_attr,   \
+        &sensor_dev_attr_fan##id##_max.dev_attr,   \
+        &sensor_dev_attr_fan##id##_enable.dev_attr,
+
+#define QSFP_DEV_ATTRS(id)                               \
+        &sensor_dev_attr_qsfp##id##_status.dev_attr,     \
+        &sensor_dev_attr_qsfp##id##_event.dev_attr,      \
+        &sensor_dev_attr_qsfp##id##_temp_input.dev_attr, \
+        &sensor_dev_attr_qsfp##id##_temp_max.dev_attr,   \
+        &sensor_dev_attr_qsfp##id##_temp_crit.dev_attr,
+
+#define CPLD_DEV_ATTRS(id)                            \
+        &sensor_dev_attr_cpld##id##_version.dev_attr,
+
+struct device_attribute *asic_dev_temp_attributes[] = {
+        TEMP_DEV_ATTRS(1)
+        TEMP_DEV_ATTRS(2)
+        TEMP_DEV_ATTRS(3)
+        TEMP_DEV_ATTRS(4)
+        TEMP_DEV_ATTRS(5)
+        TEMP_DEV_ATTRS(6)
+        TEMP_DEV_ATTRS(7)
+        TEMP_DEV_ATTRS(8)
+        TEMP_DEV_ATTRS(9)
+        TEMP_DEV_ATTRS(10)
+        TEMP_DEV_ATTRS(11)
+        TEMP_DEV_ATTRS(12)
+        TEMP_DEV_ATTRS(13)
+        TEMP_DEV_ATTRS(14)
+        TEMP_DEV_ATTRS(15)
+        TEMP_DEV_ATTRS(16)
+        TEMP_DEV_ATTRS(17)
+        TEMP_DEV_ATTRS(18)
+        TEMP_DEV_ATTRS(19)
+        TEMP_DEV_ATTRS(20)
+        TEMP_DEV_ATTRS(21)
+        TEMP_DEV_ATTRS(22)
+        TEMP_DEV_ATTRS(23)
+        TEMP_DEV_ATTRS(24)
+        TEMP_DEV_ATTRS(25)
+        TEMP_DEV_ATTRS(26)
+        TEMP_DEV_ATTRS(27)
+        TEMP_DEV_ATTRS(28)
+        TEMP_DEV_ATTRS(29)
+        TEMP_DEV_ATTRS(30)
+        TEMP_DEV_ATTRS(31)
+        TEMP_DEV_ATTRS(32)
+        TEMP_DEV_ATTRS(33)
+        TEMP_DEV_ATTRS(34)
+        TEMP_DEV_ATTRS(35)
+        TEMP_DEV_ATTRS(36)
+        TEMP_DEV_ATTRS(37)
+        TEMP_DEV_ATTRS(38)
+        TEMP_DEV_ATTRS(39)
+        TEMP_DEV_ATTRS(40)
+        TEMP_DEV_ATTRS(41)
+        TEMP_DEV_ATTRS(42)
+        TEMP_DEV_ATTRS(43)
+        TEMP_DEV_ATTRS(44)
+        TEMP_DEV_ATTRS(45)
+        TEMP_DEV_ATTRS(46)
+        TEMP_DEV_ATTRS(47)
+        TEMP_DEV_ATTRS(48)
+        TEMP_DEV_ATTRS(49)
+        TEMP_DEV_ATTRS(50)
+        TEMP_DEV_ATTRS(51)
+        TEMP_DEV_ATTRS(52)
+        TEMP_DEV_ATTRS(53)
+        TEMP_DEV_ATTRS(54)
+        TEMP_DEV_ATTRS(55)
+        TEMP_DEV_ATTRS(56)
+        TEMP_DEV_ATTRS(57)
+        TEMP_DEV_ATTRS(58)
+        TEMP_DEV_ATTRS(59)
+        TEMP_DEV_ATTRS(60)
+        TEMP_DEV_ATTRS(61)
+        TEMP_DEV_ATTRS(62)
+        TEMP_DEV_ATTRS(63)
+        TEMP_DEV_ATTRS(64)
+        TEMP_DEV_ATTRS(65)
+        NULL
+};
+
+struct device_attribute *asic_dev_fan_attributes[] = {
+        FAN_DEV_ATTRS(1)
+        FAN_DEV_ATTRS(2)
+        FAN_DEV_ATTRS(3)
+        FAN_DEV_ATTRS(4)
+        FAN_DEV_ATTRS(5)
+        FAN_DEV_ATTRS(6)
+        FAN_DEV_ATTRS(7)
+        FAN_DEV_ATTRS(8)
+        FAN_DEV_ATTRS(9)
+        FAN_DEV_ATTRS(10)
+        NULL
+};
+
+struct device_attribute *asic_dev_cpld_attributes[] = {
+        CPLD_DEV_ATTRS(1)
+        CPLD_DEV_ATTRS(2)
+        CPLD_DEV_ATTRS(3)
+        NULL
+};
+
+struct device_attribute *asic_dev_qsfp_attributes[] = {
+        QSFP_DEV_ATTRS(1)
+        QSFP_DEV_ATTRS(2)
+        QSFP_DEV_ATTRS(3)
+        QSFP_DEV_ATTRS(4)
+        QSFP_DEV_ATTRS(5)
+        QSFP_DEV_ATTRS(6)
+        QSFP_DEV_ATTRS(7)
+        QSFP_DEV_ATTRS(8)
+        QSFP_DEV_ATTRS(9)
+        QSFP_DEV_ATTRS(10)
+        QSFP_DEV_ATTRS(11)
+        QSFP_DEV_ATTRS(12)
+        QSFP_DEV_ATTRS(13)
+        QSFP_DEV_ATTRS(14)
+        QSFP_DEV_ATTRS(15)
+        QSFP_DEV_ATTRS(16)
+        QSFP_DEV_ATTRS(17)
+        QSFP_DEV_ATTRS(18)
+        QSFP_DEV_ATTRS(19)
+        QSFP_DEV_ATTRS(20)
+        QSFP_DEV_ATTRS(21)
+        QSFP_DEV_ATTRS(22)
+        QSFP_DEV_ATTRS(23)
+        QSFP_DEV_ATTRS(24)
+        QSFP_DEV_ATTRS(25)
+        QSFP_DEV_ATTRS(26)
+        QSFP_DEV_ATTRS(27)
+        QSFP_DEV_ATTRS(28)
+        QSFP_DEV_ATTRS(29)
+        QSFP_DEV_ATTRS(30)
+        QSFP_DEV_ATTRS(31)
+        QSFP_DEV_ATTRS(32)
+        QSFP_DEV_ATTRS(33)
+        QSFP_DEV_ATTRS(34)
+        QSFP_DEV_ATTRS(35)
+        QSFP_DEV_ATTRS(36)
+        QSFP_DEV_ATTRS(37)
+        QSFP_DEV_ATTRS(38)
+        QSFP_DEV_ATTRS(39)
+        QSFP_DEV_ATTRS(40)
+        QSFP_DEV_ATTRS(41)
+        QSFP_DEV_ATTRS(42)
+        QSFP_DEV_ATTRS(43)
+        QSFP_DEV_ATTRS(44)
+        QSFP_DEV_ATTRS(45)
+        QSFP_DEV_ATTRS(46)
+        QSFP_DEV_ATTRS(47)
+        QSFP_DEV_ATTRS(48)
+        QSFP_DEV_ATTRS(49)
+        QSFP_DEV_ATTRS(50)
+        QSFP_DEV_ATTRS(51)
+        QSFP_DEV_ATTRS(52)
+        QSFP_DEV_ATTRS(53)
+        QSFP_DEV_ATTRS(54)
+        QSFP_DEV_ATTRS(55)
+        QSFP_DEV_ATTRS(56)
+        QSFP_DEV_ATTRS(57)
+        QSFP_DEV_ATTRS(58)
+        QSFP_DEV_ATTRS(59)
+        QSFP_DEV_ATTRS(60)
+        QSFP_DEV_ATTRS(61)
+        QSFP_DEV_ATTRS(62)
+        QSFP_DEV_ATTRS(63)
+        QSFP_DEV_ATTRS(64)
+        NULL
+};
+
+#define DEVICE_ATTR_FAN_CREATE(hwmon, id) {                                               \
+	int i = id, j;                                                                    \
+	err = device_create_file(hwmon, asic_dev_fan_attributes[i++]);                    \
+	err = (err == 0) ? device_create_file(hwmon, asic_dev_fan_attributes[i++]) : err; \
+	err = (err == 0) ? device_create_file(hwmon, asic_dev_fan_attributes[i++]) : err; \
+	err = (err == 0) ? device_create_file(hwmon, asic_dev_fan_attributes[i++]) : err; \
+	err = (err == 0) ? device_create_file(hwmon, asic_dev_fan_attributes[i++]) : err; \
+	if (err) {                                                                        \
+		for (j = i - 1; j >= id; j--)                                             \
+			device_remove_file(hwmon, asic_dev_fan_attributes[j]);            \
+		goto fail_create_file; }}
+
+#define DEVICE_ATTR_FAN_REMOVE(hwmon, id) {                      \
+	int i = id;                                              \
+        device_remove_file(hwmon, asic_dev_fan_attributes[i++]); \
+        device_remove_file(hwmon, asic_dev_fan_attributes[i++]); \
+        device_remove_file(hwmon, asic_dev_fan_attributes[i++]); \
+        device_remove_file(hwmon, asic_dev_fan_attributes[i++]); \
+        device_remove_file(hwmon, asic_dev_fan_attributes[i]); }
+
+#define DEVICE_ATTR_QSFP_CREATE(asicdata, client, id, eeprom_id) {                                       \
+	int i = id, j;                                                                                   \
+	err = device_create_file(asicdata->hwmon_dev, asic_dev_qsfp_attributes[i++]);                    \
+	err = (err == 0) ? device_create_file(asicdata->hwmon_dev, asic_dev_qsfp_attributes[i++]) : err; \
+	err = (err == 0) ? device_create_file(asicdata->hwmon_dev, asic_dev_qsfp_attributes[i++]) : err; \
+	err = (err == 0) ? device_create_file(asicdata->hwmon_dev, asic_dev_qsfp_attributes[i++]) : err; \
+	err = (err == 0) ? device_create_file(asicdata->hwmon_dev, asic_dev_qsfp_attributes[i]) : err;   \
+	if (err) {                                                                                       \
+		for (j = i - 1; j >= id; j--)                                                            \
+			device_remove_file(asicdata->hwmon_dev, asic_dev_qsfp_attributes[j]);            \
+		goto fail_create_file; }}
+
+#define DEVICE_ATTR_QSFP_REMOVE(asicdata, client, id, eeprom_id) {              \
+	int i = id;                                                             \
+        device_remove_file(asicdata->hwmon_dev, asic_dev_qsfp_attributes[i++]); \
+        device_remove_file(asicdata->hwmon_dev, asic_dev_qsfp_attributes[i++]); \
+        device_remove_file(asicdata->hwmon_dev, asic_dev_qsfp_attributes[i++]); \
+        device_remove_file(asicdata->hwmon_dev, asic_dev_qsfp_attributes[i++]); \
+        device_remove_file(asicdata->hwmon_dev, asic_dev_qsfp_attributes[i]); }
+
+#define DEVICE_ATTR_TEMP_CREATE(hwmon, id) {                                               \
+	int i = id, j;                                                                     \
+	err = device_create_file(hwmon, asic_dev_temp_attributes[i++]);                    \
+	err = (err == 0) ? device_create_file(hwmon, asic_dev_temp_attributes[i++]) : err; \
+	err = (err == 0) ? device_create_file(hwmon, asic_dev_temp_attributes[i++]) : err; \
+	err = (err == 0) ? device_create_file(hwmon, asic_dev_temp_attributes[i]) : err;   \
+	if (err) {                                                                         \
+		for (j = i - 1; j >= id; j--)                                              \
+			device_remove_file(hwmon, asic_dev_temp_attributes[j]);            \
+		goto fail_create_file; }}
+
+#define DEVICE_ATTR_TEMP_REMOVE(hwmon, id) {                      \
+	int i = id;                                               \
+        device_remove_file(hwmon, asic_dev_temp_attributes[i++]); \
+        device_remove_file(hwmon, asic_dev_temp_attributes[i++]); \
+        device_remove_file(hwmon, asic_dev_temp_attributes[i++]); \
+        device_remove_file(hwmon, asic_dev_temp_attributes[i]); }
+
+#define DEVICE_ATTR_CPLD_CREATE(hwmon, id)                             \
+	err = device_create_file(hwmon, asic_dev_cpld_attributes[id]); \
+	if (err)                                                       \
+		goto fail_create_file;
+
+#define DEVICE_ATTR_CPLD_REMOVE(hwmon, id)                       \
+        device_remove_file(hwmon, asic_dev_cpld_attributes[id]);
+
+static inline void fan_config_clean(struct asic_data *asicdata, u8 last_ind)
+{
+	int id;
+
+	for (id = 0; id < last_ind; id++) {
+                DEVICE_ATTR_FAN_REMOVE(asicdata->hwmon_dev, id * FAN_ATTR_NUM);
+	}
+
+	if (asicdata->fan_config.cooling_levels)
+                kfree(asicdata->fan_config.cooling_levels);
+
+	if (asicdata->fan_config.fan)
+                kfree(asicdata->fan_config.fan);
+}
+
+static int fan_config(struct asic_data *asicdata)
+{
+	int err = 0, id, i, j = 0, k = 0;
+        u8 tacho_num;
+        u8 tacho_map[FAN_NUM];
+        u8 pwm_id = 0;
+
+        if (tacho_flat)
+                tacho_num = 1;
+        else
+                tacho_num = num_tachos;
+
+        /* Get mapping and available numbers for tachometers and pwm. */
+        err = fan_get_config(&asicdata->switchdevif, &asicdata->fan_config);
+
+	for (i = 0; i < TACH_PWM_MAP; i++) {
+		if (asicdata->fan_config.tacho_active & BIT(i)) {
+			asicdata->fan_config.num_fan++;
+			tacho_map[j++] = i;
+		}
+		if (asicdata->fan_config.pwm_active & BIT(i)) {
+			pwm_id = i;
+		}
+	}
+
+	asicdata->fan_config.fan = kzalloc(asicdata->fan_config.num_fan *
+                                                        sizeof(*asicdata->fan_config.fan), GFP_KERNEL);
+	if (!asicdata->fan_config.fan) {
+		return -ENOMEM;
+	}
+
+        asicdata->fan_config.num_cooling_levels = ((MAX_PWM_DUTY_CYCLE - pwm_duty_cycle) % PWM_DUTY_CYCLE_STEP) ?
+                                                        (MAX_PWM_DUTY_CYCLE - pwm_duty_cycle) / PWM_DUTY_CYCLE_STEP + 2 :
+                                                        (MAX_PWM_DUTY_CYCLE - pwm_duty_cycle) / PWM_DUTY_CYCLE_STEP + 1;
+        asicdata->fan_config.cooling_levels = kzalloc(asicdata->fan_config.num_cooling_levels *
+                                                        sizeof(*asicdata->fan_config.cooling_levels), GFP_KERNEL);
+	if (!asicdata->fan_config.cooling_levels) {
+		kfree(asicdata->fan_config.fan);
+		return -ENOMEM;
+	}
+
+	for (id = 0; id < asicdata->fan_config.num_cooling_levels; id++) {
+		asicdata->fan_config.cooling_levels[id] = pwm_duty_cycle + id * PWM_DUTY_CYCLE_STEP;
+	}
+	if (asicdata->fan_config.cooling_levels[id - 1] > MAX_PWM_DUTY_CYCLE) {
+		asicdata->fan_config.cooling_levels[id - 1] = MAX_PWM_DUTY_CYCLE;
+	}
+        asicdata->fan_config.cooling_cur_level = 0;
+
+        for (id = 0; id < asicdata->fan_config.num_fan; id++) {
+		sprintf(asicdata->fan_config.fan[id].entry.name, "%s%d", "fan", id + 1);
+                asicdata->fan_config.fan[id].entry.index = id + 1;
+                asicdata->fan_config.fan[id].pwm_id = pwm_id;
+                asicdata->fan_config.fan[id].pwm_duty_cycle = pwm_duty_cycle;
+                asicdata->fan_config.fan[id].num_tachos = tacho_num;
+                for (j = 0; j < tacho_num; j++) {
+                        asicdata->fan_config.fan[id].tacho_id[j] = tacho_map[k++];
+                        asicdata->fan_config.fan[id].speed[j] = speed_min;
+                        asicdata->fan_config.fan[id].speed_min[j] = speed_min;
+                        asicdata->fan_config.fan[id].speed_max[j] = speed_max;
+                        asicdata->fan_config.fan[id].enable[j] = 0;
+                }
+
+                DEVICE_ATTR_FAN_CREATE(asicdata->hwmon_dev, id * FAN_ATTR_NUM);
+        }
+
+	return err;
+
+fail_create_file:
+	fan_config_clean(asicdata, id);
+
+	return err;
+}
+
+static inline void qsfp_config_clean(struct asic_data *asicdata, struct i2c_client *client, u8 last_ind)
+{
+	int id;
+
+	for (id = 0; id < last_ind; id++) {
+		DEVICE_ATTR_QSFP_REMOVE(asicdata, client, id * QSFP_ATTR_NUM, id);
+        }
+
+	if (asicdata->qsfp_config.eeprom_attr_list)
+		kfree(asicdata->qsfp_config.eeprom_attr_list);
+	if (asicdata->qsfp_config.eeprom)
+		kfree(asicdata->qsfp_config.eeprom);
+	if (asicdata->qsfp_config.module)
+		kfree(asicdata->qsfp_config.module);
+}
+
+static int qsfp_config(struct asic_data *asicdata, struct i2c_client *client)
+{
+	int id, err = 0;
+
+	asicdata->qsfp_config.module = kzalloc(asicdata->qsfp_config.num_modules *
+                                                        sizeof(*asicdata->qsfp_config.module), GFP_KERNEL);
+	if (!asicdata->qsfp_config.module) {
+		return -ENOMEM;
+	}
+	asicdata->qsfp_config.eeprom = kzalloc(asicdata->qsfp_config.num_modules *
+                                                        sizeof(*asicdata->qsfp_config.eeprom), GFP_KERNEL);
+	if (!asicdata->qsfp_config.eeprom) {
+		kfree(asicdata->qsfp_config.module);
+		return -ENOMEM;
+	}
+	asicdata->qsfp_config.eeprom_attr_list = kzalloc((asicdata->qsfp_config.num_modules+1) *
+								sizeof(asicdata->qsfp_config.eeprom), GFP_KERNEL);
+	if (!asicdata->qsfp_config.eeprom_attr_list) {
+		kfree(asicdata->qsfp_config.eeprom);
+		kfree(asicdata->qsfp_config.module);
+		return -ENOMEM;
+	}
+
+        for (id = 0; id < asicdata->qsfp_config.num_modules; id++) {
+		sprintf(asicdata->qsfp_config.module[id].entry.name, "%s%d", "qsfp", id + 1);
+		asicdata->qsfp_config.module[id].entry.index = id;
+                asicdata->qsfp_config.module[id].module_index = id;
+                asicdata->qsfp_config.module[id].lock = 0;
+                asicdata->qsfp_config.module[id].status = 0;
+
+                DEVICE_ATTR_QSFP_CREATE(asicdata, client, id * QSFP_ATTR_NUM, id);
+        }
+
+	return err;
+
+fail_create_file:
+	qsfp_config_clean(asicdata, client, id);
+
+	return err;
+}
+
+static void temp_config_clean(struct asic_data *asicdata, u8 last_ind)
+{
+	int id;
+
+	for (id = 0; id < last_ind; id++) {
+		DEVICE_ATTR_TEMP_REMOVE(asicdata->hwmon_dev, id * TEMP_ATTR_NUM);
+        }
+
+	if (asicdata->temp_config.sensor)
+                kfree(asicdata->temp_config.sensor);
+}
+
+static int temp_config(struct asic_data *asicdata)
+{
+	int err = 0, id;
+
+        /* Get number of active sensors. */
+        err = temp_get_config(&asicdata->switchdevif, &asicdata->temp_config);
+        asicdata->temp_config.num_sensors = asicdata->temp_config.sensor_active;
+
+	asicdata->temp_config.sensor = kzalloc(asicdata->temp_config.num_sensors *
+						sizeof(*asicdata->temp_config.sensor), GFP_KERNEL);
+	if (!asicdata->temp_config.sensor) {
+		return -ENOMEM;
+	}
+
+        for (id = 0; id < asicdata->temp_config.num_sensors; id++) {
+		asicdata->temp_config.sensor[id].entry.index = id;
+		sprintf(asicdata->temp_config.sensor[id].entry.name, "%s%d", "temp", id + 1);
+		asicdata->temp_config.sensor[id].sensor_index = id,
+		asicdata->temp_config.sensor[id].temperature = 0,
+		asicdata->temp_config.sensor[id].mte = mte;
+		asicdata->temp_config.sensor[id].mtr = mtr;
+		asicdata->temp_config.sensor[id].max_temperature = 0;
+		asicdata->temp_config.sensor[id].tee = tee;
+		asicdata->temp_config.sensor[id].temperature_threshold = temp_threshold;
+
+		DEVICE_ATTR_TEMP_CREATE(asicdata->hwmon_dev, id * TEMP_ATTR_NUM);
+        }
+
+	return err;
+
+fail_create_file:
+	temp_config_clean(asicdata, id);
+
+	return err;
+}
+
+static void cpld_config_clean(struct asic_data *asicdata, u8 last_ind)
+{
+	int id;
+
+	for (id = 0; id < last_ind; id++) {
+		DEVICE_ATTR_CPLD_REMOVE(asicdata->hwmon_dev, id);
+        }
+
+        if (asicdata->cpld_config.cpld)
+                kfree(asicdata->cpld_config.cpld);
+
+	return;
+}
+
+static int cpld_config(struct asic_data *asicdata)
+{
+	int err = 0, id;
+
+        asicdata->cpld_config.num_cpld = num_cpld;
+	asicdata->cpld_config.cpld = kzalloc(asicdata->cpld_config.num_cpld *
+						sizeof(*asicdata->cpld_config.cpld), GFP_KERNEL);
+	if (!asicdata->cpld_config.cpld) {
+		return -ENOMEM;
+	}
+
+        for (id = 0; id < num_cpld; id++) {
+		asicdata->cpld_config.cpld[id].entry.index = id;
+		sprintf(asicdata->cpld_config.cpld[id].entry.name, "%s%d", "cpld", id + 1);
+                asicdata->cpld_config.cpld[id].index = id;
+
+                DEVICE_ATTR_CPLD_CREATE(asicdata->hwmon_dev, id);
+        }
+	return err;
+
+fail_create_file:
+	cpld_config_clean(asicdata, id);
+
+	return err;
+}
+
+static int cz_get_max_state(struct thermal_cooling_device *tcdev,
+				  unsigned long *state)
+{
+        struct asic_data *asicdata = (struct asic_data *)tcdev->devdata;
+
+	*state = asicdata->fan_config.num_cooling_levels;
+
+	return 0;
+}
+
+static int cz_get_cur_state(struct thermal_cooling_device *tcdev,
+				  unsigned long *state)
+{
+        struct asic_data *asicdata = (struct asic_data *)tcdev->devdata;
+
+	struct fan_config *fan;
+	int err;
+
+	fan = &asicdata->fan_config.fan[0];
+	err = fan_get_power(&asicdata->switchdevif, fan, 1);
+	if (err)
+		return err;
+
+	if (fan->pwm_duty_cycle ==
+		asicdata->fan_config.cooling_levels[asicdata->fan_config.cooling_cur_level]) {
+		*state = asicdata->fan_config.cooling_cur_level;
+	}
+	else {
+		*state = asicdata->fan_config.cooling_cur_level =
+                                                        ((fan->pwm_duty_cycle - pwm_duty_cycle) % PWM_DUTY_CYCLE_STEP) ?
+                                                        (fan->pwm_duty_cycle - pwm_duty_cycle) / PWM_DUTY_CYCLE_STEP + 1 :
+                                                        (fan->pwm_duty_cycle - pwm_duty_cycle) / PWM_DUTY_CYCLE_STEP;
+	}
+
+	return 0;
+}
+
+static int cz_set_cur_state(struct thermal_cooling_device *tcdev,
+				  unsigned long state)
+{
+        struct asic_data *asicdata = (struct asic_data *)tcdev->devdata;
+	struct fan_config *fan;
+	int err;
+
+	asicdata->fan_config.cooling_cur_level = (state >= asicdata->fan_config.num_cooling_levels - 1) ?
+						asicdata->fan_config.num_cooling_levels - 1 : state;
+
+	fan = &asicdata->fan_config.fan[0];
+	fan->pwm_duty_cycle = asicdata->fan_config.cooling_levels[asicdata->fan_config.cooling_cur_level];
+	err = fan_set_power(&asicdata->switchdevif, fan);
+	if (err)
+		return err;
+
+	return 0;
+}
+
+static int tz_get_temp(struct thermal_zone_device *tzdev, int *temperature)
+{
+        struct asic_data *asicdata = (struct asic_data *)tzdev->devdata;
+	struct temp_config *temp = NULL;
+	int err;
+
+	temp = &asicdata->temp_config.sensor[0];
+        err = temp_get(&asicdata->switchdevif, temp, 0, 1);
+	if (err)
+		return err;
+
+        *temperature = temp->temperature;
+
+        return 0;
+}
+
+static int tz_get_trip_type(struct thermal_zone_device *tzdev,
+				int trip, enum thermal_trip_type *type)
+{
+        struct asic_data *asicdata = (struct asic_data *)tzdev->devdata;
+
+        if (trip > asicdata->fan_config.num_cooling_levels) {
+		dev_err(asicdata->hwmon_dev, "Invalid trip point %d\n", trip);
+		return -EINVAL;
+	}
+
+	*type = THERMAL_TRIP_PASSIVE;
+
+	return 0;
+}
+
+static int tz_get_trip_temp(struct thermal_zone_device *tzdev,
+				int trip, int *temperature)
+{
+        struct asic_data *asicdata = (struct asic_data *)tzdev->devdata;
+
+        if (trip > asicdata->fan_config.num_cooling_levels) {
+		dev_err(asicdata->hwmon_dev, "Invalid trip point %d\n", trip);
+		return -EINVAL;
+	}
+
+	*temperature = asicdata->temp_config.sensor[0].temperature_threshold;
+
+	return 0;
+}
+
+static const struct thermal_cooling_device_ops asic_cool_ops = {
+        .get_max_state = cz_get_max_state,
+        .get_cur_state = cz_get_cur_state,
+        .set_cur_state = cz_set_cur_state,
+};
+
+static struct thermal_zone_device_ops asic_thermal_ops = {
+	.get_temp = tz_get_temp,
+	.get_trip_type = tz_get_trip_type,
+	.get_trip_temp = tz_get_trip_temp,
+};
+
+static const unsigned short normal_i2c[] = { 0x48, I2C_CLIENT_END };
+static int asic_probe(struct i2c_client *client, const struct i2c_device_id *devid)
+{
+        struct asic_data *asicdata;
+	u16 device_id = 0;
+        int attr_size = 0;
+        int err, i, attr_off = 0;
+
+	asicdata = kzalloc(sizeof(struct asic_data), GFP_KERNEL);
+	if (!asicdata) {
+		err = -ENOMEM;
+		goto exit_no_memory;
+	}
+	i2c_set_clientdata(client, asicdata);
+
+	INIT_LIST_HEAD(&asicdata->list);
+	list_add(&asicdata->list, &asic_instances.list);
+
+        /* Register with asic registers access interface */
+	memset(&asicdata->switchdevif, 0, sizeof(struct switchdev_if));
+	asicdata->switchdevif.DEV_CONTEXT = __symbol_get("sx_get_dev_context");
+        if (IS_ERR(asicdata->switchdevif.DEV_CONTEXT)) {
+                err = PTR_ERR(asicdata->switchdevif.DEV_CONTEXT);
+                goto exit_remove;
+        }
+	asicdata->switchdevif.REG_MFSC = __symbol_get("sx_ACCESS_REG_MFSC");
+	asicdata->switchdevif.REG_MFSM = __symbol_get("sx_ACCESS_REG_MFSM");
+	asicdata->switchdevif.REG_MTMP = __symbol_get("sx_ACCESS_REG_MTMP");
+	asicdata->switchdevif.REG_MTCAP = __symbol_get("sx_ACCESS_REG_MTCAP");
+	asicdata->switchdevif.REG_MCIA = __symbol_get("sx_ACCESS_REG_MCIA");
+	asicdata->switchdevif.REG_PMPC = __symbol_get("sx_ACCESS_REG_PMPC");
+	asicdata->switchdevif.REG_MSCI = __symbol_get("sx_ACCESS_REG_MSCI");
+	asicdata->switchdevif.REG_MJTAG = __symbol_get("sx_ACCESS_REG_MJTAG");
+	asicdata->switchdevif.REG_PMAOS = __symbol_get("sx_ACCESS_REG_PMAOS");
+	asicdata->switchdevif.REG_MFCR = __symbol_get("sx_ACCESS_REG_MFCR");
+	asicdata->switchdevif.REG_MGIR = __symbol_get("sx_ACCESS_REG_MGIR");
+
+	asicdata->switchdevif.dev_id = asic_dev_id;
+	mutex_init(&asicdata->switchdevif.access_lock);
+
+	kref_init(&asicdata->kref);
+
+        /* Set asic id and register sysfs hooks */
+	asicdata->port_cap = devid->driver_data;
+	switch (devid->driver_data) {
+	case asic_drv_32_ports:
+	default:
+		asicdata->asic_id = asic_drv_32_ports;
+		asicdata->qsfp_config.num_modules = 32;
+		break;
+	case asic_drv_64_ports:
+		asicdata->asic_id = asic_drv_64_ports;
+		asicdata->qsfp_config.num_modules = 64;
+		break;
+	case asic_drv_54_ports:
+		asicdata->asic_id = asic_drv_54_ports;
+		asicdata->qsfp_config.num_modules = 54;
+		break;
+	case asic_drv_36_ports:
+		asicdata->asic_id = asic_drv_36_ports;
+		asicdata->qsfp_config.num_modules = 36;
+		break;
+	case asic_drv_16_ports:
+		asicdata->asic_id = asic_drv_16_ports;
+		asicdata->qsfp_config.num_modules = 16;
+		break;
+	}
+
+        /* Configure sysfs infrastructure */
+        err = fan_config(asicdata);
+        err = (err == 0) ? temp_config(asicdata) : err;
+        err = (err == 0) ? qsfp_config(asicdata, client) : err;
+        err = (err == 0) ? cpld_config(asicdata) : err;
+        if (err)
+                goto exit_remove;
+
+	attr_size += ((asicdata->temp_config.num_sensors * TEMP_ATTR_NUM) +
+			(asicdata->fan_config.num_fan * FAN_ATTR_NUM) +
+			(asicdata->cpld_config.num_cpld *  CPLD_ATTR_NUM) +
+			(asicdata->qsfp_config.num_modules * QSFP_ATTR_NUM)) * sizeof(struct attribute *);
+
+	asicdata->group.attrs = kzalloc(attr_size, GFP_KERNEL);
+	if (!(asicdata->group.attrs)) {
+		err = -ENOMEM;
+		goto exit_no_memory2;
+	}
+
+	memcpy(asicdata->group.attrs, temp_attributes,
+		(asicdata->temp_config.num_sensors * TEMP_ATTR_NUM) * sizeof(struct attribute *));
+        attr_off += asicdata->temp_config.num_sensors * TEMP_ATTR_NUM;
+	memcpy(&asicdata->group.attrs[attr_off], fan_attributes,
+		(asicdata->fan_config.num_fan * FAN_ATTR_NUM) * sizeof(struct attribute *));
+        attr_off += asicdata->fan_config.num_fan * FAN_ATTR_NUM;
+	memcpy(&asicdata->group.attrs[attr_off], cpld_attributes,
+		(asicdata->cpld_config.num_cpld *  CPLD_ATTR_NUM) * sizeof(struct attribute *));
+        attr_off += asicdata->cpld_config.num_cpld *  CPLD_ATTR_NUM;
+	memcpy(&asicdata->group.attrs[attr_off], qsfp_attributes,
+		(asicdata->qsfp_config.num_modules * QSFP_ATTR_NUM) * sizeof(struct attribute *));
+	attr_off += asicdata->qsfp_config.num_modules * QSFP_ATTR_NUM;
+	asicdata->group.attrs[attr_off] = NULL;
+
+	for (i = 0; i < asicdata->qsfp_config.num_modules; i++) {
+		sysfs_bin_attr_init(&asicdata->qsfp_config.eeprom[i]);
+		asicdata->qsfp_config.eeprom[i].attr.name = kasprintf(GFP_KERNEL, "qsfp%d_eeprom", i + 1);
+		asicdata->qsfp_config.eeprom[i].attr.mode = S_IRUGO;
+		asicdata->qsfp_config.eeprom[i].read = qsfp_eeprom_bin_read;
+		asicdata->qsfp_config.eeprom[i].size = QSFP_PAGE_NUM * QSFP_PAGE_SIZE;
+		asicdata->qsfp_config.eeprom[i].private = (void *)&asicdata->qsfp_config.module[i];
+	}
+
+	asicdata->group.bin_attrs = &asicdata->qsfp_config.eeprom;
+
+        asicdata->groups[0] = &asicdata->group;
+
+        asicdata->hwmon_dev = devm_hwmon_device_register_with_groups(&client->dev,
+                                                                        "spectrum",
+                                                                        asicdata,
+                                                                        asicdata->groups);
+        if (IS_ERR(asicdata->hwmon_dev)) {
+                err = PTR_ERR(asicdata->hwmon_dev);
+                goto exit_remove;
+        }
+
+        err = mgir_get(&asicdata->switchdevif, &device_id);
+        if (err)
+                goto exit_remove;
+        switch(device_id) {
+        case 0xcb84:
+                asicdata->kind = spectrum;
+                asicdata->name = "spectrum";
+                break;
+        case 0xC738:
+                asicdata->kind = switchx2;
+                asicdata->name = "switchx2";
+                break;
+        default:
+                asicdata->kind = any_chip;
+                break;
+        }
+
+	if (auto_thermal_control) {
+		asicdata->tcdev = thermal_cooling_device_register("switchdev-cooling", asicdata,
+									&asic_cool_ops);
+		if (PTR_ERR_OR_ZERO(asicdata->tcdev)) {
+			err = PTR_ERR(asicdata->tcdev);
+			goto exit_remove;
+		}
+		asicdata->tzdev = thermal_zone_device_register("switchdev-thermal",
+								asicdata->fan_config.num_cooling_levels, 0,
+								asicdata, &asic_thermal_ops, NULL,
+								TEMP_PASSIVE_INTERVAL, TEMP_POLLING_INTERVAL);
+		if (PTR_ERR_OR_ZERO(asicdata->tzdev)) {
+			err = PTR_ERR(asicdata->tzdev);
+			goto exit_remove_cooling;
+		}
+		err = thermal_zone_bind_cooling_device(asicdata->tzdev, 0, asicdata->tcdev,
+							THERMAL_NO_LIMIT, THERMAL_NO_LIMIT,
+							THERMAL_WEIGHT_DEFAULT);
+        	if (err)
+			goto exit_remove_thermal;
+	}
+
+	return 0;
+
+exit_remove_thermal:
+	if (auto_thermal_control)
+		thermal_zone_device_unregister(asicdata->tzdev);
+
+exit_remove_cooling:
+	if (auto_thermal_control)
+		thermal_cooling_device_unregister(asicdata->tcdev);
+
+	kfree(asicdata->group.attrs);
+
+exit_no_memory2:
+	if (asicdata->switchdevif.REG_MFSC) __symbol_put("sx_ACCESS_REG_MFSC");
+	if (asicdata->switchdevif.REG_MFSM) __symbol_put("sx_ACCESS_REG_MFSM");
+	if (asicdata->switchdevif.REG_MTMP) __symbol_put("sx_ACCESS_REG_MTMP");
+	if (asicdata->switchdevif.REG_MTCAP) __symbol_put("sx_ACCESS_REG_MTCAP");
+	if (asicdata->switchdevif.REG_MCIA) __symbol_put("sx_ACCESS_REG_MCIA");
+	if (asicdata->switchdevif.REG_PMPC) __symbol_put("sx_ACCESS_REG_PMPC");
+	if (asicdata->switchdevif.REG_MSCI) __symbol_put("sx_ACCESS_REG_MSCI");
+	if (asicdata->switchdevif.REG_MJTAG) __symbol_put("sx_ACCESS_REG_MJTAG");
+	if (asicdata->switchdevif.REG_PMAOS) __symbol_put("sx_ACCESS_REG_PMAOS");
+	if (asicdata->switchdevif.REG_MFCR) __symbol_put("sx_ACCESS_REG_MFCR");
+	if (asicdata->switchdevif.REG_MGIR) __symbol_put("sx_ACCESS_REG_MGIR");
+	if (asicdata->switchdevif.DEV_CONTEXT) __symbol_put("sx_get_dev_context");
+	mutex_destroy(&asicdata->switchdevif.access_lock);
+
+exit_remove:
+        if (!list_empty(&asic_instances.list)) {
+                list_del_rcu(&asicdata->list);
+        }
+	kfree(asicdata);
+
+exit_no_memory:
+	return err;
+}
+
+static int asic_detect(struct i2c_client *client,
+                       struct i2c_board_info *info)
+{
+	strlcpy(info->type, "mlnx-asic-drv", I2C_NAME_SIZE);
+	return 0;
+}
+
+static int asic_remove(struct i2c_client *client)
+{
+        struct asic_data *asicdata = i2c_get_clientdata(client);
+
+	if (auto_thermal_control) {
+		thermal_zone_unbind_cooling_device(asicdata->tzdev, 0, asicdata->tcdev);
+		thermal_zone_device_unregister(asicdata->tzdev);
+		thermal_cooling_device_unregister(asicdata->tcdev);
+	}
+
+	temp_config_clean(asicdata, asicdata->temp_config.num_sensors);
+	fan_config_clean(asicdata, asicdata->fan_config.num_fan);
+	qsfp_config_clean(asicdata, client, asicdata->qsfp_config.num_modules);
+        cpld_config_clean(asicdata, asicdata->cpld_config.num_cpld);
+
+	kfree(asicdata->group.attrs);
+	if (asicdata->switchdevif.REG_MFSC) __symbol_put("sx_ACCESS_REG_MFSC");
+	if (asicdata->switchdevif.REG_MFSM) __symbol_put("sx_ACCESS_REG_MFSM");
+	if (asicdata->switchdevif.REG_MTMP) __symbol_put("sx_ACCESS_REG_MTMP");
+	if (asicdata->switchdevif.REG_MTCAP) __symbol_put("sx_ACCESS_REG_MTCAP");
+	if (asicdata->switchdevif.REG_MCIA) __symbol_put("sx_ACCESS_REG_MCIA");
+	if (asicdata->switchdevif.REG_PMPC) __symbol_put("sx_ACCESS_REG_PMPC");
+	if (asicdata->switchdevif.REG_MSCI) __symbol_put("sx_ACCESS_REG_MSCI");
+	if (asicdata->switchdevif.REG_MJTAG) __symbol_put("sx_ACCESS_REG_MJTAG");
+	if (asicdata->switchdevif.REG_PMAOS) __symbol_put("sx_ACCESS_REG_PMAOS");
+	if (asicdata->switchdevif.REG_MFCR) __symbol_put("sx_ACCESS_REG_MFCR");
+	if (asicdata->switchdevif.REG_MGIR) __symbol_put("sx_ACCESS_REG_MGIR");
+	if (asicdata->switchdevif.DEV_CONTEXT) __symbol_put("sx_get_dev_context");
+	mutex_destroy(&asicdata->switchdevif.access_lock);
+
+	if (!list_empty(&asic_instances.list)) {
+		list_del_rcu(&asicdata->list);
+	}
+	kfree(asicdata);
+
+	return 0;
+}
+
+static const struct i2c_device_id asic_id[] = {
+        { "mlnx-asic-drv", asic_drv_32_ports },
+        { "mlnx-asic-drv-64", asic_drv_64_ports },
+        { "mlnx-asic-drv-54", asic_drv_54_ports },
+        { "mlnx-asic-drv-36", asic_drv_36_ports },
+        { "mlnx-asic-drv-16", asic_drv_16_ports },
+        { }
+};
+MODULE_DEVICE_TABLE(i2c, asic_id);
+
+static struct i2c_driver mlnx_asic_drv = {
+        .class          = I2C_CLASS_HWMON,
+        .driver = {
+                .name   = "mlnx-asic-drv",
+        },
+        .probe          = asic_probe,
+        .remove         = asic_remove,
+        .id_table       = asic_id,
+        .detect         = asic_detect,
+};
+
+static int __init mlnx_asic_drv_init(void)
+{
+	int err;
+
+	INIT_LIST_HEAD(&asic_instances.list);
+	err = i2c_add_driver(&mlnx_asic_drv);
+
+	printk(KERN_INFO "%s Version %s\n", ASIC_DRV_DESCRIPTION, ASIC_DRV_VERSION);
+
+	return err;
+}
+
+static void __exit mlnx_asic_drv_fini(void)
+{
+	struct asic_data *data, *next;
+
+	i2c_del_driver(&mlnx_asic_drv);
+
+	list_for_each_entry_safe(data, next, &asic_instances.list, list) {
+		if (!list_empty(&asic_instances.list)) {
+			list_del_rcu(&data->list);
+			kfree(data);
+		}
+	}
+}
+
+module_init(mlnx_asic_drv_init);
+module_exit(mlnx_asic_drv_fini);
+    
diff --git a/linux/drivers/hwmon/mellanox/mlnx-asic-i2c.c b/linux/drivers/hwmon/mellanox/mlnx-asic-i2c.c
new file mode 100644
index 0000000..a5b62c7
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/mlnx-asic-i2c.c
@@ -0,0 +1,2395 @@
+/**
+ *
+ * Copyright (C) 2012-2014, Mellanox Technologies Ltd.  ALL RIGHTS RESERVED.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/version.h>
+#include <linux/init.h>
+#include <linux/slab.h>
+#include <linux/timer.h>
+#include <linux/jiffies.h>
+#include <linux/i2c.h>
+#include <linux/hwmon.h>
+#include <linux/hwmon-sysfs.h>
+#include <linux/err.h>
+#include <linux/mutex.h>
+#include <linux/sysfs.h>
+#include <linux/platform_device.h>
+#include <linux/mod_devicetable.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/delay.h>
+
+static unsigned short cir_reg_num = 1;
+static unsigned short access_rtry = 0;
+
+#define MAX_EMAD_FRAME_SIZE 1518
+#define DIR_ROUTE_TLV_SIZE   132
+#define END_TLV_SIZE           4
+#define SX_MGIR_REG  0x9020 // Misc General Information Register
+
+/* Command Interface Register 
+   Read and write accesses to the CIR must be DWORD-sized and DWORD aligned 
+   (i.e., the access address and access size must both be multiplications of 4 bytes)
+*/
+#define  CIR_GO_SW           0x0 // Software ownership
+#define  CIR_GO_HW           0x1 // Hardware ownership
+#define  CIR_EVENT_NO_REPORT 0x0
+#define  CIR_EVENT_REPORT    0x1
+#define  CIR_STATUS_OFF      0x18
+#define  IN_PARAM_OFFSET     0x00
+#define  IN_MODIFIER_OFFSET  0x08
+#define  OUT_PARAM_OFFSET    0x0c
+#define  TOKEN_OFFSET        0x14
+#define  STATUS_OFFSET       0x18
+#define  OPMOD_SHIFT         12
+#define  EVENT_BIT           22
+#define  GO_BIT              23
+struct reg_cir_layout {
+        u32 in_param_h;        // Input Parameter: parameter or pointer to input mailbox 
+        u32 in_param_l;        // Input Parameter: parameter or pointer to input mailbox 
+        u32 input_modifier;    // Input Parameter Modifier
+        u32 out_param_h;       // Output Parameter: parameter or pointer to output mailbox 
+        u32 out_param_l;       // Output Parameter: parameter or pointer to output mailbox 
+        u16 token;
+        u8 status;
+        u8 event:1;
+        u8 go;
+        u16 opcode;
+        u8 opcode_modifier;
+};
+
+/* Software Reset Register */
+#define SW_RESET 0x1
+struct reg_sw_reset_layout {
+        u32 sw_reset:1;
+        u32 reserved1:31;
+};
+
+/* DoorBell Registers 
+   Located on the device BAR. The location of the doorbell registers can
+   be queried through the QUERY_FW
+*/
+struct reg_dorbell_layout {
+        u8 reserved[100];       // ??? Dorbell size 
+};
+
+/* Message Signalled Interrupt Register */
+struct reg_msix_layout {
+        u8 reserved[100];       // ??? Message Signalled Interrupt Register size
+};
+
+/* Clear Interrupt Register */
+struct reg_clr_irq_layout {
+        u32 clr_int;            // Clear Interrupt. Write transactions to this register will clear
+                                // (de-assert) the virtual interrupt output pins. The value
+                                // to be written in this register is obtained through the
+                                // QUERY_BOARDINFO command.
+                                // This register is write-only. Reading from this register will cause
+                                // undefined result
+};
+
+/* SwitchX BAR0 Layout
+   - Command Interface Register (CIR) is used to submit commands mainly for
+     initial configuration of the device. Two commands registers are supported
+     - Primary CIR - for command submission through PCIe
+     - Secondary CIR - for command submission through i2c. The secondary CIR supports 
+       restricted set of commands.
+   - Doorbell registers are used to update the HW with new descriptors posted to send/receive descriptor
+     queues or management of completion and event queues.
+   - Clear Interrupt Register
+   - MSIX Registers - SwitchX can generate interrupts in the form of PCI interrupt or Message Signalled Interrupt (MSIX).
+   - SW Reset Register
+*/
+struct sx_bar0_layout {
+#define SX_BAR0_OFFSET_PRIM    0x71000
+        struct reg_cir_layout prim_cmd_if;   // 71000h-71018h Primary Command Interface Register
+#define SX_BAR0_OFFSET_SECOND  0x72000
+        struct reg_cir_layout second_cmd_if; // 72000h-72018h Secondary Command Interface Register
+#define SX_SW_RESET_OFFSET     0xF0010
+        struct reg_sw_reset_layout sw_reset; // F00010h SW Reset
+        struct reg_dorbell_layout door_bell; // Doorbell Registers
+        struct reg_clr_irq_layout irq;       // Clear Interrupt Register
+        struct reg_msix_layout msix;         // MSIX Register
+};
+
+#define SX_MAX_REG_SIZE 128
+ 
+/* MGIR Misc General Information Register Layout */
+struct hw_info_layout { // 00h - 1Ch
+        u16 device_hw_revision;
+        u16 device_id;
+        u16 resrved0;
+        u8 dvfs; /* 5 bits */
+        u32 resrved2[5];
+        u32 uptime;
+};
+
+struct fw_info_layout { // 20h - 5Ch
+        u8 resrved0;
+        u8 major;
+        u8 minor;
+        u8 sub_minor;
+        u32 build_id;
+        u8 month;
+        u8 day;
+        u16 year;
+        u16 resrved1;
+        u16 hour;
+        u32 psid[4];
+        u32 ini_file_version;
+        u32 extended_major;
+        u32 extended_minor;
+        u32 extended_sub_minor;
+        u32 resrved4[4];
+};
+
+struct sw_info_layout { // 60h - 7Ch
+        u8 resrved0;
+        u8 major;
+        u8 minor;
+        u8 sub_minor;
+        u32 resrved4[7];
+};
+
+struct reg_mgir_layout {
+        struct hw_info_layout hw_info; // 00h - 1Ch
+        struct fw_info_layout fw_info; // 20h - 5Ch
+        struct sw_info_layout sw_info; // 60h - 7Ch
+};
+ 
+/* SwitchX commands opcodes */
+enum switchx_cmd_opcodes {
+        SX_QUERY_FW_OP        = 0x04, // QUERY_FW command opcode
+        SX_QUERY_BOARDINFO_OP = 0x06, // QUERY_BOARDINFO command opcode
+        SX_CONFIG_PROFILE_OP  = 0x100,// CONFIG_PROFILE command opcode TBD
+        SX_ACCESS_REG_OP      = 0x40, // ACCESS_REG command opcode
+        /* Not supported by I2c */
+        SX_QUERY_AQ_CAP_OP    = 0x03,    
+        SX_SW2HW_EQ_OP        = 0x13, 
+        SX_HW2SW_EQ_OP        = 0x14,
+        SX_QUERY_EQ_OP        = 0x15,
+        SX_SW2HW_CQ_OP        = 0x16,  
+        SX_HW2SW_CQ_OP        = 0x17,  
+        SX_QUERY_CQ_OP        = 0x18,  
+        SX_2ERR_DQ_OP         = 0x1E, 
+        SX_QUERY_DQ_OP        = 0x22,
+        SX_MAD_IFC_OP         = 0x24,
+        SX_SW2HW_DQ_OP        = 0x201,  
+        SX_HW2SW_DQ_OP        = 0x202,
+        SX_INIT_MAD_DEMUX_OP  = 0x203,
+        SX_MAP_FA_OP_OP       = 0xFFF, /// Should be some other constant
+        SX_UNMAP_FA_OP_OP     = 0xFFE,       
+};
+
+enum switchx_ret_status {
+        SX_STAT_OK            = 0x00,   // Command execution succeeded.
+        SX_STAT_INTERNAL_ERR  = 0x01,   // Internal error (e.g. bus error) occurred while processing command.
+        SX_STAT_BAD_OP        = 0x02,   // Operation/command not supported or opcode modifier not supported.
+        SX_STAT_BAD_PARAM     = 0x03,   // Parameter not supported, parameter out of range.
+        SX_STAT_BAD_SYS_STATE = 0x04,   // System was not enabled or bad system state.
+        SX_STAT_BAD_RESOURCE  = 0x05,   // Attempt to access reserved or unallocated resource, or resource in
+                                        // inappropriate ownership.
+        SX_STAT_RESOURCE_BUSY = 0x06,   // Requested resource is currently executing a command.
+        SX_STAT_EXCEED_LIM    = 0x08,   // Required capability exceeds device limits.
+        SX_STAT_BAD_RES_STATE = 0x09,   // Resource is not in the appropriate state or ownership.
+        SX_STAT_BAD_INDEX     = 0x0A,   // Index out of range (might be beyond table size or attempt to 
+                                        // access a reserved resource).
+        SX_STAT_BAD_NVMEM     = 0x0B,   // checksum/CRC failed.
+        SX_STAT_BAD_PKT       = 0x30,   // Bad management packet (silently discarded).
+};
+
+/* QUERY_FW
+ Description
+ *------*------*-----*--------*------*---------*
+ |Opcode|Op Mod|Event|IN PARAM|IN Mod|OUT PARAM|
+ *------*------*-----*--------*------*---------*
+ |0x4   |Yes   |Yes  |N/A     |Yes   |Mail box |
+ *------*------*-----*--------*------*---------*
+ The QUERY_FW command retrieves information related to the firmware, command interface version and 
+ the amount of resources that should be   allocated to the firmware. The returned output parameter 
+ depends on the OpMod value. When executing QUERY_FW through the i2c, it is required to work with
+ local mailboxes. The location of the mailboxes on the i2c address space can be retrieved by 
+ issuing QUERY_FW with opcode modifier 0x1.
+*/
+
+/* Opcode Modifier */
+enum query_fw_opcode_mod {
+        SX_MB_OUTPUT      = 0x00, // An output mailbox is returned, struct query_fw_output_mbox_layout
+        SX_IMM_OUTPUT_1   = 0x01, // The output parameter is an immediate value, struct output_param_opmode1
+        SX_IMM_OUTPUT_2   = 0x02, // The output parameter is an immediate values, struct output_param_opmode2
+};
+
+/*  Input Modifier
+    Input Modifier for this command is relevant only if Opcode Modifier (OpMod) is equal to 0x2. 
+    Otherwise reserved.
+*/
+enum query_fw_input_mod {
+        SX_BAR0_OFF_NONE  = 0x00, // Input modifier parameter is not relevant
+        SX_BAR0_OFF_BSR   = 0x01, // Output parameter returns the offset in BAR0 where the 
+                                  // Boot Syndrome Register should be read from
+        SX_BAR0_OFF_FER   = 0x03  // Output parameter returns the offset in BAR0 where the
+                                  // Fatal Error Register should be read from
+};
+
+struct query_fw_output_mbox_layout {
+	u16 fw_rev_major;
+	u16 fw_pages;
+	u16 fw_rev_minor;
+	u16 fw_rev_subminor;
+	u16 cmd_interface_rev;
+	u16 core_clk;
+	u32 reserved1:31;
+	u32 dt:1;
+	u8 reserved2;
+	u8 fw_seconds;
+	u8 fw_minutes;
+	u8 fw_hour;
+	u16 fw_year;                // Firmware timestamp - year (displayed as a hexadecimal number; e.g. 0x2005)
+	u8 fw_month;                // Firmware timestamp - month (displayed as a hexadecimal number)
+	u8 fw_day;                  // Firmware timestamp - day (displayed as a hexadecimal number)
+
+	u32 reserved3;
+ 	u32 reserved4;
+ 	u32 clr_int_base_offset_h;
+ 	u32 clr_int_base_offset_l;
+ 	u32 reserved5:30; 
+ 	u32 clr_int_bar:2;
+ 	u32 reserved6;
+ 	u32 error_buf_offset_h;
+ 	u32 error_buf_offset_l;
+ 	u32 error_buf_size;
+ 	u32 reserved7:30;
+ 	u32 error_buf_bar:2;
+ 	u32 doorbell_page_offset_h;
+ 	u32 doorbell_page_offset_l;
+ 	u32 reserved8:30;
+ 	u32 doorbell_page_bar:2;
+ 	u8 reserved9[180];
+};
+
+
+struct param_h_opmode1 {
+ 	u32 local_mb_bar0_offset:20;
+ 	u32 local_mb_size:12;
+};
+
+// Format for Immediate Output Parameter With OpMod=0x1
+struct output_param_opmode1 {
+ 	struct param_h_opmode1 out_param_h; // bits 63:32 of local output mailbox - cir_reg out_param_l 
+ 	struct param_h_opmode1 out_param_l; // bits 31: 0 of local output mailbox - cir_reg out_param_h
+};
+
+struct param_h_opmode2 {
+ 	u32 local_mb_bar0_offset;
+};
+
+// Format for Immediate Output Parameter With OpMod=0x2
+struct output_param_opmode2 {
+ 	u32 out_param_h;                    // bits 63:32 of output parameter (reserved)
+ 	struct param_h_opmode2 out_param_l; // bits 31: 0 of output parameter
+};
+
+/* ACCESS_REG
+ Description
+ *------*------*-----*--------*------*---------*
+ |Opcode|Op Mod|Event|IN PARAM|IN Mod|OUT PARAM|
+ *------*------*-----*--------*------*---------*
+ |      |N/A   |Y    |Mail box|N/A   |Mail box |
+ *------*------*-----*--------*-------*--------*
+ The ACCESS_REG command supports accessing device registers through PCIe and i2c. This
+ access is mainly used for bootstrapping. Note that only a subset of registers is supported
+ as listed below
+*/
+#define MGIR_REG_ID  SX_MGIR_REG  // 0x9020 MGIR Misc General Information Register
+
+
+/* EMAD TLV Types */
+enum emad_tlv_types {
+        SX_END_OF_TLV        = 0x00, // "END"       len=1   End of TLV list. Must be present on all packets as the last TLV.
+        SX_OPERATION_TLV     = 0x01, // "OPERATION" len=4   Operation TLV, describes the method/class and response status.
+        SX_DIRECT_ROUTE_TLV  = 0x02, // "DR"        len=33  Direct Route TLV. Describes the direct route path. Must only be present
+                                     //                     in direct route EMADs. Must follow OPERATION TLV if present.
+        SX_REG_ACCESS_TLV    = 0x03, // "REG"       len=VAR Register value. Must be present for REG_ACCESS class.
+        SX_REG_USER_DATA_TLV = 0x04, // "USERDATA"  len=VAR User data for IPC class.
+};
+
+/* EMAD Classes */
+enum emad_classes {
+        SX_EMAD_RESERVED_CLASS   = 0x00, // Reserved Reserved for future use N/A
+        SX_EMAD_REG_ACCESS_CLASS = 0x01, // Access to SwitchX registers. register_id specifies the register to be accessed.
+                                         // OPERATION, (DR), REG, END
+        SX_EMAD_CPU_OPER_CLASS   = 0x02, // IPC Access to CPU OPERATION, (DR), USERDATA, END
+};
+
+struct emad_eth_header {
+        u8 dmac3;
+        u8 dmac2;
+        u8 dmac1;
+        u8 dmac0;
+        u8 smac1; 
+        u8 smac0;
+        u8 dmac5;
+        u8 dmac4;
+        u8 smac5;
+        u8 smac4;
+        u8 smac3;
+        u8 smac2;
+        u8 reserved1:4;
+        u8 ver:4;
+        u8 mlx_proto;
+        u16 et; 
+};
+
+enum oper_tlv_status {
+        OPER_TLV_STATUS_OK           = 0x00, // Good. Operation Performed.
+        OPER_TLV_STATUS_BUSY         = 0x01, // Device is busy. Can not perform the operation at the moment, requester
+                                             // should retry the operation later.
+        OPER_TLV_STATUS_VER_NSUPP    = 0x02, // Version not supported.
+        OPER_TLV_STATUS_UNKNWN       = 0x03, // Unknown TLV.
+        OPER_TLV_STATUS_REG_NSUPP    = 0x04, // Register not supported.
+        OPER_TLV_STATUS_CLASS_NSUPP  = 0x05, // Class not supported.
+        OPER_TLV_STATUS_METHOD_NSUPP = 0x06, // Method not supported
+        OPER_TLV_STATUS_BAD_PARAM    = 0x07, // Bad parameter (e.g. port out of range, non stacking port)
+        OPER_TLV_STATUS_RSRC_NAVAIL  = 0x08, // Resource not available (e.g. attempt to write to a full FDB,
+                                             // allocation failed)
+        OPER_TLV_STATUS_MSG_ACK      = 0x09, // Message Receipt Acknowledgement. Will return answer later.
+                                             // Requester should rearm retransmission timer.
+};
+
+enum oper_tlv_method {
+        OPER_TLV_METHOD_QUERY = 0x01, // Query
+        OPER_TLV_METHOD_WRITE = 0x02, // Write
+        OPER_TLV_METHOD_SEND  = 0x03, // Send (response not supported)
+        OPER_TLV_METHOD_EVENT = 0x05, // Event (optional response)
+};
+
+#define DIRECT_ROUTE_NOT_SET 0x0
+#define DIRECT_ROUTE_SET     0x1
+#define OPER_TLV_REQ         0x0
+#define OPER_TLV_RESP        0x1
+struct operation_tlv {
+        u8 type;
+        u16 len;
+        u8 dr;
+        u8 status;
+        u16 register_id;
+        u8 res_req;
+        u8 method;
+        u64 tid;                // Transaction ID - 64 bit opaque value returned as is in response message
+                                // This field simplifies software tracking of transactions.
+};
+
+struct end_tlv {
+        u8 type;
+        u16 len;
+        u8 reserved1;
+};
+
+/* Maximum size of swicth_reg filed within REG_ACCESS TLV */
+#define SX_REG_MAX_SIZE ( MAX_EMAD_FRAME_SIZE - \
+                          DIR_ROUTE_TLV_SIZE - \
+                          sizeof(struct emad_eth_header) - \
+                          sizeof(struct operation_tlv) - \
+                          struct(struct end_tlv) \
+                         ) // size in bytes
+/* Maximum size of emad frame for access register EMAD frame */
+#define SX_ACCESS_REG_EF ( SX_REG_MAX_SIZE + \
+                           sizeof(struct operation_tlv) - \
+                           sizeof(struct end_tlv) \
+                         ) // size in bytes
+struct reg_access_tlv {
+#ifdef __BIG_ENDIAN
+        u16 type:5;             // Operation - from enum emad_tlv_types
+        u16 len:11;             // Length of TLV in DWORDs
+        u16 reserved1;          //
+#else
+        u32 b1;
+#endif
+        void *switchx_reg;      // switchx register, referenced in register_id filed of operation TLV,
+                                // variable size, maximium size iz SX_REG_MAX_SIZE
+};
+
+/* Access Register Layout */
+struct access_reg_layout {
+        struct operation_tlv oper_tlv;      // Operation to perform including all fields
+        struct reg_access_tlv reg_accs_tlv; // Register access information
+};
+
+/* EMAD Frame Format Layout
+        eth_hdr;          // Ethernet Header
+        oper_tlv;         // OPERATION TLV (Mandatory)
+        dr_tlv;           // DR TLV (Present in DR packets only)
+        reg_access;       // REG TLV (present in REG_ACCESS class)  
+        user_data;        // USERDATA TLV (present in IPC class)
+        end_tlv;          // END TLV (Mandatory)
+*/
+struct emad_frame_access_reg_template {
+        struct emad_eth_header eth_hdr;
+        struct operation_tlv oper_tlv;
+        struct reg_access_tlv reg_accs_tlv;
+        struct end_tlv e_tlv;
+};
+
+#define SX_I2C_DEV_SIGNATURE 0xf0e1c2b3
+#define SXDBGMODE 1
+#define dprintk(...) do { } while(0)
+
+#define SWITCHX_ADDR32                4
+#define MAX_I2C_BUFF_SIZE           256
+#define MAX_ASIC_ID_FOR_HEALTH_TEST 250
+
+#define BUS_ID_FROM_ID(i2c_dev_id)          ( (i2c_dev_id >> 8) & 0x0000ffff)
+#define GET_PARENT_BUS_ID(bus_id)           ( (bus_id >> 8) & 0x0000ffff)
+#define GET_LOCAL_BUS_ID(asic_id)           ( ((asic_id % 2) == 1) ? 1 : 2)
+#define GET_SHIFT_FROM_LOCAL_BUS(local_bus) ( (local_bus == 1) ? 6 : 4)
+#define ADDR_FROM_DEV_ID(i2c_dev_id)        ( i2c_dev_id & 0x7f )
+#define GET_WIDTH(off)                      ( (off > 0xffff) ? 4 : (off > 0xff) ? 2 : 1 )
+
+#define _FL_ struct file *fl,
+
+#define LPCI2C_BLOCK_MAX 32
+
+#define REG_ACCESS_TLV_LEN		                0x01
+#define REG_TLV_LEN		                        0x04
+#define REG_TLV_END_LEN		                        0x01
+#define REG_TLV_TYPE		                        0x03
+#define REG_TLV_OFFSET		                        0x10
+#define ASIC_PSID_SZIE		                          16
+#define MGIR_REG_LEN					0x20
+#define MGIR_REG_HW_INFO_OFFSET				0x14
+#define MGIR_REG_HW_INFO_DEVICE_HW_REVISION_OFFSET	0x0
+#define MGIR_REG_HW_INFO_DEVICE_ID_OFFSET		0x2
+#define MGIR_REG_HW_INFO_DVFS_OFFSET			0x7
+#define MGIR_REG_HW_INFO_UPTIME_OFFSET			0x1c
+#define MGIR_REG_FW_INFO_OFFSET				0x34
+#define MGIR_REG_FW_INFO_MAJOR_OFFSET			0x01
+#define MGIR_REG_FW_INFO_MINOR_OFFSET			0x02
+#define MGIR_REG_FW_INFO_SUB_MINOR_OFFSET		0x03
+#define MGIR_REG_FW_INFO_BUILD_ID_OFFSET		0x04
+#define MGIR_REG_FW_INFO_MONTH_OFFSET			0x08
+#define MGIR_REG_FW_INFO_DAY_OFFSET			0x09
+#define MGIR_REG_FW_INFO_YEAR_OFFSET			0x0a
+#define MGIR_REG_FW_INFO_HOUR_OFFSET			0x0e
+#define MGIR_REG_FW_INFO_PSID_OFFSET			0x10
+#define MGIR_REG_FW_INFO_INI_FILE_VERSION_OFFSET	0x20
+#define MGIR_REG_FW_INFO_EXTENDED_MAJOR_OFFSET		0x24
+#define MGIR_REG_FW_INFO_EXTENDED_MINOR_OFFSET		0x28
+#define MGIR_REG_FW_INFO_EXTENDED_SUB_MINOR_OFFSET	0x2c
+#define MGIR_REG_SW_INFO_OFFSET				0x74
+#define MGIR_REG_SW_INFO_MAJOR_OFFSET			0x01
+#define MGIR_REG_SW_INFO_MINOR_OFFSET			0x02
+#define MGIR_REG_SW_INFO_SUB_MINOR_OFFSET		0x03
+
+/* Addresses to scan */
+static const unsigned short normal_i2c[] = { 0x48, I2C_CLIENT_END };
+
+
+/* Each client has this additional data */
+struct switchx_data {
+	struct list_head list;
+	int signature;
+	struct device *i2c_dev;
+	struct i2c_client *client;
+	char name[I2C_NAME_SIZE];
+	struct reg_cir_layout reg_cir;
+	u32 mb_size_in;
+	u32 mb_offset_in;
+	u32 mb_size_out;
+	u32 mb_offset_out;
+	int retries;
+	int timeout;
+	int retry_cntr;
+	u64 transact_id;
+	int bus_id;
+	int i2c_dev_addr;
+	u8 cache[SX_MAX_REG_SIZE];
+	struct mutex cmd_lock;
+#define SX_ROUTE_DEPTH 4
+	struct i2c_adapter *parent[SX_ROUTE_DEPTH];
+	int (*deselect[SX_ROUTE_DEPTH - 1])(struct i2c_adapter *, void *mux_dev, u32 chan_id);
+	u8 route_enforced;
+#define GO_BIT_STAT_OK    0
+#define GO_BIT_STAT_STUCK 1
+	u8 go_bit_status;
+	u16 asic_id;
+};
+
+/* Container structure */
+struct switchx_probe_failed {
+	struct list_head list;
+	int bus_id;
+	int probe_cntr;
+};
+
+/* Container structure */
+struct switchx_config {
+	struct list_head switchx_list;
+	struct list_head probe_failed_list;
+	u32 reg_cir_offset;
+	u32 reg_sw_reset_offset;
+	u32 last_access_dev_id;
+	int last_accessed_bus_id;
+};
+static struct switchx_config switchx_cfg;
+
+static struct kmem_cache *switchx_probe_failed_cache __read_mostly;
+
+enum chips {
+	any_chip, switchx, connectx, switchib, switchspc,
+};
+
+static inline int translate_offset_addr(u32 addr, u32 offset, u32 len, u8 width, char *msgbuf,
+				  	struct i2c_msg *msg, u8 *in_out_buf, u8 flag);
+
+#define ICR_REG_WRITE_GO_BIT(event, op_modifier, op, icr_reg)                          \
+        cpu_to_be32(((1 << GO_BIT) | (event ? (1 << EVENT_BIT) : 0) |                  \
+                        (op_modifier << OPMOD_SHIFT) | op), icr_reg);
+
+#define ASIC_GET_BUF(dest, source, offset)				\
+	do {								\
+		void *__p = (char *) (source) + (offset);		\
+		switch (sizeof(dest)) {				        \
+		case 1: (dest) = *(u8 *) __p; break;	      	        \
+		case 2: (dest) = be16_to_cpup(__p); break;	 	\
+		case 4: (dest) = be32_to_cpup(__p); break;	       	\
+		case 8: (dest) = be64_to_cpup(__p); break;	      	\
+		default: break;						\
+		}							\
+	} while (0)
+
+#define ASIC_SET_BUF(dest, source, offset)				\
+	do {								\
+		void *__d = ((char *) (dest) + (offset));		\
+		switch (sizeof(source)) {				\
+		case 1: *(u8 *) __d = (source);	 break; 		\
+		case 2: *(__be16 *) __d = cpu_to_be16(source); break; 	\
+		case 4: *(__be32 *) __d = cpu_to_be32(source); break; 	\
+		case 8: *(__be64 *) __d = cpu_to_be64(source); break; 	\
+		default:  break;					\
+		}							\
+	} while (0)
+
+#define TYPE_LEN_OFFSET         0x00
+#define DR_STATUS_OFFSET        0x02
+#define REGISTER_ID_OFFSET      0x04
+#define R_METHOD_OFFSET         0x06
+#define CLASS_OFFSET            0x07
+#define TID_OFFSET              0x08
+#define SET_OPER_TLV(buf,type,len,dr,status,res_req,reg_id,method,class,tid)	\
+{										\
+        u16 type_len = 0;						        \
+        u8 dr_status = 0;						        \
+        u8 r_method = 0;						        \
+        type_len = len | (type << 11);						\
+        ASIC_SET_BUF(buf, type_len, TYPE_LEN_OFFSET);				\
+        dr_status = status | (dr << 7);						\
+        ASIC_SET_BUF(buf, dr_status, DR_STATUS_OFFSET);				\
+        ASIC_SET_BUF(buf, reg_id, REGISTER_ID_OFFSET);		                \
+        r_method = method | (res_req << 7);					\
+        ASIC_SET_BUF(buf, r_method, R_METHOD_OFFSET);				\
+        ASIC_SET_BUF(buf, class, CLASS_OFFSET);					\
+        ASIC_SET_BUF(buf, tid, TID_OFFSET);					\
+}
+
+#define OPERATION_TLV_SIZE	0x10
+#define IN_MB_SIZE(reg_dword_size) (((reg_dword_size) * 4) + OPERATION_TLV_SIZE)
+#define TYPE_LEN_OFFSET		0x00
+#define DR_STATUS_OFFSET	0x02
+#define REGISTER_ID_OFFSET	0x04
+#define R_METHOD_OFFSET		0x06
+#define CLASS_OFFSET		0x07
+#define TID_OFFSET		0x08
+#define GET_OPER_TLV(buf,type,len,dr,status,res_req,reg_id,method,class,tid)	\
+{						        			\
+	u16 type_len = 0;						        \
+	u8 dr_status = 0;						        \
+	u8 r_method = 0;						        \
+	ASIC_GET_BUF(type_len, buf, TYPE_LEN_OFFSET);				\
+	len = type_len & 0x7ff;						        \
+	type = type_len >> 11;						        \
+	ASIC_GET_BUF(dr_status, buf, DR_STATUS_OFFSET);				\
+	status = dr_status & 0x7f;						\
+	dr = dr_status >> 7;						        \
+	ASIC_GET_BUF(reg_id, buf, REGISTER_ID_OFFSET);				\
+	ASIC_GET_BUF(r_method, buf, R_METHOD_OFFSET);				\
+	method = r_method & 0x7f;						\
+	res_req = r_method >> 7;						\
+	ASIC_GET_BUF(class, buf, CLASS_OFFSET);					\
+	ASIC_GET_BUF(tid, buf, TID_OFFSET);					\										\
+}
+
+static inline void get_mgir_reg(u8 *buf, struct reg_mgir_layout *mgir)
+{
+	ASIC_GET_BUF(mgir->hw_info.device_hw_revision, buf, MGIR_REG_HW_INFO_OFFSET + MGIR_REG_HW_INFO_DEVICE_HW_REVISION_OFFSET);
+	ASIC_GET_BUF(mgir->hw_info.device_id, buf, MGIR_REG_HW_INFO_OFFSET + MGIR_REG_HW_INFO_DEVICE_ID_OFFSET);
+	ASIC_GET_BUF(mgir->hw_info.dvfs, buf, MGIR_REG_HW_INFO_OFFSET + MGIR_REG_HW_INFO_DVFS_OFFSET);
+	mgir->hw_info.dvfs &= 0x1f;
+	ASIC_GET_BUF(mgir->hw_info.uptime, buf, MGIR_REG_HW_INFO_OFFSET + MGIR_REG_HW_INFO_UPTIME_OFFSET);
+	ASIC_GET_BUF(mgir->fw_info.major, buf, MGIR_REG_FW_INFO_OFFSET + MGIR_REG_FW_INFO_MAJOR_OFFSET);
+	ASIC_GET_BUF(mgir->fw_info.minor, buf, MGIR_REG_FW_INFO_OFFSET + MGIR_REG_FW_INFO_MINOR_OFFSET);
+	ASIC_GET_BUF(mgir->fw_info.sub_minor, buf, MGIR_REG_FW_INFO_OFFSET + MGIR_REG_FW_INFO_SUB_MINOR_OFFSET);
+	ASIC_GET_BUF(mgir->fw_info.build_id, buf, MGIR_REG_FW_INFO_OFFSET + MGIR_REG_FW_INFO_BUILD_ID_OFFSET);
+	ASIC_GET_BUF(mgir->fw_info.month, buf, MGIR_REG_FW_INFO_OFFSET + MGIR_REG_FW_INFO_MONTH_OFFSET);
+	ASIC_GET_BUF(mgir->fw_info.day, buf, MGIR_REG_FW_INFO_OFFSET + MGIR_REG_FW_INFO_DAY_OFFSET);
+	ASIC_GET_BUF(mgir->fw_info.year, buf, MGIR_REG_FW_INFO_OFFSET + MGIR_REG_FW_INFO_YEAR_OFFSET);
+	ASIC_GET_BUF(mgir->fw_info.hour, buf, MGIR_REG_FW_INFO_OFFSET + MGIR_REG_FW_INFO_HOUR_OFFSET);
+	memcpy(mgir->fw_info.psid, buf + MGIR_REG_FW_INFO_OFFSET + MGIR_REG_FW_INFO_PSID_OFFSET, ASIC_PSID_SZIE);
+	ASIC_GET_BUF(mgir->fw_info.ini_file_version, buf, MGIR_REG_FW_INFO_OFFSET + MGIR_REG_FW_INFO_INI_FILE_VERSION_OFFSET);
+	ASIC_GET_BUF(mgir->fw_info.extended_major, buf, MGIR_REG_FW_INFO_OFFSET + MGIR_REG_FW_INFO_EXTENDED_MAJOR_OFFSET);
+	ASIC_GET_BUF(mgir->fw_info.extended_minor, buf, MGIR_REG_FW_INFO_OFFSET + MGIR_REG_FW_INFO_EXTENDED_MINOR_OFFSET);
+	ASIC_GET_BUF(mgir->fw_info.extended_sub_minor, buf, MGIR_REG_FW_INFO_OFFSET + MGIR_REG_FW_INFO_EXTENDED_SUB_MINOR_OFFSET);
+	ASIC_GET_BUF(mgir->sw_info.major, buf, MGIR_REG_SW_INFO_OFFSET + MGIR_REG_SW_INFO_MAJOR_OFFSET);
+	ASIC_GET_BUF(mgir->sw_info.minor, buf, MGIR_REG_FW_INFO_OFFSET + MGIR_REG_SW_INFO_MINOR_OFFSET);
+	ASIC_GET_BUF(mgir->sw_info.sub_minor, buf, MGIR_REG_SW_INFO_OFFSET + MGIR_REG_SW_INFO_SUB_MINOR_OFFSET);
+}
+
+#define I2C_XFER(client, data, msgs, try, orig_jiffies, read_write)            \
+	orig_jiffies = jiffies;                                                \
+	for (try = 0; try <= data->retries; try++) {                           \
+		rc = i2c_transfer(client->adapter, msgs, read_write);          \
+		if (rc > 0) {		                                       \
+			break;                                                 \
+		}                                                              \
+		if (time_after(jiffies, orig_jiffies + data->timeout)) {       \
+			rc = -EIO;                                             \
+			printk( "%s try=%d rc=%d\n", __FUNCTION__, try, rc);    \
+			return rc;                                             \
+		}                                                              \
+	}                                                                      \
+	printk( "%s try=%d rc=%d\n", __FUNCTION__, try, rc);
+
+#define I2C_SET_BUF_XFER(client, data, try, orig_jiffies, tbuf, msgs, msgbuf, offset,         \
+				size, width, rw_flag)                                         \
+	translate_offset_addr(client->addr, offset, size,                                     \
+				width, msgbuf, msgs, tbuf, rw_flag);                          \
+	I2C_XFER(client, data, msgs, try, orig_jiffies, rw_flag);                             \
+	if (rc < 0) {                                                                         \
+		printk(KERN_INFO "%s(%d) rc=%d\n", __FUNCTION__, __LINE__, rc);               \
+		return rc;                                                                    \
+	}
+
+#define BUS_XFER(c,d,rw,p,q,s) I2C_XFER(client, data, msgs, try, orig_jiffies, read_write)
+
+static inline int icr_reg_write(struct i2c_client *client, struct switchx_data *data,
+				u16 token, u32 in_modifier,
+				u8 op_modifier, u16 op)
+{
+	unsigned long orig_jiffies;
+	int try;
+	struct i2c_msg msgs[1];
+	u8 msgbuf[SWITCHX_ADDR32];
+	u32 icr_reg_buf[7];
+	int rc = -EIO;
+	memset(icr_reg_buf, 0, 28);
+	icr_reg_buf[2] = cpu_to_be32(in_modifier);
+	icr_reg_buf[5] = cpu_to_be32(token << 16);
+	icr_reg_buf[6] = cpu_to_be32((op_modifier << OPMOD_SHIFT) | op);
+
+	dprintk(		 "%s(%d) icr_reg_buf[2]=0x%08x icr_reg_buf[5]=0x%08x icr_reg_buf[6]=0x%08x\n",
+		  __FUNCTION__, __LINE__, icr_reg_buf[2], icr_reg_buf[5], icr_reg_buf[6]);
+
+	I2C_SET_BUF_XFER(client, data, try, orig_jiffies,
+				(u8 *)icr_reg_buf, msgs, msgbuf,
+				SX_BAR0_OFFSET_SECOND, 28, SWITCHX_ADDR32, 0);
+
+	/* Write go bit as ceparate */
+	icr_reg_buf[6] |= cpu_to_be32(1 << GO_BIT);
+	dprintk(		  "%s(%d) icr_reg_buf[6]=0x%08x\n",
+		  __FUNCTION__, __LINE__, icr_reg_buf[6]);
+	I2C_SET_BUF_XFER(client, data, try, orig_jiffies,
+				(u8 *)&icr_reg_buf[6], msgs, msgbuf,
+				SX_BAR0_OFFSET_SECOND + STATUS_OFFSET, 4, SWITCHX_ADDR32, 0);
+
+	return rc;
+}
+
+static inline int icr_reg_read(struct i2c_client *client, u32 icr_reg_buf, u8 *status, u8 *go)
+{
+	int rc = 0;
+	*status = icr_reg_buf & 0x000000ff; //>> 24;
+	*go = icr_reg_buf & (1 << GO_BIT);
+
+	dprintk(		 "%s(%d) icr_reg_buf=0x%02x *status=0x%08x *go=0x%02x\n",
+		  __FUNCTION__, __LINE__, icr_reg_buf, *status, *go);
+
+	return rc;
+}
+
+static inline void convert_mbox(u32 *p_mbox)
+{
+	*p_mbox = cpu_to_be32(*p_mbox);
+	*(p_mbox + 1) = cpu_to_be32(*(p_mbox + 1));
+}
+
+static inline void convert_query_fw_mbox(u32 *p_query_fw_mbox)
+{
+	int i;
+	u32 *p_mbox = p_query_fw_mbox;
+	int dword_size = sizeof(struct query_fw_output_mbox_layout) / 4;
+	if (sizeof(struct query_fw_output_mbox_layout) % 4)
+		dword_size += 1;
+	for (i = 0; i < dword_size; i++, p_mbox++)
+		*p_mbox = cpu_to_be32(*p_mbox);
+}
+
+static inline void convert_oper_tlv(u32 *p_oper_tlv)
+{
+	int i;
+	u32 *p_tlv = p_oper_tlv;
+	int dword_size = sizeof(struct operation_tlv) / 4;
+	if (sizeof(struct operation_tlv) % 4)
+		dword_size += 1;
+	for (i = 0; i < dword_size; i++, p_tlv++)
+		*p_tlv = cpu_to_be32(*p_tlv);
+}
+
+static inline void convert_reg_accs_tlv(u32 *p_reg_tlv)
+{
+	int i;
+	u32 *p_tlv = p_reg_tlv;
+	int dword_size = sizeof(struct reg_access_tlv) / 4;
+	if (sizeof(struct reg_access_tlv) % 4)
+		dword_size += 1;
+	for (i = 0; i < dword_size; i++, p_tlv++)
+		*p_tlv = cpu_to_be32(*p_tlv);
+}
+
+static inline void convert_end_tlv(u32 *p_end_tlv)
+{
+	int i;
+	u32 *p_tlv = p_end_tlv;
+	int dword_size = sizeof(struct end_tlv) / 4;
+	if (sizeof(struct end_tlv) % 4)
+		dword_size += 1;
+	for (i = 0; i < dword_size; i++, p_tlv++)
+		*p_tlv = cpu_to_be32(*p_tlv);
+}
+
+static inline int translate_offset_addr(u32 addr, u32 offset, u32 len, u8 width, char *msgbuf,
+				  struct i2c_msg *msg, u8 *in_out_buf, u8 flag)
+{
+	char *msg0;
+	struct i2c_msg *msg_i2c = msg;
+
+	if (flag == I2C_M_RD) {
+		msg0 = msgbuf;
+		switch (width) {
+			case 4:
+				*msg0++ = offset >> 24;
+				*msg0++ = offset >> 16;
+				*msg0++ = offset >> 8;
+				*msg0 = offset;
+				break;
+			case 2:
+				offset &= 0xffff;
+				*msg0++ = offset >> 8;
+				*msg0 = offset;
+				break;
+			case 1:
+				offset &= 0xff;
+				*msg0 = offset;
+				break;
+		}
+		msg_i2c->addr = addr;
+		msg_i2c->buf = msgbuf;
+		msg_i2c->len = width;
+		msg_i2c++;
+		msg_i2c->addr = addr;
+		msg_i2c->flags = I2C_M_RD;
+		msg_i2c->buf = in_out_buf;
+		msg_i2c->len = len;
+	}
+	else {
+		msg0 = in_out_buf;
+		switch (width) {
+			case 4:
+				*msg0++ = offset >> 24;
+				*msg0++ = offset >> 16;
+				*msg0++ = offset >> 8;
+				*msg0 = offset;
+				break;
+			case 2:
+				offset &= 0xffff;
+				*msg0++ = offset >> 8;
+				*msg0 = offset;
+				break;
+			case 1:
+				offset &= 0xff;
+				*msg0 = offset;
+				break;
+		}
+		msg_i2c->addr = addr;
+		msg_i2c->flags = 0;
+		msg_i2c->len = width + len;
+		msg_i2c->buf = in_out_buf;
+	}
+	return 1;
+}
+
+static int i2c_software_reset(struct device *dev)
+{
+	struct i2c_client *client = to_i2c_client(dev);
+	struct switchx_data *data = i2c_get_clientdata(client);
+	unsigned long orig_jiffies;
+	int try;
+	u8 tbuf[sizeof(struct reg_sw_reset_layout) + SWITCHX_ADDR32];
+	struct reg_sw_reset_layout sw_res;
+	int rc = -EIO;
+	struct i2c_msg msgs[1];
+	u8 msgbuf[SWITCHX_ADDR32];
+
+        memset(&sw_res, 0, sizeof(struct reg_sw_reset_layout));
+        sw_res.sw_reset = SW_RESET;
+	memset(tbuf, 0, sizeof(struct reg_sw_reset_layout) + SWITCHX_ADDR32);
+	memcpy(tbuf + SWITCHX_ADDR32, (u8 *)&sw_res, sizeof(struct reg_sw_reset_layout));
+
+	translate_offset_addr(client->addr, switchx_cfg.reg_sw_reset_offset, sizeof(struct reg_sw_reset_layout),
+				SWITCHX_ADDR32, msgbuf, msgs, tbuf, 0);
+
+	orig_jiffies = jiffies;
+	for (try = 0; try <= data->retries; try++) {
+		rc = i2c_transfer(client->adapter, msgs, 1);
+		if (rc > 0) {
+			break;
+		}
+		if (time_after(jiffies, orig_jiffies + data->timeout)) {
+			rc = -EIO;
+			break;
+		}
+	}
+
+	return rc;
+}
+
+#define LEN_ACQ_GO 4
+#define OFF_ACQ_GO ( switchx_cfg.reg_cir_offset + 24 )
+static int acquire_go_bit_complete(struct i2c_client *client,
+			           struct switchx_data *data,
+			           struct reg_cir_layout *reg_cir,
+			           int len,
+			           int offset,
+			           u8 *in_out_buf,
+			           int (*f_finish)(struct i2c_client *client,
+					           struct switchx_data *data,
+					           struct reg_cir_layout *reg_cir,
+					           int len,
+					           int offset,
+					           u8 *in_out_buf))
+{
+	struct reg_cir_layout _reg_cir;
+	int rc = -EIO;
+	int try = 0;
+	unsigned long orig_jiffies;
+	struct i2c_msg msgs[2];
+	u8 msgbuf[SWITCHX_ADDR32];
+	u8 status = 1;
+	u8 go = 1;
+
+	memset(&_reg_cir, 0, sizeof(struct reg_cir_layout));
+	memset(msgs, 0, 2*sizeof(struct i2c_msg));
+
+	dprintk(		"%s(%d) before getting GO len=%d offset=%d LEN_ACQ_GO=%d OFF_ACQ_GO=0x%08x\n",
+		__FUNCTION__, __LINE__, len, offset, LEN_ACQ_GO, OFF_ACQ_GO);
+
+	translate_offset_addr(client->addr, OFF_ACQ_GO, LEN_ACQ_GO/* + 4*/, SWITCHX_ADDR32, msgbuf, msgs,
+				((u8 *)(&_reg_cir) + CIR_STATUS_OFF), I2C_M_RD);
+
+	orig_jiffies = jiffies;
+	for (try = 0; try <= data->retries; try++) {
+		rc = i2c_transfer(client->adapter, msgs, 2);
+		rc = (rc >= 0) ? icr_reg_read(client, *((u32 *)((u8 *)(&_reg_cir) + CIR_STATUS_OFF)), &status, &go) : rc;
+		if ((rc >= 0) && (go == CIR_GO_SW) /*&& (status == 0)*/) {
+			dprintk(				"%s(%d) go=%x status=%x rc=%d\n",
+				__FUNCTION__, __LINE__, go, status, rc);
+			_reg_cir.status = status;
+			_reg_cir.go = go;
+			if(f_finish)
+				rc = f_finish(client, data, &_reg_cir, len, offset, in_out_buf);
+			break;
+		}
+		if (time_after(jiffies, orig_jiffies + data->timeout)) {
+			rc = -EIO;
+			break;
+		}
+		cond_resched();
+	}
+	data->retry_cntr += try;
+
+	return (try <= data->retries) ? rc : -EIO;
+}
+
+static int acquire_go_bit(struct i2c_client *client,
+		     	  struct switchx_data *data,
+		     	  struct reg_cir_layout *reg_cir,
+			  int len,
+			  int offset,
+		     	  u8 *in_out_buf,
+		     	  int (*f_start)(struct i2c_client *client,
+				         struct switchx_data *data,
+				         struct reg_cir_layout *reg_cir,
+				         int len,
+					 int offset,
+				         u8 *in_out_buf),
+		     	  int (*f_finish)(struct i2c_client *client,
+				          struct switchx_data *data,
+				          struct reg_cir_layout *reg_cir,
+				          int len,
+					  int offset,
+				          u8 *in_out_buf))
+{
+	struct reg_cir_layout _reg_cir;
+	int try = 0;
+	int rc = -EIO;
+	struct i2c_msg msgs[2];
+	u8 msgbuf[SWITCHX_ADDR32];
+	unsigned long orig_jiffies;
+	u8 status = 0;
+	u8 go = 1;
+
+	memset(msgs, 0, 2*sizeof(struct i2c_msg));
+	memset(&_reg_cir, 0, sizeof(struct reg_cir_layout));
+	_reg_cir.go = 1; // Just for cleanup
+	mutex_lock(&data->cmd_lock);
+
+	dprintk(		"%s(%d) before getting GO len=%d offset=%d LEN_ACQ_GO=%d OFF_ACQ_GO=0x%08x 0x%08x 0x%08x\n",
+		__FUNCTION__, __LINE__, len, offset, LEN_ACQ_GO, OFF_ACQ_GO,
+		(int)offsetof(struct reg_cir_layout, status), CIR_STATUS_OFF);
+
+	translate_offset_addr(client->addr, OFF_ACQ_GO, LEN_ACQ_GO, SWITCHX_ADDR32, msgbuf, msgs,
+				((u8 *)(&_reg_cir) + CIR_STATUS_OFF), I2C_M_RD);
+
+
+	orig_jiffies = jiffies;
+	for (try = 0; try <= data->retries; try++) {
+		rc = i2c_transfer(client->adapter, msgs, 2);
+		rc = (rc >= 0) ? icr_reg_read(client, *((u32 *)((u8 *)(&_reg_cir) + CIR_STATUS_OFF)), &status, &go) : rc;
+		if ((rc >= 0) && (go == CIR_GO_SW) /*&& (!status)*/) {
+			dprintk(				"%s(%d) got GO go=%x status=%x go=%x opcode=%x opcode_mod=%x event=%x status=%x buf %x %x %x %x\n",
+				__FUNCTION__, __LINE__, go, status, _reg_cir.go,
+				_reg_cir.opcode, _reg_cir.opcode_modifier,
+        			_reg_cir.event, _reg_cir.status,
+        			*((u8 *)(&_reg_cir) + CIR_STATUS_OFF), *((u8 *)(&_reg_cir) + CIR_STATUS_OFF + 1),
+        			*((u8 *)(&_reg_cir) + CIR_STATUS_OFF + 2), *((u8 *)(&_reg_cir) + CIR_STATUS_OFF + 3));
+			if (f_start) {
+				rc = f_start(client, data, reg_cir, len, offset, in_out_buf);
+				if (!rc)
+					break;
+
+				if(f_finish)
+					rc = acquire_go_bit_complete(client, data, reg_cir, len,
+									offset, in_out_buf, f_finish);
+			}
+			else {
+				rc = 0;
+			}
+			break;
+		}
+		if (time_after(jiffies, orig_jiffies + data->timeout)) {
+			rc = -EIO;
+			break;
+		}
+		cond_resched();
+	}
+
+	mutex_unlock(&data->cmd_lock);
+	data->retry_cntr += try;
+
+	return  (try <= data->retries) ? rc : -EIO;
+}
+
+static int _cmd_start(struct i2c_client *client,
+		      struct switchx_data *data,
+		      struct reg_cir_layout *reg_cir,
+		      int len, int offset, u8 *in_out_buf)
+{
+	struct i2c_msg msgs[1];
+	u8 msgbuf[SWITCHX_ADDR32];
+	u8 tbuf[MAX_I2C_BUFF_SIZE];
+	int size = sizeof(struct reg_cir_layout);
+        int j, out_len = 0;
+	int rc;
+	u32 icr_reg = 0;
+	u32 icr_reg_buf[7];
+        int num = len/LPCI2C_BLOCK_MAX;
+        u8 *buf;
+        int off;
+        int chunk_size = (len > LPCI2C_BLOCK_MAX) ? LPCI2C_BLOCK_MAX : len;
+        int i;
+
+
+        if (len%LPCI2C_BLOCK_MAX)
+                num++;
+
+	dprintk(		"%s(%d) cir 0x%08x go=%x stat=%x tok=%d opcode=%d opmod=%d event=%d inmod %x par %x %x %x %x len %d offset %d\n",
+		__FUNCTION__, __LINE__, switchx_cfg.reg_cir_offset, reg_cir->go, reg_cir->status,
+		reg_cir->token, reg_cir->opcode, reg_cir->opcode_modifier, reg_cir->event, reg_cir->input_modifier,
+		reg_cir->in_param_h, reg_cir->in_param_l, reg_cir->out_param_h, reg_cir->out_param_l, len, offset);
+ 
+	if (reg_cir->opcode == SX_ACCESS_REG_OP) {
+		dprintk(			"%s write input at 0x%08x (len=%d)\n",
+			__FUNCTION__, data->mb_offset_in, len);
+		memset(msgs, 0, sizeof(struct i2c_msg));
+		memset(tbuf, 0, len + SWITCHX_ADDR32);
+		memcpy(tbuf + SWITCHX_ADDR32, in_out_buf, len);
+
+		j = 0;
+		while(j < len) {
+			if ((len - j) >= 32) {
+				dprintk(					"%s(%d) 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n" \
+					"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n" \
+					"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n" \
+					"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n",
+		 	 		__FUNCTION__, __LINE__,
+		  			*(tbuf+j), *(tbuf+j+1), *(tbuf+j+2), *(tbuf+j+3),
+		  			*(tbuf+j+4), *(tbuf+j+5), *(tbuf+j+6), *(tbuf+j+7),
+		  			*(tbuf+j+8), *(tbuf+j+9), *(tbuf+j+10), *(tbuf+j+11),
+		  			*(tbuf+j+12), *(tbuf+j+13), *(tbuf+j+14), *(tbuf+j+15),
+		  			*(tbuf+j+16), *(tbuf+j+17), *(tbuf+j+18), *(tbuf+j+19),
+		  			*(tbuf+j+20), *(tbuf+j+21), *(tbuf+j+22), *(tbuf+j+23),
+		  			*(tbuf+j+24), *(tbuf+j+25), *(tbuf+j+26), *(tbuf+j+27),
+		  			*(tbuf+j+28), *(tbuf+j+29), *(tbuf+j+30), *(tbuf+j+31));
+			}
+			else {
+				dprintk(					"%s(%d) 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n" \
+					"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n" \
+					"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n",
+		 	 		__FUNCTION__, __LINE__,
+		  			*(tbuf+j), *(tbuf+j+1), *(tbuf+j+2), *(tbuf+j+3),
+		  			*(tbuf+j+4), *(tbuf+j+5), *(tbuf+j+6), *(tbuf+j+7),
+		  			*(tbuf+j+8), *(tbuf+j+9), *(tbuf+j+10), *(tbuf+j+11),
+		  			*(tbuf+j+12), *(tbuf+j+13), *(tbuf+j+14), *(tbuf+j+15),
+		  			*(tbuf+j+16), *(tbuf+j+17), *(tbuf+j+18), *(tbuf+j+19),
+		  			*(tbuf+j+20), *(tbuf+j+21), *(tbuf+j+22), *(tbuf+j+23),
+		  			*(tbuf+j+24));
+			}
+			j += 32;
+		}
+		dprintk(				"%s(%d) 0x%02x bytes to be written\n",
+				__FUNCTION__, __LINE__, len);
+
+
+        	buf = tbuf;
+		off = data->mb_offset_in;
+        	for (i = 0; i < num; i++) {
+			chunk_size = (len > LPCI2C_BLOCK_MAX) ? LPCI2C_BLOCK_MAX : len;
+
+			rc = translate_offset_addr(client->addr, off, chunk_size, SWITCHX_ADDR32,
+							msgbuf, msgs, buf, 0);
+			rc = i2c_transfer(client->adapter, msgs, 1);
+
+			if (rc < 0)
+				return -EIO;
+			buf += chunk_size;
+			off += chunk_size;
+			len -= chunk_size;
+			memset(msgs, 0, 2*sizeof(struct i2c_msg));
+        	}
+	}
+
+	/* Write out Command Interface Register */
+        reg_cir->token = (u16)client->adapter->nr; /* Put bus id as token */
+	dprintk(		"%s(%d) write cir at 0x%08x go=%x stat=%x tok=%d opcode=%d opmod=%d event=%d inmod %x par %x %x %x %x\n",
+		__FUNCTION__, __LINE__, switchx_cfg.reg_cir_offset,
+		reg_cir->go, reg_cir->status, reg_cir->token,
+		reg_cir->opcode, reg_cir->opcode_modifier, reg_cir->event, reg_cir->input_modifier,
+		reg_cir->in_param_h, reg_cir->in_param_l, reg_cir->out_param_h, reg_cir->out_param_l);
+	memset(msgs, 0, sizeof(struct i2c_msg));
+	memset(tbuf, 0, size + SWITCHX_ADDR32);
+
+	memset(icr_reg_buf, 0, 28);
+	icr_reg_buf[2] = cpu_to_be32(reg_cir->input_modifier);
+	icr_reg_buf[5] = cpu_to_be32(reg_cir->token << 16);
+	icr_reg_buf[6] = cpu_to_be32((reg_cir->opcode_modifier << OPMOD_SHIFT) | reg_cir->opcode |
+					(reg_cir->event << EVENT_BIT));
+
+	dprintk(		  "%s(%d) icr_reg_buf[2]=0x%08x icr_reg_buf[5]=0x%08x icr_reg_buf[6]=0x%08x\n",
+		  __FUNCTION__, __LINE__, icr_reg_buf[2], icr_reg_buf[5], icr_reg_buf[6]);
+
+	memcpy(tbuf + SWITCHX_ADDR32, (u8 *)&icr_reg_buf, 28);
+	translate_offset_addr(client->addr, SX_BAR0_OFFSET_SECOND, 28, SWITCHX_ADDR32,
+				msgbuf, msgs, tbuf, 0);
+	out_len = 28;
+
+	rc = i2c_transfer(client->adapter, msgs, 1);
+
+	dprintk(			"%s(%d) 0x%02x bytes has been written at 0x%08x to HCR\n",
+			__FUNCTION__, __LINE__, out_len, SX_BAR0_OFFSET_SECOND);
+	j = 0;
+	while(j < out_len) {
+		if ((out_len - j) >= 32) {
+			dprintk(				"%s(%d) 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n" \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n" \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n" \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n",
+		 	 	__FUNCTION__, __LINE__,
+		  		*(tbuf+j), *(tbuf+j+1), *(tbuf+j+2), *(tbuf+j+3),
+		  		*(tbuf+j+4), *(tbuf+j+5), *(tbuf+j+6), *(tbuf+j+7),
+		  		*(tbuf+j+8), *(tbuf+j+9), *(tbuf+j+10), *(tbuf+j+11),
+		  		*(tbuf+j+12), *(tbuf+j+13), *(tbuf+j+14), *(tbuf+j+15),
+		  		*(tbuf+j+16), *(tbuf+j+17), *(tbuf+j+18), *(tbuf+j+19),
+		  		*(tbuf+j+20), *(tbuf+j+21), *(tbuf+j+22), *(tbuf+j+23),
+		  		*(tbuf+j+24), *(tbuf+j+25), *(tbuf+j+26), *(tbuf+j+27),
+		  		*(tbuf+j+28), *(tbuf+j+29), *(tbuf+j+30), *(tbuf+j+31));
+		}
+		else {
+			dprintk(				"%s(%d) 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n" \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n" \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n",
+		 	 	__FUNCTION__, __LINE__,
+		  		*(tbuf+j), *(tbuf+j+1), *(tbuf+j+2), *(tbuf+j+3),
+		  		*(tbuf+j+4), *(tbuf+j+5), *(tbuf+j+6), *(tbuf+j+7),
+		  		*(tbuf+j+8), *(tbuf+j+9), *(tbuf+j+10), *(tbuf+j+11),
+		  		*(tbuf+j+12), *(tbuf+j+13), *(tbuf+j+14), *(tbuf+j+15),
+		  		*(tbuf+j+16), *(tbuf+j+17), *(tbuf+j+18), *(tbuf+j+19),
+		  		*(tbuf+j+20), *(tbuf+j+21), *(tbuf+j+22), *(tbuf+j+23),
+		  		*(tbuf+j+24));
+
+		}
+		j += 32;
+	}
+
+	if (rc < 0)
+		return -EIO;
+
+	/* Write out Command Interface Register GO bit - do it by separate transaction */
+        reg_cir->go = CIR_GO_HW;                   /* Go (Software ownership for the CIR) */
+	dprintk(		"%s(%d) write cir at 0x%08x go=%x stat=%x tok=%d opcode=%d opmod=%d event=%d param %x %x %x %x\n",
+		__FUNCTION__, __LINE__, switchx_cfg.reg_cir_offset + STATUS_OFFSET,
+		reg_cir->go, reg_cir->status, reg_cir->token, reg_cir->opcode,
+		reg_cir->opcode_modifier, reg_cir->event, reg_cir->in_param_h, reg_cir->in_param_l,
+		reg_cir->out_param_h, reg_cir->out_param_l);
+
+	memset(msgs, 0, sizeof(struct i2c_msg));
+	memset(tbuf, 0, size + SWITCHX_ADDR32);
+
+        icr_reg = cpu_to_be32((1 << GO_BIT) | (reg_cir->event?(1 << EVENT_BIT):0) |
+                                (reg_cir->opcode_modifier << OPMOD_SHIFT) | reg_cir->opcode);
+	memcpy(tbuf + SWITCHX_ADDR32, (u8 *)&icr_reg, 4);
+	translate_offset_addr(client->addr, switchx_cfg.reg_cir_offset + STATUS_OFFSET, 4, SWITCHX_ADDR32,
+				msgbuf, msgs, tbuf, 0);
+
+	dprintk(		  "%s(%d) write GO bit at offset=0x%08x size=4 (0x%08x) buf=0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n",
+		  __FUNCTION__, __LINE__, switchx_cfg.reg_cir_offset + STATUS_OFFSET, icr_reg,
+		  *(tbuf), *(tbuf+1), *(tbuf+2), *(tbuf+3), *(tbuf+4), *(tbuf+5), *(tbuf+6), *(tbuf+7));
+
+	rc = i2c_transfer(client->adapter, msgs, 1);
+	if (rc < 0)
+		return -EIO;
+
+	return rc;
+}
+
+static int query_fw_cmd_finish_imm(struct i2c_client *client,
+			           struct switchx_data *data,
+			           struct reg_cir_layout *reg_cir,
+			           int len, int offset, u8 *in_out_buf)
+{
+	struct i2c_msg msgs[2];
+	u8 msgbuf[SWITCHX_ADDR32];
+	struct reg_cir_layout _reg_cir;
+	int rc;
+
+	switch (reg_cir->status) {
+		case SX_STAT_OK:
+			break;
+		case SX_STAT_INTERNAL_ERR:
+			printk(KERN_INFO "%s: Internal bus error occurred while processing command, client %s addr 0x%02x\n",
+				__FUNCTION__, client->name, client->addr);
+			return -EIO;
+        	case SX_STAT_BAD_OP:
+			printk(KERN_INFO "%s: Operation not supported, client %s addr 0x%02x\n",
+				__FUNCTION__, client->name, client->addr);
+			return -EIO;
+        	case SX_STAT_BAD_SYS_STATE:
+			printk(KERN_INFO "%s: Switch is not initialized, client %s addr 0x%02x\n",
+				__FUNCTION__, client->name, client->addr);
+			return -EIO;
+        	case SX_STAT_EXCEED_LIM:
+			printk(KERN_INFO "%s: Switch is not initialized, client %s addr 0x%02x\n",
+				__FUNCTION__, client->name, client->addr);
+			return -EIO;
+        	default:
+			printk(KERN_INFO "%s: Unexpected status %x is reported, client %s addr 0x%02x\n",
+				__FUNCTION__, reg_cir->status, client->name, client->addr);
+			// return -EIO;
+	}
+
+        /* Read all register, in spite only out_param_h and out_param_l are necessary anyway it happens once */
+	memset(&_reg_cir, 0, sizeof(struct reg_cir_layout));
+	memset(msgs, 0, 2*sizeof(struct i2c_msg));
+
+	rc = translate_offset_addr(client->addr, switchx_cfg.reg_cir_offset,
+				sizeof(struct reg_cir_layout),
+				SWITCHX_ADDR32, msgbuf, msgs, (u8 *)&_reg_cir, I2C_M_RD);
+
+	dprintk(		"%s len=%d(%d) 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x " \
+		"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n",
+		__FUNCTION__, len, (int)sizeof(struct reg_cir_layout),
+		*in_out_buf, *(in_out_buf+1), *(in_out_buf+2), *(in_out_buf+3),
+		*(in_out_buf+4), *(in_out_buf+5), *(in_out_buf+6), *(in_out_buf+7),
+		*(in_out_buf+8), *(in_out_buf+9), *(in_out_buf+10), *(in_out_buf+11),
+		*(in_out_buf+12), *(in_out_buf+13), *(in_out_buf+14), *(in_out_buf+15));
+
+	rc = i2c_transfer(client->adapter, msgs, 2);
+
+	dprintk(		"%s  cir  inmod=%x o=%x stat=%x tok=%d opcode=%d opmod=%d event=%d param %x %x %x %x\n",
+		__FUNCTION__, _reg_cir.input_modifier, _reg_cir.go, _reg_cir.status, _reg_cir.token,
+		_reg_cir.opcode, _reg_cir.opcode_modifier, _reg_cir.event, _reg_cir.in_param_h,
+		_reg_cir.in_param_l, _reg_cir.out_param_h, _reg_cir.out_param_l);
+
+	memcpy(in_out_buf, &_reg_cir.out_param_h, len);
+
+	dprintk(		"%s len=%d(%d) 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x " \
+		"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n",
+		__FUNCTION__, len, (int)sizeof(struct reg_cir_layout),
+		*in_out_buf, *(in_out_buf+1), *(in_out_buf+2), *(in_out_buf+3),
+		*(in_out_buf+4), *(in_out_buf+5), *(in_out_buf+6), *(in_out_buf+7),
+		*(in_out_buf+8), *(in_out_buf+9), *(in_out_buf+10), *(in_out_buf+11),
+		*(in_out_buf+12), *(in_out_buf+13), *(in_out_buf+14), *(in_out_buf+15));
+
+	return rc;
+}
+
+static int _cmd_finish_mbox(struct i2c_client *client,
+			    struct switchx_data *data,
+			    struct reg_cir_layout *reg_cir,
+			    int len, int offset, u8 *in_out_buf)
+{
+	int rc = -EIO;
+	struct i2c_msg msgs[2];
+	u8 msgbuf[SWITCHX_ADDR32];
+
+	switch (reg_cir->status) {
+		case SX_STAT_OK:
+			break;
+		case SX_STAT_INTERNAL_ERR:
+			printk(KERN_INFO "%s: Internal bus error occurred while processing command, client %s addr 0x%02x\n",
+				__FUNCTION__, client->name, client->addr);
+			return -EIO;
+		case SX_STAT_BAD_PARAM:
+			printk(KERN_INFO "%s: Bad parameters, client %s addr 0x%02x\n",
+				__FUNCTION__, client->name, client->addr);
+			return -EIO;
+        	case SX_STAT_BAD_OP:
+			printk(KERN_INFO "%s: Operation not supported, client %s addr 0x%02x\n",
+				__FUNCTION__, client->name, client->addr);
+			return -EIO;
+        	case SX_STAT_BAD_SYS_STATE:
+			printk(KERN_INFO "%s: Bad system state, client %s addr 0x%02x\n",
+				__FUNCTION__, client->name, client->addr);
+			return -EIO;
+        	case SX_STAT_EXCEED_LIM:
+			printk(KERN_INFO "%s: Exceeds device limits, client %s addr 0x%02x\n",
+				__FUNCTION__, client->name, client->addr);
+			return -EIO;
+        	default:
+			printk(KERN_INFO "%s: Unexpected status is reported, client %s addr 0x%02x\n",
+				__FUNCTION__, client->name, client->addr);
+			return -EIO;
+	}
+
+	memset(msgs, 0, 2*sizeof(struct i2c_msg));
+	dprintk(		"%s  cir read data at offset = 0x%08x width = %d len =%d param %x %x %x %x\n",
+		__FUNCTION__, data->mb_offset_out + offset, SWITCHX_ADDR32, len, reg_cir->in_param_h,
+		reg_cir->in_param_l, reg_cir->out_param_h, reg_cir->out_param_l);
+
+	if (len <= LPCI2C_BLOCK_MAX) {
+		rc = translate_offset_addr(client->addr, data->mb_offset_out + offset, len, SWITCHX_ADDR32,
+					msgbuf, msgs, in_out_buf, I2C_M_RD);
+		rc = i2c_transfer(client->adapter, msgs, 2);
+	}
+	else {
+        	int num = len/LPCI2C_BLOCK_MAX;
+        	u8 *tbuf = in_out_buf;
+        	int off = data->mb_offset_out + offset;
+        	int chunk_size = (len > LPCI2C_BLOCK_MAX) ? LPCI2C_BLOCK_MAX : len;
+        	int i;
+
+        	if (len%LPCI2C_BLOCK_MAX)
+                	num++;
+        	for (i = 0; i < num; i++) {
+			chunk_size = (len > LPCI2C_BLOCK_MAX) ? LPCI2C_BLOCK_MAX : len;
+
+			dprintk(				"%s i=%d (num=%d) chunk_size=%d off=0x%08x\n" \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x " \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n " \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x " \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n",
+				__FUNCTION__, i, num, chunk_size, off,
+				*(in_out_buf+16*i), *(in_out_buf+1+16*i), *(in_out_buf+2+16*i), *(in_out_buf+3+16*i),
+				*(in_out_buf+4+16*i), *(in_out_buf+5+16*i), *(in_out_buf+6+16*i), *(in_out_buf+7+16*i),
+				*(in_out_buf+8+16*i), *(in_out_buf+9+16*i), *(in_out_buf+10+16*i), *(in_out_buf+11+16*i),
+				*(in_out_buf+12+16*i), *(in_out_buf+13+16*i), *(in_out_buf+14+16*i), *(in_out_buf+15+16*i),
+				*(in_out_buf+16+16*i), *(in_out_buf+17+16*i), *(in_out_buf+18+16*i), *(in_out_buf+19+16*i),
+				*(in_out_buf+20+16*i), *(in_out_buf+21+16*i), *(in_out_buf+22+16*i), *(in_out_buf+23+16*i),
+				*(in_out_buf+24+16*i), *(in_out_buf+25+16*i), *(in_out_buf+26+16*i), *(in_out_buf+27+16*i),
+				*(in_out_buf+28+16*i), *(in_out_buf+29+16*i), *(in_out_buf+30+16*i), *(in_out_buf+31+16*i));
+
+			rc = translate_offset_addr(client->addr, off, chunk_size, SWITCHX_ADDR32,
+							msgbuf, msgs, tbuf, I2C_M_RD);
+			rc = i2c_transfer(client->adapter, msgs, 2);
+
+			dprintk(				"%s i=%d (num=%d) chunk_size=%d off=0x%08x rc=%d msgbuf 0x%02x 0x%02x 0x%02x 0x%02x\n" \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x " \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n " \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x " \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n",
+				__FUNCTION__, i, num, chunk_size, off, rc,
+				*(msgbuf), *(msgbuf+1), *(msgbuf+2), *(msgbuf+3),
+				*(in_out_buf+16*i), *(in_out_buf+1+16*i), *(in_out_buf+2+16*i), *(in_out_buf+3+16*i),
+				*(in_out_buf+4+16*i), *(in_out_buf+5+16*i), *(in_out_buf+6+16*i), *(in_out_buf+7+16*i),
+				*(in_out_buf+8+16*i), *(in_out_buf+9+16*i), *(in_out_buf+10+16*i), *(in_out_buf+11+16*i),
+				*(in_out_buf+12+16*i), *(in_out_buf+13+16*i), *(in_out_buf+14+16*i), *(in_out_buf+15+16*i),
+				*(in_out_buf+16+16*i), *(in_out_buf+17+16*i), *(in_out_buf+18+16*i), *(in_out_buf+19+16*i),
+				*(in_out_buf+20+16*i), *(in_out_buf+21+16*i), *(in_out_buf+22+16*i), *(in_out_buf+23+16*i),
+				*(in_out_buf+24+16*i), *(in_out_buf+25+16*i), *(in_out_buf+26+16*i), *(in_out_buf+27+16*i),
+				*(in_out_buf+28+16*i), *(in_out_buf+29+16*i), *(in_out_buf+30+16*i), *(in_out_buf+31+16*i));
+
+			if (rc < 0)
+				return -EIO;
+			tbuf += chunk_size;
+			off += chunk_size;
+			len -= chunk_size;
+			memset(msgs, 0, 2*sizeof(struct i2c_msg));
+        	}
+	}
+
+	if (rc < 0)
+		return -EIO;
+	return 0;
+}
+
+static int query_fw_cmd(struct device *dev, enum query_fw_opcode_mod opmod,
+			enum query_fw_input_mod inmod, u8 event,
+                        int len, int offset, u8 *in_out_buf)
+{
+	struct i2c_client *client = to_i2c_client(dev);
+	struct switchx_data *data = i2c_get_clientdata(client);
+	struct reg_cir_layout *reg_cir = &data->reg_cir;
+	int rc = 0;
+
+        memset(reg_cir, 0, sizeof(struct reg_cir_layout));
+        reg_cir->opcode = SX_QUERY_FW_OP;
+        reg_cir->opcode_modifier = opmod;
+        reg_cir->event = event;
+        reg_cir->input_modifier = inmod;
+
+	dprintk(         	"%s(%d) offset=0x%08x op=%x event=%d opmod=%x inmod=%x stat=%x tok=%x data=%p reg_cir=%p len=%d\n",
+         	__FUNCTION__, __LINE__, offset, reg_cir->opcode, reg_cir->opcode_modifier, reg_cir->event,
+        	reg_cir->input_modifier, reg_cir->status, reg_cir->token, data, reg_cir, len);
+
+        switch (opmod) {
+        case SX_MB_OUTPUT: /* An output mailbox is returned, struct query_fw_output_mbox_layout */
+        	if ((rc = acquire_go_bit(client, data, (void *)reg_cir, len, offset, in_out_buf,
+                                         _cmd_start, _cmd_finish_mbox)) < 0)
+			return rc;
+        	break;
+        case SX_IMM_OUTPUT_1: /* The output parameter is an immediate value, struct output_param_opmode1 */
+        	if ((rc = acquire_go_bit(client, data, (void *)reg_cir, len, offset, in_out_buf,
+                                         _cmd_start, query_fw_cmd_finish_imm)) < 0)
+			return rc;
+        	break;
+        case SX_IMM_OUTPUT_2: /* The output parameter is an immediate values, struct output_param_opmode2 */
+		rc = -ENOTSUPP;
+        	break;
+        }
+
+        return (rc > 0) ? 0 : -1;
+}
+
+static int access_reg_cmd_finish_get(struct i2c_client *client,
+				     struct switchx_data *data,
+				     struct reg_cir_layout *reg_cir,
+				     int len, int offset, u8 *in_out_buf)
+{
+	struct access_reg_layout *acc_reg = (struct access_reg_layout *)in_out_buf;
+	struct i2c_msg msgs[2];
+	u8 msgbuf[SWITCHX_ADDR32];
+	int j, out_len = len;
+	int rc;
+	unsigned long orig_jiffies;
+	int try = 0;
+        int num = len/LPCI2C_BLOCK_MAX;
+        int chunk_size = (len > LPCI2C_BLOCK_MAX) ? LPCI2C_BLOCK_MAX : len;
+	u8 *pbuf = in_out_buf;
+        int off = data->mb_offset_out + offset;
+        int i;
+	u8 status;
+	u8 go;
+
+        if (len%LPCI2C_BLOCK_MAX)
+                num++;
+        icr_reg_read(client, *((u32 *)((u8 *)(reg_cir) + CIR_STATUS_OFF)), &status, &go);
+
+
+	switch (status) {
+		case SX_STAT_OK:
+			break;
+		case SX_STAT_INTERNAL_ERR:
+			printk(KERN_INFO "%s: Internal bus error occurred while processing command, client %s addr 0x%02x\n",
+				__FUNCTION__, client->name, client->addr);
+			return -EIO;
+        	case SX_STAT_BAD_SYS_STATE:
+			printk(KERN_INFO "%s: Switch is not initialized, client %s addr 0x%02x\n",
+				__FUNCTION__, client->name, client->addr);
+			return -EIO;
+        	case SX_STAT_BAD_OP:
+			printk(KERN_INFO "%s: Operation not supported, client %s addr 0x%02x\n",
+				__FUNCTION__, client->name, client->addr);
+			return -EIO;
+        	default:
+			printk(KERN_INFO "%s: Unexpected status 0x%02x is reported, client %s addr 0x%02x\n",
+				__FUNCTION__, status, client->name, client->addr);
+			return -EIO;
+	}
+
+	/* ACCESS_REG commands uses mailboxes. In order to use mailboxes through the i2c, special area is
+	   reserved on the i2c address space that can be used for input and output mailboxes. Such mailboxes
+	   are called Local Mailboxes. When using a local mailbox, software should specify 0 as the
+	   Input/Output parameters. The location of the Local Mailbox addresses on the i2c space can be retrieved
+	   through the QUERY_FW command*/
+	memset(msgs, 0, 2*sizeof(struct i2c_msg));
+
+
+        for (i = 0; i < num; i++) {
+		chunk_size = (len > LPCI2C_BLOCK_MAX) ? LPCI2C_BLOCK_MAX : len;
+		rc = translate_offset_addr(client->addr, off, chunk_size, SWITCHX_ADDR32,
+						msgbuf, msgs, pbuf, I2C_M_RD);
+
+		orig_jiffies = jiffies;
+		for (try = 0; try <= access_rtry; try++) {
+			rc = i2c_transfer(client->adapter, msgs, 2);
+			if (rc > 0)
+				break;
+		}
+		data->retry_cntr += try;
+
+		pbuf += chunk_size;
+		off += chunk_size;
+		len -= chunk_size;
+		memset(msgs, 0, 2*sizeof(struct i2c_msg));
+        }
+
+	ASIC_GET_BUF(acc_reg->oper_tlv.status, in_out_buf, DR_STATUS_OFFSET);
+	acc_reg->oper_tlv.status &= 0x7f;
+
+	dprintk(			"%s(%d) 0x%02x bytes has been read\n",
+			__FUNCTION__, __LINE__, out_len);
+	j = 0;
+	while(j < out_len) {
+		if ((out_len - j) >= 32) {
+			dprintk(				"%s(%d) 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n" \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n" \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n" \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n",
+		 	 	__FUNCTION__, __LINE__,
+		  		*(in_out_buf+j), *(in_out_buf+j+1), *(in_out_buf+j+2), *(in_out_buf+j+3),
+		  		*(in_out_buf+j+4), *(in_out_buf+j+5), *(in_out_buf+j+6), *(in_out_buf+j+7),
+		  		*(in_out_buf+j+8), *(in_out_buf+j+9), *(in_out_buf+j+10), *(in_out_buf+j+11),
+		  		*(in_out_buf+j+12), *(in_out_buf+j+13), *(in_out_buf+j+14), *(in_out_buf+j+15),
+		  		*(in_out_buf+j+16), *(in_out_buf+j+17), *(in_out_buf+j+18), *(in_out_buf+j+19),
+		  		*(in_out_buf+j+20), *(in_out_buf+j+21), *(in_out_buf+j+22), *(in_out_buf+j+23),
+		  		*(in_out_buf+j+24), *(in_out_buf+j+25), *(in_out_buf+j+26), *(in_out_buf+j+27),
+		  		*(in_out_buf+j+28), *(in_out_buf+j+29), *(in_out_buf+j+30), *(in_out_buf+j+31));
+		}
+		else {
+			dprintk(				"%s(%d) 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n" \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n" \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n",
+		 	 	__FUNCTION__, __LINE__,
+		  		*(in_out_buf+j), *(in_out_buf+j+1), *(in_out_buf+j+2), *(in_out_buf+j+3),
+		  		*(in_out_buf+j+4), *(in_out_buf+j+5), *(in_out_buf+j+6), *(in_out_buf+j+7),
+		  		*(in_out_buf+j+8), *(in_out_buf+j+9), *(in_out_buf+j+10), *(in_out_buf+j+11),
+		  		*(in_out_buf+j+12), *(in_out_buf+j+13), *(in_out_buf+j+14), *(in_out_buf+j+15),
+		  		*(in_out_buf+j+16), *(in_out_buf+j+17), *(in_out_buf+j+18), *(in_out_buf+j+19),
+		  		*(in_out_buf+j+20), *(in_out_buf+j+21), *(in_out_buf+j+22), *(in_out_buf+j+23),
+		  		*(in_out_buf+j+24));
+		}
+		j += 32;
+	}
+
+	switch (acc_reg->oper_tlv.status) {
+        	case OPER_TLV_STATUS_OK:
+			break;
+        	case OPER_TLV_STATUS_BUSY:
+			printk(KERN_INFO "%s: Device is busy, client %s addr 0x%02x\n",
+				__FUNCTION__, client->name, client->addr);
+			return -EIO;
+        	case OPER_TLV_STATUS_VER_NSUPP:
+			printk(KERN_INFO "%s: Version not supported, client %s addr 0x%02x\n",
+				__FUNCTION__, client->name, client->addr);
+			return -EIO;
+        	case OPER_TLV_STATUS_UNKNWN:
+			printk(KERN_INFO "%s: Unknown TLV, client %s addr 0x%02x\n",
+				__FUNCTION__, client->name, client->addr);
+			return -EIO;
+        	case OPER_TLV_STATUS_REG_NSUPP:
+			printk(KERN_INFO "%s: Register not supported, client %s addr 0x%02x\n",
+				__FUNCTION__, client->name, client->addr);
+			return -EIO;
+        	case OPER_TLV_STATUS_CLASS_NSUPP:
+			printk(KERN_INFO "%s: Class not supported, client %s addr 0x%02x\n",
+				__FUNCTION__, client->name, client->addr);
+			return -EIO;
+        	case OPER_TLV_STATUS_METHOD_NSUPP:
+			printk(KERN_INFO "%s: Method not supported, client %s addr 0x%02x\n",
+				__FUNCTION__, client->name, client->addr);
+			return -EIO;
+        	case OPER_TLV_STATUS_BAD_PARAM:
+			printk(KERN_INFO "%s: Bad parameter, client %s addr 0x%02x\n",
+				__FUNCTION__, client->name, client->addr);
+			return -EIO;
+        	case OPER_TLV_STATUS_RSRC_NAVAIL:
+			printk(KERN_INFO "%s: Resource not available, client %s addr 0x%02x\n",
+				__FUNCTION__, client->name, client->addr);
+			return -EIO;
+        	case OPER_TLV_STATUS_MSG_ACK:
+			printk(KERN_INFO "%s: Message Receipt Acknowledgement, client %s addr 0x%02x\n",
+				__FUNCTION__, client->name, client->addr);
+			return -EIO;
+        	default:
+			printk(KERN_INFO "%s: Unexpected TLV status 0x%02x is reported, client %s addr 0x%02x\n",
+				__FUNCTION__, acc_reg->oper_tlv.status, client->name, client->addr);
+			return -EIO;
+	}
+
+	dprintk(		"%s read input at 0x%08x status=%x\n",
+		__FUNCTION__, data->mb_offset_out, acc_reg->oper_tlv.status);
+
+	return 0;
+}
+
+static int access_reg_cmd(struct device *dev, u32 reg_id, int reg_len, enum oper_tlv_method method,
+			  int len, int offset, u8 *in_out_buf)
+{
+	struct i2c_client *client = to_i2c_client(dev);
+	struct switchx_data *data = i2c_get_clientdata(client);
+        struct operation_tlv oper_tlv;
+        struct reg_access_tlv reg_accs_tlv;
+        struct end_tlv e_tlv;
+	u8 tlv_buf[sizeof(struct operation_tlv) + offsetof(struct reg_access_tlv, switchx_reg) +
+		   SX_MAX_REG_SIZE + sizeof(struct end_tlv)];
+	struct reg_cir_layout *reg_cir = &data->reg_cir;
+	int off = 0;
+	int offreg;
+	int j;
+	int rc = 0;
+	u16 regtlv = 0;
+
+	dprintk(               "%s input buf (len=%d) reg_len=%d reg_id=%x method=%d\n",
+               __FUNCTION__, len, reg_len, reg_id, method);
+
+	offreg = offsetof(struct reg_access_tlv, switchx_reg);
+	memset(tlv_buf, 0, sizeof(struct operation_tlv) + offreg +
+		   SX_MAX_REG_SIZE + sizeof(struct end_tlv));
+	memset(&oper_tlv, 0, sizeof(struct operation_tlv));
+	memset(&reg_accs_tlv, 0, offreg);
+	memset(&e_tlv, 0, sizeof(struct end_tlv));
+	memset(reg_cir, 0, sizeof(struct reg_cir_layout));
+
+	/* Put Oper TLV  */
+	SET_OPER_TLV(tlv_buf,
+			SX_OPERATION_TLV,
+			OPERATION_TLV_SIZE/4,
+			DIRECT_ROUTE_NOT_SET,
+			OPER_TLV_STATUS_OK,
+			OPER_TLV_REQ,
+			(u16)reg_id,
+			(u8)method,
+			(u8)SX_EMAD_REG_ACCESS_CLASS,
+			data->transact_id++);
+	/* Put Reg TLV constant part */
+        regtlv = ((REG_TLV_TYPE << 11) | (reg_len/4 + REG_ACCESS_TLV_LEN));
+	ASIC_SET_BUF(tlv_buf, regtlv, REG_TLV_OFFSET);
+	/* Put Reg TLV variable part */
+
+	/* Put End TLV */
+        regtlv = (SX_END_OF_TLV << 11) | REG_TLV_END_LEN;
+	ASIC_SET_BUF(tlv_buf, regtlv, (REG_TLV_LEN + REG_ACCESS_TLV_LEN) * 4 + reg_len);
+	off = reg_len + (REG_TLV_LEN + REG_ACCESS_TLV_LEN) * 4;
+
+	dprintk(               "%s input buf (len=%d) reg_len=%d reg_id=%x method=%d\n",
+               __FUNCTION__, len, reg_len, reg_id, method);
+
+	j = 0;
+	while(j < len) {
+		if ((len - j) >= 32) {
+			dprintk(				"%s(%d) 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n" \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n" \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n" \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n",
+		 	 	__FUNCTION__, __LINE__,
+		  		*(tlv_buf+j), *(tlv_buf+j+1), *(tlv_buf+j+2), *(tlv_buf+j+3),
+		  		*(tlv_buf+j+4), *(tlv_buf+j+5), *(tlv_buf+j+6), *(tlv_buf+j+7),
+		  		*(tlv_buf+j+8), *(tlv_buf+j+9), *(tlv_buf+j+10), *(tlv_buf+j+11),
+		  		*(tlv_buf+j+12), *(tlv_buf+j+13), *(tlv_buf+j+14), *(tlv_buf+j+15),
+		  		*(tlv_buf+j+16), *(tlv_buf+j+17), *(tlv_buf+j+18), *(tlv_buf+j+19),
+		  		*(tlv_buf+j+20), *(tlv_buf+j+21), *(tlv_buf+j+22), *(tlv_buf+j+23),
+		  		*(tlv_buf+j+24), *(tlv_buf+j+25), *(tlv_buf+j+26), *(tlv_buf+j+27),
+		  		*(tlv_buf+j+28), *(tlv_buf+j+29), *(tlv_buf+j+30), *(tlv_buf+j+31));
+		}
+		else {
+			dprintk(				"%s(%d) 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n" \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n" \
+				"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n",
+		 	 	__FUNCTION__, __LINE__,
+		  		*(tlv_buf+j), *(tlv_buf+j+1), *(tlv_buf+j+2), *(tlv_buf+j+3),
+		  		*(tlv_buf+j+4), *(tlv_buf+j+5), *(tlv_buf+j+6), *(tlv_buf+j+7),
+		  		*(tlv_buf+j+8), *(tlv_buf+j+9), *(tlv_buf+j+10), *(tlv_buf+j+11),
+		  		*(tlv_buf+j+12), *(tlv_buf+j+13), *(tlv_buf+j+14), *(tlv_buf+j+15),
+		  		*(tlv_buf+j+16), *(tlv_buf+j+17), *(tlv_buf+j+18), *(tlv_buf+j+19),
+		  		*(tlv_buf+j+20), *(tlv_buf+j+21), *(tlv_buf+j+22), *(tlv_buf+j+23),
+		  		*(tlv_buf+j+24));
+		}
+		j += 32;
+	}
+
+        reg_cir->opcode = SX_ACCESS_REG_OP;                           /* ACCESS_REG command opcode 0x40   */
+        reg_cir->event = CIR_EVENT_NO_REPORT;
+
+        switch (method) {
+        case OPER_TLV_METHOD_QUERY:
+                if ((rc = acquire_go_bit(client, data, (void *)reg_cir, off + REG_TLV_END_LEN * 4,
+                                         offset, tlv_buf, _cmd_start, access_reg_cmd_finish_get)) != 0)
+                        return rc;
+                memcpy(in_out_buf, &tlv_buf[(REG_TLV_LEN + REG_ACCESS_TLV_LEN) * 4], len);
+                break;
+        case OPER_TLV_METHOD_WRITE:
+                if ((rc = acquire_go_bit(client, data, (void *)reg_cir, off + REG_TLV_END_LEN * 4,
+                                         offset, tlv_buf, _cmd_start, NULL)) != 0)
+                        return rc;
+                break;
+        case OPER_TLV_METHOD_SEND:
+        case OPER_TLV_METHOD_EVENT:
+                rc = -ENOTSUPP;
+                break;
+        }
+
+        return rc;
+}
+
+static ssize_t switchx_sw_reset(_FL_  struct kobject *kobj, struct bin_attribute *attr,
+				  char *buf, loff_t off, size_t count)
+{
+	struct i2c_client *client = kobj_to_i2c_client(kobj);
+
+	return i2c_software_reset(&client->dev);
+}
+
+static ssize_t query_fw_read(_FL_  struct kobject *kobj, struct bin_attribute *attr,
+				  char *buf, loff_t off, size_t count)
+{
+	struct i2c_client *client = kobj_to_i2c_client(kobj);
+	int rc;
+
+	dev_dbg(&client->dev, "query_fw_read(p=%p, off=%lli, c=%zi)\n",
+		buf, off, count);
+	if (off % 4)
+		return 0;
+	if (off >= sizeof(struct query_fw_output_mbox_layout))
+		return 0;
+	if (off + count > sizeof(struct query_fw_output_mbox_layout))
+		count = sizeof(struct query_fw_output_mbox_layout) - off;
+
+	/* Read from mailbox */
+	rc = query_fw_cmd(&client->dev, SX_MB_OUTPUT, SX_BAR0_OFF_NONE,
+			  CIR_EVENT_REPORT, count, off, buf);
+
+	return count;
+}
+
+static ssize_t show_mgir_reg(_FL_  struct kobject *kobj, struct bin_attribute *attr,
+				  char *buf, loff_t off, size_t count)
+{
+#ifdef SXDBGMODE
+	struct reg_mgir_layout *p_mgir_reg = (struct reg_mgir_layout *)buf;
+#endif
+	struct i2c_client *client = kobj_to_i2c_client(kobj);
+	struct reg_mgir_layout mgir_reg;
+	int rc;
+
+	memset(&mgir_reg, 0, sizeof(struct reg_mgir_layout));
+	dev_dbg(&client->dev, "%s(p=%p, off=%lli, c=%zi)\n",
+		__FUNCTION__, buf, off, count);
+	if (off % 4)
+		return -1;
+	if (off >= sizeof(struct reg_mgir_layout))
+		return -1;
+	if (off + count > sizeof(struct reg_mgir_layout))
+		count = sizeof(struct reg_mgir_layout) - off;
+
+	rc = access_reg_cmd(&client->dev, MGIR_REG_ID,
+				MGIR_REG_LEN * 4/* sizeof(struct reg_mgir_layout)*/,
+				OPER_TLV_METHOD_QUERY, /*count*/
+				(REG_TLV_LEN + REG_ACCESS_TLV_LEN + MGIR_REG_LEN + REG_TLV_END_LEN) * 4,
+				off, buf);
+
+	get_mgir_reg((u8 *)p_mgir_reg, &mgir_reg);
+
+	dprintk(		"%s (%p) 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x " \
+		"0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x 0x%02x\n",
+		__FUNCTION__, p_mgir_reg, *buf, *(buf+1), *(buf+2), *(buf+3), *(buf+4), *(buf+5),
+		*(buf+6), *(buf+7), *(buf+8), *(buf+9), *(buf+10), *(buf+11), *(buf+12), *(buf+13),
+		*(buf+14), *(buf+15));
+	dprintk(		"%s sw_major=0x%02x sw_minor=%04x sw_sub_minor=%04x dvfs=%04x rev=%04x dev_id=%04x uptime=%04x\n"
+		"major=%04x minor=%04x sub_minor=%04x build_id=%04x month=%04x day=%04x year=%04x hour=%04x\n"
+		"ini_file_version=%08x extended_major=%08x extended_minor=%08x extended_sub_minor=%08x\n",
+		__FUNCTION__, mgir_reg.sw_info.major,
+		mgir_reg.sw_info.minor, mgir_reg.sw_info.sub_minor,
+		mgir_reg.hw_info.dvfs, mgir_reg.hw_info.device_hw_revision,
+		mgir_reg.hw_info.device_id, mgir_reg.hw_info.uptime,
+		mgir_reg.fw_info.major, mgir_reg.fw_info.minor, mgir_reg.fw_info.sub_minor,
+		mgir_reg.fw_info.build_id, mgir_reg.fw_info.month, mgir_reg.fw_info.day,
+		mgir_reg.fw_info.year, mgir_reg.fw_info.hour, mgir_reg.fw_info.ini_file_version,
+		mgir_reg.fw_info.extended_major, mgir_reg.fw_info.extended_minor,
+		mgir_reg.fw_info.extended_sub_minor);
+
+	return count;
+}
+
+static int __i2c_match_addr(struct device *dev, void *addrp)
+{
+	struct i2c_client *client = i2c_verify_client(dev);
+	int addr = *(int *)addrp;
+
+        dprintk(
+                "%s client=%p addr=%x\n", __FUNCTION__, client, addr);
+
+	if (client && client->addr == addr)
+		return 1;
+	return 0;
+}
+
+static int get_dev_by_bus_id_and_addr(int bus_id, int addr, struct device *dev)
+{
+	struct device *adap_dev;
+        struct device *client_dev;
+        struct i2c_adapter *adap;
+
+        dprintk(
+                "%s bus=%d addr=%x\n", __FUNCTION__, bus_id, addr);
+        adap = i2c_get_adapter(bus_id);
+	if (adap == NULL) {
+                printk(
+			"%s adapter not found bus=%d addr=%x\n", __FUNCTION__, bus_id, addr);
+		return -EINVAL;
+        }
+	adap_dev = &adap->dev;
+	client_dev = device_find_child(adap_dev, &addr, __i2c_match_addr);
+	if(!client_dev) {
+                printk(
+			"%s client not found bus=%d addr=%x\n", __FUNCTION__, bus_id, addr);
+		return -EINVAL;
+        }
+	dev = client_dev;
+
+	return 0;
+}
+
+struct i2c_client *sx_get_client_by_i2c_bus_and_addr(int bus_id, int addr)
+{
+	struct switchx_data *swd;
+
+	list_for_each_entry(swd, &switchx_cfg.switchx_list, list) {
+		if ((bus_id == swd->bus_id) && (addr == swd->i2c_dev_addr))
+			return swd->client;
+	}
+	return NULL;
+}
+
+struct switchx_probe_failed * find_dev_probe_context(int bus_id)
+{
+	struct switchx_probe_failed *swpf, *next;
+
+	list_for_each_entry_safe(swpf, next, &switchx_cfg.probe_failed_list, list) {
+		if (bus_id == swpf->bus_id)
+			return swpf; /* device on this bus failed probing */
+	}
+	return NULL;
+}
+
+int sx_software_reset(int i2c_dev_id)
+{
+	int bus_id = BUS_ID_FROM_ID(i2c_dev_id);
+	u8 addr = ADDR_FROM_DEV_ID(i2c_dev_id);
+	struct device *dev = NULL;
+
+	if (!(get_dev_by_bus_id_and_addr(bus_id, addr, dev)))
+		return -EINVAL;
+	return i2c_software_reset(dev);
+}
+EXPORT_SYMBOL(sx_software_reset);
+
+int i2c_get_fw_rev(int i2c_dev_id, u64* fw_rev)
+{
+	struct i2c_client *client;
+	int bus_id = BUS_ID_FROM_ID(i2c_dev_id);
+	u8 addr = ADDR_FROM_DEV_ID(i2c_dev_id);
+	struct query_fw_output_mbox_layout query_fw;
+
+	client = sx_get_client_by_i2c_bus_and_addr(bus_id, addr);
+	if(!client)
+		return -EUNATCH;
+
+	if (!query_fw_cmd(&client->dev, SX_MB_OUTPUT, SX_BAR0_OFF_NONE, \
+					  CIR_EVENT_REPORT, sizeof(struct query_fw_output_mbox_layout), 0, (u8*)&query_fw))
+		return -EIO;
+
+	*fw_rev = (u64)query_fw.fw_rev_major << 32 | (u64)query_fw.fw_rev_minor << 16 | query_fw.fw_rev_subminor;
+
+	return 0;
+}
+EXPORT_SYMBOL(i2c_get_fw_rev);
+
+int i2c_set_go_bit_stuck(int i2c_dev_id)
+{
+	struct switchx_data *data = NULL;
+	struct i2c_client *client;
+	int bus_id = BUS_ID_FROM_ID(i2c_dev_id);
+	u8 addr = ADDR_FROM_DEV_ID(i2c_dev_id);
+
+	client = sx_get_client_by_i2c_bus_and_addr(bus_id, addr);
+	if(!client)
+		return -EUNATCH;
+	data = i2c_get_clientdata(client);
+	if(!data)
+		return -EINVAL;
+	if(data->signature != SX_I2C_DEV_SIGNATURE)
+		return -EINVAL;
+	data->go_bit_status = GO_BIT_STAT_STUCK;
+	return 0;
+}
+
+int i2c_get_local_mbox(int i2c_dev_id, u32 *mb_size_in, u32 *mb_offset_in, u32 *mb_size_out, u32 *mb_offset_out)
+{
+	struct switchx_data *data;
+	struct i2c_client *client;
+	int bus_id = BUS_ID_FROM_ID(i2c_dev_id);
+	u8 addr = ADDR_FROM_DEV_ID(i2c_dev_id);
+
+	dprintk(		"%s i2c_dev_id=%x\n", __FUNCTION__, i2c_dev_id);
+
+        if (!mb_size_in || !mb_offset_in || !mb_size_out || !mb_offset_out)
+		return -EINVAL;
+	client = sx_get_client_by_i2c_bus_and_addr(bus_id, addr);
+	if(!client)
+		return -EUNATCH;
+
+	data = i2c_get_clientdata(client);
+	if(!data)
+		return -EINVAL;
+
+	*mb_size_in = data->mb_size_in;
+	*mb_offset_in = data->mb_offset_in;
+	*mb_size_out = data->mb_size_out;
+	*mb_offset_out = data->mb_offset_out;
+
+	dprintk("%s mb size=%x off=0x%08x out mb size=%x off=0x%08x\n",
+		__func__, data->mb_size_in, data->mb_offset_in, data->mb_size_out, data->mb_offset_out);
+
+	return 0;
+}
+EXPORT_SYMBOL(i2c_get_local_mbox);
+
+int i2c_enforce(int i2c_dev_id)
+{
+	return 0;
+}
+EXPORT_SYMBOL(i2c_enforce);
+
+int i2c_release(int i2c_dev_id)
+{
+	return 0;
+}
+EXPORT_SYMBOL(i2c_release);
+
+int i2c_write(int i2c_dev_id, int offset, int len, u8 *in_out_buf)
+{
+	struct switchx_data *data;
+	struct i2c_client *client;
+	int bus_id = BUS_ID_FROM_ID(i2c_dev_id);
+	u8 addr = ADDR_FROM_DEV_ID(i2c_dev_id);
+	struct i2c_msg msgs[1];
+	u8 msgbuf[SWITCHX_ADDR32];
+	u8 tbuf[MAX_I2C_BUFF_SIZE + 64];
+	unsigned long orig_jiffies;
+	int try = 0;
+	int rc = 0;
+        int num = len/LPCI2C_BLOCK_MAX;
+        int off = offset;
+        int chunk_size = (len > LPCI2C_BLOCK_MAX) ? LPCI2C_BLOCK_MAX : len;
+	u8 *pbuf = tbuf;
+        int i;
+
+        if (len%LPCI2C_BLOCK_MAX)
+                num++;
+
+	client = sx_get_client_by_i2c_bus_and_addr(bus_id, addr);
+	dprintk(		"%s i2c_dev_id=%x bus=%d addr=%x client=%p\n", __FUNCTION__, i2c_dev_id, bus_id, addr, client);
+	if(!client)
+		return -EUNATCH;
+
+	data = i2c_get_clientdata(client);
+	if(!data)
+		return -EINVAL;
+
+	memset(msgs, 0, sizeof(struct i2c_msg));
+	memset(tbuf, 0, len + SWITCHX_ADDR32);
+	memcpy(tbuf + SWITCHX_ADDR32, in_out_buf, len);
+
+        for (i = 0; i < num; i++) {
+		chunk_size = (len > LPCI2C_BLOCK_MAX) ? LPCI2C_BLOCK_MAX : len;
+		rc = translate_offset_addr(client->addr, off, chunk_size, SWITCHX_ADDR32,
+						msgbuf, msgs, pbuf, 0);
+
+	orig_jiffies = jiffies;
+	for (try = 0; try <= access_rtry; try++) {
+		rc = i2c_transfer(client->adapter, msgs, 1);
+		if (rc > 0)
+			break;
+		}
+		data->retry_cntr += try;
+		pbuf += chunk_size;
+		off += chunk_size;
+		len -= chunk_size;
+		memset(msgs, 0, 2*sizeof(struct i2c_msg));
+        }
+	dprintk(		"%s i2c_dev_id=%x bus=%d addr=%x client=%p\n", __FUNCTION__, i2c_dev_id, bus_id, addr, client);
+
+	return (rc > 0) ? 0 : rc;
+}
+EXPORT_SYMBOL(i2c_write);
+
+int i2c_read(int i2c_dev_id, int offset, int len, u8 *in_out_buf)
+{
+	struct switchx_data *data;
+	struct i2c_client *client;
+	int bus_id = BUS_ID_FROM_ID(i2c_dev_id);
+	u8 addr = ADDR_FROM_DEV_ID(i2c_dev_id);
+	struct i2c_msg msgs[2];
+	u8 msgbuf[SWITCHX_ADDR32];
+	unsigned long orig_jiffies;
+	int try = 0;
+	int rc = 0;
+        int num = len/LPCI2C_BLOCK_MAX;
+        int off = offset;
+        int chunk_size = (len > LPCI2C_BLOCK_MAX) ? LPCI2C_BLOCK_MAX : len;
+	u8 *pbuf = in_out_buf;
+        int i;
+
+        if (len%LPCI2C_BLOCK_MAX)
+                num++;
+
+	client = sx_get_client_by_i2c_bus_and_addr(bus_id, addr);
+	dprintk(		"%s i2c_dev_id=%x bus=%d addr=%x client=%p\n", __FUNCTION__, i2c_dev_id, bus_id, addr, client);
+	if(!client)
+		return -EUNATCH;
+
+	data = i2c_get_clientdata(client);
+	if(!data)
+		return -EINVAL;
+
+	memset(msgs, 0, 2*sizeof(struct i2c_msg));
+
+        for (i = 0; i < num; i++) {
+		chunk_size = (len > LPCI2C_BLOCK_MAX) ? LPCI2C_BLOCK_MAX : len;
+		rc = translate_offset_addr(client->addr, off, chunk_size, SWITCHX_ADDR32,
+						msgbuf, msgs, pbuf, I2C_M_RD);
+
+		orig_jiffies = jiffies;
+		for (try = 0; try <= access_rtry; try++) {
+			rc = i2c_transfer(client->adapter, msgs, 2);
+			if (rc > 0)
+				break;
+		}
+		data->retry_cntr += try;
+		pbuf += chunk_size;
+		off += chunk_size;
+		len -= chunk_size;
+		memset(msgs, 0, 2*sizeof(struct i2c_msg));
+        }
+
+	return (rc > 0) ? 0 : rc;
+}
+EXPORT_SYMBOL(i2c_read);
+
+int i2c_write_dword(int i2c_dev_id, int offset, u32 val)
+{
+	struct switchx_data *data;
+	struct i2c_client *client;
+	int bus_id = BUS_ID_FROM_ID(i2c_dev_id);
+	u8 addr = ADDR_FROM_DEV_ID(i2c_dev_id);
+	struct i2c_msg msgs[1];
+	u8 msgbuf[SWITCHX_ADDR32];
+	u8 tbuf[SWITCHX_ADDR32 + 4];
+	unsigned long orig_jiffies;
+	int try;
+	int rc = 0;
+
+	client = sx_get_client_by_i2c_bus_and_addr(bus_id, addr);
+	dprintk( "%s i2c_dev_id=%x bus=%d addr=%x client=%p offset=0x%08x\n",
+		    __FUNCTION__, i2c_dev_id, bus_id, addr, client, offset);
+	if(!client)
+		return -EUNATCH;
+
+	data = i2c_get_clientdata(client);
+	if(!data)
+		return -EINVAL;
+
+	memset(msgs, 0, sizeof(struct i2c_msg));
+	memset(tbuf, 0, SWITCHX_ADDR32 + 4);
+	memcpy(tbuf + SWITCHX_ADDR32, (u8 *)&val, 4);
+
+	translate_offset_addr(client->addr, offset, 4, SWITCHX_ADDR32,
+				msgbuf, msgs, tbuf, 0);
+
+	orig_jiffies = jiffies;
+	for (try = 0; try <= access_rtry; try++) {
+		rc = i2c_transfer(client->adapter, msgs, 1);
+		if (rc > 0)
+			break;
+	}
+	data->retry_cntr += try;
+	dprintk(		"%s rc=%d try=%d\n", __FUNCTION__, rc, try);
+
+	return (rc > 0) ? 0 : rc;
+}
+EXPORT_SYMBOL(i2c_write_dword);
+
+int i2c_read_dword(int i2c_dev_id, int offset, u32 *val)
+{
+	struct switchx_data *data;
+	struct i2c_client *client;
+	int bus_id = BUS_ID_FROM_ID(i2c_dev_id);
+	u8 addr = ADDR_FROM_DEV_ID(i2c_dev_id);
+	struct i2c_msg msgs[2];
+	u8 msgbuf[SWITCHX_ADDR32];
+	unsigned long orig_jiffies;
+	int try;
+	int rc = 0;
+
+	client = sx_get_client_by_i2c_bus_and_addr(bus_id, addr);
+	dprintk( "%s i2c_dev_id=%x bus=%d addr=%x client=%p offset=0x%08x\n",
+		  __FUNCTION__, i2c_dev_id, bus_id, addr, client, offset);
+	if(!client)
+		return -EUNATCH;
+
+	data = i2c_get_clientdata(client);
+	if(!data)
+		return -EINVAL;
+
+	memset(msgs, 0, 2*sizeof(struct i2c_msg));
+	translate_offset_addr(client->addr, offset, 4, SWITCHX_ADDR32,
+				msgbuf, msgs, (u8 *)val, I2C_M_RD);
+
+	orig_jiffies = jiffies;
+	for (try = 0; try <= access_rtry; try++) {
+		rc = i2c_transfer(client->adapter, msgs, 2);
+
+
+
+memcpy(msgbuf, val, 4);
+dprintk("%s(%d) 0x%02x 0x%02x 0x%02x  0x%02x\n",
+		 	 __FUNCTION__, __LINE__,
+		  	*(msgbuf), *(msgbuf+1), *(msgbuf+2), *(msgbuf+3));
+if ( *(msgbuf) == 0x30 ) {
+*(msgbuf) = 0;
+memcpy(val, msgbuf, 4);
+}
+
+
+
+		if (rc > 0) {
+			break;
+		}
+		/* This API is used for access to HCR register. In case this
+                   transaction failed - status is to be provided, since such
+                   failure in certain scenario can cause i2c bus stuck */
+	}
+	data->retry_cntr += try;
+	dprintk(		"%s rc=%d try=%d\n", __FUNCTION__, rc, try);
+
+	return (rc > 0) ? 0 : rc;
+}
+EXPORT_SYMBOL(i2c_read_dword);
+
+static struct bin_attribute query_fm_attr = {
+	.attr = {
+		.name = "queryfw",
+		.mode = S_IRUGO,
+	},
+	.size = sizeof(struct query_fw_output_mbox_layout),
+	.read = query_fw_read,
+};
+
+static struct bin_attribute mgir_reg_attr = {
+	.attr = {
+		.name = "mgir_reg",
+		.mode = S_IRUGO,
+	},
+	.size = sizeof(struct reg_mgir_layout),
+	.read = show_mgir_reg,
+};
+
+static struct bin_attribute sw_reset_attr = {
+	.attr = {
+		.name = "sw_reset",
+		.mode = S_IWUSR,
+	},
+	.size = sizeof(struct reg_sw_reset_layout),
+	.write = switchx_sw_reset,
+};
+
+/* Return 0 if detection is successful, -ENODEV otherwise */
+static int switchx_detect(struct i2c_client *client,
+			 struct i2c_board_info *info)
+{
+	struct i2c_adapter *adapter = client->adapter;
+
+	if (!i2c_check_functionality(adapter, I2C_FUNC_SMBUS_BYTE_DATA
+				     | I2C_FUNC_SMBUS_WORD_DATA
+				     | I2C_FUNC_SMBUS_WRITE_BYTE))
+		return -ENODEV;
+
+	strlcpy(info->type, "switchx", I2C_NAME_SIZE);
+
+	return 0;
+}
+
+static int switchx_probe(struct i2c_client *client,
+			const struct i2c_device_id *id)
+{
+	struct switchx_data *data = NULL;
+	struct output_param_opmode1 output_param;
+	struct i2c_adapter *adap = NULL;
+	struct query_fw_output_mbox_layout query_fw;
+	int rc = -1;
+printk("%s(%d)\n", __func__, __LINE__);
+	data = kzalloc(sizeof(struct switchx_data), GFP_KERNEL);
+	if (!data) {
+		rc = -ENOMEM;
+		goto exit;
+	}
+printk("%s(%d)\n", __func__, __LINE__);
+	memset(data, 0, sizeof(struct switchx_data));
+	data->signature = SX_I2C_DEV_SIGNATURE;
+
+	i2c_set_clientdata(client, data);
+	mutex_init(&data->cmd_lock);
+
+	if (sysfs_create_bin_file(&client->dev.kobj, &query_fm_attr))
+		goto exit_free;
+	if (sysfs_create_bin_file(&client->dev.kobj, &mgir_reg_attr))
+		goto exit_query_fw_cmd;
+	if (sysfs_create_bin_file(&client->dev.kobj, &sw_reset_attr))
+		goto exit_mgir;
+
+	data->retries = 10;    // use 10 retries only during probing, then callibrate it accrding
+			       // to number of retries of query_fm at it first probing call
+	data->timeout = 500;
+	data->transact_id = 0;
+	data->bus_id = client->adapter->nr;
+	data->i2c_dev_addr = client->addr;
+	data->i2c_dev = &client->dev;
+	data->client = client;
+	data->route_enforced = 0;
+	adap = client->adapter;
+
+	/* ACCESS_REG and CONFIG_PROFILE commands use mailboxes for input and for output parameters.
+	   QUERY_FW and QUERY_BOARDINFO commands uses mailboxes for output parameters.
+	   In order to use mailboxes through the i2c, special area is reserved on the i2c address space that
+	   can be used for input and output mailboxes.
+	   Such mailboxes are called Local Mailboxes. When using a local mailbox, software should specify 0 as the
+	   Input/Output parameters. The location of the Local Mailbox addresses on the i2c space can be retrieved
+	   through the QUERY_FW command.
+	   For this purpose QUERY_FW is to be issued with opcode modifier equal SX_IMM_OUTPUT_1 (0x01).
+	   For such command the output parameter is an immediate value, struct output_param_opmode1.
+	   Invoke QUERY_FW command for swicthx probing and for getting local mailboxes addresses.
+	   Read from immedate output parameters.
+	*/
+	memset(&output_param, 0, sizeof(struct output_param_opmode1));
+	if((rc = query_fw_cmd(&client->dev,
+				SX_IMM_OUTPUT_1,
+				SX_BAR0_OFF_NONE,
+				CIR_EVENT_NO_REPORT,
+				sizeof(struct output_param_opmode1),
+				0,
+				(u8 *)&output_param)) < 0) {
+printk("%s(%d)\n", __func__, __LINE__);
+		goto exit_query_fw_cmd;
+	}
+	list_add(&data->list, &switchx_cfg.switchx_list);
+printk("%s(%d)\n", __func__, __LINE__);
+	convert_mbox((u32 *)&output_param);
+	dprintk(		"%s(%d) 0x%08x 0x%08x 0x%08x 0x%08x\n",
+		__FUNCTION__, __LINE__,
+		output_param.out_param_h.local_mb_size, output_param.out_param_h.local_mb_bar0_offset,
+		output_param.out_param_l.local_mb_size, output_param.out_param_l.local_mb_bar0_offset);
+
+	data->mb_size_in = output_param.out_param_h.local_mb_size;
+	data->mb_offset_in = output_param.out_param_h.local_mb_bar0_offset;
+	data->mb_size_out = output_param.out_param_l.local_mb_size;
+	data->mb_offset_out = output_param.out_param_l.local_mb_bar0_offset;
+	data->retries = data->retry_cntr + 3;
+	data->go_bit_status = GO_BIT_STAT_OK;
+	memset(data->cache, 0, SX_MAX_REG_SIZE);
+	memset(&query_fw, 0, sizeof(struct query_fw_output_mbox_layout));
+	query_fw_cmd(&client->dev,
+			SX_MB_OUTPUT,
+			SX_BAR0_OFF_NONE,
+			CIR_EVENT_REPORT,
+			sizeof(struct query_fw_output_mbox_layout),
+			0,
+			(u8*)&query_fw);
+
+	convert_query_fw_mbox((u32 *)&query_fw);
+	memcpy(data->name, id->name, sizeof(id->name));
+	printk(KERN_INFO "ASIC probe device %s (rtry=%d) fw_rev: major=%d minor=%d submin=%d in mb size=%x off=0x%08x out mb size=%x off=0x%08x\n",
+		id->name, data->retry_cntr,
+		query_fw.fw_rev_major, query_fw.fw_rev_minor, query_fw.fw_rev_subminor,
+		data->mb_size_in, data->mb_offset_in, data->mb_size_out, data->mb_offset_out);
+
+	switch (id->driver_data) {
+	case any_chip:
+	case switchx:
+	case connectx:
+	case switchib:
+	case switchspc:
+		break;
+	}
+ 
+	return rc;
+
+      exit_mgir:
+	sysfs_remove_bin_file(&client->dev.kobj, &mgir_reg_attr);
+      exit_query_fw_cmd:
+	sysfs_remove_bin_file(&client->dev.kobj, &query_fm_attr);
+      exit_free:
+	i2c_set_clientdata(client, NULL);
+	kfree(data);
+      exit:
+	printk(KERN_INFO "ASIC device %s\n", id->name);
+
+	return rc;
+}
+
+static int switchx_remove(struct i2c_client *client)
+{
+	struct switchx_data *data = i2c_get_clientdata(client);
+
+	sysfs_remove_bin_file(&client->dev.kobj, &mgir_reg_attr);
+	sysfs_remove_bin_file(&client->dev.kobj, &query_fm_attr);
+	if (!list_empty(&switchx_cfg.switchx_list))
+		list_del_rcu(&data->list);
+	if(data)
+		kfree(data);
+
+	return 0;
+}
+
+static const struct i2c_device_id switchx_id[] = {
+	{ "switchx", switchx },
+	{ "connectx", connectx },
+	{ "switchib", switchib },
+	{ "switchspc", switchspc },
+	{ }
+};
+
+MODULE_DEVICE_TABLE(i2c, switchx_id);
+
+/* This is the driver that will be inserted */
+static struct i2c_driver switchx_driver = {
+	.class		= I2C_CLASS_HWMON,
+	.driver = {
+		.name	= "switchx",
+	},
+	.probe		= switchx_probe,
+	.remove		= switchx_remove,
+	.id_table	= switchx_id,
+	.detect		= switchx_detect,
+	.address_list	= normal_i2c,
+};
+
+static int __init switchx_init(void)
+{
+	switchx_probe_failed_cache = kmem_cache_create("switchx_probe_failed_cache",
+					 sizeof(struct switchx_probe_failed),
+					 0,
+					 SLAB_HWCACHE_ALIGN, NULL);
+	if (!switchx_probe_failed_cache)
+		return -ENOMEM;
+
+	INIT_LIST_HEAD(&switchx_cfg.switchx_list);
+	INIT_LIST_HEAD(&switchx_cfg.probe_failed_list);
+	if (cir_reg_num == 0)
+		switchx_cfg.reg_cir_offset = SX_BAR0_OFFSET_PRIM;
+	else
+		switchx_cfg.reg_cir_offset = SX_BAR0_OFFSET_SECOND;
+	switchx_cfg.reg_sw_reset_offset = SX_SW_RESET_OFFSET;
+	if (i2c_add_driver(&switchx_driver) != 0)
+		return -1;
+	switchx_cfg.last_access_dev_id = 0;
+
+	printk(KERN_INFO "%s: reg_cir_offset=0x%08x reg_sw_reset_offset=0x%08x\n",
+		__FUNCTION__, switchx_cfg.reg_cir_offset, switchx_cfg.reg_sw_reset_offset);
+
+{
+	struct i2c_board_info info;
+	struct i2c_adapter *adap;
+	struct i2c_client *client;
+	int i;
+
+	for (i = 2; i < 16; i++) {
+		memset(&info, 0, sizeof(struct i2c_board_info));
+		info.addr = 0x48;
+		strcpy(info.type, "switchspc");
+		adap = i2c_get_adapter(i);
+		if (adap) {
+			printk("%s bus %d %s %x\n", __func__, i, info.type, info.addr);
+			client = i2c_new_device(adap, &info);
+		}
+	}
+}
+
+	return 0;
+}
+
+static void __exit switchx_exit(void)
+{
+	struct switchx_probe_failed *swpf, *next;
+
+	i2c_del_driver(&switchx_driver);
+
+	if (switchx_probe_failed_cache) {
+                list_for_each_entry_safe(swpf, next, &switchx_cfg.probe_failed_list, list) {
+                        list_del(&swpf->list);
+                        kmem_cache_free(switchx_probe_failed_cache, swpf);
+                }
+		kmem_cache_destroy(switchx_probe_failed_cache);
+	}
+}
+
+
+MODULE_AUTHOR("Vadim Pasternak <vadimp@mellanox.com>");
+MODULE_DESCRIPTION("SWITCHX I2C/SMbus driver");
+MODULE_LICENSE("GPL v2");
+
+module_init(switchx_init);
+module_exit(switchx_exit);
diff --git a/linux/drivers/hwmon/mellanox/mlnx-common-drv.h b/linux/drivers/hwmon/mellanox/mlnx-common-drv.h
new file mode 100644
index 0000000..8fda57e
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/mlnx-common-drv.h
@@ -0,0 +1,30 @@
+/**
+ *
+ * Copyright (C) Mellanox Technologies Ltd. 2001-2015.  ALL RIGHTS RESERVED.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
+ *
+ */
+
+#define ENTRY_NAME_LEN   32
+
+struct mlnx_bsp_entry {
+        __u8          index;                 /* Entry index */
+        char          name[ENTRY_NAME_LEN];  /* Entry name  */
+        unsigned long last_updated;          /* in jiffies  */
+        u8            valid;                 /* Entry data is valid */
+};
+
+extern int (*mlnx_set_fan_hook)(u8 asic_id, u8 id, u8 speed);
diff --git a/linux/drivers/hwmon/mellanox/reset.c b/linux/drivers/hwmon/mellanox/reset.c
new file mode 100644
index 0000000..4c20211
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/reset.c
@@ -0,0 +1,175 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+#include <linux/delay.h>
+#include "sx.h"
+#include "dq.h"
+#include "alloc.h"
+
+/* taken as is from the IS4 driver, we might need to remove the part where we
+ * save the pci config headers before the reset, and restore them afterwards.
+ * Depends on the decision of the FW guys. */
+int sx_reset(struct sx_dev *dev)
+{
+	void __iomem *reset;
+	u32 *hca_header = NULL;
+	int pcie_cap;
+	u16 devctl;
+	u16 linkctl;
+	u16 vendor = 0xffff;
+	unsigned long end;
+	u32 sem;
+	int i;
+	int err = 0;
+
+	if (!dev->pdev) {
+		sx_err(dev, "SW reset will not be executed since PCI device is not present");
+	}
+
+#define SX_RESET_BASE	0xf0000
+#define SX_RESET_SIZE	0x404
+#define SX_SEM_OFFSET	0x400
+#define SX_SEM_BIT	(1 << 31)
+#define SX_RESET_OFFSET	0x10
+#define SX_RESET_VALUE	swab32(1)
+
+#ifndef INCREASED_TIMEOUT
+#define SX_SEM_TIMEOUT_JIFFIES		(10 * HZ)
+#else
+#define SX_SEM_TIMEOUT_JIFFIES		(100 * HZ)
+#endif
+#define SX_RESET_TIMEOUT_JIFFIES	(2 * HZ)
+
+	/*
+	 * Reset the chip.  This is somewhat ugly because we have to
+	 * save off the PCI header before reset and then restore it
+	 * after the chip reboots.  We skip config space offsets 22
+	 * and 23 since those have a special meaning.
+	 */
+
+	/* Do we need to save off the full 4K PCI Express header?? */
+	hca_header = kmalloc(256, GFP_KERNEL);
+	if (!hca_header) {
+		err = -ENOMEM;
+		sx_err(dev, "Couldn't allocate memory to save HCA "
+			  "PCI header, aborting.\n");
+		goto out;
+	}
+
+	pcie_cap = pci_find_capability(dev->pdev, PCI_CAP_ID_EXP);
+
+	for (i = 0; i < 64; ++i) {
+		if (i == 22 || i == 23)
+			continue;
+		if (pci_read_config_dword(dev->pdev, i * 4, hca_header + i)) {
+			err = -ENODEV;
+			sx_err(dev, "Couldn't save HCA PCI header, aborting.\n");
+			goto out;
+		}
+	}
+
+	reset = ioremap(pci_resource_start(dev->pdev, 0) + SX_RESET_BASE,
+			SX_RESET_SIZE);
+	if (!reset) {
+		err = -ENOMEM;
+		sx_err(dev, "Couldn't map HCA reset register, aborting.\n");
+		goto out;
+	}
+
+	/* grab HW semaphore to lock out flash updates */
+	end = jiffies + SX_SEM_TIMEOUT_JIFFIES;
+	do {
+		sem = be32_to_cpu(readl(reset + SX_SEM_OFFSET)) & SX_SEM_BIT;
+		if (!sem)
+			break;
+
+		msleep(1);
+	} while (time_before(jiffies, end));
+
+	if (sem) {
+		sx_err(dev, "Failed to obtain HW semaphore, aborting\n");
+		err = -EAGAIN;
+		iounmap(reset);
+		goto out;
+	}
+
+	/* actually hit reset */
+	writel(SX_RESET_VALUE, reset + SX_RESET_OFFSET);
+	iounmap(reset);
+
+	/* Wait three seconds before accessing device */
+#ifndef INCREASED_TIMEOUT
+	msleep(3000);
+#else
+	msleep(180000);
+#endif
+
+	end = jiffies + SX_RESET_TIMEOUT_JIFFIES;
+	do {
+		if (!pci_read_config_word(dev->pdev, PCI_VENDOR_ID, &vendor) &&
+		    vendor != 0xffff)
+			break;
+
+		msleep(1);
+	} while (time_before(jiffies, end));
+
+	if (vendor == 0xffff) {
+		err = -ENODEV;
+		sx_err(dev, "PCI device did not come back after reset, aborting.\n");
+		goto out;
+	}
+
+	/* Now restore the PCI headers */
+	if (pcie_cap) {
+		devctl = hca_header[(pcie_cap + PCI_EXP_DEVCTL) / 4];
+		if (pci_write_config_word(dev->pdev, pcie_cap + PCI_EXP_DEVCTL,
+					   devctl)) {
+			err = -ENODEV;
+			sx_err(dev, "Couldn't restore HCA PCI Express "
+				 "Device Control register, aborting.\n");
+			goto out;
+		}
+
+		linkctl = hca_header[(pcie_cap + PCI_EXP_LNKCTL) / 4];
+		if (pci_write_config_word(dev->pdev, pcie_cap + PCI_EXP_LNKCTL,
+					   linkctl)) {
+			err = -ENODEV;
+			sx_err(dev, "Couldn't restore HCA PCI Express "
+				 "Link control register, aborting.\n");
+			goto out;
+		}
+	}
+
+	for (i = 0; i < 16; ++i) {
+		if (i * 4 == PCI_COMMAND)
+			continue;
+
+		if (pci_write_config_dword(dev->pdev, i * 4, hca_header[i])) {
+			err = -ENODEV;
+			sx_err(dev, "Couldn't restore HCA reg %x, aborting.\n", i);
+			goto out;
+		}
+	}
+
+	if (pci_write_config_dword(dev->pdev, PCI_COMMAND,
+				   hca_header[PCI_COMMAND / 4])) {
+		err = -ENODEV;
+		sx_err(dev, "Couldn't restore HCA COMMAND, aborting.\n");
+		goto out;
+	}
+
+out:
+	kfree(hca_header);
+
+	return err;
+}
diff --git a/linux/drivers/hwmon/mellanox/sx.h b/linux/drivers/hwmon/mellanox/sx.h
new file mode 100644
index 0000000..dd4bea0
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/sx.h
@@ -0,0 +1,641 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2016 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+#ifndef SX_H
+#define SX_H
+
+
+/************************************************
+ * Includes
+ ***********************************************/
+#include <asm/atomic.h>
+#include <linux/kernel.h>
+#include <linux/semaphore.h>
+#include <linux/skbuff.h>
+#include <linux/mlx_sx/device.h>
+#include <linux/mlx_sx/driver.h>
+#include <linux/mlx_sx/sx_i2c_if.h>
+#include <linux/timer.h>
+#include "sx_sgmii.h"
+#include "eq.h"
+#include "icm.h"
+#include "sx_dpt.h"
+#include <linux/interrupt.h>
+
+/************************************************
+ *  Defines
+ ***********************************************/
+#ifdef PD_BU
+#define INCREASED_TIMEOUT
+#endif
+
+#define DRV_NAME    "sx_core"
+#define PFX         DRV_NAME ": "
+#define DRV_RELDATE "May, 2011"
+
+#define SX_MAX_DEVICES 2
+
+/* SwitchX PCI device ID */
+#define SWITCHX_PCI_DEV_ID 0xc738
+/* SwitchX in flash recovery mode */
+#define SWITCHX_FLASH_MODE_PCI_DEV_ID 0x0246
+
+/* SwitchIB PCI device ID */
+#define SWITCH_IB_PCI_DEV_ID 0xcb20
+/* SwitchIB in flash recovery mode */
+#define SWITCH_IB_FLASH_MODE_PCI_DEV_ID 0x0247
+
+/* SwitchIB PCI device ID */
+#define SPECTRUM_PCI_DEV_ID        0xcb84
+/* SwitchIB in flash recovery mode */
+#define SPECTRUM_FLASH_MODE_PCI_DEV_ID 0x0249
+
+/* SwitchIB2 PCI device ID */
+#define SWITCH_IB2_PCI_DEV_ID 		0xcf08
+/* SwitchIB in flash recovery mode */
+#define SWITCH_IB2_FLASH_MODE_PCI_DEV_ID 0x024B
+
+#define TO_FIELD(mask, shift, value) \
+    (value & mask) << shift;
+
+#ifdef CONFIG_SX_DEBUG
+extern int sx_debug_level;
+
+#define sx_dbg(mdev, format, arg ...)                       \
+    do {                                                    \
+        if (sx_debug_level) {                               \
+            dev_printk(KERN_DEBUG,                          \
+                       &mdev->pdev->dev, format, ## arg); } \
+    } while (0)
+
+#else /* CONFIG_SX_DEBUG */
+
+#define sx_dbg(mdev, format, arg ...) do { (void)mdev; } while (0)
+
+#endif /* CONFIG_SX_DEBUG */
+
+#define sx_err(mdev, format, arg ...)              \
+    if (mdev->pdev) {                              \
+        dev_err(&mdev->pdev->dev, format, ## arg); \
+    } else {                                       \
+        printk(KERN_ERR PFX format, ## arg);       \
+    }
+#define sx_warn(mdev, format, arg ...)              \
+    if (mdev->pdev) {                               \
+        dev_warn(&mdev->pdev->dev, format, ## arg); \
+    } else {                                        \
+        printk(KERN_WARNING PFX format, ## arg);    \
+    }
+#define sx_info(mdev, format, arg ...)              \
+    if (mdev->pdev) {                               \
+        dev_info(&mdev->pdev->dev, format, ## arg); \
+    } else {                                        \
+        printk(KERN_INFO PFX format, ## arg);       \
+    }
+
+#define SX_WRITE_LIMIT       1000
+#define SX_TRUNCATE_SIZE_MIN 4
+#define SXD_LAG_ID_MAX       0x3FF
+#define LAG_ID_INVALID       (SXD_LAG_ID_MAX + 1)
+
+#ifdef SX_DEBUG
+
+#define SX_CORE_HEXDUMP16(ptr, len)                   \
+    do {                                              \
+        unsigned char *data = (ptr);                  \
+        int            i;                             \
+        for (i = 0; i < len; i++) {                   \
+            if ((i % 16) == 0) {                      \
+                printk(KERN_DEBUG "\n0x%04x: ", i); } \
+            if ((i % 8) == 0) {                       \
+                printk(KERN_DEBUG " "); }             \
+            printk(KERN_DEBUG "0x%02x ", data[i]);    \
+        }                                             \
+        printk(KERN_DEBUG "\n");                      \
+    } while (0)
+
+#endif
+
+#define SX_CORE_UNUSED_PARAM(P)
+#define MAX_SYSTEM_PORTS_IN_FILTER 256
+#define MAX_LAG_PORTS_IN_FILTER    256
+#define MAX_SGMII_FLOWS            1 /* Will increase in the future */
+
+/************************************************
+ *  Enums
+ ***********************************************/
+
+enum {
+    SX_EVENT_LIST_SIZE = 1000,
+    ISX_HDR_SIZE = 16,
+    SX_MAX_MSG_SIZE = 10240
+};
+
+enum {
+    SX_FLAG_MSI_X = 1
+};
+
+enum {
+    SX_SEND_DQ_DB_BASE = 0,
+    SX_RECV_DQ_DB_BASE = 0x200,
+    SX_MAX_LOG_DQ_SIZE = 7,
+    SX_DBELL_CQ_CI_OFFSET = 0x400,
+    SX_DBELL_CQ_ARM_OFFSET = 0x800
+};
+enum TX_BASE_HEADER_DEFS {
+    TX_HDR_VER_MASK_V1 = 0xF,
+    TX_HDR_VER_SHIFT_V1 = 4,
+    TX_HDR_CTL_MASK = 0x3,
+    TX_HDR_CTL_SHIFT = 2,
+    TX_HDR_MC_MASK = 0x1,
+    TX_HDR_MC_SHIFT = 0,
+    TX_HDR_PROTOCOL_MASK = 0x7,
+    TX_HDR_PROTOCOL_SHIFT = 5,
+    TX_HDR_ETCLASS_MASK = 0x7,
+    TX_HDR_ETCLASS_SHIFT = 2,
+    TX_HDR_SWID_MASK = 0x7,
+    TX_HDR_SWID_SHIFT = 12,
+    TX_HDR_SYSTEM_PORT_MID_MASK = 0xFFFF,
+    TX_HDR_SYSTEM_PORT_MID_SHIFT = 0,
+    TX_HDR_CTCLASS3_MASK = 0x1,
+    TX_HDR_CTCLASS3_SHIFT = 14,
+    TX_HDR_RDQ_MASK = 0x1F,
+    TX_HDR_RDQ_SHIFT = 9,
+    TX_HDR_CPU_SIGNATURE_MASK = 0x1FF,
+    TX_HDR_CPU_SIGNATURE_SHIFT = 0,
+    TX_HDR_SIGNATURE_MASK = 0xFFFF,
+    TX_HDR_SIGNATURE_SHIFT = 0,
+    TX_HDR_STCLASS_MASK = 0x7,
+    TX_HDR_STCLASS_SHIFT = 13,
+    TX_HDR_EMAD_MASK = 0x1,
+    TX_HDR_EMAD_SHIFT = 5,
+    TX_HDR_TYPE_MASK = 0xF,
+    TX_HDR_TYPE_SHIFT = 0,
+    TX_HDR_RX_IS_ROUTER_MASK_V1 = 0x1,
+    TX_HDR_RX_IS_ROUTER_SHIFT_V1 = 3,
+    TX_HDR_FID_VALID_MASK_V1 = 0x1,
+    TX_HDR_FID_VALID_SHIFT_V1 = 0,
+    TX_HDR_CONTROL_MASK_V1 = 0x1,
+    TX_HDR_CONTROL_SHIFT_V1 = 6,
+    TX_HDR_ETCLASS_MASK_V1 = 0xF,
+    TX_HDR_ETCLASS_SHIFT_V1 = 0,
+    TX_HDR_FID_MASK_V1 = 0xFFFF,
+    TX_HDR_FID_SHIFT_V1 = 0
+};
+
+/************************************************
+ *  Structs
+ ***********************************************/
+struct event_data {
+    struct list_head list;
+    struct sk_buff  *skb;
+    u16              system_port;
+    u16              trap_id;
+    u8               dev_id;
+    u8               is_lag;
+    u8               lag_sub_port;
+    u8               swid;
+    struct sx_dev   *dev;
+    u32              original_packet_size;
+};
+struct sx_rsc { /* sx  resource */
+    struct event_data evlist;           /* event list           */
+    int               evlist_size;      /* the current size     */
+    spinlock_t        lock;         /* event list lock	*/
+    wait_queue_head_t poll_wait;
+    atomic_t          multi_packet_read_enable;
+    atomic_t          read_blocking_state;
+    struct semaphore  write_sem;
+};
+struct tx_base_header_v0 {
+    u8  ctl_mc;
+    u8  protocol_etclass;
+    u16 swid;
+    u16 system_port_mid;
+    u16 ctclass3_rdq_cpu_signature;
+    u32 reserved;
+    u16 signature;
+    u16 stclass_emad_type;
+};
+struct tx_base_header_v1 {
+    u8  version_ctl;
+    u8  protocol_rx_is_router_fid_valid;
+    u16 swid_control_etclass;
+    u16 system_port_mid;
+    u16 reserved1;
+    u16 fid;
+    u16 reserved2;
+    u8  reserved3[3];
+    u8  type;
+};
+struct sx_bitmap {
+    u32           max;
+    spinlock_t    lock;   /* bitmap lock */
+    unsigned long table[2];
+};
+struct sx_buf_list {
+    void      *buf;
+    dma_addr_t map;
+};
+struct sx_buf {
+    struct {
+        struct sx_buf_list  direct;
+        struct sx_buf_list *page_list;
+    } u;
+    int nbufs;
+    int npages;
+    int page_shift;
+};
+
+/************************************************
+ * EQ - Structs
+ ***********************************************/
+struct sx_eq {
+    struct sx_dev      *dev;
+    void __iomem       *ci_db;
+    void __iomem       *arm_db;
+    int                 eqn;
+    u32                 cons_index;
+    u16                 irq;
+    u16                 have_irq;
+    int                 nent;
+    struct sx_buf_list *page_list;
+};
+struct sx_eq_table {
+    struct sx_bitmap bitmap;
+    void __iomem    *clr_int;
+    u32              clr_mask;
+    struct sx_eq     eq[SX_NUM_EQ];
+    int              have_irq;
+    u8               inta_pin;
+};
+
+/************************************************
+ * CQ - Structs
+ ***********************************************/
+struct sx_cq_bkp_poll {
+    atomic_t curr_num_cq_polls;
+    int      last_interval_num_cq_polls;
+    int      last_interval_cons_index;
+    atomic_t cq_bkp_poll_mode;
+};
+struct sx_cq {
+    u32                   cons_index;
+    __be32               *set_ci_db;
+    __be32               *arm_db;
+    int                   cqn;
+    atomic_t              refcount;
+    struct completion     free;
+    spinlock_t            lock; /* sx_cq lock */
+    spinlock_t            rearm_lock; /* cq rearm lock */
+    struct sx_buf         buf;
+    int                   nent;
+    struct sx_dev        *sx_dev;
+    struct sx_cq_bkp_poll bkp_poll_data;
+};
+struct cq_rate_limiter_params {
+    u8  use_limiter;
+    int interval_credit;
+    int curr_cq_credit;
+    int max_cq_credit;
+    int num_cq_stops;
+};
+struct sx_cq_table {
+    struct sx_bitmap               bitmap;
+    spinlock_t                     lock;  /* cq_table lock */
+    struct sx_cq                 **cq;
+    struct task_struct            *cq_credit_thread;
+    unsigned int                   rl_time_interval;  /* in milliseconds */
+    struct cq_rate_limiter_params *cq_rl_params;
+    struct completion              done;  /* dummy, should never complete */
+    int                            credit_thread_active;
+};
+
+/************************************************
+ * DQ - Structs
+ ***********************************************/
+struct sx_sge_data {
+    int        len;
+    void      *vaddr;
+    dma_addr_t dma_addr;
+};
+struct sx_sge_internal {
+    struct sx_sge_data hdr_pld_sg;
+    struct sx_sge_data pld_sg_1;
+    struct sx_sge_data pld_sg_2;
+    struct sk_buff    *skb;
+};
+struct sx_wqe {
+    __be16 flags;
+    __be16 byte_count[3];
+    __be64 dma_addr[3];
+};
+struct sx_pkt {
+    struct sk_buff  *skb;
+    struct list_head list;
+    u8               set_lp;
+    enum ku_pkt_type type;
+};
+enum dq_state {
+    DQ_STATE_FREE,
+    DQ_STATE_RESET,
+    DQ_STATE_RTS,
+    DQ_STATE_ERROR,
+};
+struct sx_dq {
+    void                    (*event)(struct sx_dq *, enum sx_event);
+    struct sx_dev          *dev;
+    int                     dqn;
+    int                     is_send;
+    struct sx_buf           buf;
+    spinlock_t              lock; /* sx_dq lock */
+    u16                     head;
+    u16                     tail; /* same type as wqe_counter in cqe */
+    int                     wqe_cnt;
+    int                     wqe_shift;
+    struct sx_cq           *cq;  /* the matching cq */
+    struct sx_sge_internal *sge;
+    wait_queue_head_t       tx_full_wait; /* Not sure we need it */
+    __be32                 *db;
+    int                     is_flushing;
+    struct sx_pkt           pkts_list;
+    enum dq_state           state;
+    atomic_t                refcount;
+    struct completion       free;
+};
+struct sx_dq_table {
+    struct sx_bitmap bitmap;
+    spinlock_t       lock;    /* dq_table lock */
+    struct sx_dq   **dq;
+};
+struct sx_cmd {
+    struct pci_pool       *pool;
+    void __iomem          *hcr;
+    struct mutex           hcr_mutex;  /* the HCR's mutex */
+    struct semaphore       pci_poll_sem;
+    struct semaphore       i2c_poll_sem;
+    struct semaphore       sgmii_poll_sem;
+    struct semaphore       event_sem;
+    int                    max_cmds;
+    spinlock_t             context_lock;  /* the context lock */
+    int                    free_head;
+    struct sx_cmd_context *context;
+    u16                    token_mask;
+    u8                     use_events;
+    u8                     toggle;
+};
+struct sx_catas_err {
+    u32 __iomem      *map;
+    struct timer_list timer;
+    struct list_head  list;
+};
+struct sx_fw {
+    u64            clr_int_base;
+    u64            catas_offset;
+    u32            catas_size;
+    u8             clr_int_bar;
+    u8             catas_bar;
+    u8             debug_trace;
+    u64            fw_ver;
+    u64            doorbell_page_offset;
+    u8             doorbell_page_bar;
+    u16            core_clock;
+    struct sx_icm *fw_icm;
+    u16            fw_pages;
+    u8             fw_hour;
+    u8             fw_minutes;
+    u8             fw_seconds;
+    u16            fw_year;
+    u8             fw_month;
+    u8             fw_day;
+    u32            local_out_mb_offset;
+    u32            local_out_mb_size;
+    u32            local_in_mb_offset;
+    u32            local_in_mb_size;
+};
+union sx_cmd_ifc_registers {
+    struct ku_access_pspa_reg  pspa_reg_data;
+    struct ku_access_qsptc_reg qsptc_reg_data;
+    struct ku_access_qstct_reg qstct_reg_data;
+    struct ku_access_ptys_reg  ptys_reg_data;
+    struct ku_access_pmlp_reg  pmlp_reg_data;
+    struct ku_access_plib_reg  plib_reg_data;
+    struct ku_access_spzr_reg  spzr_reg_data;
+    struct ku_access_paos_reg  paos_reg_data;
+    struct ku_access_pmpc_reg  pmpc_reg_data;
+    struct ku_access_pmpr_reg  pmpr_reg_data;
+    struct ku_access_pmtu_reg  pmtu_reg_data;
+    struct ku_access_pelc_reg  pelc_reg_data;
+    struct ku_access_htgt_reg  htgt_reg_data;
+    struct ku_access_mfsc_reg  mfsc_reg_data;
+    struct ku_access_mfsm_reg  mfsm_reg_data;
+    struct ku_access_mfsl_reg  mfsl_reg_data;
+    struct ku_access_pvlc_reg  pvlc_reg_data;
+    struct ku_access_mcia_reg  mcia_reg_data;
+    struct ku_access_hpkt_reg  hpkt_reg_data;
+    struct ku_access_hcap_reg  hcap_reg_data;
+    struct ku_access_hdrt_reg  hdrt_reg_data;
+    struct ku_access_qprt_reg  qprt_reg_data;
+    struct ku_access_mfcr_reg  mfcr_reg_data;
+    struct ku_access_fore_reg  fore_reg_data;
+    struct ku_access_mtcap_reg mtcap_reg_data;
+    struct ku_access_mtmp_reg  mtmp_reg_data;
+    struct ku_access_mtwe_reg  mtwe_reg_data;
+    struct ku_access_mmdio_reg mmdio_reg_data;
+    struct ku_access_mmia_reg  mmia_reg_data;
+    struct ku_access_mfpa_reg  mfpa_reg_data;
+    struct ku_access_mfbe_reg  mfbe_reg_data;
+    struct ku_access_mfba_reg  mfba_reg_data;
+    struct ku_access_mjtag_reg mjtag_reg_data;
+    struct ku_access_qcap_reg  qcap_reg_data;
+    struct ku_access_pmaos_reg pmaos_reg_data;
+    struct ku_access_mfm_reg   mfm_reg_data;
+    struct ku_access_spad_reg  spad_reg_data;
+    struct ku_access_sspr_reg  sspr_reg_data;
+    struct ku_access_ppad_reg  ppad_reg_data;
+    struct ku_access_spmcr_reg spmcr_reg_data;
+    struct ku_access_pbmc_reg  pbmc_reg_data;
+    struct ku_access_pptb_reg  pptb_reg_data;
+    struct ku_access_smid_reg  smid_reg_data;
+    struct ku_access_spvid_reg spvid_reg_data;
+    struct ku_access_sfgc_reg  sfgc_reg_data;
+    struct ku_access_oepft_reg oepft_reg_data;
+    struct ku_access_mgir_reg  mgir_reg_data;
+    struct ku_access_plbf_reg  plbf_reg_data;
+    struct ku_access_mhsr_reg  mhsr_reg_data;
+    struct ku_access_mpsc_reg  mpsc_reg_data;
+};
+/************************************************
+ * Private data struct
+ ***********************************************/
+union swid_data {
+    struct {
+        int synd;
+        u64 mac;
+    } eth_swid_data;
+    struct {
+    } ib_swid_data;
+};
+/* Note - all these callbacks are called when the db_lock spinlock is locked! */
+struct dev_specific_cb {
+    int (*get_hw_etclass_cb)(struct isx_meta *meta, u8* hw_etclass);
+    int (*sx_build_isx_header_cb)(struct isx_meta *meta, struct sk_buff *skb, u8 stclass,  u8 hw_etclass);
+    int (*sx_get_sdq_cb)(struct sx_dev *dev, enum ku_pkt_type type,
+                         u8 swid, u8 etclass, u8 *stclass, u8 *sdq);
+    int (*get_send_to_rp_as_data_supported_cb)(u8 *send_to_rp_as_data_supported);
+    int (*get_rp_vid_cb)(struct sx_dev *dev, struct completion_info *comp_info, u16 *vid);
+    int (*get_swid_cb)(struct sx_dev *dev, struct completion_info *comp_info, u8 *swid);
+    int (*get_lag_mid_cb)(u16 lag_id, u16 *mid);
+#ifdef CONFIG_SX_SGMII_PRESENT
+    int (*sx_sgmii_build_cr_space_header_cb)(struct sk_buff *skb, u8 token, u8 rw, u32 address, u8 size);
+#endif
+};
+struct sx_priv {
+    struct sx_dev              dev;
+    struct list_head           dev_list;
+    struct list_head           ctx_list;
+    spinlock_t                 ctx_lock;  /* the ctx_list's lock */
+    struct sx_cmd              cmd;
+    struct sx_eq_table         eq_table;
+    struct sx_cq_table         cq_table;
+    struct sx_dq_table         sdq_table;
+    struct sx_dq_table         rdq_table;
+    struct sx_bitmap           swid_bitmap;
+    struct sx_catas_err        catas_err;
+    struct sx_fw               fw;
+    int                        is_fw_initialized;
+    int                        unregistered;
+    void __iomem              *clr_base;
+    union swid_data            swid_data[NUMBER_OF_SWIDS];
+    struct ku_l2_tunnel_params l2_tunnel_params;
+    /* IB only */
+    u8 ib_to_local_db[MAX_IBPORT_NUM + 1];
+    /* ETH only */
+    u8  system_to_local_db[MAX_SYSPORT_NUM];
+    u16 local_to_system_db[MAX_PHYPORT_NUM];
+    u8  lag_member_to_local_db[MAX_LAG_NUM][MAX_LAG_MEMBERS_NUM];
+    u8  local_is_rp[MAX_PHYPORT_NUM + 1];
+    u16 local_rp_vid[MAX_PHYPORT_NUM + 1];
+    u8  lag_is_rp[MAX_LAG_NUM];
+    u16 lag_rp_vid[MAX_LAG_NUM];
+    /* common */
+    u8         local_to_swid_db[MAX_PHYPORT_NUM + 1];
+    spinlock_t db_lock;                  /* Lock for all DBs */
+    u16        pvid_sysport_db[MAX_SYSPORT_NUM];
+    u16        pvid_lag_db[MAX_LAG_NUM];
+    u16        truncate_size_db[NUMBER_OF_RDQS];
+    u16        sysport_filter_db[NUM_HW_SYNDROMES][MAX_SYSTEM_PORTS_IN_FILTER];
+    u16        lag_filter_db[NUM_HW_SYNDROMES][MAX_LAG_PORTS_IN_FILTER];
+    /* RP helper dbs */
+    u8                     port_prio2tc[MAX_PHYPORT_NUM + 1][MAX_PRIO_NUM + 1];
+    u8                     lag_prio2tc[MAX_LAG_NUM + 1][MAX_PRIO_NUM + 1];
+    u8                     port_vtag_mode[MAX_PHYPORT_NUM + 1][MAX_VLAN_NUM];
+    u8                     lag_vtag_mode[MAX_LAG_NUM + 1][MAX_VLAN_NUM];
+    u8                     port_rp_rif_valid[MAX_PHYPORT_NUM + 1][MAX_VLAN_NUM];
+    u8                     lag_rp_rif_valid[MAX_LAG_NUM][MAX_VLAN_NUM];
+    u16                    port_rp_rif[MAX_PHYPORT_NUM + 1][MAX_VLAN_NUM];
+    u16                    lag_rp_rif[MAX_LAG_NUM][MAX_VLAN_NUM];
+    u8                     port_prio_tagging_mode[MAX_PHYPORT_NUM + 1];
+    u8                     lag_prio_tagging_mode[MAX_LAG_NUM + 1];
+    u16                    port_vid_to_fid[MAX_PHYPORT_NUM + 1][MAX_VLAN_NUM];
+    atomic_t               cq_backup_polling_refcnt;
+    struct dev_specific_cb dev_specific_cb;
+    /* ECMP redirect IP override */
+    u32                   icmp_vlan2ip_db[MAX_VLAN_NUM];
+    struct tasklet_struct intr_tasklet;
+};
+
+
+struct sx_globals {
+    spinlock_t            pci_devs_lock;   /* the devs list lock */
+    struct list_head      pci_devs_list;
+    int                   pci_devs_cnt;
+    struct sx_dev        *tmp_dev_ptr;
+    struct sx_dpt_s       sx_dpt;
+    struct sx_sgmii_ifc   sx_sgmii;
+    struct sx_i2c_ifc     sx_i2c;
+    struct sx_stats       stats;
+    struct ku_profile     profile;
+    int                   index[SX_MAX_DEVICES];
+    struct sx_priv       *priv;
+    struct listener_entry listeners_db[NUM_HW_SYNDROMES + 1];
+    spinlock_t            listeners_lock;   /* listeners' lock */
+    struct cdev           cdev;
+};
+struct isx_specific_data {
+    u8  version;
+    u8  ctl;
+    u8  mc;
+    u8  protocol;
+    u8  ctclass;
+    u16 cpu_signature;
+    u16 signature;
+    u8  emad;
+    u8  type;
+    u8  use_control_tclass;
+    u8  rx_is_router;
+    u8  fid_valid;
+    u16 fid;
+    u8  etclass;
+};
+
+/************************************************
+ * Inline Functions
+ ***********************************************/
+static inline struct sx_priv * sx_priv(struct sx_dev *p_dev)
+{
+    return container_of(p_dev, struct sx_priv, dev);
+}
+
+void inc_unconsumed_packets_global_counter(u16 hw_synd, enum sx_packet_type pkt_type);
+void inc_filtered_lag_packets_global_counter(void);
+void inc_filtered_port_packets_global_counter(void);
+void inc_unconsumed_packets_counter(struct sx_dev *dev, u16 hw_synd, enum sx_packet_type pkt_type);
+void inc_filtered_lag_packets_counter(struct sx_dev *dev);
+void inc_filtered_port_packets_counter(struct sx_dev *dev);
+int sx_reset(struct sx_dev *dev);
+int sx_core_register_device(struct sx_dev *dev);
+void sx_core_unregister_device(struct sx_dev *dev);
+int sx_cmd_init(struct sx_dev *dev);
+int sx_cmd_init_pci(struct sx_dev *dev);
+int sx_cmd_pool_create(struct sx_dev *dev);
+void sx_cmd_pool_destroy(struct sx_dev *dev);
+void sx_cmd_unmap(struct sx_dev *dev);
+#if 0
+void sx_core_start_catas_poll(struct sx_dev *dev);
+void sx_core_stop_catas_poll(struct sx_dev *dev);
+int sx_core_catas_init(struct sx_dev *dev);
+#endif
+void sx_core_catas_cleanup(struct sx_dev *dev);
+int sx_restart_one_pci(struct pci_dev *pdev);
+int sx_enable_swid(struct sx_dev *dev, int sx_dev_id, u8 swid, int synd, u64 mac);
+void sx_disable_swid(struct sx_dev *dev, u8 swid);
+int sx_change_configuration(struct sx_dev *dev);
+int sx_handle_set_profile(struct sx_dev *dev);
+int sx_build_isx_header(struct isx_meta *meta, struct sk_buff *skb, u8 stclass);
+int sx_build_isx_header_v0(struct isx_meta *meta, struct sk_buff *skb, u8 stclass,  u8 hw_etclass);
+int sx_build_isx_header_v1(struct isx_meta *meta, struct sk_buff *skb, u8 stclass,  u8 hw_etclass);
+int sx_get_sdq(struct isx_meta *meta, struct sx_dev *dev, enum ku_pkt_type type, u8 swid,
+               u8 etclass, u8 *stclass, u8 *sdq);
+int sx_get_sdq_from_profile(struct sx_dev *dev, enum ku_pkt_type type,
+                            u8 swid, u8 etclass, u8 *stclass, u8 *sdq);
+int sx_get_sdq_per_traffic_type(struct sx_dev *dev, enum ku_pkt_type type, u8 swid, u8 etclass,
+                                u8 *stclass, u8 *sdq);
+void sx_core_dispatch_event(struct sx_dev *dev, enum sx_dev_event type,
+                            union sx_event_data *event_data);
+
+#endif  /* SX_H */
+
+/************************************************
+ *                  EOF                         *
+ ***********************************************/
diff --git a/linux/drivers/hwmon/mellanox/sx_core_main.c b/linux/drivers/hwmon/mellanox/sx_core_main.c
new file mode 100644
index 0000000..ce702f0
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/sx_core_main.c
@@ -0,0 +1,7096 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2016 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/fs.h>
+#include <linux/cdev.h>
+#include <linux/poll.h>
+#include <linux/sched.h>
+#include <linux/uaccess.h>
+#include <linux/types.h>
+#include <linux/uio.h>
+#include <linux/skbuff.h>
+#include <linux/errno.h>
+#include <linux/mm.h>
+#include <linux/delay.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/pci.h>
+#include <linux/kthread.h>
+#include <linux/vmalloc.h>
+#include <linux/mlx_sx/kernel_user.h>
+#include <linux/mlx_sx/device.h>
+#include <linux/mlx_sx/cmd.h>
+#include "sx.h"
+#include "cq.h"
+#include "dq.h"
+#include "eq.h"
+#include "alloc.h"
+#include "icm.h"
+#include "fw.h"
+#include "sx_dpt.h"
+#include "sx_proc.h"
+
+#ifdef CONFIG_44x
+#include <asm/dcr.h>
+#include <asm/dcr-regs.h>
+#include <asm/reg.h>
+#endif
+
+/************************************************
+ *  Global
+ ***********************************************/
+
+/************************************************
+ *  Define
+ ***********************************************/
+
+#ifndef BUILD_VERSION
+#define BUILD_VERSION ""
+#endif
+
+#define SX_CORE_CHAR_DEVICE_NAME "sxcdev"
+#define SX_CORE_DRV_VERSION      "1.00 " BUILD_VERSION
+static const char sx_version[] =
+    DRV_NAME ": Mellanox SwitchX Core Driver "
+    SX_CORE_DRV_VERSION " (" DRV_RELDATE ")\n";
+
+#define RDQ_NUMBER_OF_ENTRIES 128
+/************************************************
+ *  Enum
+ ***********************************************/
+
+enum SX_CHAR_DEVICE {
+    SX_MAJOR = 231,
+    SX_BASE_MINOR = 193,
+};
+dev_t             char_dev;
+struct sx_globals sx_glb;
+//static int first_ib_swid = 1;
+
+/************************************************
+ *  MODULE settings
+ ***********************************************/
+MODULE_AUTHOR("Amos Hersch, Anatoly Lisenko");
+MODULE_DESCRIPTION("Mellanox SwitchX driver");
+MODULE_LICENSE("Dual BSD/GPL");
+MODULE_VERSION(SX_CORE_DRV_VERSION);
+
+int cq_thread_sched_priority = 0;
+module_param_named(cq_thread_sched_priority, cq_thread_sched_priority, int, 0644);
+MODULE_PARM_DESC(cq_thread_sched_priority, "CQ credit thread real time priority");
+
+int rx_debug_sgmii;
+module_param_named(rx_debug_sgmii,
+                   rx_debug_sgmii, int, 0644);
+MODULE_PARM_DESC(rx_debug_sgmii, "en/dis dump of SGMII pkts");
+
+int rx_debug;
+module_param_named(rx_debug,
+                   rx_debug, int, 0644);
+MODULE_PARM_DESC(rx_debug, "en/dis dump of pkts");
+
+int rx_debug_pkt_type = SX_DBG_PACKET_TYPE_ANY;
+module_param_named(rx_debug_pkt_type,
+                   rx_debug_pkt_type, int, 0644);
+MODULE_PARM_DESC(rx_debug_pkt_type, "trap/synd number to dump, 0xff dump all pkts");
+
+int rx_debug_emad_type = SX_DBG_EMAD_TYPE_ANY;
+module_param_named(rx_debug_emad_type,
+                   rx_debug_emad_type, int, 0644);
+MODULE_PARM_DESC(rx_debug_emad_type, "emad number to dump, 0xffff dump all pkts");
+
+int tx_debug_sgmii;
+module_param_named(tx_debug_sgmii,
+                   tx_debug_sgmii, int, 0644);
+MODULE_PARM_DESC(tx_debug_sgmii, "en/dis dump of SGMII pkts");
+
+char *sgmii_dev_name = "mlx4_1";
+module_param(sgmii_dev_name, charp, 0000);
+MODULE_PARM_DESC(sgmii_dev_name, "sgmii device name (string)");
+
+int sgmii_port_number = -1;
+module_param_named(sgmii_port_number,
+		sgmii_port_number, int, 0644);
+MODULE_PARM_DESC(sgmii_port_number, "out port of sgmii interface");
+
+int tx_debug;
+module_param_named(tx_debug,
+                   tx_debug, int, 0644);
+MODULE_PARM_DESC(tx_debug, "en/dis dump of pkts");
+
+int tx_debug_pkt_type = SX_DBG_PACKET_TYPE_ANY;
+module_param_named(tx_debug_pkt_type,
+                   tx_debug_pkt_type, int, 0644);
+MODULE_PARM_DESC(tx_debug_pkt_type, "trap/synd number to dump, 0xff dump all pkts");
+
+int tx_debug_emad_type = SX_DBG_EMAD_TYPE_ANY;
+module_param_named(tx_debug_emad_type,
+                   tx_debug_emad_type, int, 0644);
+MODULE_PARM_DESC(tx_debug_emad_type, "emad type to dump, 0xff dump all emads");
+
+int rx_dump_sgmii;
+module_param_named(rx_dump_sgmii,
+                   rx_dump_sgmii, int, 0644);
+MODULE_PARM_DESC(rx_dump_sgmii, " 0- don't dump, 1-dump the SGMII RX packet data");
+
+int rx_dump;
+module_param_named(rx_dump,
+                   rx_dump, int, 0644);
+MODULE_PARM_DESC(rx_dump, " 0- don't dump, 1-dump the RX packet data");
+
+int rx_dump_cnt = SX_DBG_COUNT_UNLIMITED;
+module_param_named(rx_dump_cnt,
+                   rx_dump_cnt, int, 0644);
+MODULE_PARM_DESC(rx_dump_cnt, " 0xFFFF - unlimited, dump CNT packets  only");
+
+int tx_dump_sgmii;
+module_param_named(tx_dump_sgmii,
+                   tx_dump_sgmii, int, 0644);
+MODULE_PARM_DESC(tx_dump_sgmii, " 0- don't dump, 1-dump the SGMII TX packet data");
+
+int tx_dump;
+module_param_named(tx_dump,
+                   tx_dump, int, 0644);
+MODULE_PARM_DESC(tx_dump, " 0- don't dump, 1-dump the TX packet data");
+
+int tx_dump_cnt = SX_DBG_COUNT_UNLIMITED;
+module_param_named(tx_dump_cnt,
+                   tx_dump_cnt, int, 0644);
+MODULE_PARM_DESC(tx_dump_cnt, " 0xFFFF - unlimited, dump CNT packets  only");
+
+int i2c_cmd_dump;
+module_param_named(i2c_cmd_dump,
+                   i2c_cmd_dump, int, 0644);
+MODULE_PARM_DESC(i2c_cmd_dump, " 0- don't dump, 1-dump i2c data");
+
+int i2c_cmd_op = SX_DBG_CMD_OP_TYPE_ANY;
+module_param_named(i2c_cmd_op,
+                   i2c_cmd_op, int, 0644);
+MODULE_PARM_DESC(i2c_cmd_op, " cmd op to dump, 0xFFFF - dump all cmds");
+
+int i2c_cmd_reg_id = SX_DBG_REG_TYPE_ANY;
+module_param_named(i2c_cmd_reg_id,
+                   i2c_cmd_reg_id, int, 0644);
+MODULE_PARM_DESC(i2c_cmd_reg_id, " cmd reg_id to dump, 0xFFFF - dump all cmds");
+
+int i2c_cmd_dump_cnt = SX_DBG_COUNT_UNLIMITED;
+module_param_named(i2c_cmd_dump_cnt,
+                   i2c_cmd_dump_cnt, int, 0644);
+MODULE_PARM_DESC(i2c_cmd_dump_cnt, " print CNT commands and stop, 0xFFFF - don't stop");
+
+int dis_vid2ip = 0;
+module_param_named(dis_vid2ip,
+                   dis_vid2ip, int, 0644);
+MODULE_PARM_DESC(dis_vid2ip, " disable ip override");
+
+int g_chip_type = 6;
+module_param_named(g_chip_type,
+                   g_chip_type, int, 0644);
+MODULE_PARM_DESC(g_chip_type, " set chip type for NO PCI and SGMII");
+
+int eventlist_drops_counter = 0;
+module_param_named(eventlist_drops_counter, eventlist_drops_counter, int, 0644);
+MODULE_PARM_DESC(eventlist_drops_counter, "Event list drops counter");
+
+int unconsumed_packets_counter = 0;
+module_param_named(unconsumed_packets_counter,
+		unconsumed_packets_counter, int, 0644);
+MODULE_PARM_DESC(unconsumed_packets_counter, "Unconsumed packets counter");
+
+int filtered_lag_packets_counter = 0;
+module_param_named(filtered_lag_packets_counter,
+		filtered_lag_packets_counter, int, 0644);
+MODULE_PARM_DESC(filtered_lag_packets_counter, "Filtered LAG packets counter");
+
+int filtered_port_packets_counter = 0;
+module_param_named(filtered_port_packets_counter,
+		filtered_port_packets_counter, int, 0644);
+MODULE_PARM_DESC(filtered_port_packets_counter, "Filtered port packets counter");
+
+int loopback_packets_counter = 0;
+module_param_named(loopback_packets_counter,
+		loopback_packets_counter, int, 0644);
+MODULE_PARM_DESC(loopback_packets_counter, "Loopback packets counter");
+
+#ifdef CONFIG_PCI_MSI
+
+static int msi_x = 1;
+module_param(msi_x, int, 0444);
+MODULE_PARM_DESC(msi_x, "attempt to use MSI-X if nonzero");
+
+#else /* CONFIG_PCI_MSI */
+
+static int msi_x = 0;
+
+#endif /* CONFIG_PCI_MSI */
+
+static int sx_core_dev_init_switchx_cb(struct sx_dev *dev, enum sxd_chip_types chip_type);
+static int sx_core_init_one(struct sx_priv **sx_priv);
+static void sx_core_remove_one(struct sx_priv *priv);
+static void inc_eventlist_drops_global_counter(u16 hw_synd);
+int sx_init_char_dev(struct cdev *cdev_p);
+void sx_deinit_char_dev(struct cdev *cdev_p);
+
+
+/************************************************
+ *  Functions
+ ***********************************************/
+
+u16 translate_user_port_to_sysport(struct sx_dev *dev, u32 log_port, int* is_lag)
+{
+	unsigned int port_type = SX_PORT_TYPE_ID_GET(log_port);
+	u16 ret = 0;
+	*is_lag = 0;
+	if (port_type == SX_PORT_TYPE_LAG) {
+		*is_lag = 1;
+		return SX_PORT_LAG_ID_GET(log_port);
+	}
+	else {
+		if (SX_PORT_PHY_ID_GET(log_port) == CPU_PORT_PHY_ID) {
+			/* Build CPU port route*/
+			ret = UCROUTE_CPU_PORT_PREFIX;
+			ret |= SX_PORT_DEV_ID_GET(log_port) << UCROUTE_CPU_DEV_BIT_OFFSET;
+		}
+		else {
+			ret = sx_priv(dev)->local_to_system_db[SX_PORT_PHY_ID_GET(log_port)];
+			/* For Switchx-2 it's equals to the following translation:
+			ret = SX_PORT_DEV_ID_GET(log_port) << UCROUTE_DEV_ID_BIT_OFFSET;
+			ret |= (SX_PORT_PHY_ID_GET(log_port) - 1) << UCROUTE_PHY_PORT_BITS_OFFSET;
+			*/
+		}
+	}
+	return ret;
+}
+EXPORT_SYMBOL(translate_user_port_to_sysport);
+
+u32 translate_sysport_to_user_port(struct sx_dev *dev, u16 port, u8 is_lag)
+{
+    u32 lag_id = 0;
+
+	if (is_lag) {
+	    lag_id = port << SX_PORT_LAG_ID_OFFS;
+	    lag_id |= SX_PORT_TYPE_LAG << SX_PORT_TYPE_ID_OFFS;
+		return lag_id;
+	}
+	else {
+		if ((port & UCROUTE_CPU_PORT_PREFIX) == UCROUTE_CPU_PORT_PREFIX) {
+			return ((port & ~UCROUTE_CPU_PORT_PREFIX) >> UCROUTE_CPU_DEV_BIT_OFFSET)
+				<< SX_PORT_DEV_ID_OFFS;
+		}
+		else {
+			return ((port >> UCROUTE_DEV_ID_BIT_OFFSET) << SX_PORT_DEV_ID_OFFS) |
+				(sx_priv(dev)->system_to_local_db[port]) << SX_PORT_PHY_ID_OFFS;
+			/* For Switchx-2 it's equals to the following translation:
+			return ((port >> UCROUTE_DEV_ID_BIT_OFFSET) << SX_PORT_DEV_ID_OFFS) |
+				(((port >> UCROUTE_PHY_PORT_BITS_OFFSET) & 0xFF) + 1) << SX_PORT_PHY_ID_OFFS;
+			*/
+		}
+	}
+}
+EXPORT_SYMBOL(translate_sysport_to_user_port);
+
+#define SNOOP_MISS_WA
+#if defined(CONFIG_MLNX460EX) && defined(SNOOP_MISS_WA)
+int config_l2_force_snoop(void)
+{
+    struct device_node *np;
+    u32                 r;
+    const u32          *dcrreg;
+    int                 len;
+    const u32          *prop;
+    u32                 l2_size;
+    static u32          dcrbase_l2c = 0;
+
+    if (0 == dcrbase_l2c) {
+        np = of_find_compatible_node(NULL, NULL, "ibm,l2-cache");
+        if (!np) {
+            return 0;
+        }
+
+        /* Get l2 cache size */
+        prop = of_get_property(np, "cache-size", NULL);
+        if (prop == NULL) {
+            printk(KERN_ERR "%s: Can't get cache-size!\n", np->full_name);
+            of_node_put(np);
+            return -ENODEV;
+        }
+        l2_size = prop[0];
+
+        /* Map DCRs */
+        dcrreg = of_get_property(np, "dcr-reg", &len);
+        if (!dcrreg || (len != 4 * sizeof(u32))) {
+            printk(KERN_ERR "%s: Can't get DCR register base !",
+                   np->full_name);
+            of_node_put(np);
+            return -ENODEV;
+        }
+        dcrbase_l2c = dcrreg[2];
+    }
+
+    /* Force snoop */
+    r = mfdcr(dcrbase_l2c + DCRN_L2C0_CFG);
+    r |= 0x0000010;
+    mtdcr(dcrbase_l2c + DCRN_L2C0_CFG, r);
+
+    return 0;
+}
+#endif /* SNOOP_MISS_WA */
+
+int sx_core_get_prio2tc(struct sx_dev *dev, uint16_t port_lag_id, uint8_t is_lag, uint8_t pcp, uint8_t *tc)
+{
+    struct sx_priv *dev_priv = sx_priv(dev);
+    uint16_t        local = 0;
+    unsigned long   flags;
+
+    if (pcp > MAX_PRIO_NUM) {
+        printk(KERN_ERR PFX "PCP %d is invalid. (MAX %d).\n",
+               pcp, MAX_PRIO_NUM);
+        return -EINVAL;
+    }
+
+    spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+    if (is_lag) {
+        *tc = dev_priv->lag_prio2tc[port_lag_id][pcp];
+    } else {
+        local = dev_priv->system_to_local_db[port_lag_id];
+        if (local > MAX_PHYPORT_NUM) {
+            printk(KERN_ERR PFX "Local %d is invalid. (MAX %d).\n",
+                   local, MAX_PHYPORT_NUM);
+            spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+            return -EINVAL;
+        }
+        *tc = dev_priv->port_prio2tc[local][pcp];
+    }
+    spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+
+    return 0;
+}
+EXPORT_SYMBOL(sx_core_get_prio2tc);
+
+int sx_core_get_pvid(struct sx_dev *dev,
+                     uint16_t       port_lag_id,
+                     uint8_t        is_lag,
+                     uint16_t       *pvid)
+{
+    struct sx_priv *dev_priv = sx_priv(dev);
+    uint16_t        local = 0;
+    unsigned long   flags;
+
+    spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+    if (is_lag) {
+        *pvid = dev_priv->pvid_lag_db[port_lag_id];
+    } else {
+        local = dev_priv->system_to_local_db[port_lag_id];
+        if (local > MAX_PHYPORT_NUM) {
+            printk(KERN_ERR PFX "Local %d is invalid. (MAX %d).\n",
+                   local, MAX_PHYPORT_NUM);
+            spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+            return -EINVAL;
+        }
+        *pvid = dev_priv->pvid_sysport_db[local];
+    }
+    spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+
+    return 0;
+}
+EXPORT_SYMBOL(sx_core_get_pvid);
+
+int sx_core_get_vlan_tagging(struct sx_dev *dev,
+                             uint16_t       port_lag_id,
+                             uint8_t        is_lag,
+                             uint16_t       vlan,
+                             uint8_t       *is_vlan_tagged)
+{
+    struct sx_priv *dev_priv = sx_priv(dev);
+    uint16_t        local = 0;
+    unsigned long   flags;
+
+    spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+    if (is_lag) {
+        *is_vlan_tagged = dev_priv->lag_vtag_mode[port_lag_id][vlan];
+    } else {
+        local = dev_priv->system_to_local_db[port_lag_id];
+        if (local > MAX_PHYPORT_NUM) {
+            printk(KERN_ERR PFX "Local %d is invalid. (MAX %d).\n",
+                   local, MAX_PHYPORT_NUM);
+            spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+            return -EINVAL;
+        }
+        *is_vlan_tagged = dev_priv->port_vtag_mode[local][vlan];
+    }
+    spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+
+    return 0;
+}
+EXPORT_SYMBOL(sx_core_get_vlan_tagging);
+
+int sx_core_get_prio_tagging(struct sx_dev *dev, uint16_t port_lag_id, uint8_t is_lag, uint8_t *is_port_prio_tagged)
+{
+    struct sx_priv *dev_priv = sx_priv(dev);
+    uint16_t        local = 0;
+    unsigned long   flags;
+
+    spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+    if (is_lag) {
+        *is_port_prio_tagged = dev_priv->lag_prio_tagging_mode[port_lag_id];
+    } else {
+        local = dev_priv->system_to_local_db[port_lag_id];
+        if (local > MAX_PHYPORT_NUM) {
+            printk(KERN_ERR PFX "Local %d is invalid. (MAX %d).\n",
+                   local, MAX_PHYPORT_NUM);
+            spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+            return -EINVAL;
+        }
+        *is_port_prio_tagged = dev_priv->port_prio_tagging_mode[local];
+    }
+    spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+
+    return 0;
+}
+EXPORT_SYMBOL(sx_core_get_prio_tagging);
+
+int sx_core_get_swid(struct sx_dev *dev, struct completion_info *comp_info, uint8_t *swid)
+{
+    unsigned long   flags;
+
+    *swid = 0;
+
+    spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+
+    if (dev && (sx_priv(dev)->dev_specific_cb.get_swid_cb != NULL)) {
+        sx_priv(dev)->dev_specific_cb.get_swid_cb(dev, comp_info, swid);
+    } else {
+        printk(KERN_ERR PFX "Error retrieving get_swid_cb callback\n");
+        spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+        return -EINVAL;
+    }
+
+    spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+    return 0;
+}
+EXPORT_SYMBOL(sx_core_get_swid);
+
+int sx_core_get_lag_mid(struct sx_dev *dev, u16 lag_id, u16 *mid)
+{
+    unsigned long   flags;
+    
+    spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+    
+    if (dev && (sx_priv(dev)->dev_specific_cb.get_lag_mid_cb != NULL)) {
+        sx_priv(dev)->dev_specific_cb.get_lag_mid_cb(lag_id, mid);
+    } else {
+        printk(KERN_ERR PFX "Error retrieving get_lag_mid_cb callback\n");
+        spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+        return -EINVAL;
+    }
+    
+    spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+    return 0;
+}
+EXPORT_SYMBOL(sx_core_get_lag_mid);
+
+int sx_core_get_rp_rif_id(struct sx_dev *dev, uint16_t port_lag_id,
+                          uint8_t is_lag, uint16_t vlan_id, uint16_t *rif_id)
+{
+    struct sx_priv *dev_priv = sx_priv(dev);
+    uint16_t        local = 0;
+    unsigned long   flags;
+
+    if (vlan_id >= MAX_VLAN_NUM) {
+        printk(KERN_ERR PFX "vlan_id %d is invalid. (MAX %d).\n",
+               vlan_id, MAX_VLAN_NUM);
+        return -EINVAL;
+    }
+
+    *rif_id = 0;
+
+    spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+    if (is_lag) {
+        if (port_lag_id > MAX_LAG_NUM) {
+            printk(KERN_ERR PFX "port_lag_id %d is invalid. (MAX %d).\n",
+                   port_lag_id, MAX_LAG_NUM);
+            spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+            return -EINVAL;
+        }
+        if (!dev_priv->lag_rp_rif_valid[port_lag_id][vlan_id]) {
+            printk(KERN_ERR PFX "No RP on LAG ID %d and vlan %d.\n",
+                   port_lag_id, vlan_id);
+            spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+            return -EINVAL;
+        }
+
+        *rif_id = dev_priv->lag_rp_rif[port_lag_id][vlan_id];
+    } else {
+        local = dev_priv->system_to_local_db[port_lag_id];
+        if (local > MAX_PHYPORT_NUM) {
+            printk(KERN_ERR PFX "Local %d is invalid. (MAX %d).\n",
+                   local, MAX_PHYPORT_NUM);
+            spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+            return -EINVAL;
+        }
+        if (!dev_priv->port_rp_rif_valid[local][vlan_id]) {
+            printk(KERN_ERR PFX "No RP on port %d and vlan %d.\n",
+                   local, vlan_id);
+            spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+            return -EINVAL;
+        }
+
+        *rif_id = dev_priv->port_rp_rif[local][vlan_id];
+    }
+    spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+
+    return 0;
+}
+EXPORT_SYMBOL(sx_core_get_rp_rif_id);
+
+int sx_core_get_rp_vlan(struct sx_dev *dev, struct completion_info *comp_info, uint16_t *vlan_id)
+{
+    unsigned long   flags;
+
+    *vlan_id = 0;
+
+    spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+
+    if (dev && (sx_priv(dev)->dev_specific_cb.get_rp_vid_cb != NULL)) {
+        sx_priv(dev)->dev_specific_cb.get_rp_vid_cb(dev, comp_info, vlan_id);
+    } else {
+        printk(KERN_ERR PFX "Error retrieving get_rp_vid_cb callback\n");
+        spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+        return -EINVAL;
+    }
+
+    spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+    return 0;
+}
+EXPORT_SYMBOL(sx_core_get_rp_vlan);
+
+int sx_core_get_rp_mode(struct sx_dev *dev, u8 is_lag, u16 sysport_lag_id, u16 vlan_id, u8 *is_rp)
+{
+    unsigned long flags;
+    u16 lag_id = 0;
+    uint16_t local = 0;
+
+    if (vlan_id >= MAX_VLAN_NUM) {
+        printk(KERN_ERR PFX "vlan_id %d is invalid. (MAX %d).\n",
+               vlan_id, MAX_VLAN_NUM);
+        return -EINVAL;
+    }
+
+    spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+    if (is_lag) {
+        lag_id = (sysport_lag_id >> 4) & 0xfff;
+        if (lag_id > MAX_LAG_NUM) {
+            printk(KERN_ERR PFX "LAG ID %d is invalid. (MAX %d).\n",
+                   lag_id, MAX_LAG_NUM);
+            spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+            return -EINVAL;
+        }
+        *is_rp = sx_priv(dev)->lag_rp_rif_valid[lag_id][vlan_id];
+    } else {
+        local = sx_priv(dev)->system_to_local_db[sysport_lag_id];
+        if (local > MAX_PHYPORT_NUM) {
+            printk(KERN_ERR PFX "Local %d is invalid. (MAX %d).\n",
+                   local, MAX_PHYPORT_NUM);
+            spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+            return -EINVAL;
+        }
+        *is_rp = sx_priv(dev)->port_rp_rif_valid[local][vlan_id];
+    }
+    spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+
+    return 0;
+}
+EXPORT_SYMBOL(sx_core_get_rp_mode);
+
+
+int sx_core_get_vlan2ip(struct sx_dev *dev, uint16_t vid, uint32_t *ip_addr)
+{
+    struct sx_priv *dev_priv = sx_priv(dev);
+    unsigned long   flags;
+
+    if (dis_vid2ip) {
+        return 0;
+    }
+
+    if (vid >= MAX_VLAN_NUM) {
+        printk(KERN_ERR PFX "vid %d is invalid. (MAX %d).\n",
+               vid, MAX_VLAN_NUM);
+        return -EINVAL;
+    }
+
+    spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+
+    *ip_addr = dev_priv->icmp_vlan2ip_db[vid];
+
+    spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+
+    return 0;
+}
+EXPORT_SYMBOL(sx_core_get_vlan2ip);
+
+int sx_core_get_fid_by_port_vid(struct sx_dev *dev, struct completion_info *comp_info, uint16_t *fid)
+{    
+    struct sx_priv *dev_priv = sx_priv(dev);
+    uint16_t        local = 0;
+    unsigned long   flags;
+    uint8_t         is_lag = comp_info->is_lag;
+    uint16_t        sysport_lag_id = comp_info->sysport;
+    uint16_t        lag_port_id = comp_info->lag_subport;
+    uint16_t        vid = comp_info->vid;    
+
+    if (vid >= MAX_VLAN_NUM) {
+        printk(KERN_ERR PFX "vid %d is invalid. (MAX %d).\n",
+               vid, MAX_VLAN_NUM);
+        return -EINVAL;
+    }
+
+    spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+    if (is_lag) {  
+        u16 lag_id = (sysport_lag_id >> 4) & 0xfff;
+        local = sx_priv(dev)->lag_member_to_local_db[lag_id][lag_port_id];        
+    } else {
+        local = dev_priv->system_to_local_db[sysport_lag_id];
+        if (local > MAX_PHYPORT_NUM) {
+            printk(KERN_ERR PFX "Local %d is invalid. (MAX %d).\n",
+                   local, MAX_PHYPORT_NUM);
+            spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+            return -EINVAL;
+        }        
+    }
+    *fid = dev_priv->port_vid_to_fid[local][vid];
+    spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+
+    return 0;
+}
+EXPORT_SYMBOL(sx_core_get_fid_by_port_vid);
+
+
+static int sx_raise_event(struct sx_dev *dev, void *data, void *context)
+{
+    int                    err = 0;
+    struct sk_buff        *skb = NULL;
+    struct ku_raise_trap   event_data;
+    struct completion_info ci;
+    void                  *buff;
+
+    err = copy_from_user((void*)(&event_data), data, sizeof(event_data));
+    if (err) {
+        goto out_err;
+    }
+
+    if ((event_data.buffer_size == 0) ||
+        (event_data.trap_id > NUM_HW_SYNDROMES)) {
+        printk(KERN_WARNING "sx_raise_event: Bad parameters\n");
+        err = -EINVAL;
+        goto out_err;
+    }
+
+    skb = alloc_skb(event_data.buffer_size, GFP_KERNEL);
+    if (!skb) {
+        err = -ENOMEM;
+        goto out_err;
+    }
+
+    buff = skb_put(skb, event_data.buffer_size);
+    if (buff == NULL) {
+        err = -ENOMEM;
+        goto out;
+    }
+    err = copy_from_user(buff, event_data.buffer_p,
+                         event_data.buffer_size);
+    if (err) {
+        goto out;
+    }
+
+#ifdef SX_DEBUG
+    printk(KERN_DEBUG PFX "sx_raise_event: got from user:\n");
+    printk(KERN_DEBUG PFX "sx_raise_event: trap_id = %u\n",
+           event_data.trap_id);
+    printk(KERN_DEBUG PFX "sx_raise_event: buffer_size = %u\n",
+           event_data.buffer_size);
+    SX_CORE_HEXDUMP16(event_data.buffer_p, event_data.buffer_size);
+#endif
+
+    memset(&ci, 0, sizeof(ci));
+    ci.hw_synd = event_data.trap_id;
+    ci.sysport = event_data.sysport;
+    ci.is_lag = event_data.is_lag;
+    ci.lag_subport = event_data.lag_subport;
+    ci.swid = event_data.swid;
+    ci.skb = skb;
+    ci.pkt_type = PKT_TYPE_ETH;
+    ci.info.eth.dmac = DMAC_DONT_CARE_VALUE;
+    ci.info.eth.ethtype = ETHTYPE_DONT_CARE_VALUE;
+    ci.info.eth.emad_tid = TID_DONT_CARE_VALUE;
+    ci.context = context;
+    dispatch_pkt(dev, &ci, event_data.trap_id, 0);
+out:
+    kfree_skb(skb);
+out_err:
+    return err;
+}
+
+/**
+ * Update the device's cap struct with the default capabilities of the HW
+ * (number of RDQs, SDQs, CQs Etc.)
+ */
+static void set_default_capabilities(struct sx_dev *dev)
+{
+    dev->dev_cap.log_max_rdq_sz = 7;
+    dev->dev_cap.log_max_sdq_sz = 7;
+    dev->dev_cap.log_max_cq_sz = 7;
+
+    dev->dev_cap.max_num_rdqs = NUMBER_OF_RDQS;
+    dev->dev_cap.max_num_sdqs = NUMBER_OF_SDQS;
+    dev->dev_cap.max_num_cqs = NUMBER_OF_RDQS + NUMBER_OF_SDQS;
+
+    dev->dev_cap.max_num_cpu_egress_tcs = 12;
+    dev->dev_cap.max_num_cpu_ingress_tcs = 16;
+}
+
+int ver_set_capabilities(struct sx_dev_cap *cap)
+{
+    struct sx_dev *dev = sx_glb.tmp_dev_ptr;
+
+    printk(KERN_DEBUG PFX "ver_set_capabilities: Entered function\n");
+    printk(KERN_DEBUG PFX "ver_set_capabilities: "
+           "cap->log_max_rdq_sz = %d\n", cap->log_max_rdq_sz);
+
+    dev->dev_cap.log_max_rdq_sz = cap->log_max_rdq_sz;
+    dev->dev_cap.log_max_sdq_sz = cap->log_max_sdq_sz;
+    dev->dev_cap.log_max_cq_sz = cap->log_max_cq_sz;
+
+    dev->dev_cap.max_num_rdqs = cap->max_num_rdqs;
+    dev->dev_cap.max_num_sdqs = cap->max_num_sdqs;
+    dev->dev_cap.max_num_cqs = cap->max_num_cqs;
+
+    dev->dev_cap.max_num_cpu_egress_tcs = cap->max_num_cpu_egress_tcs;
+    dev->dev_cap.max_num_cpu_ingress_tcs = cap->max_num_cpu_ingress_tcs;
+
+    return 0;
+}
+EXPORT_SYMBOL(ver_set_capabilities);
+
+/**
+ * Return a pointer to the sx device
+ *
+ * returns: Pointer to the sx device - success
+ *	    NULL                     - error
+ */
+void * sx_get_dev_context(void)
+{
+    return sx_glb.sx_dpt.dpt_info[DEFAULT_DEVICE_ID].
+           sx_pcie_info.sx_dev;
+}
+EXPORT_SYMBOL(sx_get_dev_context);
+
+/************************************************
+ *  Helper Functions
+ ***********************************************/
+void inc_unconsumed_packets_counter(struct sx_dev *dev, u16 hw_synd, enum sx_packet_type pkt_type)
+{
+    inc_unconsumed_packets_global_counter( hw_synd, pkt_type);
+	if (dev) {
+    	dev->stats.rx_unconsumed_by_synd[hw_synd][pkt_type]++;
+    	dev->unconsumed_packets_counter++;
+	}
+#ifdef SX_DEBUG
+    printk(KERN_ERR PFX "A packet with trap ID 0x%x and type %s "
+           "was not consumed\n", hw_synd, sx_cqe_packet_type_str[pkt_type]);
+#endif
+}
+
+void inc_eventlist_drops_counter(struct sx_dev* sx_dev, u16 hw_synd)
+{
+	inc_eventlist_drops_global_counter(hw_synd);
+
+    if (sx_dev == NULL) {
+        printk(KERN_ERR PFX "sx_dev is NULL\n");
+        return;
+    }
+
+    sx_dev->eventlist_drops_counter++;
+    sx_dev->stats.rx_eventlist_drops_by_synd[hw_synd]++;
+#ifdef SX_DEBUG
+    printk(KERN_ERR PFX "A packet with trap ID 0x%x "
+           "was dropped from the event list\n", hw_synd);
+#endif
+}
+
+
+
+
+void inc_filtered_lag_packets_counter(struct sx_dev *dev)
+{
+	inc_filtered_lag_packets_global_counter();
+    dev->filtered_lag_packets_counter++;
+}
+
+void inc_filtered_port_packets_counter(struct sx_dev *dev)
+{
+    inc_filtered_port_packets_global_counter();
+    dev->filtered_port_packets_counter++;
+}
+
+void inc_unconsumed_packets_global_counter(u16 hw_synd, enum sx_packet_type pkt_type)
+{
+	unconsumed_packets_counter++;
+	sx_glb.stats.rx_unconsumed_by_synd[hw_synd][pkt_type]++;
+ #ifdef SX_DEBUG
+ 	printk(KERN_ERR PFX "A packet with trap ID 0x%x and type %s "
+ 			"was not consumed\n", hw_synd, sx_cqe_packet_type_str[pkt_type]);
+ #endif
+}
+
+static void inc_eventlist_drops_global_counter( u16 hw_synd)
+{
+	eventlist_drops_counter++;
+	sx_glb.stats.rx_eventlist_drops_by_synd[hw_synd]++;
+ #ifdef SX_DEBUG
+ 	printk(KERN_ERR PFX "A packet with trap ID 0x%x "
+ 			"was dropped from the event list\n", hw_synd);
+ #endif
+}
+
+void inc_filtered_lag_packets_global_counter(void)
+{
+	filtered_lag_packets_counter++;
+}
+
+void inc_filtered_port_packets_global_counter(void)
+{
+  filtered_port_packets_counter++;
+}
+
+static int check_valid_meta(struct sx_dev *dev, struct isx_meta *meta)
+{
+    if (meta->etclass >= NUMBER_OF_ETCLASSES) {
+        if (printk_ratelimit()) {
+            printk(KERN_WARNING PFX "Error: etclass (%u) "
+                   "is invalid\n",
+                   meta->etclass);
+        }
+        return -ERANGE;
+    }
+
+    if ((meta->rdq >= NUMBER_OF_RDQS) &&
+        (meta->rdq != 0x1F)) {
+        if (printk_ratelimit()) {
+            printk(KERN_WARNING PFX "Error: rdq (%u) is invalid\n",
+                   meta->rdq);
+        }
+        return -ERANGE;
+    }
+
+    if ((meta->swid >= NUMBER_OF_SWIDS) &&
+        (meta->swid != SWID_NUM_DONT_CARE)) {
+        if (printk_ratelimit()) {
+            printk(KERN_WARNING PFX "Error: swid (%u) is invalid\n",
+                   meta->swid);
+        }
+        return -ERANGE;
+    }
+
+    /* Validate the swid really exists */
+    if (dev != NULL) {
+        if ((meta->swid != SWID_NUM_DONT_CARE) &&
+            (0 == sx_bitmap_test(&sx_priv(dev)->swid_bitmap, meta->swid))) {
+            printk(KERN_WARNING PFX "Error: swid (%u) does not exists\n", meta->swid);
+            return -EINVAL;
+        }
+    }
+
+    if ((meta->to_cpu != false) && (meta->to_cpu != true)) {
+        if (printk_ratelimit()) {
+            printk(KERN_WARNING PFX "Error: to_cpu (%u) "
+                   "is invalid\n",
+                   meta->to_cpu);
+        }
+        return -ERANGE;
+    }
+
+    if ((meta->type < SX_PKT_TYPE_MIN) ||
+        (meta->type > SX_PKT_TYPE_MAX)) {
+        if (printk_ratelimit()) {
+            printk(KERN_WARNING PFX "Error: type (%d) is invalid\n",
+                   meta->type);
+        }
+        return -ERANGE;
+    }
+
+    return 0;
+}
+
+/**
+ * Get a packet capsulated in ku struct
+ * (scattered to small buffers pointed by internal iovector)
+ * and gather them together to one buffer capslated in the given skb data.
+ * (NOTE: allocate skb)
+ *
+ * @param ku[in]   -  The given ku that capsulate the scattered packet
+ * @param reserve_hdrs[in]   -  Should the function reserve place for headers
+ * @param skb[out] -  A pointer to the pointer of allocated skb
+ *                    to be updated by the function with the gathered packet
+ *
+ * returns: 0 success
+ *	    !0 error
+ */
+static int copy_buff_to_skb(struct sk_buff **skb, struct ku_write *write_data, u8 reserve_hdrs)
+{
+    int            err = 0;
+    int            index = 0;
+    int            packet_size = 0;
+    unsigned char *p_skb_data = NULL;
+    struct iovec  *iov;
+    int            max_headers_size = 0;
+
+    if ((!skb) || (!write_data)) {
+        err = -EINVAL;
+        goto out_err;
+    }
+
+    iov = kmalloc(sizeof(*iov) * (write_data->vec_entries), GFP_KERNEL);
+    if (!iov) {
+        err = -ENOMEM;
+        goto out_err;
+    }
+
+    /*1. copy iovector */
+    err = copy_from_user((void*)iov, (void*)(write_data->iov),
+                         sizeof(*iov) * (write_data->vec_entries));
+    if (err) {
+        goto out_free;
+    }
+
+    /* 2. calc the packet size */
+    for (index = 0; index < write_data->vec_entries; index++) {
+        /*valid param check*/
+        if ((iov[index].iov_len == 0) || (iov[index].iov_base == NULL)) {
+            err = -EINVAL;
+            goto out_free;
+        }
+
+        packet_size += iov[index].iov_len;
+    }
+
+    packet_size += 2; /* ETH FCS when using SGMII */
+    if (reserve_hdrs == true) {
+        max_headers_size = ISX_HDR_SIZE +
+                           sizeof(struct sx_sgmii_ctrl_segment) +
+                           sizeof(struct sx_ethernet_header);
+    }
+
+    /* 3. allocate skb according to packet size */
+    *skb = alloc_skb(packet_size + max_headers_size, GFP_KERNEL);
+    if (!*skb) {
+        err = -ENOMEM;
+        goto out_free;
+    }
+
+    if (max_headers_size) {
+        skb_reserve(*skb, max_headers_size);
+    }
+
+    /*4. copy the scattered buffers of the           */
+    /*   packet to be gathered inside the skb buffer */
+    for (index = 0; index < write_data->vec_entries; index++) {
+        p_skb_data = skb_put(*skb, iov[index].iov_len);
+        if (p_skb_data == NULL) {
+            printk(KERN_WARNING PFX "copy_ku_to_skb: "
+                   "skb_put failed\n");
+            err = -EFAULT;
+            goto out_free_skb;
+        }
+
+        memset(p_skb_data, 0, iov[index].iov_len);
+        err = copy_from_user(p_skb_data, iov[index].iov_base,
+                             iov[index].iov_len);
+        if (err) {
+            goto out_free_skb;
+        }
+    }
+
+    goto out_free;
+
+out_free_skb:
+    kfree_skb(*skb);
+out_free:
+    kfree(iov);
+out_err:
+    return err;
+}
+
+static int sx_send_loopback(struct sx_dev *dev, struct ku_write *write_data, void *context)
+{
+    int                    err = 0;
+    struct completion_info ci;
+
+    memset(&ci, 0, sizeof(ci));
+    err = copy_buff_to_skb(&ci.skb, write_data, false);
+    if (err) {
+        printk(KERN_WARNING "sx_send_loopback: failed copying buffer to SKB\n");
+        goto out;
+    }
+
+#ifdef SX_DEBUG
+    printk(KERN_DEBUG PFX "sx_send_loopback: got from user:\n");
+    printk(KERN_DEBUG PFX "sx_send_loopback: trap_id = %u\n",
+           write_data->meta.loopback_data.trap_id);
+    printk(KERN_DEBUG PFX "sx_send_loopback: buffer_size = %u\n",
+           ci.skb->len);
+    SX_CORE_HEXDUMP16(ci.skb->data, ci.skb->len);
+#endif
+
+    ci.swid = write_data->meta.swid;
+    ci.sysport = write_data->meta.system_port_mid;
+    ci.hw_synd = write_data->meta.loopback_data.trap_id;
+    ci.is_send = 0;
+    ci.pkt_type = PKT_TYPE_ETH;
+    ci.info.eth.dmac = DMAC_DONT_CARE_VALUE;
+    ci.info.eth.ethtype = ETHTYPE_DONT_CARE_VALUE;
+    ci.info.eth.emad_tid = TID_DONT_CARE_VALUE;
+    ci.is_lag = write_data->meta.loopback_data.is_lag;
+    ci.lag_subport = write_data->meta.loopback_data.lag_subport;
+    ci.is_tagged = 0;
+    ci.vid = 0;
+    ci.context = context;
+    dispatch_pkt(dev, &ci, ci.hw_synd, 0);
+out:
+    kfree_skb(ci.skb);
+    return err;
+}
+
+
+/**
+ * Get edata which is the packet parameters and a place to copy it (buf).
+ *
+ * Create a meta-data struct from the edata and copy it to the user
+ * After it in sequential way copy the packet
+ * (NOTE: the function assuming there is enough place in the given buf )
+ *
+ * param[in] buf   - The place the meta-data + packet should be copied to
+ * param[in] edata - The packet and meta-data parameters
+ *
+ * returns: positive number on success (the copied size of meta-data + packet)
+ *	    !0 error
+ */
+static int copy_pkt_to_user(char __user *buf, struct event_data *edata)
+{
+    struct ku_read metadata;
+    int            copied_size = 0;
+
+#ifdef SX_DEBUG
+    printk(KERN_DEBUG PFX "copy_pkt_to_user()\n");
+#endif
+    /* copy the packet  */
+    metadata.length = edata->skb->len;
+    metadata.system_port = edata->system_port;
+    metadata.trap_id = edata->trap_id;
+    metadata.is_lag = edata->is_lag;
+    metadata.swid = edata->swid;
+    metadata.original_packet_size = edata->original_packet_size;
+    if (edata->is_lag) {
+        metadata.lag_subport = edata->lag_sub_port;
+    }
+
+    if (copy_to_user(buf, &metadata, sizeof(metadata))) {
+        return -EFAULT;
+    }
+
+    copied_size += sizeof(metadata);
+    if (copy_to_user(buf + copied_size, edata->skb->data, edata->skb->len)) {
+        return -EFAULT;
+    }
+
+    copied_size += edata->skb->len;
+#ifdef SX_DEBUG
+    printk(KERN_DEBUG PFX " copy_pkt_to_user() metadata.length=[%llu] " \
+           "metadata.system_port=[%d] metadata.trap_id=[%d]\n",
+           metadata.length, metadata.system_port,
+           metadata.trap_id);
+    SX_CORE_HEXDUMP16((void*)&metadata, sizeof(metadata));
+    SX_CORE_HEXDUMP16(edata->skb->data, edata->skb->len);
+#endif
+
+    return copied_size;
+}
+
+/**
+ *
+ * Copy the edata that holds packet/s to the given buf
+ * NOTE: The function assumes that there is enough free space in
+ * the buffer (which was already checked before calling here)
+ *
+ * param[in] edata - The packet and meta-data parameters
+ * param[in] buf   - the data buffer, ku_read struct
+ *
+ * returns: The size of data which copied to the user buffer
+ *	   !0 error
+ */
+static int copy_edata_to_user(struct event_data *edata, char __user *buf)
+{
+    struct event_data *tmp = NULL;
+    int                so_far_copied = 0;
+    int                copied_size = 0;
+    int                err = 0;
+    struct list_head  *pos, *q;
+    struct sx_dev    * sx_dev = NULL;
+
+#ifdef SX_DEBUG
+    printk(KERN_DEBUG PFX " copy_edata_to_user()\n");
+#endif
+    list_for_each_safe(pos, q, &edata->list) {
+        tmp = list_entry(pos, struct event_data, list);
+        copied_size = copy_pkt_to_user(buf + so_far_copied, tmp);
+        if (copied_size < 0) {
+            err = copied_size;
+            goto out_free;
+        }
+
+        so_far_copied += copied_size;
+        list_del(pos);
+        kfree_skb(tmp->skb);
+        kfree(tmp);
+    }
+
+    return so_far_copied;
+
+out_free:
+    list_for_each_safe(pos, q, &edata->list) {
+        tmp = list_entry(pos, struct event_data, list);
+        sx_dev = tmp->dev;
+        if (sx_dev && !sx_dev->eventlist_drops_counter) {
+            printk(KERN_WARNING PFX
+                   "copy_pkt_to_user failed, " \
+                   "dropping RX packet\n");
+        }
+
+        inc_eventlist_drops_counter(sx_dev, tmp->trap_id);
+        list_del(pos);
+        kfree_skb(tmp->skb);
+        kfree(tmp);
+    }
+
+    return so_far_copied ? so_far_copied : err;
+}
+
+
+/**
+ *
+ * Create edata linked-list and return it
+ *
+ * while the event list is not empty, pop event-data ,
+ * if the packet size in the event-data is not to big then add it to
+ * the buffer counter and continue while the buffer counter is
+ * less then the user buffer
+ *
+ * param[in] evlist - The given event list
+ * param[in] user_counter  - The number of bites to send(the user buffer size)
+ *
+ *
+ * returns:
+ *	edata list on - success 
+ *      NULL          - error
+ * retval:
+ * 	0 on success, otherwise the size of the buffer needed for the 
+ * 	first packet if there is enough space in the buffer for 
+ * 	metadata, -ENOMEM if not 
+ */
+static int get_edata_from_elist(int               *evlist_size,
+                                struct event_data *edata_list,
+                                struct list_head  *evlist,
+                                size_t             user_counter,
+                                int                multi_packet_read_enable)
+{
+    struct  list_head  *pos, *q;
+    struct  event_data *edata = NULL;
+    size_t              buf_counter = 0;
+    int                 pkt_size = 0;
+
+    list_for_each_safe(pos, q, evlist) {
+        edata = list_entry(pos, struct event_data, list);
+        pkt_size = sizeof(struct ku_read) + edata->skb->len;
+        if (buf_counter + pkt_size > user_counter) {
+            break;
+        }
+
+        buf_counter += pkt_size;
+        list_del(pos);
+        list_add_tail(&edata->list, &edata_list->list);
+        (*evlist_size)--;
+        if (multi_packet_read_enable == false) {
+            break;
+        }
+    }
+
+    /* Not enough place for even a single packet */
+    if (0 == buf_counter) {
+        if (sizeof(struct ku_read) > user_counter) {
+            return -ENOMEM;
+        }
+        return pkt_size;
+    }
+    return 0;
+}
+
+/**
+ * Create new listener with the given swid,type,critireas and add it to an entry
+ * in sx device listeners data-base when the entry is according to the hw_synd
+ *
+ * Note: A default listener is a listener which receives all the packets
+ *       that haven't been consumed by another non-default listener.
+ *
+ * param swid            [in] - The listener swid
+ * param hw_synd         [in] - The listener syndrome number
+ * param type            [in] - The listener type - ETH,IB or FC, or Don't care
+ * param is_default      [in] - If the listener is a default listener
+ * param filter_critireas[in] - The listener additional filter critireas
+ * param handler         [in] -
+ * param context         [in] -
+ * param check_dup       [in] - block the listener from registering twice to same trap.
+ *
+ * returns: 0 success
+ *	   !0 error
+ *
+ */
+int sx_core_add_synd(u8                        swid,
+                     u16                       hw_synd,
+                     enum l2_type              type,
+                     u8                        is_default,
+                     union ku_filter_critireas crit,
+                     cq_handler                handler,
+                     void                     *context,
+                     check_dup_e               check_dup,
+                     struct sx_dev           * sx_dev)
+{
+    unsigned long          flags;
+    struct listener_entry *new_listener = NULL;
+    struct list_head      *pos;
+    struct listener_entry *listener;
+    unsigned int           found_same_listener;
+
+    /* if NULL use default sx_dev */
+    if (sx_dev == NULL) {
+        sx_dev = sx_glb.tmp_dev_ptr;
+    }
+
+#if 0
+    if (!context) {
+        printk(KERN_WARNING PFX "sx_core_add_synd: Cannot add listener, context is NULL\n");
+        return -EINVAL;
+    }
+#endif
+
+    if (!handler) {
+        printk(KERN_WARNING PFX "sx_core_add_synd: Cannot add listener, handler is NULL\n");
+        return -EINVAL;
+    }
+
+    new_listener = kmalloc(sizeof(*new_listener), GFP_ATOMIC);
+    if (!new_listener) {
+        printk(KERN_WARNING PFX "sx_core_add_synd: Failed allocating memory for the new listener\n");
+        return -ENOMEM;
+    }
+
+    new_listener->swid = swid;
+    new_listener->critireas = crit;
+    new_listener->handler = handler;
+    new_listener->context = context;
+    new_listener->listener_type = type;
+    new_listener->is_default = is_default;
+    new_listener->rx_pkts = 0;
+    spin_lock_irqsave(&sx_glb.listeners_lock, flags);
+    /* default listeners are stored at Don't care */
+    /* entry, at the end of the list              */
+
+    if (is_default) {
+        list_add_tail(&(new_listener->list),
+                      &(sx_glb.listeners_db[hw_synd].list));
+    } else {
+        found_same_listener = 0;
+        if ((check_dup == CHECK_DUP_ENABLED_E) &&
+            !list_empty(&sx_glb.listeners_db[hw_synd].list)) {
+            list_for_each(pos, &sx_glb.listeners_db[hw_synd].list) {
+                listener = list_entry(pos, struct listener_entry, list);
+                if (listener->context == context) {
+                    found_same_listener = 1;
+                    printk(KERN_WARNING PFX "this listener is already " \
+                           "listening for that trap \n");
+                }
+            }
+        }
+
+        if (found_same_listener == 0) {
+            list_add(&(new_listener->list),
+                     &(sx_glb.listeners_db[hw_synd].list));
+        } else {
+            kfree(new_listener);
+        }
+    }
+    spin_unlock_irqrestore(&sx_glb.listeners_lock, flags);
+
+    return 0;
+}
+EXPORT_SYMBOL(sx_core_add_synd);
+
+static int sx_core_add_synd_l3(u8 swid, u16 hw_synd, struct sx_dev *dev)
+{
+	union sx_event_data event_data;
+
+	event_data.eth_l3_synd.swid = swid;
+	event_data.eth_l3_synd.hw_synd = hw_synd;
+	sx_core_dispatch_event(dev, SX_DEV_EVENT_ADD_SYND_NETDEV, &event_data);
+	return 0;
+}
+
+static int sx_core_remove_synd_l3(u8 swid, u16 hw_synd, struct sx_dev *dev)
+{
+	union sx_event_data event_data;
+
+	event_data.eth_l3_synd.swid = swid;
+	event_data.eth_l3_synd.hw_synd = hw_synd;
+	sx_core_dispatch_event(dev, SX_DEV_EVENT_REMOVE_SYND_NETDEV, &event_data);
+	return 0;
+}
+
+static int sx_core_add_synd_l2(u8 swid, u16 hw_synd, struct sx_dev *dev)
+{
+	union sx_event_data event_data;
+
+	event_data.eth_l3_synd.swid = swid;
+	event_data.eth_l3_synd.hw_synd = hw_synd;
+	sx_core_dispatch_event(dev, SX_DEV_EVENT_ADD_SYND_L2_NETDEV, &event_data);
+	return 0;
+}
+
+static int sx_core_remove_synd_l2(u8 swid, u16 hw_synd, struct sx_dev *dev)
+{
+	union sx_event_data event_data;
+
+	event_data.eth_l3_synd.swid = swid;
+	event_data.eth_l3_synd.hw_synd = hw_synd;
+	sx_core_dispatch_event(dev, SX_DEV_EVENT_REMOVE_SYND_L2_NETDEV, &event_data);
+	return 0;
+}
+
+static int sx_core_add_synd_ipoib(u8 swid, u16 hw_synd, struct sx_dev *dev)
+{
+    union sx_event_data event_data;
+
+    event_data.ipoib_synd.swid = swid;
+    event_data.ipoib_synd.hw_synd = hw_synd;
+    sx_core_dispatch_event(dev, SX_DEV_EVENT_ADD_SYND_IPOIB, &event_data);
+
+    return 0;
+}
+
+static int sx_core_remove_synd_ipoib(u8 swid, u16 hw_synd, struct sx_dev *dev)
+{
+    union sx_event_data event_data;
+
+    event_data.ipoib_synd.swid = swid;
+    event_data.ipoib_synd.hw_synd = hw_synd;
+    sx_core_dispatch_event(dev, SX_DEV_EVENT_REMOVE_SYND_IPOIB, &event_data);
+
+    return 0;
+}
+
+static int is_to_remove_listener(u8                        swid,
+                                 enum l2_type              type,
+                                 u8                        is_default,
+                                 union ku_filter_critireas critireas,
+                                 void                     *context,
+                                 struct listener_entry    *listener)
+{
+    if (listener->context != context) {
+        return false;
+    }
+
+    if (listener->is_default != is_default) {
+        return false;
+    }
+
+    if (is_default) {
+        return true;
+    }
+
+    if (listener->swid != swid) {
+        return false;
+    }
+
+    if (listener->listener_type != type) {
+        return false;
+    }
+
+    switch (type) {
+    case L2_TYPE_DONT_CARE:
+        if (listener->critireas.dont_care.sysport != critireas.dont_care.sysport) {
+            return false;
+        }
+
+        break;
+
+    case L2_TYPE_ETH:
+        if (listener->critireas.eth.ethtype != critireas.eth.ethtype) {
+            return false;
+        }
+
+        if (listener->critireas.eth.dmac != critireas.eth.dmac) {
+            return false;
+        }
+
+        if (listener->critireas.eth.emad_tid != critireas.eth.emad_tid) {
+            return false;
+        }
+
+        if (listener->critireas.eth.from_rp != critireas.eth.from_rp) {
+            return false;
+        }
+
+        if (listener->critireas.eth.from_bridge != critireas.eth.from_bridge) {
+            return false;
+        }
+
+        break;
+
+    case L2_TYPE_IB:
+        if (listener->critireas.ib.qpn != critireas.ib.qpn) {
+            return false;
+        }
+
+        break;
+
+    default:
+        break;
+    }
+
+    return true;
+}
+
+int sx_core_remove_synd(u8                        swid,
+                        u16                       hw_synd,
+                        enum l2_type              type,
+                        u8                        is_default,
+                        union ku_filter_critireas critireas,
+                        void                     *context,
+                        struct sx_dev           * sx_dev)
+{
+    unsigned long          flags;
+    struct listener_entry *listener = NULL;
+    struct list_head      *pos = NULL;
+    struct list_head      *q = NULL;
+    int                    listener_removed = 0;
+    int                    entry = 0;
+
+    /*
+     * if NULL use default sx_dev
+     * TODO: add instead logic for all_devs
+     */
+    if (sx_dev == NULL) {
+        sx_dev = sx_glb.tmp_dev_ptr;
+    }
+
+#if 0
+    if (context == NULL) {
+        printk(KERN_DEBUG PFX "sx_core_remove_synd: Err: context = "
+               "NULL, exiting...\n");
+        return -EINVAL;
+    }
+#endif
+
+    spin_lock_irqsave(&sx_glb.listeners_lock, flags);
+    /* Used to be "is_default ? NUM_HW_SYNDROMES : hw_synd;" removed, since we 
+       would like to use is_default for PUDE */
+    entry = hw_synd; 
+    if (!list_empty(&(sx_glb.listeners_db[entry].list))) {
+        list_for_each_safe(pos, q, &sx_glb.listeners_db[entry].list) {
+            listener = list_entry(pos, struct listener_entry, list);
+            if (is_to_remove_listener(swid, type, is_default,
+                                      critireas, context, listener)) {
+                list_del(pos);
+                /* Listener was allocated as atomic memory
+                 * so it's OK to free it under spinlock */
+                kfree(listener);
+                listener_removed = 1;
+                break;
+            }
+        }
+    }
+
+    spin_unlock_irqrestore(&sx_glb.listeners_lock, flags);
+    if (listener_removed == 0) {
+        printk(KERN_WARNING PFX "sx_core_remove_synd: no matching "
+               "listener was found\n");
+        return -EAGAIN;
+    }
+
+    return 0;
+}
+EXPORT_SYMBOL(sx_core_remove_synd);
+static void sx_cq_handler(struct completion_info *comp_info, void *context)
+{
+    unsigned long      flags;
+    struct event_data *edata = NULL;
+    struct sx_rsc     *file = ((struct file *)(context))->private_data;
+    struct sx_dev     *sx_dev = comp_info->dev;
+    struct sk_buff    *skb = comp_info->skb;
+
+    skb_get(skb);
+    edata = kmalloc(sizeof(*edata), GFP_ATOMIC);
+    if (edata == NULL) {
+        if (sx_dev && !sx_dev->eventlist_drops_counter) {
+            printk(KERN_WARNING PFX "Memory allocation "
+                   "for event data failed, "
+                   "dropping RX packet\n");
+        }
+        inc_eventlist_drops_counter(sx_dev, comp_info->hw_synd);
+        goto out_free;
+    }
+
+    /* update edata params */
+    INIT_LIST_HEAD(&edata->list);
+    edata->skb = skb;
+    edata->system_port = comp_info->sysport;
+    edata->trap_id = comp_info->hw_synd;
+    edata->dev_id = 0;
+    edata->dev = sx_dev;
+    edata->is_lag = comp_info->is_lag;
+    edata->lag_sub_port = comp_info->lag_subport;
+    edata->swid = comp_info->swid;
+    edata->original_packet_size = comp_info->original_packet_size;
+#ifdef SX_DEBUG
+    printk(KERN_DEBUG PFX " sx_cq_handler(): skb->len=[%d]  sysport=[%d]"
+           " hw_synd(trap_id)=[%d]\n",
+           skb->len, edata->system_port, edata->trap_id);
+#endif
+
+    spin_lock_irqsave(&file->lock, flags);
+
+    if (file->evlist_size < SX_EVENT_LIST_SIZE) {
+        list_add_tail(&edata->list, &file->evlist.list);
+        file->evlist_size++;
+        wake_up_interruptible(&file->poll_wait);
+        spin_unlock_irqrestore(&file->lock, flags);
+        goto out_ok;
+    }
+
+    spin_unlock_irqrestore(&file->lock, flags);
+
+    if ((sx_dev != NULL) && (!sx_dev->eventlist_drops_counter)) {
+        printk(KERN_WARNING PFX "Event list is full, "
+               "dropping RX packet\n");
+    }
+    inc_eventlist_drops_counter(sx_dev, comp_info->hw_synd);
+
+out_free:
+    kfree_skb(skb);
+    kfree(edata);
+out_ok:
+    return;
+}
+
+static void sx_l2_tunnel_handler(struct completion_info *comp_info, void *context)
+{
+    struct sx_dev  *dev = (struct sx_dev *)context;
+    struct sk_buff *skb = comp_info->skb;
+
+    /* sx_priv(dev) has the relevant info for building the headers and sending */
+    dev = NULL;
+    skb_get(skb);
+    kfree_skb(skb);
+
+    return;
+}
+
+/**
+ * This function is used to check the validity of the given ku
+ *
+ * param[in] ku_synd_ioctl - The given ku
+ *
+ * returns: 0 success
+ *	   !0 error
+ */
+static int check_valid_ku_synd(struct ku_synd_ioctl *ku)
+{
+    int err = 0;
+
+    if ((ku->is_default != false) && (ku->is_default != true)) {
+        err = -EINVAL;
+#ifdef SX_DEBUG
+        printk(KERN_DEBUG PFX "The given ku_synd_ioctl not valid: "
+               " ku->is_default=[%d]\n", ku->is_default);
+#endif
+    }
+
+    if ((ku->swid >= NUMBER_OF_SWIDS) && (ku->swid != SWID_NUM_DONT_CARE)) {
+        err = -EINVAL;
+#ifdef SX_DEBUG
+        printk(KERN_DEBUG PFX "The given ku_synd_ioctl not valid: "
+               " ku->swid=[%d]\n", ku->swid);
+#endif
+    }
+
+    if (ku->syndrome_num > NUM_HW_SYNDROMES) {
+        err = -EINVAL;
+#ifdef SX_DEBUG
+        printk(KERN_DEBUG PFX "The given ku_synd_ioctl not valid: "
+               " ku->syndrome_num=[%d]\n", ku->syndrome_num);
+#endif
+    }
+
+    return err;
+}
+
+static int check_valid_profile(struct sx_dev *dev, struct sx_pci_profile *profile)
+{
+    int i, j;
+
+    for (i = 0; i < NUMBER_OF_SWIDS; i++) {
+        for (j = 0; j < NUMBER_OF_ETCLASSES; j++) {
+            if (profile->tx_prof[i][j].sdq >=
+                dev->dev_cap.max_num_sdqs) {
+                printk(KERN_WARNING PFX "sdq num is > max\n");
+                return -EINVAL;
+            }
+            if (profile->tx_prof[i][j].stclass >= 8) {
+                printk(KERN_WARNING PFX "stclass num is > "
+                       "max\n");
+                return -EINVAL;
+            }
+        }
+    }
+    if (profile->emad_tx_prof.sdq >= dev->dev_cap.max_num_sdqs) {
+        printk(KERN_WARNING PFX "emad sdq num is > max\n");
+        return -EINVAL;
+    }
+    if (profile->emad_tx_prof.stclass >= 8) {
+        printk(KERN_WARNING PFX "emad stclass num is > max\n");
+        return -EINVAL;
+    }
+    for (i = 0; i < NUMBER_OF_SWIDS; i++) {
+        if (profile->rdq_count[i] > dev->dev_cap.max_num_rdqs) {
+            printk(KERN_WARNING PFX "sdq num is > max\n");
+            return -EINVAL;
+        }
+        for (j = 0; j < profile->rdq_count[i]; j++) {
+            if (profile->rdq[i][j] >= dev->dev_cap.max_num_rdqs) {
+                printk(KERN_WARNING PFX "rdq[%d][%d] = %d "
+                       "> max\n", i, j, profile->rdq[i][j]);
+                return -EINVAL;
+            }
+        }
+    }
+    if ((profile->pci_profile >= PCI_PROFILE_EN_SINGLE_SWID) &&
+        (profile->emad_rdq >= dev->dev_cap.max_num_rdqs)) {
+        printk(KERN_WARNING PFX "emad_rdq = %d > max\n",
+               profile->emad_rdq);
+        return -EINVAL;
+    }
+
+    for (i = 0; i < NUMBER_OF_SWIDS; i++) {
+        if ((profile->swid_type[i] != SX_KU_L2_TYPE_IB) &&
+            (profile->swid_type[i] != SX_KU_L2_TYPE_ETH) &&
+            (profile->swid_type[i] != SX_KU_L2_TYPE_ROUTER_PORT) &&
+            (profile->swid_type[i] != SX_KU_L2_TYPE_DONT_CARE)) {
+            printk(KERN_WARNING PFX "for swid %d type = "
+                   "%d is wrong!\n",
+                   i, profile->swid_type[i]);
+            return -EINVAL;
+        }
+    }
+
+    for (i = 0; i < NUMBER_OF_RDQS; i++) {
+        if ((profile->rdq_properties[i].number_of_entries >
+             1 << dev->dev_cap.log_max_rdq_sz) ||
+            (profile->rdq_properties[i].entry_size >
+             SX_MAX_MSG_SIZE)) {
+            printk(KERN_WARNING PFX "%s:%d\n", __FILE__, __LINE__);
+            printk(KERN_WARNING PFX "%d: n_enties %d , max_rdq_sz:"
+                   " %d , entry_sz:%d, SX_MAX_MSG_SZ:%d\n",
+                   i,
+                   profile->rdq_properties[i].number_of_entries,
+                   1 << dev->dev_cap.log_max_rdq_sz,
+                   profile->rdq_properties[i].entry_size,
+                   SX_MAX_MSG_SIZE);
+            return -EINVAL;
+        }
+    }
+
+    for (i = 0; i < NUMBER_OF_SDQS; i++) {
+        if (profile->cpu_egress_tclass[i] >= 64) {
+            printk(KERN_WARNING PFX "cpu_egress_tclass[%d] = "
+                   "%d > max\n",
+                   i, profile->cpu_egress_tclass[i]);
+            return -EINVAL;
+        }
+    }
+
+    return 0;
+}
+
+int sx_send_enable_ib_swid_events(struct sx_dev *dev, u8 swid)
+{
+    int                 i;
+    int                 err = 0;
+    union sx_event_data event_data;
+    u8                  first_ib_swid;
+        
+    if (dev->profile.swid_type[swid] == SX_KU_L2_TYPE_IB ) {
+        memset(&event_data, 0, sizeof(event_data));
+        event_data.ib_swid_change.swid = swid;
+        event_data.ib_swid_change.dev_id = dev->device_id;
+        sx_core_dispatch_event(dev, SX_DEV_EVENT_IB_SWID_UP, &event_data);
+
+        spin_lock(&dev->profile_lock);        
+        first_ib_swid = dev->first_ib_swid;
+        dev->first_ib_swid = 0;
+        spin_unlock(&dev->profile_lock);
+        if (first_ib_swid) {
+            union sx_event_data tca_init_event_data;
+
+            memset(&tca_init_event_data, 0, sizeof(tca_init_event_data));
+            for (i = 0; i < NUMBER_OF_SWIDS; i++) {
+                if (dev->profile.ipoib_router_port_enable[i]) {
+                    tca_init_event_data.tca_init.swid[tca_init_event_data.tca_init.num_of_ib_swids++] = i;
+                }
+            }
+
+            if (tca_init_event_data.tca_init.num_of_ib_swids != 0) {
+                tca_init_event_data.tca_init.max_pkey = dev->profile.max_pkey;
+                sx_core_dispatch_event(dev, SX_DEV_EVENT_TYPE_TCA_INIT, &tca_init_event_data);
+            }
+        }
+    }
+    else{
+        printk(KERN_INFO PFX "Error: try to send IB_SWID_UP event on swid %d from non-IB type %d, ",
+               swid, dev->profile.swid_type[swid]);
+        err = -EINVAL;
+    }
+    
+    return err;   
+}
+
+
+int sx_enable_swid(struct sx_dev *dev, int sx_dev_id, u8 swid, int synd, u64 mac)
+{
+    int                 i;
+    int                 err = 0;
+    u8                  dqn;
+    u32                 dq_bitmap = 0;
+    struct sx_dq       *dq;
+    union sx_event_data event_data;
+    unsigned long       flags;
+    u8                  dev_profile_set;
+    u8                  first_ib_swid;
+
+    /* IF PCI-E path is not valid, no need to open DQs */
+    if (!sx_dpt_is_path_valid(sx_dev_id, DPT_PATH_PCI_E)) {
+        printk(KERN_INFO PFX "PCIe path is not valid for device %u, "
+               "will not open DQs\n",
+               sx_dev_id);
+        goto send_events;
+    }
+
+    /* TODO: handle errors */
+    for (i = 0; i < NUMBER_OF_ETCLASSES; i++) {
+        dqn = dev->profile.tx_prof[swid][i].sdq;
+        spin_lock_irqsave(&sx_priv(dev)->sdq_table.lock, flags);
+        dq = sx_priv(dev)->sdq_table.dq[dqn];
+        spin_unlock_irqrestore(&sx_priv(dev)->sdq_table.lock, flags);
+        if (!dq) {
+            err = sx_core_create_sdq(dev,
+                                     1 << dev->dev_cap.log_max_sdq_sz, dqn, &dq);
+            if (err) {
+                goto out;
+            }
+
+            dq_bitmap |= (1 << dqn);
+            /* We only want to increase the refcount if the dq is in use in another swid */
+        } else if (!(dq_bitmap & (1 << dqn))) {
+            atomic_inc(&dq->refcount);
+        }
+    }
+
+    dq_bitmap = 0;
+    for (i = 0; i < dev->profile.rdq_count[swid]; i++) {
+        dqn = dev->profile.rdq[swid][i];
+        spin_lock_irqsave(&sx_priv(dev)->rdq_table.lock, flags);
+        dq = sx_priv(dev)->rdq_table.dq[dqn];
+        spin_unlock_irqrestore(&sx_priv(dev)->rdq_table.lock, flags);
+        if (!dq) {
+            err = sx_core_create_rdq(dev, RDQ_NUMBER_OF_ENTRIES, dqn, &dq);
+            if (err) {
+                goto out;
+            }
+
+            dq_bitmap |= (1 << dqn);
+        } else if (!(dq_bitmap & (1 << dqn))) {
+            atomic_inc(&dq->refcount);
+        }
+    }
+
+send_events:
+    memset(&event_data, 0, sizeof(event_data));   
+    spin_lock(&dev->profile_lock);
+    dev_profile_set = dev->dev_profile_set;
+    spin_unlock(&dev->profile_lock);
+    if ( (dev->profile.swid_type[swid] == SX_KU_L2_TYPE_IB) &&
+         dev_profile_set ) {        
+        event_data.ib_swid_change.swid = swid;
+        event_data.ib_swid_change.dev_id = sx_dev_id;
+        sx_core_dispatch_event(dev, SX_DEV_EVENT_IB_SWID_UP, &event_data);
+
+        spin_lock(&dev->profile_lock);
+        first_ib_swid = dev->first_ib_swid;
+        dev->first_ib_swid = 0;
+        spin_unlock(&dev->profile_lock);
+        if (first_ib_swid) {
+            union sx_event_data tca_init_event_data;
+
+            memset(&tca_init_event_data, 0, sizeof(tca_init_event_data));
+            for (i = 0; i < NUMBER_OF_SWIDS; i++) {
+                if (dev->profile.ipoib_router_port_enable[i]) {
+                    tca_init_event_data.tca_init.swid[tca_init_event_data.tca_init.num_of_ib_swids++] = i;
+                }
+            }
+
+            if (tca_init_event_data.tca_init.num_of_ib_swids != 0) {
+                tca_init_event_data.tca_init.max_pkey = dev->profile.max_pkey;
+                sx_core_dispatch_event(dev, SX_DEV_EVENT_TYPE_TCA_INIT, &tca_init_event_data);
+            }
+        }
+    } else if (dev->profile.swid_type[swid] == SX_KU_L2_TYPE_ETH) {
+        sx_priv(dev)->swid_data[swid].eth_swid_data.synd = synd;
+        sx_priv(dev)->swid_data[swid].eth_swid_data.mac = mac;
+        event_data.eth_swid_up.swid = swid;
+        event_data.eth_swid_up.synd = synd;
+        event_data.eth_swid_up.mac = mac;
+        sx_core_dispatch_event(dev, SX_DEV_EVENT_ETH_SWID_UP, &event_data);
+    }    
+
+    sx_bitmap_set(&sx_priv(dev)->swid_bitmap, swid);
+    if (sx_priv(dev)->cq_table.cq_credit_thread &&
+        !sx_priv(dev)->cq_table.credit_thread_active) {
+        wake_up_process(sx_priv(dev)->cq_table.cq_credit_thread);
+        if (cq_thread_sched_priority != 0) {
+            struct sched_param param = { .sched_priority = cq_thread_sched_priority };
+
+            err = sched_setscheduler(sx_priv(dev)->cq_table.cq_credit_thread,
+                                     SCHED_FIFO, &param);
+            if (err) {
+                printk(KERN_INFO PFX "Failed setting RT prio %d to the "
+                       "cq_credit_thread, err = %d\n",
+                       cq_thread_sched_priority, err);
+            } else {
+                printk(KERN_INFO PFX "Successfully set the real time priority of the "
+                       "cq_credit_thread to %d\n", cq_thread_sched_priority);
+            }
+        }
+
+        sx_priv(dev)->cq_table.credit_thread_active = 1;
+    }
+
+out:
+    return err;
+}
+
+void sx_disable_swid(struct sx_dev *dev, u8 swid)
+{
+    int                 i;
+    u8                  dqn;
+    struct sx_dq       *dq;
+    union sx_event_data event_data;
+    u8                  dev_profile_set;
+
+    for (i = 0; i < NUMBER_OF_ETCLASSES; i++) {
+        dqn = dev->profile.tx_prof[swid][i].sdq;
+        dq = sx_priv(dev)->sdq_table.dq[dqn];
+        if (dq && (atomic_read(&dq->refcount) == 1)) {
+            sx_core_destroy_sdq(dev, dq);
+            sx_priv(dev)->sdq_table.dq[dqn] = NULL;
+        } else if (dq) {
+            atomic_dec(&dq->refcount);
+        }
+    }
+
+    for (i = 0; i < dev->profile.rdq_count[swid]; i++) {
+        dqn = dev->profile.rdq[swid][i];
+        dq = sx_priv(dev)->rdq_table.dq[dqn];
+        if (dq && (atomic_read(&dq->refcount) == 1)) {
+            sx_core_destroy_rdq(dev, dq);
+            sx_priv(dev)->rdq_table.dq[dqn] = NULL;
+        } else if (dq) {
+            atomic_dec(&dq->refcount);
+        }
+    }
+
+    spin_lock(&dev->profile_lock);
+    dev_profile_set = dev->dev_profile_set;
+    spin_unlock(&dev->profile_lock);
+    event_data.ib_swid_change.swid = swid;
+    event_data.ib_swid_change.dev_id = dev->profile.dev_id;
+    if (dev->profile.swid_type[swid] == SX_KU_L2_TYPE_IB && dev_profile_set ) {
+        sx_core_dispatch_event(dev, SX_DEV_EVENT_IB_SWID_DOWN, &event_data);
+    } else if (dev->profile.swid_type[swid] == SX_KU_L2_TYPE_ETH) {
+        sx_core_dispatch_event(dev, SX_DEV_EVENT_ETH_SWID_DOWN, &event_data);
+    }
+
+    sx_bitmap_free(&sx_priv(dev)->swid_bitmap, swid);
+}
+
+/*
+ *  Workaround for rdq stucked because of wqe_too_short error.
+ *  Force rdq size to be SX_MAX_MSG_SIZE on SX
+ */
+void __sx_adjust_rdq_size(struct sx_dev *dev)
+{
+    int i;
+
+    if ((sx_glb.profile.chip_type == SXD_CHIP_TYPE_UNKNOWN) ||
+        (sx_glb.profile.chip_type == SXD_CHIP_TYPE_SWITCHX_A1) ||
+        (sx_glb.profile.chip_type == SXD_CHIP_TYPE_SWITCHX_A2)) {
+        for (i = 0; i < NUMBER_OF_RDQS; i++) {
+            if (dev->profile.rdq_properties[i].entry_size != 0) {
+                dev->profile.rdq_properties[i].entry_size = SX_MAX_MSG_SIZE;
+            }
+        }
+    }
+}
+
+int sx_handle_set_profile(struct sx_dev *dev)
+{
+    int           err = 0;
+    struct sx_dq *sdq, *rdq;
+    u8            dqn;
+
+    __sx_adjust_rdq_size(dev);
+
+    err = check_valid_profile(dev, &dev->profile);
+    if (err) {
+        printk(KERN_ERR PFX "input profile is not valid\n");
+        goto out;
+    }
+
+    if (!sx_dpt_is_path_valid(dev->profile.dev_id, DPT_PATH_PCI_E)) {
+        printk(KERN_INFO PFX "PCIe path is not valid for device %u, "
+               "will not open EMAD DQs\n", dev->profile.dev_id);
+        goto out;
+    }
+
+    /* no need to open EMAD RDQ on IB only systems */
+    if ((dev->profile.pci_profile >= PCI_PROFILE_EN_SINGLE_SWID) &&
+        (sx_dpt_get_emad_path(dev->device_id) == DPT_PATH_PCI_E)) {
+        dqn = dev->profile.emad_tx_prof.sdq;
+        err = sx_core_create_sdq(dev, 1 << dev->dev_cap.log_max_sdq_sz, dqn, &sdq);
+        if (err) {
+            printk(KERN_ERR PFX "create EMAD sdq %d failed. err: %d\n",
+                   dqn, err);
+            goto out;
+        }
+
+        dqn = dev->profile.emad_rdq;
+        err = sx_core_create_rdq(dev, RDQ_NUMBER_OF_ENTRIES, dqn, &rdq);
+        if (err) {
+            printk(KERN_ERR PFX "create EMAD rdq %d failed. err: %d\n",
+                   dqn, err);
+            goto out;
+        }
+    }
+
+out: return err;
+}
+
+static int sx_core_get_hw_etclass_impl(struct isx_meta *meta, u8* hw_etclass)
+{
+    /* According to the SX PRM
+     * etclass should be set to the following value: (7 - Egress Tclass) */
+
+    *hw_etclass = 7 - meta->etclass;
+
+    return 0;
+}
+
+static int sx_core_get_hw_etclass_impl_spectrum(struct isx_meta *meta, u8* hw_etclass)
+{
+    /* According to the SX PRM
+     * etclass should be set to the following value: (7 - Egress Tclass) */
+
+    *hw_etclass = meta->etclass;
+
+    return 0;
+}
+
+static int sx_core_get_send_to_rp_as_data_supported(u8* send_to_rp_as_data_supported)
+{
+    *send_to_rp_as_data_supported = false;
+
+    return 0;
+}
+
+static int sx_core_get_send_to_rp_as_data_supported_spectrum(u8* send_to_rp_as_data_supported)
+{
+    *send_to_rp_as_data_supported = true;
+
+    return 0;
+}
+
+static int get_rp_vid_from_db(struct sx_dev *dev, struct completion_info *comp_info, u16 *vlan_id)
+{
+    struct sx_priv *dev_priv = sx_priv(dev);
+    uint16_t        local = 0;
+    uint16_t        port_lag_id = comp_info->sysport;
+    uint8_t         is_lag = comp_info->is_lag;
+
+    *vlan_id = 0;
+
+    if (is_lag) {
+        *vlan_id = dev_priv->lag_rp_vid[port_lag_id];
+    } else {
+        local = dev_priv->system_to_local_db[port_lag_id];
+        if (local > MAX_PHYPORT_NUM) {
+            printk(KERN_ERR PFX "Local %d is invalid. (MAX %d).\n",
+                   local, MAX_PHYPORT_NUM);
+            return -EINVAL;
+        }
+        *vlan_id = dev_priv->local_rp_vid[local];
+    }
+
+    return 0;
+}
+
+static int get_rp_vid_from_ci(struct sx_dev *dev, struct completion_info *comp_info, u16 *vlan_id)
+{
+    if (comp_info->is_tagged) {
+        *vlan_id = comp_info->vid;
+    }
+    else {
+        *vlan_id = 0;
+    }
+
+    return 0;
+}
+
+static int get_swid_from_db(struct sx_dev *dev, struct completion_info *comp_info, u8 *swid)
+{
+    enum sx_packet_type pkt_type = comp_info->pkt_type;
+    u8 is_lag = comp_info->is_lag;
+    u16 sysport_lag_id = comp_info->sysport;
+    u16 lag_port_id = comp_info->lag_subport;
+
+    u16 system_port, local_port, ib_port;
+
+    switch (pkt_type) {
+    case PKT_TYPE_ETH:
+    case PKT_TYPE_FCoETH:
+        if (is_lag) {
+            u16 lag_id = (sysport_lag_id >> 4) & 0xfff;
+            local_port = sx_priv(dev)->lag_member_to_local_db[lag_id][lag_port_id];
+        }
+        else{
+            system_port = sysport_lag_id;
+            local_port = sx_priv(dev)->system_to_local_db[system_port];
+        }
+
+        break;
+
+    case PKT_TYPE_IB_Raw: /* TODO: Extract qpn from IB Raw pkts */
+    case PKT_TYPE_IB_non_Raw:
+    case PKT_TYPE_FCoIB:
+    case PKT_TYPE_ETHoIB:
+        ib_port = (sysport_lag_id >> 4) & 0x7f;
+        local_port = sx_priv(dev)->ib_to_local_db[ib_port];
+        break;
+
+    default:
+        if (printk_ratelimit())
+            printk(KERN_WARNING PFX "Received packet type is FC, "
+                "and therefore unsupported right now\n");
+        return 0;
+    }
+
+    *swid = sx_priv(dev)->local_to_swid_db[local_port];
+
+    return 0;
+}
+
+static int get_swid_from_ci(struct sx_dev *dev, struct completion_info *comp_info, u8 *swid)
+{
+    *swid = comp_info->swid;
+
+    return 0;
+}
+
+/* Used for SX */
+static int sx_get_lag_mid(u16 lag_id, u16 *mid)
+{
+    if (mid)
+        *mid = lag_id + 0xC000 + sx_glb.profile.max_mid;
+    return 0;
+}
+
+/* Used for Spectrum */
+static int sdk_get_lag_mid(u16 lag_id, u16 *mid)
+{
+    if (mid)
+        *mid = lag_id + 0x100;
+    return 0;
+}
+
+struct dev_specific_cb spec_cb_sx_a1 = {
+    sx_core_get_hw_etclass_impl,      /* get_hw_etclass_cb   */
+    sx_build_isx_header_v0,           /* sx_build_isx_header_cb */
+    sx_get_sdq_from_profile,           /* sx_get_sdq_cb */
+    sx_core_get_send_to_rp_as_data_supported,
+    get_rp_vid_from_db,
+    get_swid_from_db,
+    sx_get_lag_mid,                /* get_lag_mid_cb */
+#ifdef CONFIG_SX_SGMII_PRESENT
+	sx_sgmii_build_cr_space_header_switchx
+#endif
+};
+struct dev_specific_cb spec_cb_sx_a2 = {
+    sx_core_get_hw_etclass_impl,      /* get_hw_etclass_cb   */
+    sx_build_isx_header_v0,           /* sx_build_isx_header_cb */
+    sx_get_sdq_from_profile,           /* sx_get_sdq_cb */
+    sx_core_get_send_to_rp_as_data_supported,
+    get_rp_vid_from_ci,
+    get_swid_from_db,
+    sx_get_lag_mid,                   /* get_lag_mid_cb */
+#ifdef CONFIG_SX_SGMII_PRESENT
+	sx_sgmii_build_cr_space_header_switchx
+#endif
+};
+struct dev_specific_cb spec_cb_pelican = {
+    sx_core_get_hw_etclass_impl_spectrum,      /* get_hw_etclass_cb   */
+    sx_build_isx_header_v0,                    /* sx_build_isx_header_cb */
+    sx_get_sdq_from_profile,                     /* sx_get_sdq_cb */
+    sx_core_get_send_to_rp_as_data_supported,
+    get_rp_vid_from_db,
+    get_swid_from_db,
+    NULL,                                      /* get_lag_mid_cb */
+#ifdef CONFIG_SX_SGMII_PRESENT
+	sx_sgmii_build_cr_space_header_spectrum
+#endif
+};
+struct dev_specific_cb spec_cb_spectrum = {
+    sx_core_get_hw_etclass_impl_spectrum,      /* get_hw_etclass_cb   */
+    sx_build_isx_header_v1,                     /* sx_build_isx_header_cb */
+    sx_get_sdq_per_traffic_type,                /* sx_get_sdq_cb */
+    sx_core_get_send_to_rp_as_data_supported_spectrum,
+    get_rp_vid_from_ci,
+    get_swid_from_ci,
+    sdk_get_lag_mid,                            /* get_lag_mid_cb */
+#ifdef CONFIG_SX_SGMII_PRESENT
+	sx_sgmii_build_cr_space_header_spectrum
+#endif
+};
+
+static int sx_core_dev_init_switchx_cb(struct sx_dev *dev, enum sxd_chip_types chip_type)
+{
+    int           err = 0;
+    unsigned long flags;
+
+    spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+
+    memset(&(sx_priv(dev)->dev_specific_cb), 0, sizeof(sx_priv(dev)->dev_specific_cb));
+
+    /* init specific and common callbacks per device revision */
+    switch (chip_type) {
+    case SXD_CHIP_TYPE_SWITCHX_A0:
+        sx_err(dev, "Cannot add device , the SDK does not support "
+               "SwitchX with revision A0\n");
+        err = -EINVAL;
+        break;
+
+    case SXD_CHIP_TYPE_SWITCHX_A1:
+        /* for A1 revision add specific cb */
+        sx_priv(dev)->dev_specific_cb = spec_cb_sx_a1;
+        break;
+
+    case SXD_CHIP_TYPE_SWITCHX_A2:
+        sx_priv(dev)->dev_specific_cb = spec_cb_sx_a2;
+        break;
+
+    case SXD_CHIP_TYPE_SWITCH_IB:
+    case SXD_CHIP_TYPE_SWITCH_IB2:
+        /* for pelican/eagle add specific cb */
+        sx_priv(dev)->dev_specific_cb = spec_cb_pelican;
+        break;
+
+    case SXD_CHIP_TYPE_SPECTRUM:
+        /* for condor add specific cb */
+        sx_priv(dev)->dev_specific_cb = spec_cb_spectrum;
+        break;
+
+    default:
+        err = -EINVAL;
+        sx_err(dev, "ERROR:hw_ver: 0x%x unsupported. \n",
+               chip_type);
+        break;
+    }
+
+    spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+
+    printk(KERN_DEBUG PFX "sx_core_dev_init_switchx_cb chip_type [%d]\n", chip_type);
+
+    return err;
+}
+
+static void sx_skb_destructor(struct sk_buff *skb)
+{
+#ifndef NO_PCI
+    struct sx_rsc *rsc;
+
+    memcpy(&rsc, skb->cb, sizeof(rsc));
+    up(&rsc->write_sem);
+#endif
+}
+
+static int sx_flush_dq(struct sx_dev *dev, struct sx_dq *dq)
+{
+    int           err;
+    unsigned long flags;
+    unsigned long end;
+
+    dq->is_flushing = 1;
+    err = sx_dq_modify_2err(dev, dq);
+    if (err) {
+        sx_warn(dev, "Failed to modify dq to error. "
+                "May cause resource leak\n");
+        goto out_mod;
+    }
+
+    end = jiffies + 5 * HZ;
+    spin_lock_irqsave(&dq->lock, flags);
+    while ((int)(dq->head - dq->tail) > 0) {
+        spin_unlock_irqrestore(&dq->lock, flags);
+        msleep(1000 / HZ);
+        if (time_after(jiffies, end)) {
+            spin_lock_irqsave(&dq->lock, flags);
+            break;
+        }
+
+        spin_lock_irqsave(&dq->lock, flags);
+    }
+    if ((int)(dq->head - dq->tail) > 0) {
+        err = -ETIMEDOUT;
+    } else {
+        err = 0;
+    }
+
+    dq->is_flushing = 0;
+    spin_unlock_irqrestore(&dq->lock, flags);
+
+out_mod:
+    return err;
+}
+
+static void sx_flush_dqs(struct sx_dev *dev, u8 send)
+{
+    struct sx_priv     *priv = sx_priv(dev);
+    int                 err;
+    int                 i;
+    int                 max = send ? dev->dev_cap.max_num_sdqs : dev->dev_cap.max_num_rdqs;
+    struct sx_dq_table *dq_table = send ?
+                                   &priv->sdq_table : &priv->rdq_table;
+
+    for (i = 0; i < max; i++) {
+        if (dq_table->dq[i]) {
+            err = sx_flush_dq(dev, dq_table->dq[i]);
+            if (err && dq_table->dq[i]->is_send) {
+                sx_warn(dev, "failed to flush dq %d. err %d\n", i, err);
+            }
+
+            sx_hw2sw_dq(dev, dq_table->dq[i]);
+        }
+    }
+}
+
+int sx_change_configuration(struct sx_dev *dev)
+{
+    int i;
+
+    spin_lock(&dev->profile_lock);
+    if (dev->profile_set == 1) {
+        dev->profile_set = 0;
+        spin_unlock(&dev->profile_lock);
+        /* we unregister the device first, so sx_ib resources will be
+         * cleaned (if there are such) because they might try to send
+         * packets during the unregister process */
+        sx_core_unregister_device(dev);
+        if (dev->pdev) {
+            sx_flush_dqs(dev, true);
+            sx_core_destroy_sdq_table(dev, false);
+            sx_flush_dqs(dev, false);
+            sx_core_destroy_rdq_table(dev, false);
+        }
+
+        for (i = 0; i < NUMBER_OF_SWIDS; i++) {
+            sx_bitmap_free(&sx_priv(dev)->swid_bitmap, i);
+        }
+
+        sx_core_register_device(dev);
+        goto out;
+    }
+
+    spin_unlock(&dev->profile_lock);
+out:
+    return 0;
+}
+
+/************************************************
+ *  Char device Functions
+ ***********************************************/
+
+/**
+ * This function opens the device for file operations.
+ * We support multiple file opens for the same device.
+ * Initialize an event list per file and keep it in the file context.
+ * The maximum size of the event list is constant.
+ * When the file is opened, we can't read or write to it until the
+ * profile is set with ioctl
+ *
+ * param[in] inode - the associated inode.
+ * param[in] filp - a pointer to the associated file.
+ *
+ * returns: 0 success
+ *        !0 error
+ */
+static int sx_core_open(struct inode *inode, struct file *filp)
+{
+    struct sx_rsc *file = NULL;
+
+#ifdef SX_DEBUG
+    printk(KERN_DEBUG PFX " sx_core_open() \n");
+#endif
+    SX_CORE_UNUSED_PARAM(inode);
+    file = kzalloc(sizeof(*file), GFP_KERNEL);
+    if (!file) {
+#ifdef SX_DEBUG
+        printk(KERN_DEBUG PFX " sx_core_open() \n");
+#endif
+        return -ENOMEM;
+    }
+
+    INIT_LIST_HEAD(&file->evlist.list);
+    file->evlist_size = 0;
+    spin_lock_init(&file->lock);
+    init_waitqueue_head(&file->poll_wait);
+    atomic_set(&file->multi_packet_read_enable, false);
+    atomic_set(&file->read_blocking_state, true);
+    sema_init(&file->write_sem, SX_WRITE_LIMIT);
+    filp->private_data = file; /* connect the fd with its resources */
+
+    return 0;
+}
+
+
+/**
+ * Send packets - EMADs, Ethernet packets. We copy the packets
+ * from user space, as is, and post them to the HW SDQ. The
+ * header is built by user space. SDQ number is calculated
+ * according to the profile. Buf is formatted according to
+ * ku_write struct. Count is the size of ku_write buffer
+ * (without the packets data). We support sending multiple
+ * packets in a single operation.
+ *
+ * param[in] filp  - a pointer to the associated file
+ * param[in] buf   - ku_write struct/s
+ * param[in] count - the number of bytes to send
+ * param[in] pos   - not in use
+ *
+ *
+ * returns: 0<=res - The size of the given buffer that was written
+ *          res<0  - Error
+ */
+static ssize_t sx_core_write(struct file *filp, const char __user *buf, size_t count, loff_t *pos)
+{
+    struct ku_write write_data;
+    int             err = 0;
+    int             user_buffer_copied_size = 0;
+    struct sk_buff *skb = NULL;
+    struct sx_dev  *dev = NULL;
+    struct sx_rsc  *rsc = filp->private_data;
+    int             i;
+
+    if ((count == 0) || (buf == NULL)) {
+        err = -EINVAL;
+        goto out;
+    }
+
+    while (user_buffer_copied_size + sizeof(write_data) <= count) {
+        err = copy_from_user((void*)&write_data,
+                             ((void*)buf) + user_buffer_copied_size,
+                             sizeof(write_data));
+        if (err) {
+            goto out;
+        }
+
+        if (((write_data.vec_entries != 0) && (write_data.iov == NULL)) ||
+            ((write_data.vec_entries == 0) && (write_data.iov != NULL))) {
+            err = -EINVAL;
+            goto out;
+        }
+
+        if (write_data.vec_entries == 0) {
+            break;
+        }
+
+        err = sx_dpt_get_sx_dev_by_id(write_data.meta.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_write: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+#if 0
+        if (dev && !dev->profile_set) {
+            printk(KERN_WARNING PFX "sx_core_write() cannot "
+                   "execute because the profile is not "
+                   "set\n");
+            err = -ENOEXEC;
+            goto out;
+        }
+#endif
+        err = check_valid_meta(dev, &write_data.meta);
+        if (err) {
+            printk(KERN_WARNING PFX "Cannot execute because meta "
+                   "is invalid\n");
+            goto out;
+        }
+
+        if (write_data.meta.type == SX_PKT_TYPE_LOOPBACK_CTL) {
+            err = sx_send_loopback(dev, &write_data, filp);
+            if (err) {
+                printk(KERN_WARNING PFX "sx_core_write: "
+                       "Failed seding loopback packet\n");
+                goto out;
+            }
+
+			if (dev) {
+				loopback_packets_counter++;
+            	dev->loopback_packets_counter++;
+			}
+			else if (sx_glb.tmp_dev_ptr) {
+                sx_glb.tmp_dev_ptr->loopback_packets_counter++;
+			}
+            user_buffer_copied_size += sizeof(write_data);
+            continue;
+        }
+
+        /* according to the PRM, emads should get "any ethernet swid" */
+        if ((write_data.meta.type == SX_PKT_TYPE_DROUTE_EMAD_CTL) ||
+            (write_data.meta.type == SX_PKT_TYPE_EMAD_CTL)) {
+            if (!dev || !dev->profile_set) {
+                write_data.meta.swid = 0;
+            } else {
+                for (i = 0; i < NUMBER_OF_SWIDS; i++) {
+                    if (dev->profile.swid_type[i] ==
+                        SX_KU_L2_TYPE_ETH) {
+                        write_data.meta.swid = i;
+                        break;
+                    }
+                }
+
+                if (i == NUMBER_OF_SWIDS) { /* no ETH swids found */
+                    printk(KERN_WARNING PFX "sx_core_write: Err: "
+                           "trying to send an emad from "
+                           "an IB only system\n");
+                    err = -EFAULT;
+                    write_data.meta.swid = 0;
+                    goto out;
+                }
+            }
+        }
+
+        user_buffer_copied_size += sizeof(write_data);
+        err = copy_buff_to_skb(&skb, &write_data, true);
+        if (err) {
+            goto out;
+        }
+
+        memcpy(skb->cb, &rsc, sizeof(rsc));
+        skb->destructor = sx_skb_destructor;
+#ifndef NO_PCI
+        down(&rsc->write_sem);
+#endif
+        err = sx_core_post_send(dev, skb, &write_data.meta);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_write: got error"
+                   " from sx_core_post_send\n");
+            /* we don't free the packet because sx_core_post_send free
+             * the packet in case of an error */
+            goto out;
+        }
+    }
+
+    SX_CORE_UNUSED_PARAM(pos);
+    return user_buffer_copied_size;
+
+out:
+#ifdef SX_DEBUG
+    printk(KERN_DEBUG PFX "sx_core_write: return "
+           "value is %d (error)\n", err);
+#endif
+    return err;
+}
+
+
+/**
+ * Read a bulk of packets up to count size (count includes packet and metadata)
+ * Format to buf according to ku_en_read struct.
+ * Read packets from the file event list.
+ * The event list holds pointers to cloned skbs.
+ * On each read, we free the skb.
+ * If multi-packets mode (see ioctl) is enabled,
+ * we read multiple packets up to count.
+ * If multi-packets mode is disabled, we read a single packet up to count.
+ *
+ * param[in] filp  - a pointer to the associated file
+ * param[in] buf   - the data buffer, ku_read struct
+ * param[in] count - the number of bites to send (the size of the buffer)
+ * param[in] pos   - not in use
+ *
+ *
+ * returns: The size of data which copied to the user buffer
+ *	   !0 error
+ */
+static ssize_t sx_core_read(struct file *filp, char __user *buf, size_t count, loff_t *pos)
+{
+    unsigned long     flags;
+    struct event_data edata_list;
+    struct sx_rsc    *file = filp->private_data;
+    int               multi_packet_read_enable = false;
+    int               read_blocking_state = true;
+    int               err;
+
+    SX_CORE_UNUSED_PARAM(pos);
+#ifdef SX_DEBUG
+    printk(KERN_DEBUG PFX " sx_core_read()\n");
+#endif
+    if ((count == 0) || (buf == NULL)) {
+        return -EINVAL;
+    }
+
+    spin_lock_irqsave(&file->lock, flags);
+    while (list_empty(&file->evlist.list)) {
+        spin_unlock_irqrestore(&file->lock, flags);
+        read_blocking_state = atomic_read(&file->read_blocking_state);
+        if (read_blocking_state == false) { /* non-blocking */
+            return -EAGAIN;
+        }
+
+        if (wait_event_interruptible(file->poll_wait,
+                                     !list_empty(&file->evlist.list))) {
+            return -ERESTARTSYS;
+        }
+
+        spin_lock_irqsave(&file->lock, flags);
+    }
+
+    multi_packet_read_enable = atomic_read(&file->multi_packet_read_enable);
+    INIT_LIST_HEAD(&edata_list.list);
+    err = get_edata_from_elist(&file->evlist_size, &edata_list,
+                         &file->evlist.list, count, multi_packet_read_enable);
+    spin_unlock_irqrestore(&file->lock, flags);
+
+    /* 	
+        not enough room for single packet (meta + packet)
+	but enough room for meta
+    */
+    if (err > 0) {
+        struct ku_read 	metadata;
+
+        memset(&metadata, 0, sizeof(metadata));
+        metadata.length = err;
+        if (copy_to_user(buf, &metadata, sizeof(metadata)))
+            return -EFAULT;
+        return 0;
+    }
+    /* not enough room for meta */
+    else if (err < 0) {	
+        return err;
+    }
+
+    return copy_edata_to_user(&edata_list, buf);
+}
+
+static int sx_core_handle_access_reg_ioctl(unsigned int cmd, unsigned long data)
+{
+    int            err = 0;
+    struct sx_dev *dev = NULL;
+
+    switch (cmd) {
+    case CTRL_CMD_ACCESS_REG_PSPA:
+    {
+        struct ku_access_pspa_reg pspa_reg_data;
+
+        err = copy_from_user(&pspa_reg_data, (void*)data,
+                             sizeof(pspa_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(pspa_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg PSPA: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_PSPA(dev, &pspa_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &pspa_reg_data,
+                           sizeof(pspa_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_QSPTC:
+    {
+        struct ku_access_qsptc_reg qsptc_reg_data;
+
+        err = copy_from_user(&qsptc_reg_data, (void*)data,
+                             sizeof(qsptc_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(qsptc_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg QSPTC: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_QSPTC(dev, &qsptc_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &qsptc_reg_data,
+                           sizeof(qsptc_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_QSTCT:
+    {
+        struct ku_access_qstct_reg qstct_reg_data;
+
+        err = copy_from_user(&qstct_reg_data, (void*)data,
+                             sizeof(qstct_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(qstct_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg QSTCT: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_QSTCT(dev, &qstct_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &qstct_reg_data,
+                           sizeof(qstct_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_PTYS:
+    {
+        struct ku_access_ptys_reg ptys_reg_data;
+
+        err = copy_from_user(&ptys_reg_data, (void*)data,
+                             sizeof(ptys_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(ptys_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg PTYS: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_PTYS(dev, &ptys_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &ptys_reg_data,
+                           sizeof(ptys_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_MHSR:
+    {
+        struct ku_access_mhsr_reg mhsr_reg_data;
+
+        err = copy_from_user(&mhsr_reg_data, (void*)data,
+                             sizeof(mhsr_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(mhsr_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg MHSR: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_MHSR(dev, &mhsr_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &mhsr_reg_data,
+                           sizeof(mhsr_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_PMLP:
+    {
+        struct ku_access_pmlp_reg pmlp_reg_data;
+
+        err = copy_from_user(&pmlp_reg_data, (void*)data,
+                             sizeof(pmlp_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(pmlp_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg PMLP: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_PMLP(dev, &pmlp_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &pmlp_reg_data,
+                           sizeof(pmlp_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_PLIB:
+    {
+        struct ku_access_plib_reg plib_reg_data;
+
+        err = copy_from_user(&plib_reg_data, (void*)data,
+                             sizeof(plib_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(plib_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg PLIB: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_PLIB(dev, &plib_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &plib_reg_data,
+                           sizeof(plib_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_SPZR:
+    {
+        struct ku_access_spzr_reg spzr_reg_data;
+        union sx_event_data       event_data;
+
+        err = copy_from_user(&spzr_reg_data, (void*)data,
+                             sizeof(spzr_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(spzr_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg SPZR: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_SPZR(dev, &spzr_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        /* If the user updates the node description of the local device
+        * through this path we should notify sx_ib about this change */
+        if (spzr_reg_data.spzr_reg.ndm && (spzr_reg_data.op_tlv.method == 2) &&
+            (sx_glb.sx_dpt.dpt_info[spzr_reg_data.dev_id].cmd_path == DPT_PATH_PCI_E)) {
+            event_data.node_desc_update.swid = spzr_reg_data.spzr_reg.swid;
+            memcpy(event_data.node_desc_update.NodeDescription, spzr_reg_data.spzr_reg.NodeDescription, 64);
+            sx_core_dispatch_event(dev, SX_DEV_EVENT_NODE_DESC_UPDATE, &event_data);
+        }
+
+        err = copy_to_user((void*)data, &spzr_reg_data,
+                           sizeof(spzr_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_PAOS:
+    {
+        struct ku_access_paos_reg paos_reg_data;
+
+        err = copy_from_user(&paos_reg_data, (void*)data,
+                             sizeof(paos_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(paos_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg PAOS: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_PAOS(dev, &paos_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &paos_reg_data,
+                           sizeof(paos_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_PLPC:
+     {
+         struct ku_access_plpc_reg plpc_reg_data;
+
+         err = copy_from_user(&plpc_reg_data, (void*)data,
+                              sizeof(plpc_reg_data));
+         if (err) {
+             goto out;
+         }
+
+         err = sx_dpt_get_cmd_sx_dev_by_id(plpc_reg_data.dev_id, &dev);
+         if (err) {
+             printk(KERN_WARNING PFX "sx_core_access_reg PLPC: "
+                    "Device doesn't exist. Aborting\n");
+             goto out;
+         }
+
+         err = sx_ACCESS_REG_PLPC(dev, &plpc_reg_data);
+         if (err) {
+             goto out;
+         }
+
+         err = copy_to_user((void*)data, &plpc_reg_data,
+                            sizeof(plpc_reg_data));
+         if (err) {
+             goto out;
+         }
+         break;
+     }
+
+    case CTRL_CMD_ACCESS_REG_PPLM:
+     {
+         struct ku_access_pplm_reg pplm_reg_data;
+
+         err = copy_from_user(&pplm_reg_data, (void*)data,
+                              sizeof(pplm_reg_data));
+         if (err) {
+             goto out;
+         }
+
+         err = sx_dpt_get_cmd_sx_dev_by_id(pplm_reg_data.dev_id, &dev);
+         if (err) {
+             printk(KERN_WARNING PFX "sx_core_access_reg PPLM: "
+                    "Device doesn't exist. Aborting\n");
+             goto out;
+         }
+
+         err = sx_ACCESS_REG_PPLM(dev, &pplm_reg_data);
+         if (err) {
+             goto out;
+         }
+
+         err = copy_to_user((void*)data, &pplm_reg_data,
+                            sizeof(pplm_reg_data));
+         if (err) {
+             goto out;
+         }
+         break;
+     }
+
+    case CTRL_CMD_ACCESS_REG_PMPC:
+    {
+        struct ku_access_pmpc_reg pmpc_reg_data;
+
+        err = copy_from_user(&pmpc_reg_data, (void*)data,
+                             sizeof(pmpc_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(pmpc_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg PMPC: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_PMPC(dev, &pmpc_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &pmpc_reg_data,
+                           sizeof(pmpc_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_PPSC:
+    {
+        struct ku_access_ppsc_reg ppsc_reg_data;
+
+        err = copy_from_user(&ppsc_reg_data, (void*)data,
+                             sizeof(ppsc_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(ppsc_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg PPSC: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_PPSC(dev, &ppsc_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &ppsc_reg_data,
+                           sizeof(ppsc_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_PMPR:
+    {
+        struct ku_access_pmpr_reg pmpr_reg_data;
+
+        err = copy_from_user(&pmpr_reg_data, (void*)data,
+                             sizeof(pmpr_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(pmpr_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg PMPR: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_PMPR(dev, &pmpr_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &pmpr_reg_data,
+                           sizeof(pmpr_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_PMTU:
+    {
+        struct ku_access_pmtu_reg pmtu_reg_data;
+
+        err = copy_from_user(&pmtu_reg_data, (void*)data,
+                             sizeof(pmtu_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(pmtu_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg PMTU: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_PMTU(dev, &pmtu_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &pmtu_reg_data,
+                           sizeof(pmtu_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_PELC:
+    {
+        struct ku_access_pelc_reg pelc_reg_data;
+
+        err = copy_from_user(&pelc_reg_data, (void*)data,
+                             sizeof(pelc_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(pelc_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg PELC: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_PELC(dev, &pelc_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &pelc_reg_data,
+                           sizeof(pelc_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_PLBF:
+    {
+        struct ku_access_plbf_reg plbf_reg_data;
+
+        err = copy_from_user(&plbf_reg_data, (void*)data,
+                             sizeof(plbf_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(plbf_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg PLBF: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_PLBF(dev, &plbf_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &plbf_reg_data,
+                           sizeof(plbf_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_SGCR:
+    {
+        struct ku_access_sgcr_reg sgcr_reg_data;
+
+        err = copy_from_user(&sgcr_reg_data, (void*)data,
+                             sizeof(sgcr_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(sgcr_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg SGCR: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_SGCR(dev, &sgcr_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &sgcr_reg_data,
+                           sizeof(sgcr_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_MSCI:
+    {
+        struct ku_access_msci_reg msci_reg_data;
+
+        err = copy_from_user(&msci_reg_data, (void*)data,
+                             sizeof(msci_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(msci_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg MSCI: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_MSCI(dev, &msci_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &msci_reg_data,
+                           sizeof(msci_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_SPAD:
+    {
+        struct ku_access_spad_reg spad_reg_data;
+
+        err = copy_from_user(&spad_reg_data, (void*)data,
+                             sizeof(spad_reg_data));
+        if (err) {
+            sx_err(dev, "CTRL_CMD_ACCESS_REG_SPAD: copy_from_user failed");
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(spad_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg SPAD: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_SPAD(dev, &spad_reg_data);
+        if (err) {
+            sx_err(dev, "CTRL_CMD_ACCESS_REG_SPAD: sx_ACCESS_REG_SPAD failed");
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &spad_reg_data,
+                           sizeof(spad_reg_data));
+        if (err) {
+            sx_err(dev, "CTRL_CMD_ACCESS_REG_SPAD: copy_to_user failed");
+            goto out;
+        }
+
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_HTGT:
+    {
+        struct ku_access_htgt_reg htgt_reg_data;
+
+        err = copy_from_user(&htgt_reg_data, (void*)data,
+                             sizeof(htgt_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(htgt_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg HTGT: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_HTGT(dev, &htgt_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &htgt_reg_data,
+                           sizeof(htgt_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_MFSC:
+    {
+        struct ku_access_mfsc_reg mfsc_reg_data;
+
+        err = copy_from_user(&mfsc_reg_data, (void*)data,
+                             sizeof(mfsc_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(mfsc_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg MFSC: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_MFSC(dev, &mfsc_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &mfsc_reg_data,
+                           sizeof(mfsc_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_MFSM:
+    {
+        struct ku_access_mfsm_reg mfsm_reg_data;
+
+        err = copy_from_user(&mfsm_reg_data, (void*)data,
+                             sizeof(mfsm_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(mfsm_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg MSFS: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_MFSM(dev, &mfsm_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &mfsm_reg_data,
+                           sizeof(mfsm_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_MFSL:
+    {
+        struct ku_access_mfsl_reg mfsl_reg_data;
+
+        err = copy_from_user(&mfsl_reg_data, (void*)data,
+                             sizeof(mfsl_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(mfsl_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg MFSL: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_MFSL(dev, &mfsl_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &mfsl_reg_data,
+                           sizeof(mfsl_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_PVLC:
+    {
+        struct ku_access_pvlc_reg pvlc_reg_data;
+
+        err = copy_from_user(&pvlc_reg_data, (void*)data,
+                             sizeof(pvlc_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(pvlc_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg PVLC: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_PVLC(dev, &pvlc_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &pvlc_reg_data,
+                           sizeof(pvlc_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_MCIA:
+    {
+        struct ku_access_mcia_reg mcia_reg_data;
+
+        err = copy_from_user(&mcia_reg_data, (void*)data,
+                             sizeof(mcia_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(mcia_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg MCIA: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_MCIA(dev, &mcia_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &mcia_reg_data,
+                           sizeof(mcia_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_HPKT:
+    {
+        struct ku_access_hpkt_reg hpkt_reg_data;
+
+        err = copy_from_user(&hpkt_reg_data, (void*)data,
+                             sizeof(hpkt_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(hpkt_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg HPKT: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_HPKT(dev, &hpkt_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &hpkt_reg_data,
+                           sizeof(hpkt_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_HCAP:
+    {
+        struct ku_access_hcap_reg hcap_reg_data;
+
+        err = copy_from_user(&hcap_reg_data, (void*)data,
+                             sizeof(hcap_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(hcap_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg HCAP: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_HCAP(dev, &hcap_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &hcap_reg_data,
+                           sizeof(hcap_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_HDRT:
+    {
+        struct ku_access_hdrt_reg hdrt_reg_data;
+
+        err = copy_from_user(&hdrt_reg_data, (void*)data,
+                             sizeof(hdrt_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(hdrt_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg HDRT: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_HDRT(dev, &hdrt_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &hdrt_reg_data,
+                           sizeof(hdrt_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_QPRT:
+    {
+        struct ku_access_qprt_reg qprt_reg_data;
+
+        err = copy_from_user(&qprt_reg_data, (void*)data,
+                             sizeof(qprt_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(qprt_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg QPRT: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_QPRT(dev, &qprt_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &qprt_reg_data,
+                           sizeof(qprt_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_MFCR:
+    {
+        struct ku_access_mfcr_reg mfcr_reg_data;
+
+        err = copy_from_user(&mfcr_reg_data, (void*)data,
+                             sizeof(mfcr_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(mfcr_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg MFCR: "
+                   "Device doesn't exist. Aborting\n");
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_MFCR(dev, &mfcr_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &mfcr_reg_data,
+                           sizeof(mfcr_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_FORE:
+    {
+        struct ku_access_fore_reg fore_reg_data;
+
+        err = copy_from_user(&fore_reg_data, (void*)data,
+                             sizeof(fore_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(fore_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_FORE(dev, &fore_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &fore_reg_data,
+                           sizeof(fore_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_MTCAP:
+    {
+        struct ku_access_mtcap_reg mtcap_reg_data;
+
+        err = copy_from_user(&mtcap_reg_data, (void*)data,
+                             sizeof(mtcap_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(mtcap_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+        err = sx_ACCESS_REG_MTCAP(dev, &mtcap_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &mtcap_reg_data,
+                           sizeof(mtcap_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_MTMP:
+    {
+        struct ku_access_mtmp_reg mtmp_reg_data;
+
+        err = copy_from_user(&mtmp_reg_data, (void*)data,
+                             sizeof(mtmp_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(mtmp_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_MTMP(dev, &mtmp_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &mtmp_reg_data,
+                           sizeof(mtmp_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_MTWE:
+    {
+        struct ku_access_mtwe_reg mtwe_reg_data;
+
+        err = copy_from_user(&mtwe_reg_data, (void*)data,
+                             sizeof(mtwe_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(mtwe_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_MTWE(dev, &mtwe_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &mtwe_reg_data,
+                           sizeof(mtwe_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_MMDIO:
+    {
+        struct ku_access_mmdio_reg mmdio_reg_data;
+
+        err = copy_from_user(&mmdio_reg_data, (void*)data,
+                             sizeof(mmdio_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(mmdio_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_MMDIO(dev, &mmdio_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &mmdio_reg_data,
+                           sizeof(mmdio_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_MMIA:
+    {
+        struct ku_access_mmia_reg mmia_reg_data;
+
+        err = copy_from_user(&mmia_reg_data, (void*)data,
+                             sizeof(mmia_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(mmia_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_MMIA(dev, &mmia_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &mmia_reg_data,
+                           sizeof(mmia_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_MFPA:
+    {
+        struct ku_access_mfpa_reg mfpa_reg_data;
+
+        err = copy_from_user(&mfpa_reg_data, (void*)data,
+                             sizeof(mfpa_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(mfpa_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_MFPA(dev, &mfpa_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &mfpa_reg_data,
+                           sizeof(mfpa_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_MFBE:
+    {
+        struct ku_access_mfbe_reg mfbe_reg_data;
+
+        err = copy_from_user(&mfbe_reg_data, (void*)data,
+                             sizeof(mfbe_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(mfbe_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_MFBE(dev, &mfbe_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &mfbe_reg_data,
+                           sizeof(mfbe_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_MFBA:
+    {
+        struct ku_access_mfba_reg mfba_reg_data;
+
+        err = copy_from_user(&mfba_reg_data, (void*)data,
+                             sizeof(mfba_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(mfba_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_MFBA(dev, &mfba_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &mfba_reg_data,
+                           sizeof(mfba_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_QCAP:
+    {
+        struct ku_access_qcap_reg qcap_reg_data;
+
+        err = copy_from_user(&qcap_reg_data, (void*)data,
+                             sizeof(qcap_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(qcap_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_QCAP(dev, &qcap_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &qcap_reg_data,
+                           sizeof(qcap_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_RAW:
+    {
+        struct ku_access_raw_reg raw_reg_data;
+
+        err = copy_from_user(&raw_reg_data, (void*)data,
+                             sizeof(raw_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(raw_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_RAW(dev, &raw_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &raw_reg_data,
+                           sizeof(raw_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_RAW_BUFF:
+    {
+        struct ku_access_reg_raw_buff raw_buff_data;
+
+        err = copy_from_user(&raw_buff_data, (void*)data,
+                             sizeof(raw_buff_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(raw_buff_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_RAW_BUFF(dev, &raw_buff_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &raw_buff_data,
+                           sizeof(raw_buff_data));
+        if (err) {
+            goto out;
+        }
+
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_PMAOS:
+    {
+        struct ku_access_pmaos_reg pmaos_reg_data;
+
+        err = copy_from_user(&pmaos_reg_data, (void*)data,
+                             sizeof(pmaos_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(pmaos_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_PMAOS(dev, &pmaos_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &pmaos_reg_data,
+                           sizeof(pmaos_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_MFM:
+    {
+        struct ku_access_mfm_reg mfm_reg_data;
+
+        err = copy_from_user(&mfm_reg_data, (void*)data,
+                             sizeof(mfm_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(mfm_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_MFM(dev, &mfm_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &mfm_reg_data,
+                           sizeof(mfm_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_MGIR:
+    {
+        struct ku_access_mgir_reg mgir_reg_data;
+
+        err = copy_from_user(&mgir_reg_data, (void*)data,
+                             sizeof(mgir_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(mgir_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_MGIR(dev, &mgir_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &mgir_reg_data,
+                           sizeof(mgir_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_SSPR:
+    {
+        struct ku_access_sspr_reg sspr_reg_data;
+
+        err = copy_from_user(&sspr_reg_data, (void*)data,
+                             sizeof(sspr_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(sspr_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_SSPR(dev, &sspr_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &sspr_reg_data,
+                           sizeof(sspr_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_PPAD:
+    {
+        struct ku_access_ppad_reg ppad_reg_data;
+
+        err = copy_from_user(&ppad_reg_data, (void*)data,
+                             sizeof(ppad_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(ppad_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_PPAD(dev, &ppad_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &ppad_reg_data,
+                           sizeof(ppad_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_SPMCR:
+    {
+        struct ku_access_spmcr_reg spmcr_reg_data;
+
+        err = copy_from_user(&spmcr_reg_data, (void*)data,
+                             sizeof(spmcr_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(spmcr_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_SPMCR(dev, &spmcr_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &spmcr_reg_data,
+                           sizeof(spmcr_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_PBMC:
+    {
+        struct ku_access_pbmc_reg pbmc_reg_data;
+
+        err = copy_from_user(&pbmc_reg_data, (void*)data,
+                             sizeof(pbmc_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(pbmc_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_PBMC(dev, &pbmc_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &pbmc_reg_data,
+                           sizeof(pbmc_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_PPTB:
+    {
+        struct ku_access_pptb_reg pptb_reg_data;
+
+        err = copy_from_user(&pptb_reg_data, (void*)data,
+                             sizeof(pptb_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(pptb_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_PPTB(dev, &pptb_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &pptb_reg_data,
+                           sizeof(pptb_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_SPVID:
+    {
+        struct ku_access_spvid_reg spvid_reg_data;
+
+        err = copy_from_user(&spvid_reg_data, (void*)data,
+                             sizeof(spvid_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(spvid_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_SPVID(dev, &spvid_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &spvid_reg_data,
+                           sizeof(spvid_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_SFGC:
+    {
+        struct ku_access_sfgc_reg sfgc_reg_data;
+
+        err = copy_from_user(&sfgc_reg_data, (void*)data,
+                             sizeof(sfgc_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(sfgc_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_SFGC(dev, &sfgc_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &sfgc_reg_data,
+                           sizeof(sfgc_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_OEPFT:
+    {
+        struct ku_access_oepft_reg oepft_reg_data;
+
+        err = copy_from_user(&oepft_reg_data, (void*)data,
+                             sizeof(oepft_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(oepft_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_OEPFT(dev, &oepft_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &oepft_reg_data,
+                           sizeof(oepft_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_MRSR:
+    {
+        struct ku_access_mrsr_reg mrsr_reg_data;
+
+        err = copy_from_user(&mrsr_reg_data, (void*)data,
+                             sizeof(mrsr_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(mrsr_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_MRSR(dev, &mrsr_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &mrsr_reg_data,
+                           sizeof(mrsr_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    case CTRL_CMD_ACCESS_REG_MPSC:
+    {
+        struct ku_access_mpsc_reg mpsc_reg_data;
+
+        err = copy_from_user(&mpsc_reg_data, (void*)data,
+                             sizeof(mpsc_reg_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_get_cmd_sx_dev_by_id(mpsc_reg_data.dev_id, &dev);
+        if (err) {
+            printk(KERN_WARNING PFX "sx_core_access_reg , line %d "
+                   "Device doesn't exist. Aborting\n", __LINE__);
+            goto out;
+        }
+
+        err = sx_ACCESS_REG_MPSC(dev, &mpsc_reg_data);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &mpsc_reg_data,
+                           sizeof(mpsc_reg_data));
+        if (err) {
+            goto out;
+        }
+        break;
+    }
+
+    default:
+        return -EINVAL;
+    }
+
+out:
+
+    return err;
+}
+
+/**
+ * This function is used to perform some configuration commands
+ * on the local device.
+ *
+ * param[in] inode - the associated inode
+ * param[in] filp  - a pointer to the associated file
+ * param[in] cmd   - the ioctl command to be performed
+ * param[in] data  - a data to be passed to the invoked function
+ *
+ * returns: 0 success
+ *	   !0 error
+ */
+static long sx_core_ioctl(struct file *filp, unsigned int cmd, unsigned long data)
+{
+    struct sx_rsc            *file = filp->private_data;
+    int                       err = 0;
+    unsigned long             flags;
+    struct sx_dev            *dev = NULL;
+    struct ku_dpt_path_add    dpt_path_add_data;
+    struct ku_dpt_path_modify dpt_path_modify_data;
+
+    SX_CORE_UNUSED_PARAM(filp);
+
+#ifdef SX_DEBUG
+    printk(KERN_DEBUG PFX " sx_core_ioctl() cmd=[%d]\n", cmd);
+#endif
+
+#if 0 /* Not sure we need this check */
+    spin_lock(&sx_glb.pci_devs_lock);
+    if (list_empty(&sx_glb.pci_devs_list)) {
+        spin_unlock(&sx_glb.pci_devs_lock);
+        printk(KERN_WARNING PFX "sx_core_ioctl: "
+               "NO PCI Device is present. Aborting\n");
+        return -ENXIO;
+    }
+
+    spin_unlock(&sx_glb.pci_devs_lock);
+#endif
+
+    if ((cmd >= CTRL_CMD_ACCESS_REG_MIN) &&
+        (cmd <= CTRL_CMD_ACCESS_REG_MAX)) {
+        return sx_core_handle_access_reg_ioctl(cmd, data);
+    }
+
+    dev = sx_glb.tmp_dev_ptr; /* TODO: temporary, should use device id instead */
+    if (!dev) {
+        printk(KERN_WARNING PFX "sx_core_ioctl: "
+               "Device doesn't exist. Aborting\n");
+        return -ENXIO;
+    }
+
+    switch (cmd) {
+    case CTRL_CMD_GET_CAPABILITIES:
+        err = copy_to_user((void*)data, (void*)&dev->dev_cap,
+                           sizeof(dev->dev_cap));
+        if (err) {
+            goto out;
+        }
+        break;
+
+    case CTRL_CMD_RESET:
+        err = sx_change_configuration(dev);
+        if (err) {
+            goto out;
+        }
+        break;
+
+    case CTRL_CMD_PCI_DEVICE_RESTART:
+    {
+        struct sx_dev *tmp_dev = NULL, *curr_dev = NULL;
+        u8             found = 0;
+
+        printk(KERN_DEBUG PFX "ioctl device restart called "
+               "for device %lu\n", data);
+        spin_lock(&sx_glb.pci_devs_lock);
+        list_for_each_entry_safe(curr_dev, tmp_dev, &sx_glb.pci_devs_list, list) {
+            if (curr_dev->device_id == data) {
+                found = 1;
+                break;
+            }
+        }
+
+        spin_unlock(&sx_glb.pci_devs_lock);
+        if (!found) {
+            sx_warn(dev, "ioctl device restart: the device "
+                    "wasn't found\n");
+            err = -ENODEV;
+            goto out;
+        }
+
+        err = sx_restart_one_pci(curr_dev->pdev);
+        if (err) {
+            goto out;
+        }
+
+        break;
+    }
+
+    case CTRL_CMD_SET_PCI_PROFILE:
+        printk(KERN_DEBUG PFX "ioctl set pci profile called\n");
+        spin_lock(&dev->profile_lock);
+        if (dev->profile_set == 1) {
+            printk(KERN_WARNING PFX "Err: cannot set "
+                   "profile twice\n");
+            spin_unlock(&dev->profile_lock);
+            err = -EINVAL;
+            goto out;
+        }
+
+        dev->profile_set = 1;
+        spin_unlock(&dev->profile_lock);
+
+        err = copy_from_user((void*)&dev->profile,
+                             (void*)data, sizeof(dev->profile));
+        if (err) {
+            err = -ENOMEM;
+            goto out;
+        }
+
+        err = sx_handle_set_profile(dev);
+        if (err) {
+            spin_lock(&dev->profile_lock);
+            dev->profile_set = 0;
+            spin_unlock(&dev->profile_lock);
+            goto out;
+        }
+
+        break;
+
+    case CTRL_CMD_GET_PCI_PROFILE:
+    {
+        struct ku_get_pci_profile pci_prof;
+        spin_lock(&dev->profile_lock);
+        if (!dev->profile_set) {
+            printk(KERN_WARNING PFX "Err: profile is not set\n");
+            spin_unlock(&dev->profile_lock);
+            err = -EINVAL;
+            goto out;
+        }
+
+        pci_prof.pci_profile = dev->profile.pci_profile;
+        spin_unlock(&dev->profile_lock);
+        err = copy_to_user((struct ku_get_pci_profile *)data,
+                           &pci_prof, sizeof(pci_prof));
+        break;
+    }
+
+    case CTRL_CMD_ADD_SYND:
+    case CTRL_CMD_REMOVE_SYND:
+    {
+        struct ku_synd_ioctl      ku;
+        union ku_filter_critireas critireas;
+        enum l2_type              listener_type = L2_TYPE_DONT_CARE;
+
+        err = copy_from_user((void*)&ku, (void*)data, sizeof(ku));
+        if (err) {
+            goto out;
+        }
+
+        err = check_valid_ku_synd(&ku);
+        if (err) {
+            goto out;
+        }
+
+        memset(&critireas, 0, sizeof(critireas));
+
+        switch (ku.type) {
+        case SX_KU_L2_TYPE_DONT_CARE:
+            listener_type = L2_TYPE_DONT_CARE;
+            critireas.dont_care.sysport = ku.critireas.dont_care.sysport;
+            break;
+
+        case SX_KU_L2_TYPE_IB:
+            listener_type = L2_TYPE_IB;
+            critireas.ib.qpn = ku.critireas.ib.qpn;
+            break;
+
+        case SX_KU_L2_TYPE_ETH:
+            listener_type = L2_TYPE_ETH;
+            critireas.eth.dmac = ku.critireas.eth.dmac;
+            critireas.eth.ethtype = ku.critireas.eth.ethtype;
+            critireas.eth.emad_tid = ku.critireas.eth.emad_tid;
+            critireas.eth.from_rp = ku.critireas.eth.from_rp;
+            critireas.eth.from_bridge = ku.critireas.eth.from_bridge;
+            break;
+
+        case SX_KU_L2_TYPE_FC:
+            listener_type = L2_TYPE_FC;
+            critireas.fc.TBD = ku.critireas.fc.TBD;
+            break;
+
+        default:
+            printk(KERN_ERR PFX "Err: invalid listener type : %d \n",
+                   ku.type);
+            err = -EINVAL;
+            goto out;
+        }
+    if (ku.channel_type == SX_KU_USER_CHANNEL_TYPE_L2_NETDEV) {
+		printk("%s: Adding SX_KU_USER_CHANNEL_TYPE_L2_NETDEV\n", __func__);
+		if (dev->profile.swid_type[ku.swid] != SX_KU_L2_TYPE_ETH) {
+			printk("%s: Adding L2 netdev on non ethernet device is not implemented\n", __func__);
+			err = -EFAULT;
+		}
+		
+		if (cmd == CTRL_CMD_ADD_SYND) 
+			err = sx_core_add_synd_l2(ku.swid, ku.syndrome_num, dev);
+		else 
+			err = sx_core_remove_synd_l2(ku.swid, ku.syndrome_num, dev);
+
+		if (err)
+			goto out;
+	} else if (ku.channel_type == SX_KU_USER_CHANNEL_TYPE_L3_NETDEV) {
+            if (dev->profile.swid_type[ku.swid] == SX_KU_L2_TYPE_ETH) {
+                /* L3 traps registration */
+                if (cmd == CTRL_CMD_ADD_SYND) {
+                    err = sx_core_add_synd_l3(ku.swid, ku.syndrome_num, dev);
+                } else {
+                    err = sx_core_remove_synd_l3(ku.swid, ku.syndrome_num, dev);
+                }
+
+                if (err) {
+                    goto out;
+                }
+            } else {
+                /* IPoIB traps registration */
+                if (cmd == CTRL_CMD_ADD_SYND) {
+                    err = sx_core_add_synd_ipoib(ku.swid, ku.syndrome_num, dev);
+                } else {
+                    err = sx_core_remove_synd_ipoib(ku.swid, ku.syndrome_num, dev);
+                }
+
+                if (err) {
+                    goto out;
+                }
+            }
+        } else if (ku.channel_type == SX_KU_USER_CHANNEL_TYPE_FD) {
+            if ((ku.type == SX_KU_L2_TYPE_ETH) &&
+                ((sx_glb.tmp_dev_ptr->profile.swid_type[ku.swid] == SX_KU_L2_TYPE_ETH) ||
+                 (sx_glb.tmp_dev_ptr->profile.swid_type[ku.swid] == SX_KU_L2_TYPE_ROUTER_PORT))) {
+                ku.swid = SWID_NUM_DONT_CARE;
+            } else if ((ku.type == SX_KU_L2_TYPE_DONT_CARE) &&
+                       (ku.critireas.dont_care.sysport == SYSPORT_DONT_CARE_VALUE)) {
+                if ((sx_glb.tmp_dev_ptr->profile.swid_type[ku.swid] == SX_KU_L2_TYPE_ETH) ||
+                    (sx_glb.tmp_dev_ptr->profile.swid_type[ku.swid] == SX_KU_L2_TYPE_ROUTER_PORT)) {
+                    ku.swid = SWID_NUM_DONT_CARE;
+                    ku.type = SX_KU_L2_TYPE_ETH;
+                } else if (sx_glb.tmp_dev_ptr->profile.swid_type[ku.swid] == SX_KU_L2_TYPE_IB) {
+                    ku.type = SX_KU_L2_TYPE_IB;
+                }
+            }
+
+            if (cmd == CTRL_CMD_ADD_SYND) {
+                err = sx_core_add_synd(ku.swid, ku.syndrome_num,
+                                       listener_type, ku.is_default,
+                                       critireas, sx_cq_handler, filp,
+                                       CHECK_DUP_ENABLED_E, dev);
+            } else {
+                err = sx_core_remove_synd(ku.swid, ku.syndrome_num,
+                                          listener_type, ku.is_default,
+                                          critireas, filp, dev);
+            }
+
+            if (err) {
+                goto out;
+            }
+        } else if (ku.channel_type == SX_KU_USER_CHANNEL_TYPE_L2_TUNNEL) {
+            if (cmd == CTRL_CMD_ADD_SYND) {
+                sx_priv(dev)->l2_tunnel_params = ku.l2_tunnel_params;
+                err = sx_core_add_synd(ku.swid, ku.syndrome_num,
+                                       listener_type, ku.is_default,
+                                       critireas, sx_l2_tunnel_handler, dev,
+                                       CHECK_DUP_DISABLED_E, dev);
+            } else {
+                err = sx_core_remove_synd(ku.swid, ku.syndrome_num,
+                                          listener_type, ku.is_default,
+                                          critireas, dev, dev);
+            }
+
+            if (err) {
+                goto out;
+            }
+        } else {
+            return -EINVAL;
+        }
+
+        break;
+    }
+
+    case CTRL_CMD_MULTI_PACKET_ENABLE:
+        if ((data != false) && (data != true)) {
+            printk(KERN_WARNING PFX "ioctl MULTI_PACKET_ENABLE: " \
+                   "error data = %lu > 1\n", data);
+            return -EINVAL;
+        }
+
+        atomic_set(&file->multi_packet_read_enable, data);
+        break;
+
+    case CTRL_CMD_BLOCKING_ENABLE:
+        if ((data != false) && (data != true)) {
+            return -EINVAL;
+        }
+
+        atomic_set(&file->read_blocking_state, data);
+        break;
+
+    case CTRL_CMD_RAISE_EVENT:
+        /* filp is the listener context when registering through the char device */
+        err = sx_raise_event(dev, (void*)data, filp);
+        if (err) {
+            goto out;
+        }
+
+        break;
+
+    case CTRL_CMD_ENABLE_SWID:
+    {
+        struct ku_swid_details swid_data;
+
+        err = copy_from_user(&swid_data, (void*)data,
+                             sizeof(swid_data));
+        if (err) {
+            goto out;
+        }
+
+        printk(KERN_DEBUG PFX "ioctl enable_swid called with swid %d\n", swid_data.swid);
+        spin_lock(&dev->profile_lock);
+        if (!dev->profile_set || (swid_data.swid >= NUMBER_OF_SWIDS)
+            || (dev->profile.swid_type[swid_data.swid] ==
+                SX_KU_L2_TYPE_DONT_CARE) ||
+            sx_bitmap_test(&sx_priv(dev)->swid_bitmap,
+                           swid_data.swid)) {
+            spin_unlock(&dev->profile_lock);
+            return -EINVAL;
+        }
+
+        spin_unlock(&dev->profile_lock);
+        err = sx_enable_swid(dev, swid_data.dev_id, swid_data.swid, swid_data.iptrap_synd, swid_data.mac);
+        if (err) {
+            goto out;
+        }
+
+        break;
+    }
+
+    case CTRL_CMD_DISABLE_SWID:
+    {
+        struct ku_swid_details swid_data;
+
+        printk(KERN_DEBUG PFX "ioctl disable_swid called\n");
+        err = copy_from_user(&swid_data, (void*)data,
+                             sizeof(swid_data));
+        if (err) {
+            goto out;
+        }
+
+        spin_lock(&dev->profile_lock);
+        if (!dev->profile_set || (swid_data.swid >= NUMBER_OF_SWIDS) ||
+            !sx_bitmap_test(&sx_priv(dev)->swid_bitmap,
+                            swid_data.swid)) {
+            spin_unlock(&dev->profile_lock);
+            return -EINVAL;
+        }
+        spin_unlock(&dev->profile_lock);
+        sx_disable_swid(dev, swid_data.swid);
+        break;
+    }
+
+    case CTRL_CMD_GET_SYNDROME_STATUS:
+    {
+        struct ku_synd_query_ioctl *synd_query = NULL;
+        struct ku_synd_query_ioctl  tmp_synd_query;
+        u8                          is_registered = 0;
+
+        err = copy_from_user(&tmp_synd_query, (void*)data,
+                             sizeof(tmp_synd_query));
+        if (err) {
+            goto out;
+        }
+
+        if (tmp_synd_query.syndrome_num > NUM_HW_SYNDROMES) {
+            err = -EINVAL;
+            goto out;
+        }
+
+        spin_lock_irqsave(&sx_glb.listeners_lock, flags);
+        if (!list_empty(&sx_glb.listeners_db[tmp_synd_query.syndrome_num].list)) {
+            is_registered = 1;
+        }
+        spin_unlock_irqrestore(&sx_glb.listeners_lock, flags);
+
+        synd_query = (struct ku_synd_query_ioctl *)data;
+        err = copy_to_user(&synd_query->is_registered, &is_registered,
+                           sizeof(is_registered));
+        if (err) {
+            goto out;
+        }
+
+        break;
+    }
+
+    case CTRL_CMD_QUERY_FW:
+    {
+        struct ku_query_fw query_fw;
+        err = copy_from_user(&query_fw, (struct ku_query_fw *)data,
+                             sizeof(query_fw));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_QUERY_FW(dev, &query_fw);
+        if (err) {
+            goto out;
+        }
+        err = copy_to_user((void*)data, &query_fw, sizeof(query_fw));
+        if (err) {
+            goto out;
+        }
+
+        break;
+    }
+
+    case CTRL_CMD_QUERY_BOARD_INFO:
+    {
+        struct ku_query_board_info board_info;
+
+        board_info.vsd_vendor_id = dev->vsd_vendor_id;
+        memcpy(board_info.board_id, dev->board_id,
+               sizeof(dev->board_id));
+        break;
+    }
+
+    case CTRL_CMD_SET_DEVICE_PROFILE:
+    {
+        struct ku_profile profile;
+        struct ku_swid_config *swid_config_type_arr = 
+                &profile.swid0_config_type;
+        u8  swid;
+        
+        err = copy_from_user(&profile, (void*)data, sizeof(profile));
+        if (err) {
+            goto out;
+        }
+
+        if (profile.dev_id != DEFAULT_DEVICE_ID) {
+            err = sx_SET_PROFILE(dev, &profile);
+            if (err) {
+                goto out;
+            }
+        }
+
+        /*
+         * if chip is PCI attached set the global profile and
+         * create ib devices if needed
+         */
+        if (sx_dpt_is_dev_pci_attached(profile.dev_id)){
+            sx_glb.profile = profile;
+            
+            /*
+             * set events for all IB enabled swids if
+             * swid is enabled
+             * */
+            for (swid=0; swid<NUMBER_OF_SWIDS; swid++){
+                /* if swid isnt enable skip it */
+                if (!sx_bitmap_test(&sx_priv(dev)->swid_bitmap, swid)) {
+                    continue;
+                }
+
+                /* check the swid is IB in device profile */
+                if (swid_config_type_arr[swid].type != KU_SWID_TYPE_INFINIBAND){
+                    continue;
+                }
+
+                spin_lock(&dev->profile_lock);
+                if (dev->dev_profile_set != 0) {
+                    spin_unlock(&dev->profile_lock);
+                    break;
+                }
+                dev->dev_profile_set = 1;
+                spin_unlock(&dev->profile_lock);
+
+                err = sx_send_enable_ib_swid_events(dev, swid);
+                if (err) {
+                    spin_lock(&dev->profile_lock);
+                    dev->dev_profile_set = 0;
+                    spin_unlock(&dev->profile_lock);
+                    goto out;
+                }
+            }
+        }
+
+        break;
+    }
+
+    case CTRL_CMD_GET_DEVICE_PROFILE:
+    {
+        struct ku_profile profile;
+
+        err = sx_GET_PROFILE(dev, &profile);
+        if (err) {
+            goto out;
+        }
+
+        err = copy_to_user((void*)data, &profile, sizeof(profile));
+        if (err) {
+            goto out;
+        }
+
+        break;
+    }
+
+    case CTRL_CMD_ADD_DEV_PATH:
+
+        err = copy_from_user(&dpt_path_add_data, (void*)data,
+                             sizeof(dpt_path_add_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_add_dev_path(dpt_path_add_data.dev_id,
+                                  dpt_path_add_data.path_type,
+                                  dpt_path_add_data.path_info,
+                                  dpt_path_add_data.is_local);
+        if (err) {
+            goto out;
+        }
+
+        break;
+
+    case CTRL_CMD_REMOVE_DEV_PATH:
+
+        err = copy_from_user(&dpt_path_add_data, (void*)data,
+                             sizeof(dpt_path_add_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_remove_dev_path(dpt_path_add_data.dev_id,
+                                     dpt_path_add_data.path_type);
+        if (err) {
+            goto out;
+        }
+        break;
+
+    case CTRL_CMD_REMOVE_DEV:
+
+        err = copy_from_user(&dpt_path_add_data, (void*)data,
+                             sizeof(dpt_path_add_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_remove_dev(dpt_path_add_data.dev_id, 0 );
+        if (err) {
+            goto out;
+        }
+        break;
+
+    case CTRL_CMD_SET_CMD_PATH:
+
+        err = copy_from_user(&dpt_path_modify_data, (void*)data,
+                             sizeof(dpt_path_modify_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_set_cmd_path(dpt_path_modify_data.dev_id,
+                                  dpt_path_modify_data.path_type);
+        if (err) {
+            goto out;
+        }
+        break;
+
+    case CTRL_CMD_SET_EMAD_PATH:
+        err = copy_from_user(&dpt_path_modify_data, (void*)data,
+                             sizeof(dpt_path_modify_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_set_emad_path(dpt_path_modify_data.dev_id,
+                                   dpt_path_modify_data.path_type);
+        if (err) {
+            goto out;
+        }
+
+        break;
+
+    case CTRL_CMD_SET_MAD_PATH:
+        err = copy_from_user(&dpt_path_modify_data, (void*)data,
+                             sizeof(dpt_path_modify_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_set_mad_path(dpt_path_modify_data.dev_id,
+                                  dpt_path_modify_data.path_type);
+        if (err) {
+            goto out;
+        }
+
+        break;
+
+    case CTRL_CMD_SET_CR_ACCESS_PATH:
+        err = copy_from_user(&dpt_path_modify_data, (void*)data,
+                             sizeof(dpt_path_modify_data));
+        if (err) {
+            goto out;
+        }
+
+        err = sx_dpt_set_cr_access_path(dpt_path_modify_data.dev_id,
+                                        dpt_path_modify_data.path_type);
+        if (err) {
+            goto out;
+        }
+
+        break;
+
+    case CTRL_CMD_GET_SWID_2_RDQ:
+    {
+        struct ku_swid_2_rdq_query swid_2_rdq;
+
+        if (!dev->profile_set) {
+            return -EFAULT;
+        }
+
+        err = copy_from_user(&swid_2_rdq, (void*)data,
+                             sizeof(swid_2_rdq));
+        if (err) {
+            goto out;
+        }
+
+        if ((dev->profile.swid_type[swid_2_rdq.swid] ==
+             SX_KU_L2_TYPE_DONT_CARE) ||
+            !sx_bitmap_test(&sx_priv(dev)->swid_bitmap,
+                            swid_2_rdq.swid)) {
+            err = -EINVAL;
+            goto out;
+        }
+
+        swid_2_rdq.rdq = dev->profile.rdq[swid_2_rdq.swid][0];
+        err = copy_to_user((void*)data, &swid_2_rdq,
+                           sizeof(swid_2_rdq));
+        if (err) {
+            goto out;
+        }
+
+        break;
+    }
+
+    case CTRL_CMD_TRAP_FILTER_ADD:
+    {
+        struct ku_trap_filter_data filter_data;
+        int                        i, idx = -1;
+
+        err = copy_from_user(&filter_data, (void*)data,
+                             sizeof(filter_data));
+        if (err) {
+            goto out;
+        }
+
+        if (filter_data.trap_id >= NUM_HW_SYNDROMES) {
+            printk(KERN_ERR PFX "Received TRAP ID %d "
+                   "is invalid\n", filter_data.trap_id);
+            err = -EINVAL;
+            goto out;
+        }
+
+        spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+        if (filter_data.is_lag) {
+            if (filter_data.lag_id >= MAX_LAG_NUM) {
+                printk(KERN_ERR PFX "Received LAG ID 0x%x "
+                       "is invalid\n",
+                       filter_data.lag_id);
+                spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+                err = -EINVAL;
+                goto out;
+            }
+
+            for (i = 0; i < MAX_LAG_PORTS_IN_FILTER; i++) {
+                if (sx_priv(dev)->lag_filter_db[filter_data.trap_id][i] ==
+                    filter_data.lag_id) {
+                    printk(KERN_ERR PFX "Received LAG ID %d "
+                           "is already in the filter list "
+                           "of trap ID 0x%x\n",
+                           filter_data.lag_id,
+                           filter_data.trap_id);
+                    spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+                    err = -EEXIST;
+                    goto out;
+                }
+
+                if (sx_priv(dev)->lag_filter_db[filter_data.trap_id][i] ==
+                    LAG_ID_INVALID) {
+                    idx = i;
+                }
+            }
+
+            if (idx == -1) {
+                printk(KERN_ERR PFX "Cannot add LAG ID 0x%x to trap %d filter "
+                       "because DB filter DB for this trap is full\n",
+                       filter_data.lag_id, filter_data.trap_id);
+                err = -EFAULT;
+            } else {
+                sx_priv(dev)->lag_filter_db[filter_data.trap_id][idx] =
+                    filter_data.lag_id;
+                printk(KERN_INFO PFX "LAG ID %u was added to filter list "
+                       "of trap ID 0x%x\n", filter_data.lag_id,
+                       filter_data.trap_id);
+            }
+        } else {
+            for (i = 0; i < MAX_SYSTEM_PORTS_IN_FILTER; i++) {
+                if (sx_priv(dev)->sysport_filter_db[filter_data.trap_id][i] ==
+                    filter_data.sysport) {
+                    printk(KERN_ERR PFX "Received system port 0x%x "
+                           "is already in the filter list "
+                           "of trap ID 0x%x\n",
+                           filter_data.sysport,
+                           filter_data.trap_id);
+                    spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+                    err = -EEXIST;
+                    goto out;
+                }
+
+                if (sx_priv(dev)->sysport_filter_db[filter_data.trap_id][i] == 0) {
+                    idx = i;
+                }
+            }
+
+            if (idx == -1) {
+                printk(KERN_ERR PFX "Cannot add system port 0x%x to trap %d filter "
+                       "because DB filter DB for this trap is full\n",
+                       filter_data.sysport, filter_data.trap_id);
+                err = -EFAULT;
+            } else {
+                sx_priv(dev)->sysport_filter_db[filter_data.trap_id][idx] =
+                    filter_data.sysport;
+                printk(KERN_INFO PFX "system port 0x%x was added to filter "
+                       "list of trap ID 0x%x\n", filter_data.sysport,
+                       filter_data.trap_id);
+            }
+        }
+
+        spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+        break;
+    }
+
+    case CTRL_CMD_TRAP_FILTER_REMOVE:
+    {
+        struct ku_trap_filter_data filter_data;
+        int                        i;
+
+        err = copy_from_user(&filter_data, (void*)data,
+                             sizeof(filter_data));
+        if (err) {
+            goto out;
+        }
+
+        if (filter_data.trap_id >= NUM_HW_SYNDROMES) {
+            printk(KERN_ERR PFX "Received TRAP ID %d "
+                   "is invalid\n", filter_data.trap_id);
+            err = -EINVAL;
+            goto out;
+        }
+
+        spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+        if (filter_data.is_lag) {
+            if (filter_data.lag_id >= MAX_LAG_NUM) {
+                printk(KERN_ERR PFX "Received LAG ID 0x%x "
+                       "is invalid\n",
+                       filter_data.lag_id);
+                spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+                err = -EINVAL;
+                goto out;
+            }
+
+            for (i = 0; i < MAX_LAG_PORTS_IN_FILTER; i++) {
+                if (sx_priv(dev)->lag_filter_db[filter_data.trap_id][i] ==
+                    filter_data.lag_id) {
+                    sx_priv(dev)->lag_filter_db[filter_data.trap_id][i] =
+                        LAG_ID_INVALID;
+                    printk(KERN_INFO PFX "LAG ID %u was removed from filter list "
+                           "of trap ID 0x%x\n", filter_data.lag_id,
+                           filter_data.trap_id);
+                    break;
+                }
+            }
+
+            if (i == MAX_LAG_PORTS_IN_FILTER) {
+                printk(KERN_ERR PFX "Cannot find LAG ID 0x%x in trap ID %d filter DB\n",
+                       filter_data.lag_id, filter_data.trap_id);
+                err = -EINVAL;
+            }
+        } else {
+            for (i = 0; i < MAX_SYSTEM_PORTS_IN_FILTER; i++) {
+                if (sx_priv(dev)->sysport_filter_db[filter_data.trap_id][i] ==
+                    filter_data.sysport) {
+                    sx_priv(dev)->sysport_filter_db[filter_data.trap_id][i] = 0;
+                    printk(KERN_INFO PFX "system port 0x%x was removed from filter "
+                           "list of trap ID 0x%x\n", filter_data.sysport,
+                           filter_data.trap_id);
+                    break;
+                }
+            }
+
+            if (i == MAX_SYSTEM_PORTS_IN_FILTER) {
+                printk(KERN_ERR PFX "Cannot find system port 0x%x in trap ID %d "
+                       "filter DB\n", filter_data.sysport, filter_data.trap_id);
+                err = -EINVAL;
+            }
+        }
+
+        spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+        break;
+    }
+
+    case CTRL_CMD_TRAP_FILTER_REMOVE_ALL:
+    {
+        struct ku_trap_filter_data filter_data;
+        int                        i;
+
+        err = copy_from_user(&filter_data, (void*)data,
+                             sizeof(filter_data));
+        if (err) {
+            goto out;
+        }
+
+        if (filter_data.trap_id >= NUM_HW_SYNDROMES) {
+            printk(KERN_ERR PFX "Received TRAP ID %d "
+                   "is invalid\n", filter_data.trap_id);
+            err = -EINVAL;
+            goto out;
+        }
+
+        spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+
+        for (i = 0; i < MAX_LAG_PORTS_IN_FILTER; i++) {
+            sx_priv(dev)->lag_filter_db[filter_data.trap_id][i] = LAG_ID_INVALID;
+        }
+
+        for (i = 0; i < MAX_SYSTEM_PORTS_IN_FILTER; i++) {
+            sx_priv(dev)->sysport_filter_db[filter_data.trap_id][i] = 0;
+        }
+
+        spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+        printk(KERN_INFO PFX "Removed all ports and LAGs from the filter list "
+               "of trap ID %d\n", filter_data.trap_id);
+        break;
+    }
+
+    case CTRL_CMD_SET_DEFAULT_VID:
+    {
+        struct ku_default_vid_data default_vid_data;
+
+        err = copy_from_user(&default_vid_data, (void*)data,
+                             sizeof(default_vid_data));
+        if (err) {
+            goto out;
+        }
+
+        spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+        if (default_vid_data.is_lag) {
+            if (default_vid_data.lag_id >= MAX_LAG_NUM) {
+                printk(KERN_ERR PFX "Received LAG ID 0x%x "
+                       "is invalid\n",
+                       default_vid_data.lag_id);
+                err = -EINVAL;
+                spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+                goto out;
+            }
+
+            sx_priv(dev)->pvid_lag_db[default_vid_data.lag_id] =
+                default_vid_data.default_vid;
+        } else {
+            sx_priv(dev)->pvid_sysport_db[default_vid_data.sysport] =
+                default_vid_data.default_vid;
+        }
+
+        spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+        break;
+    }
+
+    case CTRL_CMD_SET_VID_MEMBERSHIP:
+    {
+        struct ku_vid_membership_data vid_data;
+
+        err = copy_from_user(&vid_data, (void*)data,
+                             sizeof(vid_data));
+        if (err) {
+            goto out;
+        }
+
+        if (vid_data.vid >= MAX_VLAN_NUM) {
+            printk(KERN_ERR PFX "Received VID %d "
+                   "is invalid\n", vid_data.vid);
+            err = -EINVAL;
+            goto out;
+        }
+
+        spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+        if (vid_data.is_lag) {
+            if (vid_data.lag_id >= MAX_LAG_NUM) {
+                printk(KERN_ERR PFX "Received LAG ID 0x%x "
+                       "is invalid\n",
+                       vid_data.lag_id);
+                err = -EINVAL;
+                spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+                goto out;
+            }
+
+            sx_priv(dev)->lag_vtag_mode[vid_data.lag_id][vid_data.vid] =
+                vid_data.is_tagged;
+        } else {
+            if (vid_data.phy_port > MAX_PHYPORT_NUM) {
+                printk(KERN_ERR PFX "Phy_port %d isn invalid. (MAX %d)\n",
+                       vid_data.phy_port, MAX_PHYPORT_NUM);
+                err = -EINVAL;
+                spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+                goto out;
+            }
+            sx_priv(dev)->port_vtag_mode[vid_data.phy_port][vid_data.vid] =
+                vid_data.is_tagged;
+        }
+        spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+        break;
+    }
+
+    case CTRL_CMD_SET_PRIO_TAGGING:
+    {
+        struct ku_prio_tagging_data prio_tag_data;
+
+        err = copy_from_user(&prio_tag_data, (void*)data,
+                             sizeof(prio_tag_data));
+        if (err) {
+            goto out;
+        }
+
+        spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+        if (prio_tag_data.is_lag) {
+            if (prio_tag_data.lag_id >= MAX_LAG_NUM) {
+                printk(KERN_ERR PFX "Received LAG ID 0x%x "
+                       "is invalid\n",
+                       prio_tag_data.lag_id);
+                err = -EINVAL;
+                spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+                goto out;
+            }
+
+            sx_priv(dev)->port_prio_tagging_mode[prio_tag_data.lag_id] =
+                prio_tag_data.is_prio_tagged;
+        } else {
+            if (prio_tag_data.phy_port > MAX_PHYPORT_NUM) {
+                printk(KERN_ERR PFX "Phy_port %d is invalid. (MAX %d)\n",
+                       prio_tag_data.phy_port, MAX_PHYPORT_NUM);
+                err = -EINVAL;
+                spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+                goto out;
+            }
+            sx_priv(dev)->port_prio_tagging_mode[prio_tag_data.phy_port] =
+                prio_tag_data.is_prio_tagged;
+        }
+
+        spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+        break;
+    }
+
+    case CTRL_CMD_SET_PRIO_TO_TC:
+    {
+        struct ku_prio_to_tc_data prio_to_tc_data;
+
+        err = copy_from_user(&prio_to_tc_data, (void*)data,
+                             sizeof(prio_to_tc_data));
+        if (err) {
+            goto out;
+        }
+
+        if (prio_to_tc_data.priority > MAX_PRIO_NUM) {
+            printk(KERN_ERR PFX "Received PRIO %d "
+                   "is invalid (MAX %d). \n",
+                   prio_to_tc_data.priority, MAX_PRIO_NUM);
+            err = -EINVAL;
+            goto out;
+        }
+
+        if (prio_to_tc_data.traffic_class > (NUMBER_OF_ETCLASSES - 1)) {
+            printk(KERN_ERR PFX "Received TC %d "
+                   "is invalid (MAX %d).\n",
+                   prio_to_tc_data.traffic_class, (NUMBER_OF_ETCLASSES - 1));
+            err = -EINVAL;
+            goto out;
+        }
+
+        spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+
+        if (prio_to_tc_data.is_lag) {
+            if (prio_to_tc_data.lag_id >= MAX_LAG_NUM) {
+                printk(KERN_ERR PFX "Received LAG ID 0x%x "
+                       "is invalid\n",
+                       prio_to_tc_data.lag_id);
+                err = -EINVAL;
+                spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+                goto out;
+            }
+
+            sx_priv(dev)->lag_prio2tc[prio_to_tc_data.lag_id][prio_to_tc_data.priority] =
+                prio_to_tc_data.traffic_class;
+        } else {
+            if (prio_to_tc_data.phy_port > MAX_PHYPORT_NUM) {
+                printk(KERN_ERR PFX "Received Local %d "
+                       "is invalid. (MAX %d).\n",
+                       prio_to_tc_data.phy_port, MAX_PHYPORT_NUM);
+                err = -EINVAL;
+                spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+                goto out;
+            }
+
+            sx_priv(dev)->port_prio2tc[prio_to_tc_data.phy_port][prio_to_tc_data.priority] =
+                prio_to_tc_data.traffic_class;
+        }
+
+        spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+        break;
+    }
+
+    case CTRL_CMD_SET_LOCAL_PORT_TO_SWID:
+    {
+        struct ku_local_port_swid_data local_port_swid_data;
+
+        err = copy_from_user(&local_port_swid_data, (void*)data,
+                             sizeof(local_port_swid_data));
+        if (err) {
+            goto out;
+        }
+
+        if (local_port_swid_data.local_port > MAX_PHYPORT_NUM) {
+            printk(KERN_ERR PFX "Received Local port 0x%x "
+                   "is invalid (max. %d) \n",
+                   local_port_swid_data.local_port, MAX_PHYPORT_NUM);
+            err = -EINVAL;
+            goto out;
+        }
+
+        sx_priv(dev)->local_to_swid_db[local_port_swid_data.local_port] =
+            local_port_swid_data.swid;
+
+        #ifdef SX_DEBUG
+        printk(KERN_DEBUG PFX " sx_ioctl() (PSPA) LOC_PORT_TO_SWID lp:%d, swid: %d \n",
+               local_port_swid_data.local_port, local_port_swid_data.swid);
+        #endif
+
+        break;
+    }
+
+    case CTRL_CMD_SET_IB_TO_LOCAL_PORT:
+    {
+        struct ku_ib_local_port_data ib_local_port_data;
+
+        err = copy_from_user(&ib_local_port_data, (void*)data,
+                             sizeof(ib_local_port_data));
+        if (err) {
+            goto out;
+        }
+
+        if (ib_local_port_data.local_port > MAX_PHYPORT_NUM) {
+            printk(KERN_ERR PFX "Received Local port 0x%x "
+                   "is invalid (max. %d) \n",
+                   ib_local_port_data.local_port, MAX_PHYPORT_NUM);
+            err = -EINVAL;
+            goto out;
+        }
+
+        if (ib_local_port_data.ib_port > MAX_IBPORT_NUM) {
+            printk(KERN_ERR PFX "Received IB port 0x%x "
+                   "is invalid (max. %d) \n",
+                   ib_local_port_data.ib_port, MAX_IBPORT_NUM);
+            err = -EINVAL;
+            goto out;
+        }
+
+        sx_priv(dev)->ib_to_local_db[ib_local_port_data.ib_port] =
+            ib_local_port_data.local_port;
+
+        #ifdef SX_DEBUG
+        printk(KERN_DEBUG PFX " sx_ioctl() (PLIB) IB_TO_LOCAL_PORT ib_p:%d, lc_p:%d \n",
+               ib_local_port_data.ib_port, ib_local_port_data.local_port);
+        #endif
+
+        break;
+    }
+
+    case CTRL_CMD_SET_SYSTEM_TO_LOCAL_PORT:
+    {
+        struct ku_system_local_port_data system_local_port_data;
+
+        err = copy_from_user(&system_local_port_data, (void*)data,
+                             sizeof(system_local_port_data));
+        if (err) {
+            goto out;
+        }
+
+        if (system_local_port_data.local_port > MAX_PHYPORT_NUM) {
+            printk(KERN_ERR PFX "Received Local port 0x%x "
+                   "is invalid (max. %d) \n",
+                   system_local_port_data.local_port, MAX_PHYPORT_NUM);
+            err = -EINVAL;
+            goto out;
+        }
+
+	spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+        sx_priv(dev)->system_to_local_db[system_local_port_data.system_port] =
+		system_local_port_data.local_port;
+	sx_priv(dev)->local_to_system_db[system_local_port_data.local_port] = 
+		system_local_port_data.system_port;
+	spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+
+#ifdef SX_DEBUG
+        printk(KERN_DEBUG PFX " sx_ioctl() (SSPR) SYSTEM_TO_LOCAL_PORT sys_p:0x%x, lc_p:%d \n",
+               system_local_port_data.system_port, system_local_port_data.local_port);
+#endif
+
+        break;
+    }
+
+    case CTRL_CMD_SET_PORT_RP_MODE:
+    {
+        struct ku_port_rp_mode_data port_rp_mode_data;
+        uint16_t                    local_port;
+        uint8_t                     lag_port_index;
+
+        err = copy_from_user(&port_rp_mode_data, (void*)data,
+                             sizeof(port_rp_mode_data));
+        if (err) {
+            goto out;
+        }
+
+        spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+
+        if (port_rp_mode_data.is_lag) {
+            if (port_rp_mode_data.lag_id >= MAX_LAG_NUM) {
+                printk(KERN_ERR PFX "Received LAG ID 0x%x "
+                       "is invalid\n",
+                       port_rp_mode_data.lag_id);
+                err = -EINVAL;
+                spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+                goto out;
+            }
+
+            sx_priv(dev)->lag_is_rp[port_rp_mode_data.lag_id] =
+                port_rp_mode_data.is_rp;
+            sx_priv(dev)->lag_rp_vid[port_rp_mode_data.lag_id] =
+                port_rp_mode_data.vlan_id;
+            sx_priv(dev)->lag_rp_rif[port_rp_mode_data.lag_id][port_rp_mode_data.vlan_id] =
+                port_rp_mode_data.rif_id;
+            /* If opcode = create = 0, set IS_RP value,
+             * else opcode = delete = 1, set DONT_CARE value */
+            sx_priv(dev)->lag_rp_rif_valid[port_rp_mode_data.lag_id][port_rp_mode_data.vlan_id] =
+                    port_rp_mode_data.opcode ? IS_RP_DONT_CARE_E : IS_RP_FROM_RP_E;
+
+            for (lag_port_index = 0; lag_port_index < MAX_LAG_MEMBERS_NUM; lag_port_index++) {
+                local_port =
+                    sx_priv(dev)->lag_member_to_local_db[port_rp_mode_data.lag_id][lag_port_index];
+                /* if the LAG is RP than assign swid 0 to this local port ,
+                 * because we want all RP traffic to arrive with swid 0 */
+                if (local_port != 0) {
+                    if (port_rp_mode_data.is_rp) {
+                        sx_priv(dev)->local_to_swid_db[local_port] = ROUTER_PORT_SWID;
+                    } else {
+                        sx_priv(dev)->local_to_swid_db[local_port] = 255;
+                    }
+                }
+            }
+        } else {
+            local_port = sx_priv(dev)->system_to_local_db[port_rp_mode_data.sysport];
+            if (local_port > MAX_PHYPORT_NUM) {
+                printk(KERN_ERR PFX "Received Local %d "
+                       "is invalid. (MAX %d).\n",
+                       local_port, MAX_PHYPORT_NUM);
+                err = -EINVAL;
+                spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+                goto out;
+            }
+            sx_priv(dev)->local_is_rp[local_port] = port_rp_mode_data.is_rp;
+            sx_priv(dev)->local_rp_vid[local_port] = port_rp_mode_data.vlan_id;
+            sx_priv(dev)->port_rp_rif[local_port][port_rp_mode_data.vlan_id] =
+                    port_rp_mode_data.rif_id;
+            /* If opcode = create = 0, set IS_RP value,
+             * else opcode = delete = 1, set DONT_CARE value */
+            sx_priv(dev)->port_rp_rif_valid[local_port][port_rp_mode_data.vlan_id] =
+                    port_rp_mode_data.opcode ? IS_RP_DONT_CARE_E : IS_RP_FROM_RP_E;
+            if (port_rp_mode_data.is_rp) {
+                sx_priv(dev)->local_to_swid_db[local_port] = ROUTER_PORT_SWID;
+            } else {
+                sx_priv(dev)->local_to_swid_db[local_port] = 255;
+            }
+        }
+
+        #ifdef SX_DEBUG
+        printk(KERN_DEBUG PFX " sx_ioctl() (RITR) RP_MODE is_lag:%d,lid:%d,sys_p:0x%x,is_rp:%d \n",
+               port_rp_mode_data.is_lag,
+               port_rp_mode_data.lag_id,
+               port_rp_mode_data.sysport,
+               port_rp_mode_data.is_rp);
+        #endif
+
+        spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+        break;
+    }
+
+    case CTRL_CMD_SET_LOCAL_PORT_TO_LAG:
+    {
+        struct ku_local_port_to_lag_data local_to_lag_data;
+        uint16_t                         lag_id;
+        uint16_t                         lag_port_index;
+
+        err = copy_from_user(&local_to_lag_data, (void*)data,
+                             sizeof(local_to_lag_data));
+        if (err) {
+            goto out;
+        }
+
+        if (local_to_lag_data.lag_id >= MAX_LAG_NUM) {
+            printk(KERN_ERR PFX "Received LAG ID 0x%x "
+                   "is invalid\n", local_to_lag_data.lag_id);
+            err = -EINVAL;
+            goto out;
+        }
+
+        if (local_to_lag_data.lag_port_index >= MAX_LAG_MEMBERS_NUM) {
+            printk(KERN_ERR PFX "Received LAG port index 0x%x "
+                   "is invalid\n", local_to_lag_data.lag_port_index);
+            err = -EINVAL;
+            goto out;
+        }
+
+        if (local_to_lag_data.local_port > MAX_PHYPORT_NUM) {
+            printk(KERN_ERR PFX "Received Local port 0x%x "
+                   "is invalid (max. %d) \n",
+                   local_to_lag_data.local_port, MAX_PHYPORT_NUM);
+            err = -EINVAL;
+            goto out;
+        }
+
+        lag_id = local_to_lag_data.lag_id;
+        lag_port_index = local_to_lag_data.lag_port_index;
+        if ((lag_id >= MAX_LAG_NUM) ||
+            (lag_port_index >= MAX_LAG_MEMBERS_NUM)) {
+            printk(KERN_ERR PFX "Received LAG ID 0x%x or LAG_INDEX %d "
+                   "is invalid\n", lag_id, lag_port_index);
+            err = -EINVAL;
+            goto out;
+        }
+
+        if (local_to_lag_data.is_lag) {
+            /* Adding the port to LAG */
+            sx_priv(dev)->lag_member_to_local_db[lag_id][lag_port_index] =
+                local_to_lag_data.local_port;
+            /* if the LAG is RP than assign swid 0 to this local port ,
+             * because we want all RP traffic to arrive with swid 0 */
+            if (sx_priv(dev)->lag_is_rp[lag_id]) {
+                sx_priv(dev)->local_to_swid_db[local_to_lag_data.local_port] = ROUTER_PORT_SWID;
+            }
+        } else {
+            /* Removing the port from LAG */
+            sx_priv(dev)->lag_member_to_local_db[lag_id][lag_port_index] = 0;
+
+            /* if the LAG is RP than assign swid 255 to this local port ,
+             * because we want all RP traffic to arrive with swid 0 */
+            if (sx_priv(dev)->lag_is_rp[lag_id]) {
+                sx_priv(dev)->local_to_swid_db[local_to_lag_data.local_port] = 255;
+            }
+        }
+
+#ifdef SX_DEBUG
+        printk(KERN_DEBUG PFX " sx_ioctl() (SLCOR) LOCAL_PORT_TO_LAG is_lag:%d,lid:%d,port_id:%x,loc_port:%d \n",
+               local_to_lag_data.is_lag,
+               lag_id,
+               lag_port_index,
+               local_to_lag_data.local_port);
+#endif
+
+        break;
+    }
+
+    case CTRL_CMD_SET_RDQ_RATE_LIMITER:
+    {
+        struct ku_set_rdq_rate_limiter rate_limiter_params;
+        int                            cqn;
+
+        printk(KERN_DEBUG PFX "ioctl CTRL_CMD_SET_RDQ_RATE_LIMITER called\n");
+        if (!dev->pdev) {
+            printk(KERN_DEBUG PFX "will not set rate limiter since there's no  PCI device\n");
+            goto out;
+        }
+
+        err = copy_from_user(&rate_limiter_params, (void*)data,
+                             sizeof(rate_limiter_params));
+        if (err) {
+            goto out;
+        }
+
+        if (rate_limiter_params.rdq >= NUMBER_OF_RDQS) {
+            printk(KERN_WARNING PFX "Cannot set the rate limiter, RDQ value (%u) is not valid\n",
+                   rate_limiter_params.rdq);
+            err = -EINVAL;
+            goto out;
+        }
+
+        cqn = rate_limiter_params.rdq + NUMBER_OF_SDQS;
+        if (rate_limiter_params.use_limiter) {
+            sx_priv(dev)->cq_table.rl_time_interval =
+                max((int)rate_limiter_params.time_interval, 50);
+            sx_priv(dev)->cq_table.cq_rl_params[cqn].interval_credit =
+                rate_limiter_params.interval_credit;
+            sx_priv(dev)->cq_table.cq_rl_params[cqn].max_cq_credit =
+                rate_limiter_params.max_credit;
+        }
+
+        sx_priv(dev)->cq_table.cq_rl_params[cqn].use_limiter =
+            rate_limiter_params.use_limiter;
+        if (!rate_limiter_params.use_limiter) {
+            /* If the RDQ is not allocated we can finish here */
+            if (!sx_bitmap_test(&sx_priv(dev)->rdq_table.bitmap,
+                                rate_limiter_params.rdq)) {
+                goto out;
+            }
+
+            sx_priv(dev)->cq_table.cq_rl_params[cqn].curr_cq_credit = 0;
+            /* The CQ might not be armed if it ran out of credits */
+            sx_cq_arm(sx_priv(dev)->cq_table.cq[cqn]);
+        }
+
+        if (!sx_priv(dev)->cq_table.cq_credit_thread &&
+            rate_limiter_params.use_limiter) {
+            char kth_name[20];
+
+            sprintf(kth_name, "cq_credit_thread");
+            sx_priv(dev)->cq_table.cq_credit_thread =
+                kthread_create(sx_cq_credit_thread_handler,
+                               (void*)sx_priv(dev), kth_name);
+            if (IS_ERR(sx_priv(dev)->cq_table.cq_credit_thread)) {
+                printk(KERN_ERR PFX "Failed creating the CQ credit thread\n");
+                return -ENOMEM;
+            } else {
+                printk(KERN_INFO PFX "cq_credit_thread has been created\n");
+            }
+        }
+
+        /* start the CQ credit task */
+        if (rate_limiter_params.use_limiter &&
+            sx_bitmap_test(&sx_priv(dev)->cq_table.bitmap, cqn) &&
+            !sx_priv(dev)->cq_table.credit_thread_active) {
+            wake_up_process(sx_priv(dev)->cq_table.cq_credit_thread);
+            if (cq_thread_sched_priority != 0) {
+                struct sched_param param = { .sched_priority = cq_thread_sched_priority };
+
+                err = sched_setscheduler(sx_priv(dev)->cq_table.cq_credit_thread,
+                                         SCHED_FIFO, &param);
+                if (err) {
+                    printk(KERN_INFO PFX "Failed setting RT prio %d to the "
+                           "cq_credit_thread, err = %d\n",
+                           cq_thread_sched_priority, err);
+                } else {
+                    printk(KERN_INFO PFX "Successfully set the real time priority of the "
+                           "cq_credit_thread to %d\n", cq_thread_sched_priority);
+                }
+            }
+
+
+            sx_priv(dev)->cq_table.credit_thread_active = 1;
+        }
+
+        if (rate_limiter_params.use_limiter) {
+            printk(KERN_DEBUG PFX "Added a rate limiter to RDQ %d: "
+                   "interval_credit=%u, "
+                   "max_credit=%u,"
+                   "time_interval=%u\n",
+                   rate_limiter_params.rdq,
+                   sx_priv(dev)->cq_table.cq_rl_params[cqn].interval_credit,
+                   sx_priv(dev)->cq_table.cq_rl_params[cqn].max_cq_credit,
+                   sx_priv(dev)->cq_table.rl_time_interval);
+        } else {
+            printk(KERN_DEBUG PFX "Removed the rate limiter from RDQ %d\n",
+                   rate_limiter_params.rdq);
+        }
+
+        break;
+    }
+
+    case CTRL_CMD_SET_TRUNCATE_PARAMS:
+    {
+        struct ku_set_truncate_params truncate_params;
+
+        if (!dev->pdev) {
+            printk(KERN_DEBUG PFX "will not set truncate params since there's no "
+                   "PCI device\n");
+            goto out;
+        }
+
+        err = copy_from_user(&truncate_params, (void*)data,
+                             sizeof(truncate_params));
+        if (err) {
+            goto out;
+        }
+
+        if (truncate_params.rdq >= dev->dev_cap.max_num_rdqs) {
+            printk(KERN_ERR PFX "CTRL_CMD_SET_TRUNCATE_PARAMS: RDQ %d is not valid\n",
+                   truncate_params.rdq);
+            return -EINVAL;
+        }
+
+        spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+        if (truncate_params.truncate_enable) {
+            if (truncate_params.truncate_size < SX_TRUNCATE_SIZE_MIN) {
+                printk(KERN_ERR PFX "CTRL_CMD_SET_TRUNCATE_PARAMS: Truncate size %u is not valid\n",
+                       truncate_params.truncate_size);
+                spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+                err = -EINVAL;
+                goto out;
+            }
+
+            sx_priv(dev)->truncate_size_db[truncate_params.rdq] = truncate_params.truncate_size;
+        } else {
+            sx_priv(dev)->truncate_size_db[truncate_params.rdq] = 0;
+        }
+
+        spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+        break;
+    }
+
+    case CTRL_CMD_CR_SPACE_READ:
+    {
+        struct ku_cr_space_read read_data;
+        unsigned char          *buf = NULL;
+
+        err = copy_from_user(&read_data, (void*)data,
+                             sizeof(read_data));
+        if (err) {
+            goto out;
+        }
+
+        buf = kmalloc(read_data.size, GFP_KERNEL);
+        if (!buf) {
+            err = -ENOMEM;
+            goto out;
+        }
+
+        err = sx_dpt_cr_space_read(read_data.dev_id, read_data.address,
+                                   buf, read_data.size);
+        if (err) {
+            kfree(buf);
+            goto out;
+        }
+
+        err = copy_to_user(read_data.data, buf, read_data.size);
+        kfree(buf);
+        break;
+    }
+
+    case CTRL_CMD_CR_SPACE_WRITE:
+    {
+        struct ku_cr_space_write write_data;
+        unsigned char           *buf = NULL;
+
+        err = copy_from_user(&write_data, (void*)data,
+                             sizeof(write_data));
+        if (err) {
+            goto out;
+        }
+
+        buf = kmalloc(write_data.size, GFP_KERNEL);
+        if (!buf) {
+            err = -ENOMEM;
+            goto out;
+        }
+
+        err = copy_from_user(buf, write_data.data, write_data.size);
+        if (err) {
+            kfree(buf);
+            goto out;
+        }
+
+        err = sx_dpt_cr_space_write(write_data.dev_id, write_data.address,
+                                    buf, write_data.size);
+        kfree(buf);
+        break;
+    }
+
+    case CTRL_CMD_CREATE_PORT_NETDEV:
+    {
+        struct ku_port_netdev netdev_data;
+        union sx_event_data   netdev_event_data;
+
+        printk(KERN_DEBUG PFX "ioctl CTRL_CMD_CREATE_PORT_NETDEV called\n");
+        err = copy_from_user(&netdev_data, (void*)data,
+                             sizeof(netdev_data));
+        if (err) {
+            goto out;
+        }
+
+        spin_lock(&dev->profile_lock);
+        if (!dev->profile_set) {
+            spin_unlock(&dev->profile_lock);
+            err = -EINVAL;
+            goto out;
+        }
+
+        spin_unlock(&dev->profile_lock);
+        netdev_event_data.port_netdev_set.sysport = netdev_data.sysport;
+        netdev_event_data.port_netdev_set.is_lag = netdev_data.is_lag;
+        if (netdev_data.is_lag) {
+            netdev_event_data.port_netdev_set.mid = netdev_data.sysport +
+                                                    0xC000 + sx_glb.profile.max_mid;
+            printk(KERN_DEBUG "port_netdev_set.mid : 0x%x, is_lag: %d"
+                   "sys_port:0x%x, max_mid: 0x%x \n",
+                   netdev_event_data.port_netdev_set.mid,
+                   netdev_data.is_lag,
+                   netdev_data.sysport,
+                   sx_glb.profile.max_mid);
+        } else {
+            printk(KERN_DEBUG "NOT LAG : port_netdev_set.mid : 0x%x \n", netdev_data.is_lag);
+        }
+
+        netdev_event_data.port_netdev_set.name = netdev_data.name;
+        netdev_event_data.port_netdev_set.swid = netdev_data.swid;
+
+        spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+        if (sx_priv(dev)->dev_specific_cb.get_send_to_rp_as_data_supported_cb != NULL) {
+            sx_priv(dev)->dev_specific_cb.get_send_to_rp_as_data_supported_cb(&netdev_event_data.port_netdev_set.send_to_rp_as_data_supported);
+        } else {
+            netdev_event_data.port_netdev_set.send_to_rp_as_data_supported = false;
+        }
+        spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+
+        sx_core_dispatch_event(dev, SX_DEV_EVENT_OPEN_PORT_NETDEV, &netdev_event_data);
+        break;
+    }
+
+    case CTRL_CMD_REMOVE_PORT_NETDEV:
+    {
+        struct ku_port_netdev netdev_data;
+        union sx_event_data   netdev_event_data;
+
+        printk(KERN_DEBUG PFX "ioctl CTRL_CMD_REMOVE_PORT_NETDEV called\n");
+        err = copy_from_user(&netdev_data, (void*)data,
+                             sizeof(netdev_data));
+        if (err) {
+            goto out;
+        }
+
+        spin_lock(&dev->profile_lock);
+        if (!dev->profile_set) {
+            spin_unlock(&dev->profile_lock);
+            err = -EINVAL;
+            goto out;
+        }
+
+        spin_unlock(&dev->profile_lock);
+        netdev_event_data.port_netdev_set.sysport = netdev_data.sysport;
+        sx_core_dispatch_event(dev, SX_DEV_EVENT_CLOSE_PORT_NETDEV, &netdev_event_data);
+        break;
+    }
+
+    case CTRL_CMD_SET_SGMII_BASE_SMAC:
+    {
+        struct ku_sgmii_smac smac_data;
+
+        if (sx_glb.sx_sgmii.initialized == 1) {
+            printk(KERN_WARNING PFX "ioctl CTRL_CMD_SET_SGMII_BASE_SMAC: "
+                   "Cannot set SGMII base SMAC after the SGMII was initialized\n");
+            err = -EPERM;
+            goto out;
+        }
+
+        err = copy_from_user(&smac_data, (void*)data,
+                             sizeof(smac_data));
+        if (err) {
+            goto out;
+        }
+
+        if ((smac_data.number_of_macs == 0) ||
+            (smac_data.number_of_macs > MAX_SGMII_FLOWS)) {
+            printk(KERN_WARNING PFX "ioctl CTRL_CMD_SET_SGMII_BASE_SMAC: "
+                   "number_of_macs %d is not valid\n", smac_data.number_of_macs);
+            err = -EINVAL;
+            goto out;
+        }
+
+        sx_glb.sx_sgmii.base_smac = smac_data.base_smac;
+        sx_glb.sx_sgmii.number_of_macs = smac_data.number_of_macs;
+        printk(KERN_INFO PFX "SGMII SMAC set to 0x%llx, number of "
+               "MACs (%u)\n", sx_glb.sx_sgmii.base_smac,
+               sx_glb.sx_sgmii.number_of_macs);
+        break;
+    }
+
+    case CTRL_CMD_SET_VID_2_IP:
+    {
+        struct ku_vid2ip_data vid2ip_data;
+
+        err = copy_from_user(&vid2ip_data, (void*)data,
+                             sizeof(vid2ip_data));
+        if (err) {
+            goto out;
+        }
+
+        if (vid2ip_data.vid >= MAX_VLAN_NUM) {
+            printk(KERN_ERR PFX "Received VID %d "
+                   "is invalid\n", vid2ip_data.vid);
+            err = -EINVAL;
+            goto out;
+        }
+
+        spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+
+        if (vid2ip_data.valid) {
+            sx_priv(dev)->icmp_vlan2ip_db[vid2ip_data.vid] =
+                vid2ip_data.ip_addr;
+        } else {
+            sx_priv(dev)->icmp_vlan2ip_db[vid2ip_data.vid] = 0;
+        }
+
+        spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+        break;
+    }
+    
+    case CTRL_CMD_SET_PORT_VID_TO_FID_MAP:
+    {        
+        struct ku_port_vlan_to_fid_map_data port_vlan_to_fid_map_data;
+
+        err = copy_from_user(&port_vlan_to_fid_map_data, (void*)data,
+                             sizeof(port_vlan_to_fid_map_data));
+        if (err) {
+            goto out;
+        }
+        
+        if (port_vlan_to_fid_map_data.local_port > MAX_PHYPORT_NUM) {
+            printk(KERN_ERR PFX "Received Local port 0x%x "
+                   "is invalid (max. %d) \n",
+                   port_vlan_to_fid_map_data.local_port, MAX_PHYPORT_NUM);
+            err = -EINVAL;
+            goto out;
+        }
+
+        if (port_vlan_to_fid_map_data.vid >= MAX_VLAN_NUM) {
+            printk(KERN_ERR PFX "Received VID %d "
+                   "is invalid\n", port_vlan_to_fid_map_data.vid);
+            err = -EINVAL;
+            goto out;
+        }
+
+        spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+
+        if (port_vlan_to_fid_map_data.is_mapped_to_fid) {
+            sx_priv(dev)->port_vid_to_fid[port_vlan_to_fid_map_data.local_port][port_vlan_to_fid_map_data.vid] =
+                port_vlan_to_fid_map_data.fid;
+        } else {
+            sx_priv(dev)->port_vid_to_fid[port_vlan_to_fid_map_data.local_port][port_vlan_to_fid_map_data.vid] = 0;
+        }
+
+        spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+        break;
+    }
+
+    default:
+        return -EINVAL;
+    }
+
+out:
+    return err;
+}
+
+static unsigned int sx_core_poll(struct file *filp, poll_table *wait)
+{
+    unsigned int   mask = 0;
+    struct sx_rsc *file = NULL;
+    unsigned long  flags;
+
+    file = filp->private_data;
+    poll_wait(filp, &file->poll_wait, wait);
+    spin_lock_irqsave(&file->lock, flags);
+    if (!list_empty(&file->evlist.list)) {
+        mask |= POLLIN | POLLRDNORM;  /* readable */
+    }
+    if (file->evlist_size < SX_EVENT_LIST_SIZE) {
+        mask |= POLLOUT | POLLWRNORM; /* writeable */
+    }
+    spin_unlock_irqrestore(&file->lock, flags);
+
+    return mask;
+}
+
+static int sx_core_close(struct inode *inode, struct file *filp)
+{
+    struct event_data     *edata;
+    unsigned long          flags;
+    struct listener_entry *listener;
+    struct list_head      *pos, *q;
+    int                    entry;
+    struct sx_rsc         *file = filp->private_data;
+
+#ifdef SX_DEBUG
+    printk(KERN_DEBUG PFX " sx_core_close() \n");
+#endif
+
+    /* delete all listener entries belong to this fd */
+    spin_lock_irqsave(&sx_glb.listeners_lock, flags);
+    for (entry = 0; entry < NUM_HW_SYNDROMES + 1; entry++) {
+        if (!list_empty(&sx_glb.listeners_db[entry].list)) {
+            list_for_each_safe(pos, q,
+                               &sx_glb.listeners_db[entry].list) {
+                listener = list_entry(pos,
+                                      struct listener_entry, list);
+
+                if ((struct file *)listener->context == filp) {
+                    list_del(pos);
+                    kfree(listener);
+                }
+            }
+        }
+    }
+
+    spin_unlock_irqrestore(&sx_glb.listeners_lock, flags);
+    spin_lock_irqsave(&file->lock, flags);
+    list_for_each_safe(pos, q, &file->evlist.list) {
+        edata = list_entry(pos, struct event_data, list);
+        list_del(pos);
+        kfree_skb(edata->skb);
+        kfree(edata);
+    }
+
+    spin_unlock_irqrestore(&file->lock, flags);
+    kfree(file);
+    SX_CORE_UNUSED_PARAM(inode);
+
+    return 0;
+}
+
+int sx_core_flush_synd_by_context(void * context)
+{
+    unsigned long          flags;
+    struct listener_entry *listener;
+    struct list_head      *pos, *q;
+    int                    entry;
+
+    /* delete all listener entries belong to this context */
+    spin_lock_irqsave(&sx_glb.listeners_lock, flags);
+    for (entry = 0; entry < NUM_HW_SYNDROMES + 1; entry++) {
+        if (!list_empty(&sx_glb.listeners_db[entry].list)) {
+            list_for_each_safe(pos, q,
+                               &sx_glb.listeners_db[entry].list) {
+                listener = list_entry(pos,
+                                      struct listener_entry, list);
+
+                if (listener->context == context) {
+                    list_del(pos);
+                    kfree(listener);
+                }
+            }
+        }
+    }
+
+    spin_unlock_irqrestore(&sx_glb.listeners_lock, flags);
+
+    return 0;
+}
+EXPORT_SYMBOL(sx_core_flush_synd_by_context);
+
+int sx_core_flush_synd_by_handler(cq_handler handler)
+{
+    unsigned long          flags;
+    struct listener_entry *listener;
+    struct list_head      *pos, *q;
+    int                    entry;
+
+    /* delete all listener entries belong to this context */
+    spin_lock_irqsave(&sx_glb.listeners_lock, flags);
+    for (entry = 0; entry < NUM_HW_SYNDROMES + 1; entry++) {
+        if (!list_empty(&sx_glb.listeners_db[entry].list)) {
+            list_for_each_safe(pos, q,
+                               &sx_glb.listeners_db[entry].list) {
+                listener = list_entry(pos,
+                                      struct listener_entry, list);
+
+                if (listener->handler == handler) {
+                    list_del(pos);
+                    kfree(listener);
+                }
+            }
+        }
+    }
+
+    spin_unlock_irqrestore(&sx_glb.listeners_lock, flags);
+
+    return 0;
+}
+EXPORT_SYMBOL(sx_core_flush_synd_by_handler);
+
+/************************************************
+ *  Data-Structures
+ ***********************************************/
+
+static const struct file_operations sx_core_fops = {
+    .owner = THIS_MODULE,
+    .open = sx_core_open,
+    .read = sx_core_read,
+    .write = sx_core_write,
+	.unlocked_ioctl	 =	sx_core_ioctl,
+    .poll = sx_core_poll,
+    .release = sx_core_close
+};
+
+/************************************************
+ *  Module Functions
+ ***********************************************/
+static int sx_load_fw(struct sx_dev *dev)
+{
+    struct sx_priv *priv = sx_priv(dev);
+    int             err;
+
+    priv->fw.fw_icm = sx_alloc_icm(dev, priv->fw.fw_pages,
+                                   GFP_HIGHUSER | __GFP_NOWARN, 0);
+    if (!priv->fw.fw_icm) {
+        sx_err(dev, "Couldn't allocate FW area, aborting.\n");
+        return -ENOMEM;
+    }
+
+    err = sx_MAP_FA(dev, priv->fw.fw_icm);
+    if (err) {
+        sx_err(dev, "MAP_FA command failed, aborting.\n");
+        goto err_free;
+    }
+
+    return 0;
+
+err_free:
+    sx_free_icm(dev, priv->fw.fw_icm, 0);
+    return err;
+}
+
+static int sx_core_init_cb(struct sx_dev *dev,
+                           uint16_t device_id,
+                           uint16_t device_hw_revision)
+{
+    int err = 0;
+    enum sxd_chip_types chip_type;
+
+    switch (device_id) {
+    case SXD_MGIR_HW_DEV_ID_SX:
+        if (device_hw_revision == 0xA1) {
+            chip_type = SXD_CHIP_TYPE_SWITCHX_A1;
+        } else if (device_hw_revision == 0xA2) {
+            chip_type = SXD_CHIP_TYPE_SWITCHX_A2;
+        } else if (device_hw_revision == 0xA0) {
+            printk(KERN_ERR PFX "The SwitchX device revision is A0, "
+                   "and therefore it is not supported by SX driver\n");
+            return -EFAULT;
+        } else {
+            printk(KERN_ERR PFX "The SwitchX device revision (0x%x) "
+                   "is not supported by SX driver\n", device_hw_revision);
+            return -EFAULT;
+        }
+        break;
+
+    case SXD_MGIR_HW_DEV_ID_SWITCH_IB:
+        chip_type = SXD_CHIP_TYPE_SWITCH_IB;
+        break;
+
+    case SXD_MGIR_HW_DEV_ID_SPECTRUM:
+        chip_type = SXD_CHIP_TYPE_SPECTRUM;
+        break;
+
+    case SXD_MGIR_HW_DEV_ID_SWITCH_IB2:
+		chip_type = SXD_CHIP_TYPE_SWITCH_IB2;
+		break;
+
+    default:
+        printk(KERN_ERR PFX "ERROR: Unresolved chip type. device_id (%u)\n", device_id);
+        return -EFAULT;
+    }
+
+    err = sx_core_dev_init_switchx_cb(dev, chip_type);
+    if (err) {
+        printk(KERN_ERR PFX "callback device init failed for device (%u)\n",
+               dev->profile.dev_id);
+        return err;
+    }
+
+    return err;
+}
+
+static int sx_init_board(struct sx_dev *dev)
+{
+    struct sx_board           board;
+    int                       err;
+    struct ku_access_mgir_reg reg_data;
+    int                       retry_num = 0;
+
+    /*
+        This is a workaround to race condition occured when FW 
+        boot isn't finished and we start to read MGIR. 
+        We post the in_mailbox but FW zero GO bit. So we think 
+        that command is done.
+        After this race we get 0 in all MGIR fields.
+        The temporary solution is to reread again.
+        The real solution should provide interface to read HEALTH 
+        bits which will indicate that FW boot is finished.
+    */
+    while (retry_num < 3) {
+        memset(&reg_data, 0, sizeof(reg_data));
+        reg_data.dev_id = dev->device_id;
+        reg_data.op_tlv.type = 1;
+        reg_data.op_tlv.length = 4;
+        reg_data.op_tlv.dr = 0;
+        reg_data.op_tlv.status = 0;
+        reg_data.op_tlv.register_id = 0x9020; /* MGIR register ID */
+        reg_data.op_tlv.r = 0;
+        reg_data.op_tlv.method = 1; /* Query */
+        reg_data.op_tlv.op_class = 1;
+        reg_data.op_tlv.tid = 0;
+
+        err = sx_ACCESS_REG_MGIR(dev, &reg_data);
+        /* Only if we managed to read MGIR successfully we check the HW revision
+         * to see it's not A0 */
+        if (!err && !reg_data.op_tlv.status &&
+            (reg_data.mgir_reg.hw_info.device_id == SXD_MGIR_HW_DEV_ID_SX) &&
+            (reg_data.mgir_reg.hw_info.device_hw_revision == SXD_MGIR_HW_REV_ID_SX_A0)) {
+            printk(KERN_ERR PFX "The SwitchX device revision is A0, "
+                   "and therefore it is not supported by SX driver\n");
+            return -EFAULT;
+        }
+
+        if (reg_data.mgir_reg.hw_info.device_id != 0) {
+            break;
+        }
+
+        msleep(500*retry_num);
+        retry_num++;
+    } 
+ 
+    err = sx_core_init_cb(dev, reg_data.mgir_reg.hw_info.device_id,
+                          reg_data.mgir_reg.hw_info.device_hw_revision);
+    if (err) {
+        printk(KERN_ERR PFX "callback dev init failed for device (%u)\n",
+               dev->profile.dev_id);
+        return err;
+    }
+
+    err = sx_QUERY_FW(dev, NULL);
+    if (err) {
+        sx_err(dev, "QUERY_FW command failed, aborting.\n");
+        return err;
+    }
+
+    /* init local mailboxes */
+    err = sx_QUERY_FW_2(dev, dev->device_id);
+    if (err) {
+        sx_err(dev, "QUERY_FW_2 command failed, aborting.\n");
+        return err;
+    }
+
+    dev->bar0_dbregs_offset = sx_priv(dev)->fw.doorbell_page_offset;
+    dev->bar0_dbregs_bar = sx_priv(dev)->fw.doorbell_page_bar;
+
+    err = sx_load_fw(dev);
+    if (err) {
+        sx_err(dev, "Failed to start FW, aborting.\n");
+        return err;
+    }
+
+    err = sx_QUERY_AQ_CAP(dev);
+    if (err) {
+        sx_err(dev, "QUERY_AQ_CAP command failed, aborting.\n");
+        goto err_stop_fw;
+    }
+    dev->dev_cap.max_num_cpu_egress_tcs = 12;
+    dev->dev_cap.max_num_cpu_ingress_tcs = 16;
+
+    err = sx_QUERY_BOARDINFO(dev, &board);
+    if (err) {
+        sx_err(dev, "QUERY_BOARDINFO command failed, aborting.\n");
+        goto err_stop_fw;
+    }
+
+    sx_priv(dev)->eq_table.inta_pin = board.inta_pin;
+    memcpy(dev->board_id, board.board_id, sizeof(dev->board_id));
+    dev->vsd_vendor_id = board.vsd_vendor_id;
+    return 0;
+
+err_stop_fw:
+    sx_UNMAP_FA(dev);
+    sx_free_icm(dev, sx_priv(dev)->fw.fw_icm, 0);
+
+    return err;
+}
+
+static void sx_enable_msi_x(struct sx_dev *dev)
+{
+#if 0
+    struct sx_priv   *priv = sx_priv(dev);
+    struct msix_entry entry;
+    int               err;
+    int               i;
+
+    if (msi_x) {
+        entry.entry = 0;
+        err = pci_enable_msix(dev->pdev, &entry, 1);
+        if (err) {
+            if (err > 0) {
+                printk(KERN_INFO PFX "Only %d MSI-X vectors available, "
+                       "not using MSI-X\n", err);
+            } else {
+                printk(KERN_DEBUG PFX "Failed enabling MSI-X interrupts. "
+                       "Going to use standard interrupts instead\n");
+            }
+
+            goto no_msi;
+        }
+
+        sx_info(dev, "MSI-X interrupts were enabled successfully\n");
+        for (i = 0; i < SX_NUM_EQ; ++i) {
+            priv->eq_table.eq[i].irq = entry.vector;
+        }
+
+        dev->flags |= SX_FLAG_MSI_X;
+        return;
+    }
+
+no_msi:
+    msi_x = 0;
+    for (i = 0; i < SX_NUM_EQ; ++i) {
+        priv->eq_table.eq[i].irq = dev->pdev->irq;
+    }
+#endif
+}
+
+static int sx_map_doorbell_area(struct sx_dev *dev)
+{
+    dev->db_base =
+        ioremap(pci_resource_start(dev->pdev, dev->bar0_dbregs_bar)
+                + dev->bar0_dbregs_offset,
+                SX_DBELL_REGION_SIZE);
+    if (!dev->db_base) {
+        printk(KERN_ERR "%s(): bar: %d virt: is NULL \n",
+               __func__, dev->bar0_dbregs_bar);
+
+        return -EINVAL;
+    }
+
+    printk(KERN_DEBUG "%s(): bar: %d dev->db_base phys: 0x%llx , virt: 0x%p \n",
+           __func__,
+           dev->bar0_dbregs_bar,
+           pci_resource_start(dev->pdev, dev->bar0_dbregs_bar)
+           + dev->bar0_dbregs_offset,
+           dev->db_base);
+
+    return 0;
+}
+
+static void sx_doorbell_cleanup(struct sx_dev *dev)
+{
+    iounmap(dev->db_base);
+}
+
+static void sx_close_board(struct sx_dev *dev)
+{
+    sx_UNMAP_FA(dev);
+    sx_free_icm(dev, sx_priv(dev)->fw.fw_icm, 0);
+}
+
+#ifdef NO_PCI
+static void sx_destroy_sx(struct sx_dev *dev)
+{
+    /**Clearing the rdq tables**/
+    sx_core_destroy_rdq_table(dev, true);
+
+    /**Clearing the sdq tables**/
+    sx_core_destroy_sdq_table(dev, true);
+
+    /**Clearing the cq tables**/
+    sx_core_destroy_cq_table(dev);
+
+    /**Returning to polling (not command)**/
+    sx_cmd_use_polling(dev);
+
+    /**Clearing the eq tables**/
+    sx_cleanup_eq_table(dev);
+
+    return;
+}
+#endif
+
+static int sx_setup_sx(struct sx_dev *dev)
+{
+    int err = 0;
+
+    err = sx_init_eq_table(dev);
+    if (err) {
+        sx_err(dev, "Failed to initialize "
+               "event queue table, aborting.\n");
+        goto out_ret;
+    }
+
+    err = sx_cmd_use_events(dev);
+    if (err) {
+        sx_err(dev, "Failed to switch to event-driven "
+               "firmware commands, aborting.\n");
+        goto err_eq_table_free;
+    }
+
+    err = sx_core_init_cq_table(dev);
+    if (err) {
+        sx_err(dev, "Failed to initialize CQ table, aborting.\n");
+        goto err_cmd_poll;
+    }
+
+    err = sx_core_init_sdq_table(dev);
+    if (err) {
+        sx_err(dev, "Failed to initialize SDQ table, aborting.\n");
+        goto err_cq_table_free;
+    }
+
+    err = sx_core_init_rdq_table(dev);
+    if (err) {
+        sx_err(dev, "Failed to initialize RDQ table, aborting.\n");
+        goto err_sdq_table_free;
+    }
+
+    return 0;
+
+err_sdq_table_free:
+    sx_core_destroy_sdq_table(dev, true);
+
+err_cq_table_free:
+    sx_core_destroy_cq_table(dev);
+
+err_cmd_poll:
+    sx_cmd_use_polling(dev);
+
+err_eq_table_free:
+    sx_cleanup_eq_table(dev);
+
+out_ret:
+    return err;
+}
+
+static int sx_core_init_one_pci(struct pci_dev *pdev, const struct pci_device_id *id)
+{
+#if 0
+    struct sx_priv *priv = NULL;
+    struct sx_dev  *dev = NULL;
+    int             err = 0;
+
+    sx_glb.pci_devs_cnt++;
+    
+    printk(KERN_INFO PFX "Probe %s(%d) device %u\n", __FUNCTION__, __LINE__, \
+           pdev->device);
+    err = sx_core_init_one(&priv);
+    if (err) {
+        dev_err(&pdev->dev, "sx_core_init_one failed with err: %d , aborting.\n",
+                err);
+        goto out;
+    }
+
+    dev = &priv->dev;
+
+    err = pci_enable_device(pdev);
+    if (err) {
+        dev_err(&pdev->dev, "Cannot enable PCI device, aborting.\n");
+        goto err_enable_pdev;
+    }
+
+    /* Check for BARs.  We expect 0: 1MB in Baz and 4MB in Pelican */
+    if (!(pci_resource_flags(pdev, 0) & IORESOURCE_MEM) ||
+        ((pci_resource_len(pdev, 0) != 1 << 20) &&
+         (pci_resource_len(pdev, 0) != 1 << 22))) {
+        dev_err(&pdev->dev, "Missing BAR0, aborting.\n");
+        err = -ENODEV;
+        goto err_disable_pdev;
+    }
+
+    err = pci_request_region(pdev, 0, DRV_NAME);
+    if (err) {
+        dev_err(&pdev->dev, "Cannot request control region, "
+                "aborting.\n");
+        goto err_disable_pdev;
+    }
+
+    pci_set_master(pdev);
+    err = pci_set_dma_mask(pdev, DMA_BIT_MASK(64));
+    if (err) {
+        dev_warn(&pdev->dev, "Warning: couldn't set 64-bit PCI "
+                 "DMA mask.\n");
+        err = pci_set_dma_mask(pdev, DMA_BIT_MASK(32));
+        if (err) {
+            dev_err(&pdev->dev, "Can't set PCI DMA mask, aborting.\n");
+            goto err_release_bar0;
+        }
+    }
+
+    err = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(64));
+    if (err) {
+        dev_warn(&pdev->dev, "Warning: couldn't set 64-bit "
+                 "consistent PCI DMA mask.\n");
+        err = pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(32));
+        if (err) {
+            dev_err(&pdev->dev, "Can't set consistent PCI DMA "
+                    "mask, aborting.\n");
+            goto err_release_bar0;
+        }
+    }
+
+    dev->pdev = pdev;
+#ifndef PD_BU
+    err = sx_reset(dev);
+    if (err) {
+        sx_err(dev, "Failed to reset HW, aborting.\n");
+        goto err_release_bar0;
+    }
+#else
+    printk(KERN_INFO PFX "Performing SW reset is SKIPPED in PD mode.\n");
+#endif
+
+    if (sx_cmd_pool_create(dev)) {
+        sx_err(dev, "Failed to create command buffer pool, aborting.\n");
+        goto err_release_bar0;
+    }
+
+    if (sx_cmd_init_pci(dev)) {
+        sx_err(dev, "Failed to initialize command interface, aborting.\n");
+        goto err_free_pool;
+    }
+
+    pci_set_drvdata(pdev, dev);
+    spin_lock(&sx_glb.pci_devs_lock);
+    list_add(&dev->list, &sx_glb.pci_devs_list);
+    spin_unlock(&sx_glb.pci_devs_lock);
+    err = sx_dpt_init_dev_pci(dev);
+    if (err) {
+        sx_err(dev, "Failed initializing default PCI device "
+               "attibutes in the DPT, aborting.\n");
+        goto err_free_pool;
+    }
+
+    err = sx_init_board(dev);
+    if (err) {
+        goto err_cmd;
+    }
+
+    sx_enable_msi_x(dev);
+    err = sx_map_doorbell_area(dev);
+    if (err) {
+        goto err_dbell;
+    }
+
+    /* Only if the device is not registered */
+    if (priv->unregistered) {
+        err = sx_core_register_device(dev);
+        if (err) {
+            sx_err(dev, "Failed to register the device, aborting.\n");
+            goto err_dbell_clean;
+        }
+
+        priv->unregistered = 0;
+    }
+
+    err = sx_setup_sx(dev);
+    if ((err == -EBUSY) && (dev->flags & SX_FLAG_MSI_X)) {
+        dev->flags &= ~SX_FLAG_MSI_X;
+        pci_disable_msix(dev->pdev);
+        err = sx_setup_sx(dev);
+    }
+
+    if (err) {
+        goto out_unregister;
+    }
+
+    dev->global_flushing = 0;
+    dev->dev_stuck = 0;
+    sx_core_start_catas_poll(dev);
+#endif
+    return 0;
+#if 0
+out_unregister:
+    if (!priv->unregistered) {
+        sx_core_unregister_device(dev);
+        priv->unregistered = 1;
+    }
+
+err_dbell_clean:
+    sx_doorbell_cleanup(dev);
+
+err_dbell:
+    if (dev->flags & SX_FLAG_MSI_X) {
+        pci_disable_msix(pdev);
+    }
+
+    sx_close_board(dev);
+
+err_free_pool:
+    sx_cmd_pool_destroy(dev);
+    list_del(&dev->list);
+
+err_cmd:
+    sx_cmd_unmap(dev);
+
+err_release_bar0:
+    pci_release_region(pdev, 0);
+
+err_disable_pdev:
+    pci_disable_device(pdev);
+    pci_set_drvdata(pdev, NULL);
+
+err_enable_pdev:
+    sx_core_remove_one(priv);
+
+out:
+    return err;
+#endif
+}
+
+static int sx_core_init_one(struct sx_priv **sx_priv)
+{
+    struct sx_priv *priv;
+    struct sx_dev  *dev;
+    int             i, j, err;
+
+#ifdef NO_PCI
+    printk(KERN_INFO PFX "Initializing in NO_PCI mode\n");
+#endif
+
+    if (!sx_priv) {
+        printk(KERN_ERR PFX "Invalid param %s\n", __func__);
+        return -EINVAL;
+    }
+
+    priv = vmalloc(sizeof(struct sx_priv));
+    if (!priv) {
+        printk(KERN_ERR PFX "Device struct alloc failed, aborting.\n");
+        err = -ENOMEM;
+        goto out;
+    }
+    memset(priv, 0, sizeof *priv);
+    dev = &priv->dev;
+
+    /* default pvid for all ports is 1 */
+    for (i = 0; i < MAX_SYSPORT_NUM; i++) {
+        if (i < MAX_LAG_NUM) {
+            priv->pvid_lag_db[i] = 1;
+        }
+
+        priv->pvid_sysport_db[i] = 1;
+    }
+
+    /* initialize lag_filter_db with invalid value */
+    for (i = 0; i < NUM_HW_SYNDROMES; i++) {
+        for (j = 0; j < MAX_LAG_PORTS_IN_FILTER; j++) {
+            priv->lag_filter_db[i][j] = LAG_ID_INVALID;
+        }
+    }
+
+    err = sx_dpt_init_default_dev(dev);
+    if (err) {
+        sx_err(dev, "Failed initializing default device "
+               "attibutes in the DPT, aborting.\n");
+        goto out_free_priv;
+    }
+
+    err = sx_cmd_init(dev);
+    if (err) {
+        sx_err(dev, "Failed initializing command interface, aborting.\n");
+        goto out_free_priv;
+    }
+
+    spin_lock_init(&dev->profile_lock);
+    dev->profile_set = 0;
+    dev->dev_profile_set = 0;
+    dev->first_ib_swid = 1;
+    spin_lock_init(&priv->ctx_lock);
+    spin_lock_init(&priv->db_lock);
+    INIT_LIST_HEAD(&priv->ctx_list);
+    INIT_LIST_HEAD(&priv->dev_list);
+    atomic_set(&priv->cq_backup_polling_refcnt, 0);
+
+#if 0
+    err = sx_core_catas_init(dev);
+    if (err) {
+        printk(KERN_ERR PFX "Couldn't start catas. Aborting...\n");
+        goto out_free_priv;
+    }
+#endif
+
+    err = sx_bitmap_init(&priv->swid_bitmap, NUMBER_OF_SWIDS);
+    if (err) {
+        sx_err(dev, "Failed to initialize SWIDs bitmap, aborting.\n");
+        goto catas_stop;
+    }
+        
+    set_default_capabilities(dev);
+    err = sx_core_register_device(dev);
+    if (err) {
+        sx_err(dev, "Failed to register the device, aborting.\n");
+        goto catas_stop;
+    }
+    memset(&dev->stats, 0, sizeof(dev->stats));    
+
+#ifdef NO_PCI
+    err = sx_setup_sx(dev);
+    if (err) {
+        sx_err(dev, "Failed to in sx_setup_sx, aborting.\n");
+        goto out_unregister;
+    }
+
+    spin_lock(&sx_glb.pci_devs_lock);
+    list_add(&dev->list, &sx_glb.pci_devs_list);
+    spin_unlock(&sx_glb.pci_devs_lock);
+#endif
+
+    if (sx_priv != NULL) {
+        *sx_priv = priv;
+    }
+
+    return 0;
+
+#ifdef NO_PCI
+out_unregister:
+    sx_core_unregister_device(dev);
+    priv->unregistered = 1;
+#endif
+
+catas_stop:
+#if 0
+    sx_core_catas_cleanup(dev);
+#endif
+
+out_free_priv:
+    vfree(priv);
+
+out:
+    return err;
+}
+
+static void sx_core_remove_one(struct sx_priv *priv)
+{
+    struct sx_dev *dev;
+
+    if (priv == NULL) {
+        dev = sx_glb.tmp_dev_ptr;
+        sx_glb.tmp_dev_ptr = NULL;
+        priv = sx_priv(dev);
+    } else {
+        dev = &priv->dev;
+    }
+
+    if (!dev) {
+        return;
+    }
+
+    if (!priv->unregistered) {
+        sx_core_unregister_device(dev);
+        priv->unregistered = 1;
+    }
+#if 0
+    sx_core_catas_cleanup(dev);
+#endif
+#ifdef NO_PCI
+    spin_lock(&sx_glb.pci_devs_lock);
+    list_del(&dev->list);
+    spin_unlock(&sx_glb.pci_devs_lock);
+
+    /*Freeing the memmory for the device tables*/
+    sx_destroy_sx(dev);
+#endif
+
+    sx_dpt_remove_dev(dev->device_id, 1);
+
+    vfree(priv);
+}
+
+static void sx_core_remove_one_pci(struct pci_dev *pdev)
+{
+    struct sx_priv *priv;
+    struct sx_dev  *dev;
+    int             i;
+
+    dev = pci_get_drvdata(pdev);
+
+    if (!dev) {
+        return;
+    }
+
+    spin_lock(&sx_glb.pci_devs_lock);
+    list_del(&dev->list);
+    spin_unlock(&sx_glb.pci_devs_lock);
+    sx_glb.pci_devs_cnt--;
+
+    priv = sx_priv(dev);
+    if (!priv->unregistered) {
+        sx_core_unregister_device(dev);
+        priv->unregistered = 1;
+    }
+
+    /* Destroy the cq_credit_thread before we flush the DQs
+     * otherwise it can stop pulling completion during the flush process */
+    if (priv->cq_table.cq_credit_thread) {
+        kthread_stop(priv->cq_table.cq_credit_thread);
+        priv->cq_table.cq_credit_thread = NULL;
+        for (i = 0; i < dev->dev_cap.max_num_cqs; i++) {
+            priv->cq_table.cq_rl_params[i].use_limiter = 0;
+        }
+
+        printk(KERN_DEBUG PFX "sx_core_remove_one_pci: cq_credit_thread was killed\n");
+    }
+
+    dev->global_flushing = 1;
+    sx_flush_dqs(dev, false);
+    sx_flush_dqs(dev, true);
+
+    for (i = 0; i < NUMBER_OF_SWIDS; i++) {
+        if (sx_bitmap_test(&sx_priv(dev)->swid_bitmap, i)) {
+            sx_disable_swid(dev, i);
+        }
+    }
+
+    sx_core_destroy_rdq_table(dev, true);
+    sx_core_destroy_sdq_table(dev, true);
+    sx_cmd_use_polling(dev);
+    sx_cleanup_eq_table(dev);
+    sx_core_destroy_cq_table(dev);
+    sx_doorbell_cleanup(dev);
+#if 0
+    if (dev->flags & SX_FLAG_MSI_X) {
+        pci_disable_msix(dev->pdev);
+    }
+#endif
+
+    sx_UNMAP_FA(dev);
+    sx_free_icm(dev, sx_priv(dev)->fw.fw_icm, 0);
+    sx_cmd_pool_destroy(dev);
+    sx_cmd_unmap(dev);
+#if 0
+    pci_release_region(pdev, 0);
+    pci_disable_device(pdev);
+    pci_set_drvdata(pdev, NULL);
+#endif
+    sx_core_remove_one(priv);
+}
+
+int sx_restart_one_pci(struct pci_dev *pdev)
+{
+    if (pdev == NULL){
+        printk(KERN_ERR PFX "sx_restart_one_pci error: pdev == NULL, exit \n");
+        return -ENODEV;        
+    }
+    sx_core_remove_one_pci(pdev);
+    return sx_core_init_one_pci(pdev, NULL);
+}
+
+struct pci_device_id sx_pci_table[] = {
+#ifndef NO_PCI
+    /* SwitchX PCI device ID */
+    { PCI_VDEVICE(MELLANOX, SWITCHX_PCI_DEV_ID) },
+
+    /* SwitchIB PCI device ID */
+    { PCI_VDEVICE(MELLANOX, SWITCH_IB_PCI_DEV_ID) },
+
+    /* Spectrum PCI device ID */
+    { PCI_VDEVICE(MELLANOX, SPECTRUM_PCI_DEV_ID) },
+
+    /* SwitchIB2 PCI device ID */
+    { PCI_VDEVICE(MELLANOX, SWITCH_IB2_PCI_DEV_ID) },
+#endif
+    { 0, }
+};
+
+MODULE_DEVICE_TABLE(pci, sx_pci_table);
+
+static struct pci_driver sx_driver = {
+    .name = DRV_NAME,
+    .id_table = sx_pci_table,
+    .probe = sx_core_init_one_pci,
+	.remove		= sx_core_remove_one_pci
+};
+
+int sx_init_char_dev(struct cdev *cdev_p)
+{
+    int ret = 0;
+    int devno, major, minor;
+
+    major = MAJOR(char_dev);
+    minor = MINOR(char_dev);
+    devno = MKDEV(major, minor);
+    printk("%s: Create char dev with major:%d minor:%d \n",
+           __func__, major, minor);
+
+    cdev_init(cdev_p, &sx_core_fops);
+    cdev_p->owner = THIS_MODULE;
+
+    ret = cdev_add(cdev_p, devno, 1);
+    if (ret) {
+        printk(KERN_ERR PFX "Couldn't add char device. Aborting... err: %d\n",
+               ret);
+        goto out;
+    }
+
+out:
+    return ret;
+}
+
+void sx_deinit_char_dev(struct cdev *cdev_p)
+{
+    printk("Deinit char dev: %p , usage:%d\n",
+           cdev_p, cdev_p->count);
+    cdev_del(cdev_p);
+}
+
+static int __init sx_core_init(void)
+{
+    int ret = 0;
+    int i = 0;
+
+    printk(KERN_INFO "%s", sx_version);
+
+    memset(&sx_glb, 0, sizeof(sx_glb));
+
+#ifndef NO_PCI
+#if defined(CONFIG_MLNX460EX) && defined(SNOOP_MISS_WA)
+    config_l2_force_snoop();
+#endif
+#endif
+
+    sx_core_init_proc_fs();
+    sx_dpt_init();
+
+    spin_lock_init(&sx_glb.pci_devs_lock);
+    INIT_LIST_HEAD(&sx_glb.pci_devs_list);
+
+    spin_lock_init(&sx_glb.listeners_lock);
+    for (i = 0; i < NUM_HW_SYNDROMES + 1; i++) {
+        INIT_LIST_HEAD(&sx_glb.listeners_db[i].list);
+    }
+
+    char_dev = MKDEV(SX_MAJOR, SX_BASE_MINOR);
+    ret = register_chrdev_region(char_dev, SX_MAX_DEVICES,
+                                 SX_CORE_CHAR_DEVICE_NAME);
+    if (ret) {
+        printk(KERN_INFO PFX "Couldn't register the default device number. "
+               "Trying to allocate one dynamically\n");
+        ret = alloc_chrdev_region(&char_dev, SX_BASE_MINOR, SX_MAX_DEVICES,
+                                  SX_CORE_CHAR_DEVICE_NAME);
+        if (ret) {
+            printk(KERN_ERR PFX "Couldn't register device number. "
+                   "Aborting...\n");
+            goto out_close_proc;
+        }
+    }
+
+    sx_glb.pci_devs_cnt = 0;
+
+#if defined(NO_PCI) || defined(CONFIG_SX_SGMII_PRESENT)
+    ret = sx_core_init_one(&sx_glb.priv);
+    if (ret) {
+        printk(KERN_ERR PFX "Couldn't initialize the device. "
+               "Aborting...\n");
+        goto out_cdev;
+    }
+
+    if (g_chip_type == 0) {
+        printk(KERN_ERR PFX "Chip type is not defined for device.\n");
+        goto out_remove_one;
+    }
+
+    ret = sx_core_dev_init_switchx_cb(&sx_glb.priv->dev, g_chip_type);
+    if (ret) {
+        printk(KERN_ERR PFX "callback dev init failed for device (%u)\n",
+               sx_glb.priv->dev.profile.dev_id);
+        goto out_remove_one;
+    }
+#endif
+
+    ret = pci_register_driver(&sx_driver);
+    if (ret < 0) {
+        goto out_remove_one;
+    }
+
+    ret = sx_init_char_dev(&sx_glb.cdev);
+    if (ret < 0) {
+        goto out_unreg_pci;
+    }
+
+    return 0;
+
+out_unreg_pci:
+    pci_unregister_driver(&sx_driver);
+
+out_remove_one:
+#if defined(NO_PCI) || defined(CONFIG_SX_SGMII_PRESENT)
+    sx_core_remove_one(sx_glb.priv);
+
+out_cdev:
+#endif
+    sx_deinit_char_dev(&sx_glb.cdev);
+    unregister_chrdev_region(char_dev, SX_MAX_DEVICES);
+
+out_close_proc:
+    sx_core_close_proc_fs();
+
+    return ret;
+}
+
+static void __exit sx_core_cleanup(void)
+{
+    unsigned long          flags;
+    struct listener_entry *listener;
+    struct list_head      *pos, *q;
+    int                    entry;
+
+    printk(KERN_INFO PFX "sx_core_cleanup_module \n");
+
+    sx_core_close_proc_fs();
+    pci_unregister_driver(&sx_driver);
+    sx_deinit_char_dev(&sx_glb.cdev);
+
+#if defined(NO_PCI) || defined(CONFIG_SX_SGMII_PRESENT)
+    sx_core_remove_one(sx_glb.priv);
+#endif /* #ifdef NO_PCI */
+
+    unregister_chrdev_region(char_dev, SX_MAX_DEVICES);
+    sx_dpt_dereg_i2c_ifc();
+    if (sx_glb.sx_sgmii.deinit) {
+        sx_glb.sx_sgmii.deinit();
+    }
+
+    /* delete all remaining listener entries */
+    spin_lock_irqsave(&sx_glb.listeners_lock, flags);
+    for (entry = 0; entry < NUM_HW_SYNDROMES + 1; entry++) {
+        if (!list_empty(&sx_glb.listeners_db[entry].list)) {
+            list_for_each_safe(pos, q, &sx_glb.listeners_db[entry].list) {
+                listener = list_entry(pos, struct listener_entry, list);
+                list_del(pos);
+                kfree(listener);
+            }
+        }
+    }
+    spin_unlock_irqrestore(&sx_glb.listeners_lock, flags);
+}
+
+/************************************************
+ *  MODULE init/exit
+ ***********************************************/
+module_init(sx_core_init);
+module_exit(sx_core_cleanup);
+
+
+/************************************************
+ *                  EOF                         *
+ ***********************************************/
diff --git a/linux/drivers/hwmon/mellanox/sx_dpt.c b/linux/drivers/hwmon/mellanox/sx_dpt.c
new file mode 100644
index 0000000..20dcb78
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/sx_dpt.c
@@ -0,0 +1,2041 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2016 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+#include <linux/skbuff.h>
+#include <linux/pci.h>
+#include <linux/kernel.h>
+#include <linux/if_ether.h>
+#include <linux/mlx_sx/kernel_user.h>
+#include <linux/mlx_sx/device.h>
+#include <linux/mlx_sx/driver.h>
+#include "sx_dpt.h"
+#include "sx.h"
+#include "fw.h"
+#include "sx_sgmii.h"
+
+#define MAX_I2C_RETRIES 1
+
+extern struct sx_globals    sx_glb;
+extern struct pci_device_id sx_pci_table[];
+extern int                  tx_debug;
+
+#include <linux/module.h>
+#include <linux/mlx_sx/sx_i2c_if.h>
+
+#ifdef SX_DEBUG
+#define DPRINTK(fmt, args ...)                                     \
+    do {                                                           \
+        printk(KERN_ERR DRV_NAME " [%s: %s() line %d]: " fmt "\n", \
+               __FILE__, __func__, __LINE__, ## args);             \
+    } while (0)
+#else
+#define DPRINTK(fmt, args ...) \
+    do {} while (0)
+#endif
+
+#define ENTER_FUNC() DPRINTK("ENTER {");
+#define EXIT_FUNC()  DPRINTK("EXIT }");
+
+#define PRINTK_ERR(fmt, args ...)                                  \
+    do {                                                           \
+        printk(KERN_ERR DRV_NAME " [%s: %s() line %d]: " fmt "\n", \
+               __FILE__, __func__, __LINE__, ## args);             \
+    } while (0)
+
+static char *dpt_type2str[] = {
+    /* DPT_PATH_INVALID = 0*/
+    "NONE",
+    /* DPT_PATH_I2C = 1*/
+    " I2C",
+    /* DPT_PATH_SGMII = 2*/
+    "SGMII",
+    /* DPT_PATH_PCI_E = 3*/
+    "PCIE",
+};
+
+/*
+ * Init default DPT device 255.
+ * On systems without PCI adding virtual PCI device.
+ * On systems with PCI , PCI devices will be added to
+ * DPT from pci_probe callbacks.
+ */
+int sx_dpt_init_default_dev(struct sx_dev *sx_dev)
+{
+    union ku_dpt_path_info path_data;
+    int                    dev_id = DEFAULT_DEVICE_ID;
+    int                    err;
+
+    sx_dev->device_id = dev_id;
+    path_data.sx_i2c_info.sx_i2c_dev = 0x448;
+    err = sx_dpt_add_dev_path(dev_id, DPT_PATH_I2C, path_data, 1);
+    if (err) {
+        sx_err(sx_dev, "Failed adding I2C path to the default device\n");
+        return err;
+    }
+
+    err = sx_dpt_set_cr_access_path(dev_id, DPT_PATH_I2C);
+    if (err) {
+        sx_err(sx_dev, "Failed setting CR access path to I2C\n");
+        return err;
+    }
+
+#ifdef NO_PCI
+    /* In NO_PCI mode we act as if we had PCI path */
+    sx_glb.tmp_dev_ptr = sx_dev;
+    return sx_dpt_init_dev_pci(sx_dev);
+#endif
+
+#ifdef CONFIG_SX_SGMII_PRESENT
+    /* In NO_PCI mode we act as if we had PCI path */
+    sx_glb.tmp_dev_ptr = sx_dev;
+#endif
+
+    return err;
+}
+
+/*
+ * In Barracuda system SwitchIB should be
+ * the default (255) device
+ */
+int sx_dpt_alloc_pci_dev_id(struct sx_dev *sx_dev, int* dpt_dev_id)
+{
+    int err;
+    int is_path_valid;
+
+#ifndef NO_PCI
+    struct sx_dev *tmp_sx_dev = NULL;
+#endif
+    int i, alloc_dev_id, free_id_found = 0;
+
+    is_path_valid = sx_dpt_is_path_valid(DEFAULT_DEVICE_ID, DPT_PATH_PCI_E);
+    if (!is_path_valid) {
+        *dpt_dev_id = DEFAULT_DEVICE_ID;
+        goto out;
+    }
+
+    /* lookup for next pci dev. */
+    for (i = DEFAULT_DEVICE_ID; i > 0; i--) {
+        is_path_valid = sx_dpt_is_path_valid(i, DPT_PATH_PCI_E);
+        if (!is_path_valid) {
+            free_id_found = 1;
+            alloc_dev_id = i;
+            break;
+        }
+    }
+
+    if (!free_id_found) {
+        sx_err(sx_dev, "No free dev_id found.\n");
+        err = -EINVAL;
+        goto out;
+    }
+
+#ifndef NO_PCI
+    /*
+     * In Barracude systems SwitchIB always should be default device
+     */
+    err = sx_dpt_get_sx_dev_by_id(DEFAULT_DEVICE_ID, &tmp_sx_dev);
+
+    if (tmp_sx_dev == NULL || sx_dev == NULL) {
+		printk("Error null ptr: tmp_sx_dev: %p,sx_dev: %p \n",
+		       tmp_sx_dev, sx_dev);
+		err = -EINVAL;
+	    goto out;
+	}
+    
+    if ( tmp_sx_dev->pdev == NULL ) {
+        printk("Error null ptr: tmp_sx_dev->pdev: 0x%p \n",
+                   tmp_sx_dev->pdev );
+        err = -EINVAL;
+        goto out;
+    }
+
+	if ( sx_dev->pdev == NULL ) {
+		printk("Error null ptr: sx_dev->pdev: 0x%p \n",
+				sx_dev->pdev );
+		err = -EINVAL;
+		goto out;
+    }
+
+    if ((tmp_sx_dev->pdev->device == SWITCHX_PCI_DEV_ID) &&
+        (sx_dev->pdev->device == SWITCH_IB_PCI_DEV_ID ||
+         sx_dev->pdev->device == SWITCH_IB2_PCI_DEV_ID)) {
+        err = sx_dpt_move(alloc_dev_id, DEFAULT_DEVICE_ID);
+        if (err) {
+            goto out;
+        }
+
+        *dpt_dev_id = DEFAULT_DEVICE_ID;
+        goto out;
+    }
+#endif
+
+    *dpt_dev_id = alloc_dev_id;
+out:
+    return 0;
+}
+
+/*
+ * Build pci dev id [8 bit busno|8 bit slot|8 bit devfn]
+ * for example on piranha ppc : 0x18100
+ */
+unsigned int sx_dpt_build_pci_dev_id(struct sx_dev *sx_dev)
+{
+    unsigned int dpt_pci_dev_id = 0;
+
+#ifdef NO_PCI
+    return 0x18100;
+#endif
+
+    dpt_pci_dev_id |= sx_dev->pdev->bus->number << 16;
+    dpt_pci_dev_id |= PCI_SLOT(sx_dev->pdev->devfn) << 8;
+    dpt_pci_dev_id |= PCI_FUNC(sx_dev->pdev->devfn);
+    printk("%s: Build pci_dev_id: 0x%x , bus:%d , slot(dev):%d, fn:%d \n",
+           __func__, dpt_pci_dev_id, sx_dev->pdev->bus->number,
+           PCI_SLOT(sx_dev->pdev->devfn), PCI_FUNC(sx_dev->pdev->devfn));
+    return dpt_pci_dev_id;
+}
+
+int sx_dpt_init_dev_pci(struct sx_dev *sx_dev)
+{
+    union ku_dpt_path_info path_data;
+    int                    dev_id;
+    int                    err;
+
+    err = sx_dpt_alloc_pci_dev_id(sx_dev, &dev_id);
+    if (err) {
+        sx_err(sx_dev, "Failed adding PCI path to the default device\n");
+        return err;
+    }
+
+    sx_dev->device_id = dev_id;
+    printk("Called %s with device_id: %d \n", __func__, sx_dev->device_id);
+    path_data.sx_pcie_info.pci_id = sx_dpt_build_pci_dev_id(sx_dev);
+    path_data.sx_pcie_info.sx_dev = sx_dev;
+    err = sx_dpt_add_dev_path(dev_id, DPT_PATH_PCI_E, path_data, 1);
+    if (err) {
+        sx_err(sx_dev, "Failed adding PCI path to the default device\n");
+        return err;
+    }
+
+#ifndef NO_PCI
+    sx_dpt_set_cmd_path(dev_id, DPT_PATH_PCI_E);
+    sx_dpt_set_emad_path(dev_id, DPT_PATH_PCI_E);
+    sx_dpt_set_mad_path(dev_id, DPT_PATH_PCI_E);
+#else
+    sx_dpt_set_cmd_path(dev_id, DPT_PATH_I2C);
+    sx_dpt_set_emad_path(dev_id, DPT_PATH_I2C);
+    sx_dpt_set_mad_path(dev_id, DPT_PATH_I2C);
+#endif
+
+    if (dev_id == DEFAULT_DEVICE_ID) {
+        sx_glb.tmp_dev_ptr = sx_dev;
+    }
+
+
+    return err;
+}
+
+int sx_dpt_init()
+{
+    int err = 0;
+    int i;
+
+    ENTER_FUNC();
+
+    for (i = 0; i < MAX_NUM_OF_REMOTE_SWITCHES; i++) {
+        sx_glb.sx_dpt.dpt_info[i].cmd_path = DPT_PATH_INVALID;
+        sx_glb.sx_dpt.dpt_info[i].emad_path = DPT_PATH_INVALID;
+        sx_glb.sx_dpt.dpt_info[i].mad_path = DPT_PATH_INVALID;
+        sx_glb.sx_dpt.dpt_info[i].cr_access_path = DPT_PATH_INVALID;
+
+        memset(sx_glb.sx_dpt.dpt_info[i].is_ifc_valid, 0,
+               sizeof(sx_glb.sx_dpt.dpt_info[i].is_ifc_valid));
+    }
+#ifdef NO_PCI
+    sx_glb.sx_i2c.read = NULL;
+    sx_glb.sx_i2c.write = NULL;
+    sx_glb.sx_i2c.read_dword = NULL;
+    sx_glb.sx_i2c.write_dword = NULL;
+    sx_glb.sx_i2c.enforce = NULL;
+    sx_glb.sx_i2c.release = NULL;
+#else
+    sx_glb.sx_i2c.read = sx_dpt_stub_i2c_read;
+    sx_glb.sx_i2c.write = sx_dpt_stub_i2c_write;
+    sx_glb.sx_i2c.read_dword = sx_dpt_stub_i2c_read_dword;
+    sx_glb.sx_i2c.write_dword = sx_dpt_stub_i2c_write_dword;
+    sx_glb.sx_i2c.enforce = sx_dpt_stub_i2c_enforce;
+    sx_glb.sx_i2c.release = sx_dpt_stub_i2c_release;
+#endif
+    sx_glb.sx_i2c.get_fw_rev = NULL;
+    sx_glb.sx_i2c.set_go_bit_stuck = NULL;
+    sx_glb.sx_sgmii.initialized = 0;
+    sx_glb.sx_sgmii.base_smac = 0;
+    sx_sgmii_init_cb(&sx_glb.sx_sgmii);
+
+    EXIT_FUNC();
+    return err;
+}
+
+#if 0
+int sx_dpt_find_pci_dev_old(unsigned int sx_pci_dev_id, int vendor, int device, struct pci_dev **sx_pci_dev)
+{
+    int             pci_bus_id;
+    int             pci_dev_id;
+    int             pci_func_id;
+    struct pci_dev *tmp_dev = NULL;
+    int             err = 1;
+
+    *sx_pci_dev = NULL;
+
+    pci_bus_id = SX_GET_PCI_BUS_ID(sx_pci_dev_id);
+    pci_dev_id = SX_GET_PCI_DEV_ID(sx_pci_dev_id);
+    pci_func_id = SX_GET_PCI_FUNC_ID(sx_pci_dev_id);
+    printk(KERN_DEBUG "%s(): find pci ven:0x%x, dev: %x "
+           "pci addr %x:%x:%x\n", __func__, vendor, device,
+           pci_bus_id, pci_dev_id, pci_func_id);
+
+    do {
+        tmp_dev = pci_get_device(vendor, device, tmp_dev);
+        *sx_pci_dev = tmp_dev;
+        if ((tmp_dev != NULL) && tmp_dev->bus) {
+            printk(KERN_DEBUG "bus_num: %d , %s , dev: %d , "
+                   "fn: %d, tmp_dev->devfn %d \n",
+                   tmp_dev->bus->number,
+                   tmp_dev->bus->name,
+                   PCI_SLOT(tmp_dev->devfn),
+                   PCI_FUNC(tmp_dev->devfn),
+                   tmp_dev->devfn);
+            printk(KERN_DEBUG "PCI Device found successfully\n");
+            err = 0;
+            break;
+        }
+    } while (tmp_dev != NULL);
+
+    return err;
+}
+#endif
+
+int sx_dpt_find_pci_dev(unsigned int sx_pci_dev_id, int vendor, int device, struct pci_dev **sx_pci_dev)
+{
+    int            pci_bus_id;
+    int            pci_dev_id;
+    int            pci_func_id;
+    int            pci_devfn;
+    int            err = 1;
+    struct sx_dev *tmp_dev = NULL, *curr_dev = NULL;
+    int            is_found = 0;
+
+    *sx_pci_dev = NULL;
+
+    pci_bus_id = SX_GET_PCI_BUS_ID(sx_pci_dev_id);
+    pci_dev_id = SX_GET_PCI_DEV_ID(sx_pci_dev_id);
+    pci_func_id = SX_GET_PCI_FUNC_ID(sx_pci_dev_id);
+    pci_devfn = PCI_DEVFN(pci_dev_id, pci_func_id);
+
+    printk(KERN_DEBUG "%s(): lookup for sx_pci_dev_id: 0x%x "
+           "pci addr %x:%x:%x\n", __func__, sx_pci_dev_id,
+           pci_bus_id, pci_dev_id, pci_func_id);
+
+    /* The user gives us dev_id in the data param (immediate) */
+    list_for_each_entry_safe(curr_dev, tmp_dev, &sx_glb.pci_devs_list, list) {
+        printk(KERN_DEBUG "%s(): CHECK vendor: 0x%x , device : 0x%x ,"
+               "pci addr %x:%x:%x\n", __func__,
+               curr_dev->pdev->vendor, curr_dev->pdev->device,
+               curr_dev->pdev->bus->number,
+               PCI_SLOT(curr_dev->pdev->devfn),
+               PCI_FUNC(curr_dev->pdev->devfn));
+        if ((curr_dev->pdev->vendor == PCI_VENDOR_ID_MELLANOX) &&
+            (curr_dev->pdev->bus->number == pci_bus_id) &&
+            (curr_dev->pdev->devfn == pci_devfn)) {
+            is_found = 1;
+            err = 0;
+            *sx_pci_dev = curr_dev->pdev;
+            break;
+        }
+    }
+
+    if (!is_found &&
+        (sx_glb.sx_dpt.dpt_info[DEFAULT_DEVICE_ID].sx_pcie_info.sx_dev != NULL)) {
+        curr_dev = sx_glb.sx_dpt.dpt_info[DEFAULT_DEVICE_ID].sx_pcie_info.sx_dev;
+        *sx_pci_dev = curr_dev->pdev;
+        err = 0;
+    }
+
+    return err;
+}
+
+bool sx_dpt_is_path_valid(int sx_dev_id, enum  ku_dpt_path_type path)
+{
+    return sx_glb.sx_dpt.dpt_info[sx_dev_id].is_ifc_valid[path];
+}
+
+/*
+ * This function will add path to entry dev_id in DPT. The path
+ * should be provided with remote device description. For
+ * example , if added path is I2C than path_data should be
+ * i2c_dev_id.
+ */
+int sx_dpt_add_dev_path(int sx_dev_id, enum ku_dpt_path_type path, union ku_dpt_path_info path_data, uint8_t is_local)
+{
+    int err = 0;
+
+#ifndef NO_PCI
+    struct pci_dev *sx_pci_dev = NULL;
+#endif
+    unsigned int sx_pci_dev_id;
+
+    ENTER_FUNC();
+
+    switch (path) {
+    case DPT_PATH_I2C:
+        if (sx_glb.sx_i2c.is_registered == 0) {
+            sx_dpt_reg_i2c_ifc(&sx_glb.sx_i2c);
+            sx_glb.sx_i2c.is_registered = 1;
+        }
+
+        sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_i2c_info = path_data.sx_i2c_info;
+        sx_glb.sx_dpt.dpt_info[sx_dev_id].is_ifc_valid[DPT_PATH_I2C] = true;
+        if (sx_glb.sx_i2c.get_local_mbox) {
+            sx_glb.sx_i2c.get_local_mbox(
+                path_data.sx_i2c_info.sx_i2c_dev,
+                &sx_glb.sx_dpt.dpt_info[sx_dev_id].in_mb_size,
+                &sx_glb.sx_dpt.dpt_info[sx_dev_id].in_mb_offset,
+                &sx_glb.sx_dpt.dpt_info[sx_dev_id].out_mb_size,
+                &sx_glb.sx_dpt.dpt_info[sx_dev_id].out_mb_offset);
+        }
+        if (!is_local && sx_glb.sx_i2c.get_fw_rev) {
+            err = sx_glb.sx_i2c.enforce(path_data.sx_i2c_info.sx_i2c_dev);
+            if (err) {
+                PRINTK_ERR("I2C bus %d of device %d is not ready. "
+                           "query_fw will not be performed. err=%d\n",
+                           path_data.sx_i2c_info.sx_i2c_dev, sx_dev_id, err);
+                err = -EBUSY;
+                break;
+            }
+
+            sx_glb.sx_i2c.get_fw_rev(path_data.sx_i2c_info.sx_i2c_dev,
+                                     &sx_glb.sx_dpt.dpt_info[sx_dev_id].fw_rev);
+            err = sx_glb.sx_i2c.release(path_data.sx_i2c_info.sx_i2c_dev);
+            if (err) {
+                PRINTK_ERR("I2C bus %d of device %d is not ready. "
+                           "i2c release failed. err=%d\n",
+                           path_data.sx_i2c_info.sx_i2c_dev, sx_dev_id, err);
+                err = -EBUSY;
+                break;
+            }
+        }
+
+        break;
+
+    case DPT_PATH_SGMII:
+        if (!sx_glb.sx_sgmii.init) {
+            PRINTK_ERR("Cannot add SGMII path. SGMII is not supported\n");
+            err = -EINVAL;
+            break;
+        }
+
+        if (!sx_glb.sx_sgmii.initialized) {
+            if (sx_glb.sx_sgmii.init()) {
+                err = -EINVAL;
+                break;
+            }
+        }
+
+        sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_sgmii_info = path_data.sx_sgmii_info;
+        sx_glb.sx_dpt.dpt_info[sx_dev_id].is_ifc_valid[DPT_PATH_SGMII] = true;
+        break;
+
+    case DPT_PATH_PCI_E:
+        sx_pci_dev_id = path_data.sx_pcie_info.pci_id;
+#ifndef NO_PCI
+        err = sx_dpt_find_pci_dev(sx_pci_dev_id, 0, 0, &sx_pci_dev);
+        if (err) {
+            PRINTK_ERR("sx_dpt_find_pci_dev failed, sx_pci_dev_id: 0x%x \n",
+                       sx_pci_dev_id);
+            err = -EINVAL;
+            goto out;
+        }
+
+        sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_pcie_info.pci_id = sx_pci_dev_id;
+        sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_pcie_info.sx_dev = pci_get_drvdata(sx_pci_dev);
+        if (is_local) {
+            ((struct sx_dev *)(sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_pcie_info.sx_dev))->device_id = sx_dev_id;
+        }
+#else
+        sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_pcie_info.pci_id = sx_pci_dev_id;
+        sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_pcie_info.sx_dev = sx_glb.tmp_dev_ptr;
+        sx_glb.tmp_dev_ptr->device_id = sx_dev_id;
+#endif
+        sx_glb.sx_dpt.dpt_info[sx_dev_id].is_ifc_valid[DPT_PATH_PCI_E] = true;
+        printk(KERN_INFO PFX "Successfully added path %s to device %d pci_dev_id %u\n",
+               dpt_type2str[path], sx_dev_id, sx_pci_dev_id);
+        break;
+
+    default:
+        PRINTK_ERR("Try to set invalid path to sx_dev_id: %d, dpt_path: %s\n",
+                   sx_dev_id, dpt_type2str[path]);
+        err = -EINVAL;
+        break;
+    }
+
+#ifndef NO_PCI
+out:
+#endif
+
+    if (!err) {
+        printk(KERN_INFO PFX "Successfully added path %s to device %d\n",
+               dpt_type2str[path], sx_dev_id);
+    }
+
+    EXIT_FUNC();
+    return err;
+}
+
+/**
+ * This function with params dev_id and path will invalidate the
+ * given "path" at index DEV_ID in DPT. All other pathes will
+ * remains without changes.
+ */
+int sx_dpt_remove_dev_path(int sx_dev_id, enum ku_dpt_path_type path)
+{
+    int err = 0;
+
+    ENTER_FUNC();
+
+    if (sx_glb.sx_dpt.dpt_info[sx_dev_id].cmd_path == path) {
+        PRINTK_ERR("Can't remove path because %d "
+                   "is current active cmd_path.\n", path);
+        return -EINVAL;
+    }
+
+    if (sx_glb.sx_dpt.dpt_info[sx_dev_id].emad_path == path) {
+        PRINTK_ERR("Can't remove path because %d "
+                   "is current active emad_path.\n", path);
+        return -EINVAL;
+    }
+
+    if (sx_glb.sx_dpt.dpt_info[sx_dev_id].mad_path == path) {
+        PRINTK_ERR("Can't remove path because %d "
+                   "is current active mad_path.\n", path);
+        return -EINVAL;
+    }
+
+    if (sx_glb.sx_dpt.dpt_info[sx_dev_id].cr_access_path == path) {
+        PRINTK_ERR("Can't remove path because %d "
+                   "is current active cr_access_path.\n", path);
+        return -EINVAL;
+    }
+
+    switch (path) {
+    case DPT_PATH_I2C:
+        sx_glb.sx_dpt.dpt_info[sx_dev_id].is_ifc_valid[DPT_PATH_I2C] =
+            false;
+        memset(&sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_i2c_info, 0,
+               sizeof(sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_i2c_info));
+        break;
+
+    case DPT_PATH_SGMII:
+        sx_glb.sx_dpt.dpt_info[sx_dev_id].is_ifc_valid[DPT_PATH_SGMII]
+            = false;
+        memset(&sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_sgmii_info, 0,
+               sizeof(sx_glb.sx_dpt.dpt_info[sx_dev_id].
+                      sx_sgmii_info));
+        break;
+
+    case DPT_PATH_PCI_E:
+        sx_glb.sx_dpt.dpt_info[sx_dev_id].is_ifc_valid[DPT_PATH_PCI_E]
+            = false;
+        memset(&sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_pcie_info, 0,
+               sizeof(sx_glb.sx_dpt.dpt_info[sx_dev_id].
+                      sx_pcie_info));
+        break;
+
+    default:
+        PRINTK_ERR("Invalid path.\n");
+        err = -EINVAL;
+    }
+
+    if (!err) {
+        printk(KERN_INFO PFX "Successfully removed path %s from device %d\n",
+               dpt_type2str[path], sx_dev_id);
+    }
+
+    EXIT_FUNC();
+    return err;
+}
+
+/**
+ * This function with params dev_id and path will invalidate all pathes
+ * at index DEV_ID in DPT.
+ */
+void __sx_dpt_remove_dev(int sx_dev_id)
+{
+    ENTER_FUNC();
+    printk(KERN_DEBUG "%s() Remove dev %d \n", __func__, sx_dev_id);
+
+    sx_glb.sx_dpt.dpt_info[sx_dev_id].cmd_path = DPT_PATH_INVALID;
+    sx_glb.sx_dpt.dpt_info[sx_dev_id].emad_path = DPT_PATH_INVALID;
+    sx_glb.sx_dpt.dpt_info[sx_dev_id].mad_path = DPT_PATH_INVALID;
+    sx_glb.sx_dpt.dpt_info[sx_dev_id].cr_access_path = DPT_PATH_INVALID;
+
+    sx_glb.sx_dpt.dpt_info[sx_dev_id].is_ifc_valid[DPT_PATH_I2C] = false;
+    sx_glb.sx_dpt.dpt_info[sx_dev_id].is_ifc_valid[DPT_PATH_SGMII] = false;
+    sx_glb.sx_dpt.dpt_info[sx_dev_id].is_ifc_valid[DPT_PATH_PCI_E] = false;
+
+    memset(&sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_i2c_info, 0,
+           sizeof(sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_i2c_info));
+    memset(&sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_sgmii_info, 0,
+           sizeof(sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_sgmii_info));
+    memset(&sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_pcie_info, 0,
+           sizeof(sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_pcie_info));
+
+    printk(KERN_INFO PFX "Successfully removed device %d from the DPT\n", sx_dev_id);
+    EXIT_FUNC();
+}
+
+int sx_dpt_is_dev_pci_attached(int sx_dev_id)
+{
+    if (sx_glb.sx_dpt.dpt_info[sx_dev_id].is_ifc_valid[DPT_PATH_PCI_E] &&
+            (sx_glb.sx_dpt.dpt_info[sx_dev_id].cmd_path == DPT_PATH_PCI_E) ){
+        return 1;
+    }
+
+    return 0;
+}
+
+int sx_dpt_remove_dev(int sx_dev_id, int restart_flow)
+{
+    int err = 0;
+    void  *sx_dev = NULL; 
+    void  *def_sx_dev = NULL;
+    
+    if (sx_glb.sx_dpt.dpt_info[sx_dev_id].is_ifc_valid[DPT_PATH_PCI_E]){
+        sx_dev = sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_pcie_info.sx_dev;
+    }
+    
+    if (sx_glb.sx_dpt.dpt_info[DEFAULT_DEVICE_ID].is_ifc_valid[DPT_PATH_PCI_E]){
+        def_sx_dev = sx_glb.sx_dpt.dpt_info[DEFAULT_DEVICE_ID].sx_pcie_info.sx_dev;
+    }        
+    
+    /* 
+     * if the removed device is PCI (CMD_PATH=PCI) and 
+     * the it is the same as default device then remove default device too 
+     */ 
+    if ( (sx_glb.sx_dpt.dpt_info[sx_dev_id].cmd_path == DPT_PATH_PCI_E) && 
+         (sx_dev == def_sx_dev) && restart_flow) {
+        __sx_dpt_remove_dev(DEFAULT_DEVICE_ID);
+    }
+    
+    __sx_dpt_remove_dev(sx_dev_id);
+
+    return err;
+}
+
+/**
+ * This function is used to change dpt path : i2c , pci-e , sgmii
+ * by changing the callbacks to approprite interface functions
+ * To validate if the change is suceessed need to make some
+ * wr/rd to check if the new interface is working
+ */
+int sx_dpt_set_cmd_path(int sx_dev_id, enum  ku_dpt_path_type cmd_path)
+{
+    int                   err = 0;
+    enum ku_dpt_path_type old_cmd_path;
+
+    ENTER_FUNC();
+
+    if (sx_glb.sx_dpt.dpt_info[sx_dev_id].is_ifc_valid[cmd_path] == false) {
+        PRINTK_ERR("sx_dev_id:%d , path %d is not valid !\n", sx_dev_id, cmd_path);
+        return -EINVAL;
+    }
+
+    if ((cmd_path != DPT_PATH_I2C) && (cmd_path != DPT_PATH_PCI_E) && (cmd_path != DPT_PATH_SGMII)) {
+        PRINTK_ERR("path %d is not valid ! Supported CMD pathes: "
+                   "I2C and PCI-E\n", cmd_path);
+        return -EINVAL;
+    }
+
+    /* save old path */
+    old_cmd_path = sx_glb.sx_dpt.dpt_info[sx_dev_id].cmd_path;
+    /* change the pathes */
+    sx_glb.sx_dpt.dpt_info[sx_dev_id].cmd_path = cmd_path;
+    /* validate new cmd path */
+    if (old_cmd_path != sx_glb.sx_dpt.dpt_info[sx_dev_id].cmd_path) {
+        err = sx_dpt_validate_new_cmd_path(sx_dev_id, cmd_path);
+        if (err) {
+            PRINTK_ERR("sx_dpt_validate_new_cmd_path() "
+                       "for path %d failed. Rollback cmd_path to %d\n",
+                       cmd_path, old_cmd_path);
+            sx_glb.sx_dpt.dpt_info[sx_dev_id].cmd_path =
+                old_cmd_path;
+        }
+    }
+
+    if (!err) {
+        printk(KERN_INFO PFX "Successfully set CMD path for device %d to %s\n",
+               sx_dev_id, dpt_type2str[cmd_path]);
+    }
+
+    EXIT_FUNC();
+    return err;
+}
+
+int sx_dpt_get_i2c_info(int sx_dev_id, struct ku_dpt_i2c_info** i2c_info)
+{
+    if ((i2c_info == NULL) || (sx_dev_id > MAX_NUM_OF_REMOTE_SWITCHES)) {
+        return -EINVAL;
+    }
+
+    *i2c_info = &(sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_i2c_info);
+    return 0;
+}
+
+int sx_dpt_get_cmd_path(int sx_dev_id)
+{
+    /* DEFAULT_DEVICE_ID is a WA for FW burn on local device
+     * before device ID is known. Burning FW will be done through CMD IFC
+     * over the PCI */
+#ifndef NO_PCI
+    if (sx_dev_id == DEFAULT_DEVICE_ID) {
+        return DPT_PATH_PCI_E;
+    }
+#endif
+
+    return sx_glb.sx_dpt.dpt_info[sx_dev_id].cmd_path;
+}
+
+/**
+ * This function is used to set the emad path in the dpt
+ */
+int sx_dpt_set_emad_path(int sx_dev_id, enum  ku_dpt_path_type emad_path)
+{
+    int                   err = 0;
+    enum ku_dpt_path_type old_emad_path;
+
+    ENTER_FUNC();
+
+    if (sx_glb.sx_dpt.dpt_info[sx_dev_id].is_ifc_valid[emad_path] ==
+        false) {
+        PRINTK_ERR("path %d is not valid !\n", emad_path);
+        return -EINVAL;
+    }
+
+    /* save old path */
+    old_emad_path = sx_glb.sx_dpt.dpt_info[sx_dev_id].emad_path;
+    /* change the pathes */
+    sx_glb.sx_dpt.dpt_info[sx_dev_id].emad_path = emad_path;
+    /* validate new emad path */
+    if (old_emad_path != sx_glb.sx_dpt.dpt_info[sx_dev_id].emad_path) {
+        err = sx_dpt_validate_new_emad_path(sx_dev_id);
+        if (err) {
+            PRINTK_ERR("sx_dpt_validate_new_emad_path() for "
+                       "path %d failed. Rollback emad_path to %d\n",
+                       emad_path, old_emad_path);
+            sx_glb.sx_dpt.dpt_info[sx_dev_id].emad_path =
+                old_emad_path;
+        }
+    }
+
+    if (!err) {
+        printk(KERN_INFO PFX "Successfully set EMAD path for device %d to %s\n",
+               sx_dev_id, dpt_type2str[emad_path]);
+    }
+
+    EXIT_FUNC();
+    return err;
+}
+
+int sx_dpt_get_emad_path(int sx_dev_id)
+{
+    return sx_glb.sx_dpt.dpt_info[sx_dev_id].emad_path;
+}
+
+int sx_dpt_set_mad_path(int sx_dev_id, enum  ku_dpt_path_type mad_path)
+{
+    int                   err = 0;
+    enum ku_dpt_path_type old_mad_path;
+
+    ENTER_FUNC();
+
+    if ((sx_dev_id < 0) ||
+        (sx_dev_id > MAX_NUM_OF_REMOTE_SWITCHES)) {
+        PRINTK_ERR("Device id is not valid");
+        return -EINVAL;
+    }
+
+    if ((mad_path < DPT_PATH_MIN) ||
+        (mad_path > DPT_PATH_MAX)) {
+        PRINTK_ERR("MAD path is not valid");
+        return -EINVAL;
+    }
+
+    if (sx_glb.sx_dpt.dpt_info[sx_dev_id].is_ifc_valid[mad_path] == false) {
+        PRINTK_ERR("Path %d is not valid !\n", mad_path);
+        return -EINVAL;
+    }
+
+    /* save old path */
+    old_mad_path = sx_glb.sx_dpt.dpt_info[sx_dev_id].mad_path;
+    /* change the pathes */
+    sx_glb.sx_dpt.dpt_info[sx_dev_id].mad_path = mad_path;
+    /* validate new emad path */
+    if (old_mad_path != sx_glb.sx_dpt.dpt_info[sx_dev_id].mad_path) {
+        err = sx_dpt_validate_new_emad_path(sx_dev_id);
+        if (err) {
+            PRINTK_ERR("sx_dpt_validate_new_emad_path() for path "
+                       "%d failed. Rollback emad_path to %d\n",
+                       mad_path, old_mad_path);
+            sx_glb.sx_dpt.dpt_info[sx_dev_id].mad_path =
+                old_mad_path;
+        }
+    }
+
+    if (!err) {
+        printk(KERN_INFO PFX "Successfully set MAD path for device %d to %s\n",
+               sx_dev_id, dpt_type2str[mad_path]);
+    }
+
+    EXIT_FUNC();
+    return err;
+}
+
+int sx_dpt_set_cr_access_path(int sx_dev_id, enum ku_dpt_path_type cr_access_path)
+{
+    int err = 0;
+
+    ENTER_FUNC();
+
+    if ((sx_dev_id < 0) ||
+        (sx_dev_id > MAX_NUM_OF_REMOTE_SWITCHES)) {
+        PRINTK_ERR("Device id is not valid.\n");
+        return -EINVAL;
+    }
+
+    if ((cr_access_path < DPT_PATH_MIN) ||
+        (cr_access_path > DPT_PATH_MAX)) {
+        PRINTK_ERR("CR_ACCESS path is not valid.\n");
+        return -EINVAL;
+    }
+
+    if (sx_glb.sx_dpt.dpt_info[sx_dev_id].is_ifc_valid[cr_access_path] == false) {
+        PRINTK_ERR("path %d is not valid !\n", cr_access_path);
+        return -EINVAL;
+    }
+
+    /* change the path */
+    sx_glb.sx_dpt.dpt_info[sx_dev_id].cr_access_path = cr_access_path;
+
+    printk(KERN_INFO PFX "Successfully set CR_ACCESS path for device %d to %s\n",
+               sx_dev_id, dpt_type2str[cr_access_path]);
+
+    EXIT_FUNC();
+    return err;
+}
+
+enum ku_dpt_path_type sx_dpt_get_cr_access_path(int sx_dev_id)
+{
+    return sx_glb.sx_dpt.dpt_info[sx_dev_id].cr_access_path;
+}
+
+int sx_get_sdq_from_profile(struct sx_dev *dev, enum ku_pkt_type type, u8 swid, u8 etclass, u8 *stclass, u8 *sdq)
+{
+    if ((type == SX_PKT_TYPE_DROUTE_EMAD_CTL) ||
+        (type == SX_PKT_TYPE_EMAD_CTL)) {
+        if (stclass) {
+            *stclass = dev->profile.emad_tx_prof.stclass;
+        }
+        *sdq = dev->profile.emad_tx_prof.sdq;
+        goto out;
+    }
+
+    if (swid >= NUMBER_OF_SWIDS) {
+        printk("swid %d is out of range [0..%d], force to 0\n",
+               swid, NUMBER_OF_SWIDS);
+        swid = 0;
+    }
+
+    if (etclass >= NUMBER_OF_ETCLASSES) {
+        printk("etclass %d is out of range [0..%d], force to 0\n",
+               etclass, NUMBER_OF_ETCLASSES);
+        etclass = 0;
+    }
+
+    if (stclass) {
+        *stclass = dev->profile.tx_prof[swid][etclass].stclass;
+    }
+    *sdq = dev->profile.tx_prof[swid][etclass].sdq;
+
+out:
+    return 0;
+}
+
+int sx_get_sdq_per_traffic_type(struct sx_dev *dev, enum ku_pkt_type type, u8 swid, u8 etclass, u8 *stclass, u8 *sdq)
+{
+    if ((type == SX_PKT_TYPE_DROUTE_EMAD_CTL) ||
+        (type == SX_PKT_TYPE_EMAD_CTL)) {
+        *sdq = 0;
+    } else if ((type == SX_PKT_TYPE_ETH_CTL_UC) ||
+               (type == SX_PKT_TYPE_ETH_CTL_MC)) {
+        *sdq = 1;
+    } else {
+        *sdq = 2;
+    }
+    return 0;
+}
+static void get_specific_data(enum ku_pkt_type type, u8 to_cpu, u8 etclass, u8 lp, struct isx_specific_data *data)
+{
+    if ((type == SX_PKT_TYPE_ETH_DATA) || (type == SX_PKT_TYPE_IB_RAW_DATA)
+        || (type == SX_PKT_TYPE_IB_TRANSPORT_DATA /*&& !lp*/)) {
+        data->type = 0;
+    } else {
+        data->type = 6;
+    }
+    if ((type == SX_PKT_TYPE_DROUTE_EMAD_CTL) || (type == SX_PKT_TYPE_EMAD_CTL)) {
+        data->emad = 1;
+    } else {
+        data->emad = 0;
+    }
+    if ((type == SX_PKT_TYPE_ETH_CTL_MC) || (type == SX_PKT_TYPE_FC_CTL_MC)
+        || (type == SX_PKT_TYPE_FCOE_CTL_MC)) {
+        data->mc = 1;
+    } else {
+        data->mc = 0;
+    }
+    if (type >= SX_PKT_TYPE_IB_RAW_CTL) {
+        data->protocol = 0;
+    } else {
+        data->protocol = 1;
+    }
+
+    switch (type) {
+    case SX_PKT_TYPE_ETH_CTL_UC:
+    case SX_PKT_TYPE_ETH_CTL_MC:
+    case SX_PKT_TYPE_EMAD_CTL:
+    case SX_PKT_TYPE_FC_CTL_UC:
+    case SX_PKT_TYPE_FC_CTL_MC:
+    case SX_PKT_TYPE_FCOE_CTL_UC:
+    case SX_PKT_TYPE_FCOE_CTL_MC:
+        data->ctl = 0;
+        break;
+
+    case SX_PKT_TYPE_ETH_DATA:
+    case SX_PKT_TYPE_IB_RAW_DATA:
+    case SX_PKT_TYPE_IB_TRANSPORT_DATA:
+        data->ctl = 1;
+        break;
+
+    case SX_PKT_TYPE_DROUTE_EMAD_CTL:
+        data->ctl = 2;
+        break;
+
+    case SX_PKT_TYPE_IB_RAW_CTL:
+    case SX_PKT_TYPE_IB_TRANSPORT_CTL:
+    case SX_PKT_TYPE_EOIB_CTL:
+    case SX_PKT_TYPE_FCOIB_CTL:
+        data->ctl = 3;
+        break;
+
+    default:
+        break;
+    }
+
+    if (to_cpu) {
+        data->ctclass = (etclass & 0x8) >> 3;
+    } else {
+        data->ctclass = 0;
+    }
+
+    data->cpu_signature = 0;
+    data->signature = 0xE0E0;
+}
+
+static void get_specific_data_version_1(enum ku_pkt_type          type,
+                                        u8                        to_cpu,
+                                        u8                        etclass,
+                                        u8                        lp,
+                                        u8                        rx_is_router,
+                                        u8                        fid_valid,
+                                        u16                       fid,
+                                        struct isx_specific_data *data)
+{
+    switch (type) {
+    case SX_PKT_TYPE_ETH_CTL_UC:
+    case SX_PKT_TYPE_ETH_CTL_MC:
+        data->type = 6;
+        data->protocol = 1;
+        data->ctl = 0;
+        data->use_control_tclass = 1;
+        data->etclass = 0; /* When control_tclass = 1, this field is reserved */
+        data->fid = 0;
+        data->fid_valid = 0;
+        data->rx_is_router = 0;
+        break;
+
+    case SX_PKT_TYPE_ETH_DATA:
+        data->type = 0;
+        data->protocol = 1;
+        data->ctl = 1;
+        data->use_control_tclass = 0;
+        data->etclass = 0;
+        data->rx_is_router = rx_is_router;
+        data->fid_valid = fid_valid;
+        data->fid = fid;
+        break;
+
+    case SX_PKT_TYPE_DROUTE_EMAD_CTL:
+    case SX_PKT_TYPE_EMAD_CTL:
+        data->type = 6;
+        data->protocol = 1;
+        data->ctl = 2;
+        data->use_control_tclass = 0;
+        data->etclass = 0;
+        data->fid = 0;
+        data->fid_valid = 0;
+        data->rx_is_router = 0;
+        break;
+
+    default:
+        break;
+    }
+
+    data->version = 1;
+}
+
+
+/**
+ * get HW etclass from device callback structure
+ */
+int sx_get_hw_etclass(struct isx_meta *meta, u8* hw_etclass)
+{
+    struct sx_dev *dev;
+    unsigned long  flags;
+    int            err;
+
+    err = sx_dpt_get_sx_dev_by_id(meta->dev_id, &dev);
+    if (err) {
+        PRINTK_ERR("Invalid device id %d\n", meta->dev_id);
+        return -EINVAL;
+    }
+    
+    /* case of SGMII dev */
+    if (dev == NULL){
+        *hw_etclass = meta->etclass;
+        return 0;
+    }
+
+    spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+    if (sx_priv(dev)->dev_specific_cb.get_hw_etclass_cb != NULL) {
+        sx_priv(dev)->dev_specific_cb.get_hw_etclass_cb(meta, hw_etclass);
+    } else {
+        *hw_etclass = meta->etclass;
+    }
+    spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+
+    return 0;
+}
+
+/**
+ * callback structure for selecting SDQ based on condor and BAZ
+ */
+int sx_get_sdq(struct isx_meta *meta,
+               struct sx_dev   *dev,
+               enum ku_pkt_type type,
+               u8               swid,
+               u8               etclass,
+               u8              *stclass,
+               u8              *sdq)
+{
+    unsigned long flags;
+    u8            hw_etclass = 0;
+
+
+    /* this func also take priv->db_lock */
+    if (sx_get_hw_etclass(meta, &hw_etclass) != 0) {
+        PRINTK_ERR("Error retrieving HW Etclass!\n");
+        return -EINVAL;
+    }
+
+    spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+    if (dev && (sx_priv(dev)->dev_specific_cb.sx_get_sdq_cb != NULL)) {
+        sx_priv(dev)->dev_specific_cb.sx_get_sdq_cb(dev, type, swid,
+                                                    etclass, stclass, sdq);
+    } else {
+        PRINTK_ERR("Error retrieving sx_get_sdq_cb callback structure!\n");
+        spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+        return -EINVAL;
+    }
+
+    spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+    return 0;
+}
+
+/**
+ * callback structure for filling meta based on condor and BAZ
+ */
+int sx_build_isx_header(struct isx_meta *meta, struct sk_buff *skb, u8 stclass)
+{
+    struct sx_dev *dev = sx_glb.tmp_dev_ptr;
+    unsigned long  flags;
+    u8             hw_etclass = 0;
+
+    /* this func also take priv->db_lock */
+    if (sx_get_hw_etclass(meta, &hw_etclass) != 0) {
+        PRINTK_ERR("Error retrieving HW Etclass!\n");
+        return -EINVAL;
+    }
+
+    spin_lock_irqsave(&sx_priv(dev)->db_lock, flags);
+
+    if (dev && (sx_priv(dev)->dev_specific_cb.sx_build_isx_header_cb != NULL)) {
+        sx_priv(dev)->dev_specific_cb.sx_build_isx_header_cb(meta, skb, stclass, hw_etclass);
+    } else {
+        PRINTK_ERR("Error retrieving sx_build_isx_header callback structure!\n");
+        spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+        return -EINVAL;
+    }
+
+    spin_unlock_irqrestore(&sx_priv(dev)->db_lock, flags);
+    return 0;
+}
+
+/**
+ * Build isx_header from the given meta (meta-data) and push the isx_headr
+ * to the beginning of the skb (before the pkt-buffer part) for Version 0
+ */
+int sx_build_isx_header_v0(struct isx_meta *meta, struct sk_buff *skb, u8 stclass, u8 hw_etclass)
+{
+    struct tx_base_header_v0 *p_hdr;
+    struct isx_specific_data  specific_meta;
+
+    p_hdr = (struct tx_base_header_v0 *)skb_push(skb, ISX_HDR_SIZE);
+    if (p_hdr == NULL) {
+        if (printk_ratelimit()) {
+            printk(KERN_WARNING PFX "sx_build_isx_header: "
+                   "Error in skb_push\n");
+        }
+        return -ENOMEM;
+    }
+
+    memset(p_hdr, 0, sizeof(*p_hdr));
+    memset(&specific_meta, 0, sizeof(specific_meta));
+    get_specific_data(meta->type, meta->to_cpu,
+                      meta->etclass, meta->lp, &specific_meta);
+    p_hdr->ctl_mc |=
+        TO_FIELD(TX_HDR_CTL_MASK,
+                 TX_HDR_CTL_SHIFT, specific_meta.ctl);
+    p_hdr->ctl_mc |=
+        TO_FIELD(TX_HDR_MC_MASK,
+                 TX_HDR_MC_SHIFT, specific_meta.mc);
+    p_hdr->protocol_etclass |=
+        TO_FIELD(TX_HDR_PROTOCOL_MASK,
+                 TX_HDR_PROTOCOL_SHIFT,
+                 specific_meta.protocol);
+    p_hdr->protocol_etclass |=
+        TO_FIELD(TX_HDR_ETCLASS_MASK,
+                 TX_HDR_ETCLASS_SHIFT, hw_etclass);
+    p_hdr->swid |= (u16)TO_FIELD(TX_HDR_SWID_MASK,
+                                 TX_HDR_SWID_SHIFT, meta->swid);
+    p_hdr->swid = cpu_to_be16(p_hdr->swid);
+    p_hdr->system_port_mid |=
+        (u16)TO_FIELD(TX_HDR_SYSTEM_PORT_MID_MASK,
+                      TX_HDR_SYSTEM_PORT_MID_SHIFT,
+                      meta->system_port_mid);
+    p_hdr->system_port_mid = cpu_to_be16(p_hdr->system_port_mid);
+    p_hdr->ctclass3_rdq_cpu_signature |=
+        (u16)TO_FIELD(TX_HDR_CTCLASS3_MASK,
+                      TX_HDR_CTCLASS3_SHIFT,
+                      specific_meta.ctclass);
+    p_hdr->ctclass3_rdq_cpu_signature |=
+        (u16)TO_FIELD(TX_HDR_RDQ_MASK,
+                      TX_HDR_RDQ_SHIFT, meta->rdq);
+    p_hdr->ctclass3_rdq_cpu_signature |=
+        (u16)TO_FIELD(TX_HDR_CPU_SIGNATURE_MASK,
+                      TX_HDR_CPU_SIGNATURE_SHIFT,
+                      specific_meta.cpu_signature);
+    p_hdr->ctclass3_rdq_cpu_signature =
+        cpu_to_be16(p_hdr->ctclass3_rdq_cpu_signature);
+    p_hdr->signature |=
+        (u16)TO_FIELD(TX_HDR_SIGNATURE_MASK,
+                      TX_HDR_SIGNATURE_SHIFT,
+                      specific_meta.signature);
+    p_hdr->signature = cpu_to_be16(p_hdr->signature);
+    p_hdr->stclass_emad_type |=
+        (u16)TO_FIELD(TX_HDR_STCLASS_MASK,
+                      TX_HDR_STCLASS_SHIFT, stclass);
+    p_hdr->stclass_emad_type |=
+        (u16)TO_FIELD(TX_HDR_EMAD_MASK,
+                      TX_HDR_EMAD_SHIFT, specific_meta.emad);
+    p_hdr->stclass_emad_type |=
+        (u16)TO_FIELD(TX_HDR_TYPE_MASK,
+                      TX_HDR_TYPE_SHIFT, specific_meta.type);
+    p_hdr->stclass_emad_type = cpu_to_be16(p_hdr->stclass_emad_type);
+    if (tx_debug) {
+        printk(KERN_DEBUG PFX "sx_core_post_send: Sending for version 0 "
+               "pkt with specific meta: "
+               "sysport:0x%x, specific_meta.ctl: %d,"
+               "specific_meta.protocol: %d\n",
+               meta->system_port_mid,
+               specific_meta.ctl, specific_meta.protocol);
+    }
+    return 0;
+}
+/**
+ * Build isx_header from the given meta (meta-data) and push the isx_headr
+ * to the beginning of the skb (before the pkt-buffer part) for condor (Version 1)
+ */
+int sx_build_isx_header_v1(struct isx_meta *meta, struct sk_buff *skb, u8 stclass,  u8 hw_etclass)
+{
+    struct tx_base_header_v1 *p_hdr;
+    struct isx_specific_data  specific_meta;
+
+    p_hdr = (struct tx_base_header_v1 *)skb_push(skb, ISX_HDR_SIZE);
+    if (p_hdr == NULL) {
+        if (printk_ratelimit()) {
+            printk(KERN_WARNING PFX "sx_build_isx_header: "
+                   "Error in skb_push\n");
+        }
+        return -ENOMEM;
+    }
+
+    memset(p_hdr, 0, sizeof(*p_hdr));
+    memset(&specific_meta, 0, sizeof(specific_meta));
+    get_specific_data_version_1(meta->type, meta->to_cpu,
+                                meta->etclass, meta->lp, meta->rx_is_router,
+                                meta->fid_valid, meta->fid, &specific_meta);
+
+    p_hdr->version_ctl |=
+        TO_FIELD(TX_HDR_VER_MASK_V1,
+                 TX_HDR_VER_SHIFT_V1, specific_meta.version);
+
+    p_hdr->version_ctl |=
+        TO_FIELD(TX_HDR_CTL_MASK,
+                 TX_HDR_CTL_SHIFT, specific_meta.ctl);
+
+    p_hdr->protocol_rx_is_router_fid_valid |=
+        TO_FIELD(TX_HDR_PROTOCOL_MASK,
+                 TX_HDR_PROTOCOL_SHIFT,
+                 specific_meta.protocol);
+
+    p_hdr->protocol_rx_is_router_fid_valid |=
+        TO_FIELD(TX_HDR_RX_IS_ROUTER_MASK_V1,
+                 TX_HDR_RX_IS_ROUTER_SHIFT_V1,
+                 specific_meta.rx_is_router);
+
+    p_hdr->protocol_rx_is_router_fid_valid |=
+        TO_FIELD(TX_HDR_FID_VALID_MASK_V1,
+                 TX_HDR_FID_VALID_SHIFT_V1,
+                 specific_meta.fid_valid);
+
+    p_hdr->swid_control_etclass |= (u16)TO_FIELD(TX_HDR_SWID_MASK,
+                                                 TX_HDR_SWID_SHIFT, meta->swid);
+
+    p_hdr->swid_control_etclass |=
+        (u16)TO_FIELD(TX_HDR_CONTROL_MASK_V1,
+                      TX_HDR_CONTROL_SHIFT_V1,
+                      specific_meta.use_control_tclass);
+    p_hdr->swid_control_etclass |=
+        (u16)TO_FIELD(TX_HDR_ETCLASS_MASK_V1,
+                      TX_HDR_ETCLASS_SHIFT_V1, specific_meta.etclass);
+
+    p_hdr->swid_control_etclass = cpu_to_be16(p_hdr->swid_control_etclass);
+
+    p_hdr->system_port_mid |=
+        (u16)TO_FIELD(TX_HDR_SYSTEM_PORT_MID_MASK,
+                      TX_HDR_SYSTEM_PORT_MID_SHIFT,
+                      meta->system_port_mid);
+
+    p_hdr->system_port_mid = cpu_to_be16(p_hdr->system_port_mid);
+
+    p_hdr->fid |=
+        (u16)TO_FIELD(TX_HDR_FID_MASK_V1,
+                      TX_HDR_FID_SHIFT_V1,
+                      specific_meta.fid);
+    p_hdr->fid = cpu_to_be16(p_hdr->fid);
+
+    p_hdr->type |=
+        TO_FIELD(TX_HDR_TYPE_MASK,
+                 TX_HDR_TYPE_SHIFT, specific_meta.type);
+
+    if (tx_debug) {
+        printk(KERN_DEBUG PFX "sx_core_post_send: Sending "
+               "pkt with specific meta: "
+               "specific_meta.version: %d , swid: %d , sysport:0x%x, specific_meta.ctl: %d,"
+               "specific_meta.protocol: %d, specific_meta.rx_is_router:%d, type: %d, "
+               "specific_meta.fid_valid: %d,  specific_meta.use_control_tclass: %d, specific_meta.etclass :%d,"
+               "specific_meta.fid: %d\n",
+               specific_meta.version, meta->swid,
+               meta->system_port_mid,
+               specific_meta.ctl, specific_meta.protocol, specific_meta.rx_is_router, specific_meta.type,
+               specific_meta.fid_valid, specific_meta.use_control_tclass, specific_meta.etclass, specific_meta.fid);
+    }
+
+    return 0;
+}
+
+/**
+ * This functions will send emads according emad_path defined in DPT : I2C,PCI,SGMII.
+ */
+int sx_dpt_send_emad(int sx_dev_id, struct sk_buff *skb, struct isx_meta *meta)
+{
+    int            err = 0;
+    struct sx_dev *dev = sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_pcie_info.sx_dev;
+
+    ENTER_FUNC();
+
+    DPRINTK(KERN_DEBUG "send emad to dev_id %d, skb: %p\n", sx_dev_id, skb);
+    switch (sx_glb.sx_dpt.dpt_info[sx_dev_id].emad_path) {
+    case DPT_PATH_I2C:
+        PRINTK_ERR("Send emad over I2C isn't supported.\n");
+        err = -EINVAL;
+        break;
+
+    case DPT_PATH_SGMII:
+        err = sx_build_isx_header(meta, skb, 0);
+        if (err) {
+            if (printk_ratelimit()) {
+                printk(KERN_WARNING PFX "sx_dpt_send_emad: "
+                       "error in build header/stub\n");
+            }
+            sx_skb_free(skb);
+            break;
+        }
+
+        err = sx_glb.sx_sgmii.build_ctrl_segment(skb, meta->lp, 0, SX_SGMII_PKT_TYPE_ETH);
+        if (err) {
+            sx_skb_free(skb);
+            break;
+        }
+
+        err = sx_glb.sx_sgmii.build_encapsulation_header(skb,
+                                                         sx_glb.sx_dpt.dpt_info[sx_dev_id] \
+                                                         .sx_sgmii_info.dmac,
+                                                         sx_glb.sx_sgmii.base_smac,
+                                                         ETHTYPE_EMAD, 0, 0);
+        if (err) {
+            sx_skb_free(skb);
+            break;
+        }
+#ifdef SX_DEBUG
+        printk(KERN_DEBUG PFX "Going to send an EMAD through SGMII "
+               "to device %d\n", sx_dev_id);
+#endif
+        err = sx_glb.sx_sgmii.send(skb);
+        break;
+
+    case DPT_PATH_PCI_E:
+        DPRINTK("Send emad over PCI-E.\n");
+        err = __sx_core_post_send(dev, skb, meta);
+        break;
+
+    default:
+        PRINTK_ERR("dpt_info[%d].emad_path: %d is invalid\n",
+                   sx_dev_id,
+                   sx_glb.sx_dpt.dpt_info[sx_dev_id].emad_path);
+        err = -EINVAL;
+        break;
+    }
+
+    EXIT_FUNC();
+    return err;
+}
+
+/**
+ * This functions will send mads according mad_path defined in DPT :
+ * I2C,PCI,SGMII.
+ */
+int sx_dpt_send_mad(int sx_dev_id, struct sk_buff *skb, struct isx_meta *meta)
+{
+    int            err = 0;
+    struct sx_dev *dev = sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_pcie_info.sx_dev;
+
+    ENTER_FUNC();
+
+    DPRINTK("send mad to dev_id %d, skb: %p\n", sx_dev_id, skb);
+    switch (sx_glb.sx_dpt.dpt_info[sx_dev_id].mad_path) {
+    case DPT_PATH_I2C:
+        PRINTK_ERR("Send mad over I2C isn't supported.\n");
+        err = -EINVAL;
+        break;
+
+    case DPT_PATH_SGMII:
+        err = sx_build_isx_header(meta, skb, 0);
+        if (err) {
+            if (printk_ratelimit()) {
+                printk(KERN_WARNING PFX "sx_dpt_send_emad: "
+                       "error in build header/stub\n");
+            }
+            sx_skb_free(skb);
+            break;
+        }
+
+        err = sx_glb.sx_sgmii.build_ctrl_segment(skb, meta->lp, 0,
+                                                 SX_SGMII_PKT_TYPE_IB_TRANSPORT);
+        if (err) {
+            sx_skb_free(skb);
+            break;
+        }
+
+        err = sx_glb.sx_sgmii.build_encapsulation_header(skb,
+                                                         sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_sgmii_info.dmac,
+                                                         sx_glb.sx_sgmii.base_smac,
+                                                         ETHTYPE_EMAD, 0, 0);
+        if (err) {
+            sx_skb_free(skb);
+            break;
+        }
+
+        printk(KERN_DEBUG PFX "Going to send a MAD through SGMII to "
+               "device %d\n", sx_dev_id);
+        err = sx_glb.sx_sgmii.send(skb);
+        break;
+
+    case DPT_PATH_PCI_E:
+        DPRINTK("Send mad over PCI-E\n");
+        err = __sx_core_post_send(dev, skb, meta);
+        break;
+
+    default:
+        PRINTK_ERR("dpt_info[%d].mad_path: %d is INVALID !\n",
+                   sx_dev_id,
+                   sx_glb.sx_dpt.dpt_info[sx_dev_id].mad_path);
+        err = -EINVAL;
+        break;
+    }
+
+    EXIT_FUNC();
+    return err;
+}
+
+int sx_dpt_get_sx_dev_by_id(int sx_dev_id, struct sx_dev **dev)
+{
+    int ret = 0;
+
+    ENTER_FUNC();    
+
+#ifdef NO_PCI
+    if (sx_glb.tmp_dev_ptr == NULL) {
+        *dev = NULL;
+        ret = -EINVAL;
+		PRINTK_ERR("There is no device to configure for DPT\n");
+        goto out;
+    }
+
+    *dev = sx_glb.tmp_dev_ptr;
+    goto out;
+#endif
+
+#ifdef CONFIG_SX_SGMII_PRESENT
+    /* for SXD_SIGNAL sw event handling on system close */
+    if (sx_dev_id == DEFAULT_DEVICE_ID) {
+    	*dev = NULL;
+		goto out;
+	}
+#endif
+
+    *dev = NULL;
+    if (sx_dev_id > MAX_NUM_OF_REMOTE_SWITCHES) {
+        PRINTK_ERR("Device id is not valid\n");
+        ret = -EINVAL;
+        goto out;
+    }
+    
+
+    if (sx_glb.sx_dpt.dpt_info[sx_dev_id].is_ifc_valid[DPT_PATH_PCI_E] == true) {
+        *dev = (struct sx_dev *)sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_pcie_info.sx_dev;
+    } else if (sx_glb.sx_dpt.dpt_info[sx_dev_id].is_ifc_valid[DPT_PATH_SGMII] == true) {
+        *dev = NULL;
+    } else {
+        PRINTK_ERR("no path configured for dev_id %d\n", sx_dev_id);
+        ret = -ENXIO;
+        goto out;
+    }
+    
+out:
+    EXIT_FUNC();
+    return ret;
+}
+
+int sx_dpt_get_cmd_sx_dev_by_id(int sx_dev_id, struct sx_dev **dev)
+{
+    int ret = 0;
+
+    ENTER_FUNC();
+
+#ifdef NO_PCI
+    if (sx_glb.tmp_dev_ptr == NULL) {
+        *dev = NULL;
+        ret = -EINVAL;
+        goto out;
+    }
+
+    *dev = sx_glb.tmp_dev_ptr;
+    goto out;
+#endif
+
+    *dev = NULL;
+    if (sx_dev_id > MAX_NUM_OF_REMOTE_SWITCHES) {
+        PRINTK_ERR("Device id is not valid\n");
+        ret = -EINVAL;
+        goto out;
+    }
+
+    if (sx_glb.sx_dpt.dpt_info[sx_dev_id].cmd_path == DPT_PATH_PCI_E) {
+        if (sx_glb.sx_dpt.dpt_info[sx_dev_id].is_ifc_valid[DPT_PATH_PCI_E] == true) {
+            *dev = (struct sx_dev *)sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_pcie_info.sx_dev;
+        } else {
+            PRINTK_ERR("no PCIE path configured for dev_id %d\n", sx_dev_id);
+            ret = -ENXIO;
+            goto out;
+        }
+    } else {
+        *dev = sx_glb.tmp_dev_ptr;
+    }
+
+out:
+    EXIT_FUNC();
+    return ret;
+}
+
+int sx_dpt_get_i2c_dev_by_id(int sx_dev_id, int *i2c_dev)
+{
+    ENTER_FUNC();
+
+    if (sx_glb.sx_dpt.dpt_info[sx_dev_id].is_ifc_valid[DPT_PATH_I2C] == false) {
+        PRINTK_ERR("Path DPT_PATH_I2C is not valid.\n");
+        *i2c_dev = 0;
+        return -EINVAL;
+    }
+
+    *i2c_dev = sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_i2c_info.sx_i2c_dev;
+
+    EXIT_FUNC();
+
+    return 0;
+}
+
+int sx_dpt_get_sgmii_dev_mac(int sx_dev_id, u64 *mac)
+{
+    ENTER_FUNC();
+    if (sx_glb.sx_dpt.dpt_info[sx_dev_id].is_ifc_valid[DPT_PATH_SGMII] == false) {
+        PRINTK_ERR("Path DPT_PATH_SGMII is not valid.\n");
+        *mac = 0;
+        return -EINVAL;
+    }
+
+    *mac = sx_glb.sx_dpt.dpt_info[sx_dev_id].sx_sgmii_info.dmac;
+
+    EXIT_FUNC();
+
+    return 0;
+}
+
+int sx_dpt_validate_new_cmd_path(int sx_dev_id, enum  ku_dpt_path_type cmd_path)
+{
+    if (cmd_path == DPT_PATH_SGMII) {
+        return sx_QUERY_FW_2(sx_glb.tmp_dev_ptr, sx_dev_id);
+    }
+
+    return 0;
+}
+
+int sx_dpt_validate_new_emad_path(int sx_dev_id)
+{
+    int err = 0;
+
+    ENTER_FUNC();
+    EXIT_FUNC();
+
+    return err;
+}
+
+int sx_dpt_validate_new_mad_path(int sx_dev_id)
+{
+    int err = 0;
+
+    ENTER_FUNC();
+    EXIT_FUNC();
+
+    return err;
+}
+
+/**
+ * reg - CR space adress to write to
+ * value - the value we want to write
+ */
+int sx_dpt_i2c_writel(int dev_id, u32 reg, u32 value)
+{
+    int err = 0;
+    int i2c_dev_id;
+    int i;
+
+    ENTER_FUNC();
+    err = sx_dpt_get_i2c_dev_by_id(dev_id, &i2c_dev_id);
+    if (err) {
+        PRINTK_ERR("sx_dpt_get_i2c_dev_by_id for dev_id: %d "
+                   "failed !\n", dev_id);
+        return -EINVAL;
+    }
+
+    DPRINTK("dev_id: 0x%x, i2c_dev: 0x%x, reg:0x%x, val: 0x%x\n",
+            dev_id, i2c_dev_id, reg, value);
+
+    for (i = 0; i < MAX_I2C_RETRIES; i++) {
+        err = sx_glb.sx_i2c.write_dword(i2c_dev_id, reg, value);
+        if (!err) {
+            break;
+        }
+    }
+
+    if (err) {
+        printk(KERN_WARNING "sx_dpt_i2c_writel for dev_id: %d failed "
+               "after %d tries!\n", dev_id, i);
+        return -EINVAL;
+    }
+
+    if (i > 0) {
+        printk(KERN_INFO "sx_dpt_i2c_writel for dev_id: %d succeeded "
+               "after %d tries!\n", dev_id, i);
+    }
+
+    EXIT_FUNC();
+    return err;
+}
+
+int sx_dpt_i2c_write_buf(int dev_id, unsigned int i2c_offset, unsigned char *buf, int size)
+{
+    int err = 0;
+    int i2c_dev_id;
+    int i;
+
+    ENTER_FUNC();
+    err = sx_dpt_get_i2c_dev_by_id(dev_id, &i2c_dev_id);
+    if (err) {
+        PRINTK_ERR("sx_dpt_get_i2c_dev_by_id for dev_id: "
+                   "%d failed !\n", dev_id);
+        return -EINVAL;
+    }
+
+    DPRINTK("dev_id: 0x%x, i2c_dev_id: 0x%x, i2c_offset:0x%x, buf: %p, "
+            "size: %d\n", dev_id, i2c_dev_id,
+            i2c_offset, buf, size);
+    for (i = 0; i < MAX_I2C_RETRIES; i++) {
+        err = sx_glb.sx_i2c.write(i2c_dev_id, i2c_offset, size, buf);
+        if (!err) {
+            break;
+        }
+    }
+
+    if (err) {
+        printk(KERN_ERR "sx_dpt_i2c_write_buf for dev_id: %d "
+               "failed after %d tries! err: %d\n", dev_id, i, err);
+        return -EINVAL;
+    }
+
+    if (i > 0) {
+        printk(KERN_INFO "sx_dpt_i2c_write_buf for dev_id: %d "
+               "succeeded after %d tries!\n", dev_id, i);
+    }
+
+    EXIT_FUNC();
+    return err;
+}
+
+int sx_dpt_i2c_read_buf(int dev_id, unsigned int i2c_offset, unsigned char *buf, int size)
+{
+    int err = 0;
+    int i2c_dev_id;
+    int i;
+
+    ENTER_FUNC();
+    err = sx_dpt_get_i2c_dev_by_id(dev_id, &i2c_dev_id);
+    if (err) {
+        PRINTK_ERR("sx_dpt_get_i2c_dev_by_id for dev_id: "
+                   "%d failed !\n", dev_id);
+        return -EINVAL;
+    }
+
+    DPRINTK("dev_id: 0x%x, i2c_dev_id: 0x%x, i2c_offset:0x%x, buf: %p, "
+            "size: %d\n", dev_id, i2c_dev_id,
+            i2c_offset, buf, size);
+
+    for (i = 0; i < MAX_I2C_RETRIES; i++) {
+        err = sx_glb.sx_i2c.read(i2c_dev_id, i2c_offset, size, buf);
+        if (!err) {
+            break;
+        }
+    }
+
+    if (err) {
+        PRINTK_ERR("sx_dpt_i2c_read_buf for dev_id: %d failed "
+                   "after %d tries! err: %d \n", dev_id, i, err);
+        return -EINVAL;
+    }
+
+    if (i > 0) {
+        printk(KERN_INFO "sx_dpt_i2c_read_buf for dev_id: %d "
+               "succeeded after %d tries!\n", dev_id, i);
+    }
+
+    EXIT_FUNC();
+    return err;
+}
+
+u32 sx_dpt_i2c_readl(int dev_id, u32 address, int *err)
+{
+    int value = 0;
+    int i2c_dev_id;
+    int i;
+
+    ENTER_FUNC();
+    *err = sx_dpt_get_i2c_dev_by_id(dev_id, &i2c_dev_id);
+    if (*err) {
+        PRINTK_ERR("sx_dpt_get_i2c_dev_by_id for dev_id: %d "
+                   "failed !\n", dev_id);
+        return 0;
+    }
+
+    for (i = 0; i < MAX_I2C_RETRIES; i++) {
+        *err = sx_glb.sx_i2c.read_dword(i2c_dev_id, address, &value);
+        if (!(*err)) {
+            break;
+        }
+    }
+
+    if (*err) {
+        printk(KERN_DEBUG "sx_dpt_i2c_readl for dev_id: %d failed "
+               "after %d tries!\n", dev_id, i);
+        return 0;
+    }
+
+    if (i > 0) {
+        printk(KERN_INFO "sx_dpt_i2c_read_buf for dev_id: %d "
+               "succeeded after %d tries!\n", dev_id, i);
+    }
+
+    EXIT_FUNC();
+    return value;
+}
+
+void sx_dump_dpt_entry(int i)
+{
+    struct sx_dpt_info dpt_info = sx_glb.sx_dpt.dpt_info[i];
+
+    printk(KERN_INFO "%d:   c:%s e:%s m:%s cr:%s ; ",
+           i,
+           dpt_type2str[dpt_info.cmd_path],
+           dpt_type2str[dpt_info.emad_path],
+           dpt_type2str[dpt_info.mad_path],
+           dpt_type2str[dpt_info.cr_access_path]);
+
+    if (sx_glb.sx_dpt.dpt_info[i].is_ifc_valid[DPT_PATH_I2C]) {
+        printk(KERN_INFO "i2c:0x%x mbox in 0x%x out 0x%x",
+               dpt_info.sx_i2c_info.sx_i2c_dev,
+               dpt_info.in_mb_offset,
+               dpt_info.out_mb_offset);
+    }
+
+    if (sx_glb.sx_dpt.dpt_info[i].is_ifc_valid[DPT_PATH_PCI_E]) {
+        printk(KERN_INFO "pcie: %p ", dpt_info.sx_pcie_info.sx_dev);
+    }
+
+    if (sx_glb.sx_dpt.dpt_info[i].is_ifc_valid[DPT_PATH_SGMII]) {
+        printk(KERN_INFO "sgmii: dmac 0x%llx smac 0x%llx mbox in 0x%x out 0x%x",
+               dpt_info.sx_sgmii_info.dmac,
+               sx_glb.sx_sgmii.base_smac,
+               dpt_info.in_mb_offset,
+               dpt_info.out_mb_offset);
+    }
+
+    printk(KERN_INFO "\n");
+}
+
+void sx_dpt_dump(void)
+{
+    int cnt = 0;
+    int i;
+
+    printk(KERN_INFO "DUMP DPT:\n");
+    for (i = 0; i < MAX_NUM_OF_REMOTE_SWITCHES + 1; i++) {
+        if (sx_glb.sx_dpt.dpt_info[i]. \
+            is_ifc_valid[DPT_PATH_I2C] ||
+            sx_glb.sx_dpt.dpt_info[i]. \
+            is_ifc_valid[DPT_PATH_SGMII] ||
+            sx_glb.sx_dpt.dpt_info[i]. \
+            is_ifc_valid[DPT_PATH_PCI_E]) {
+            sx_dump_dpt_entry(i);
+            cnt++;
+        }
+    }
+
+    printk(KERN_INFO "Active DPT entries : %d \n", cnt);
+}
+
+int sx_dpt_move(int dst_dev_id, int src_dev_id)
+{
+    struct sx_dev *tmp_sx_dev = NULL;
+
+    if (dst_dev_id > (MAX_NUM_OF_REMOTE_SWITCHES - 1)) {
+        printk(KERN_INFO "%s: dst_dev_id %d is out of range [0 .. %d] \n",
+               __func__, dst_dev_id, (MAX_NUM_OF_REMOTE_SWITCHES - 1));
+        return -EINVAL;
+    }
+
+    if (src_dev_id > (MAX_NUM_OF_REMOTE_SWITCHES - 1)) {
+        printk(KERN_INFO "%s: src_dev_id %d is out of range [0 .. %d] \n",
+               __func__, src_dev_id, (MAX_NUM_OF_REMOTE_SWITCHES - 1));
+        return -EINVAL;
+    }
+
+
+    sx_glb.sx_dpt.dpt_info[dst_dev_id] = sx_glb.sx_dpt.dpt_info[src_dev_id];
+    memset(&sx_glb.sx_dpt.dpt_info[src_dev_id], 0, sizeof(sx_glb.sx_dpt.dpt_info[src_dev_id]));
+    sx_dpt_get_sx_dev_by_id(dst_dev_id, &tmp_sx_dev);
+    tmp_sx_dev->device_id = dst_dev_id;
+
+    printk(KERN_INFO "Move DPT entry %d to %d .\n", src_dev_id, dst_dev_id);
+    return 0;
+}
+
+void sx_dpt_set_cmd_dbg(int dev_id, int path)
+{
+    sx_dpt_set_cmd_path(dev_id, path);
+}
+
+void sx_dpt_reg_i2c_ifc(struct sx_i2c_ifc *i2c_ifc)
+{
+    sx_glb.sx_i2c.read = __symbol_get("i2c_read");
+    sx_glb.sx_i2c.write = __symbol_get("i2c_write");
+    sx_glb.sx_i2c.read_dword = __symbol_get("i2c_read_dword");
+    sx_glb.sx_i2c.write_dword = __symbol_get("i2c_write_dword");
+    sx_glb.sx_i2c.enforce = __symbol_get("i2c_enforce");
+    sx_glb.sx_i2c.release = __symbol_get("i2c_release");
+    sx_glb.sx_i2c.get_local_mbox = __symbol_get("i2c_get_local_mbox");
+    sx_glb.sx_i2c.get_fw_rev = __symbol_get("i2c_get_fw_rev");
+    sx_glb.sx_i2c.set_go_bit_stuck = __symbol_get("i2c_set_go_bit_stuck");
+}
+EXPORT_SYMBOL(sx_dpt_reg_i2c_ifc);
+
+void sx_dpt_dereg_i2c_ifc(void)
+{
+    if (!sx_glb.sx_i2c.read || (sx_glb.sx_i2c.read == sx_dpt_stub_i2c_read)) {
+        return;
+    }
+
+    if (sx_glb.sx_i2c.read) {
+        __symbol_put("i2c_read");
+    }
+    if (sx_glb.sx_i2c.write) {
+        __symbol_put("i2c_write");
+    }
+    if (sx_glb.sx_i2c.read_dword) {
+        __symbol_put("i2c_read_dword");
+    }
+    if (sx_glb.sx_i2c.write_dword) {
+        __symbol_put("i2c_write_dword");
+    }
+    if (sx_glb.sx_i2c.enforce) {
+        __symbol_put("i2c_enforce");
+    }
+    if (sx_glb.sx_i2c.release) {
+        __symbol_put("i2c_release");
+    }
+    if (sx_glb.sx_i2c.get_local_mbox) {
+        __symbol_put("i2c_get_local_mbox");
+    }
+    if (sx_glb.sx_i2c.get_fw_rev) {
+        __symbol_put("i2c_get_fw_rev");
+    }
+    if (sx_glb.sx_i2c.set_go_bit_stuck) {
+        __symbol_put("i2c_set_go_bit_stuck");
+    }
+}
+EXPORT_SYMBOL(sx_dpt_dereg_i2c_ifc);
+
+int sx_dpt_stub_i2c_enforce(int i2c_dev_id)
+{
+    PRINTK_ERR("i2c module non registered !!!\n");
+
+    return 0;
+}
+
+int sx_dpt_stub_i2c_release(int i2c_dev_id)
+{
+    PRINTK_ERR("i2c module non registered !!!\n");
+
+    return 0;
+}
+
+int sx_dpt_stub_i2c_write(int i2c_dev_id, int offset, int len, u8 *in_out_buf)
+{
+    PRINTK_ERR("i2c module non registered !!!\n");
+
+    return 0;
+}
+
+int sx_dpt_stub_i2c_read(int i2c_dev_id, int offset, int len, u8 *in_out_buf)
+{
+    PRINTK_ERR("i2c module not registered !\n");
+
+    return 0;
+}
+
+int sx_dpt_stub_i2c_write_dword(int i2c_dev_id, int offset, u32 val)
+{
+    PRINTK_ERR("i2c module not registered!\n");
+
+    return 0;
+}
+
+int sx_dpt_stub_i2c_read_dword(int i2c_dev_id, int offset, u32 *val)
+{
+    PRINTK_ERR("i2c module not registered !\n");
+
+    return 0;
+}
+
+int sx_dpt_cr_space_read(int dev_id, unsigned int address, unsigned char *buf, int size)
+{
+    enum ku_dpt_path_type   cr_access_path = sx_dpt_get_cr_access_path(dev_id);
+    int                   err = 0;
+    struct ku_dpt_i2c_info* sx_i2c_info;
+
+    ENTER_FUNC();
+
+    switch (cr_access_path){
+    case (DPT_PATH_I2C):
+		if (!sx_glb.sx_i2c.enforce) {
+		    printk(KERN_ERR PFX "enforce is NULL!!!\n");
+		    err = -EINVAL;
+		    goto out;
+		}
+
+		err = sx_dpt_get_i2c_info(dev_id, &sx_i2c_info);
+		if (err) {
+		    printk(KERN_WARNING PFX "Can't get I2C info of device %d. "
+		           "cr space read will not be performed. err = %d\n",
+		           dev_id, err);
+		    goto out;
+		}
+		err = sx_glb.sx_i2c.enforce(sx_i2c_info->sx_i2c_dev);
+		if (err) {
+		    printk(KERN_WARNING PFX "I2C bus of device %d is not ready. "
+		           "cr space read will not be performed. err = %d\n",
+		           dev_id, err);
+		    goto out;
+		}
+
+		err = sx_dpt_i2c_read_buf(dev_id, address, buf, size);
+		sx_glb.sx_i2c.release(sx_i2c_info->sx_i2c_dev);
+		if (err) {
+		        printk(KERN_WARNING PFX "I2C bus of device %d is not ready. "
+		           "cr space read will not be performed. err = %d\n",
+		           dev_id, err);
+			goto out;
+		}
+
+		break;
+#ifdef CONFIG_SX_SGMII_PRESENT
+	case (DPT_PATH_SGMII):
+        err = sx_sgmii_cr_space_read_buf(dev_id, address, buf, size);
+        if (err) {
+            printk(KERN_WARNING PFX "Can't read CR space value from device %d  "
+                   "cr space read will not be performed. err = %d\n",
+                   dev_id, err);
+        }
+        break;
+#endif
+    case (DPT_PATH_PCI_E):
+        printk(KERN_WARNING PFX "sx_dpt_cr_space_read: reading from CR space "
+               "of device %u over PCI isn't supported\n", dev_id);
+        err = -EINVAL;
+        break;
+    
+    default:
+        printk(KERN_ERR PFX "sx_dpt_cr_space_read: Can't read from CR space "
+               "of device %u because the DPT path isn't valid\n", dev_id);
+        err = -EINVAL;
+        break;
+    }
+
+out:
+    EXIT_FUNC();
+    return err;
+}
+
+int sx_dpt_cr_space_write(int dev_id, unsigned int address, unsigned char *buf, int size)
+{
+    enum ku_dpt_path_type   cr_access_path = sx_dpt_get_cr_access_path(dev_id);
+    int                   err = 0;
+    struct ku_dpt_i2c_info* sx_i2c_info;
+
+    ENTER_FUNC();
+
+    switch (cr_access_path) {
+    case (DPT_PATH_I2C):
+		if (!sx_glb.sx_i2c.enforce) {
+		    printk(KERN_ERR PFX "enforce is NULL!!!\n");
+		    err = -EINVAL;
+		    goto out;
+		}
+
+		err = sx_dpt_get_i2c_info(dev_id, &sx_i2c_info);
+		if (err) {
+		    printk(KERN_WARNING PFX "Can't get I2C info of device %d. "
+		           "cr space read will not be performed. err = %d\n",
+		           dev_id, err);
+		    goto out;
+		}
+		err = sx_glb.sx_i2c.enforce(sx_i2c_info->sx_i2c_dev);
+		if (err) {
+		    printk(KERN_WARNING PFX "I2C bus of device %d is not ready. "
+		           "cr space write will not be performed. err = %d\n",
+		           dev_id, err);
+		    goto out;
+		}
+
+		err = sx_dpt_i2c_write_buf(dev_id, address, buf, size);
+		sx_glb.sx_i2c.release(sx_i2c_info->sx_i2c_dev);
+		if (err) {
+		    printk(KERN_WARNING PFX "I2C bus of device %d is not ready. "
+		           "cr space write will not be performed. err = %d\n",
+		           dev_id, err);
+		    goto out;
+		}
+        break;
+#ifdef CONFIG_SX_SGMII_PRESENT
+	case (DPT_PATH_SGMII):
+        err = sx_sgmii_cr_space_write_buf(dev_id, address, buf, size);
+        if (err) {
+            printk(KERN_WARNING PFX "Can't write CR space value to device %d  "
+                   "cr space write will not be performed. err = %d\n",
+                   dev_id, err);
+        }
+        break;
+#endif
+    case (DPT_PATH_PCI_E):
+        printk(KERN_WARNING PFX "sx_dpt_cr_space_write: reading from CR space "
+               "of device %u over PCI isn't supported\n", dev_id);
+        err = -EINVAL;
+        break;
+
+    default:
+        printk(KERN_ERR PFX "sx_dpt_cr_space_write: Can't write to CR space "
+               "of device %u because the DPT path isn't valid\n", dev_id);
+        err = -EINVAL;
+        break;
+    }
+
+out:
+    EXIT_FUNC();
+    return err;
+}
diff --git a/linux/drivers/hwmon/mellanox/sx_dpt.h b/linux/drivers/hwmon/mellanox/sx_dpt.h
new file mode 100644
index 0000000..63577ad
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/sx_dpt.h
@@ -0,0 +1,217 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+#ifndef _SX_DPT_H_
+#define _SX_DPT_H_
+
+#include <linux/pci.h>
+#include <linux/mlx_sx/device.h>
+
+#define MAX_NUM_OF_REMOTE_SWITCHES	256
+
+#define SX_GET_PCI_BUS_ID(x)	(((x) & 0xFF0000)>>16)
+#define SX_GET_PCI_DEV_ID(x)	(((x) & 0x00FF00)>>8)
+#define SX_GET_PCI_FUNC_ID(x)	((x) & 0xFF)
+
+/*
+	struct per SX chip
+*/
+struct sx_dpt_info{
+	/* spinlock_t				dpt_lock; */
+	enum ku_dpt_path_type		cmd_path;
+	enum ku_dpt_path_type		emad_path;
+	enum ku_dpt_path_type		mad_path;
+	enum ku_dpt_path_type		cr_access_path;
+	bool						is_ifc_valid[DPT_PATH_MAX+1];
+	struct ku_dpt_i2c_info 		sx_i2c_info;
+	struct ku_dpt_sgmii_info	sx_sgmii_info;
+	struct ku_dpt_pcie_info 	sx_pcie_info;
+	u32							in_mb_size;
+	u32							in_mb_offset;
+	u32							out_mb_size;
+	u32							out_mb_offset;
+	u64							fw_rev;
+};
+
+struct sx_dpt_s{
+	struct sx_dpt_info	dpt_info[MAX_NUM_OF_REMOTE_SWITCHES + 1];
+};
+
+/**
+* This function is used to init dpt table: 1. set the default
+* dpt path to i2c 2. set all dpt devs is_exists to FALSE
+*
+* param[in] returns: 0 success
+*	   !0 error
+*/
+int sx_dpt_init(void);
+int sx_dpt_init_default_dev(struct sx_dev *sx_dev);
+int sx_dpt_init_dev_pci(struct sx_dev *sx_dev);
+/**
+* This function returns true if the given path is valid for
+* the given device, or false otherwise.
+*
+* param[in] returns: 1 true
+*	   0 false
+*/
+bool sx_dpt_is_path_valid(int sx_dev_id, enum  ku_dpt_path_type path);
+
+/**
+* This function will add path to entry dev_id in DPT. The path
+* should be provided with remote device description. For example
+* , if added path is I2C than path_data should be i2c_dev_id.
+*
+* param[in] returns: 0 success
+*	   !0 error
+*/
+int sx_dpt_add_dev_path(int sx_dev_id,
+		enum  ku_dpt_path_type path,
+		union ku_dpt_path_info path_data,
+		uint8_t is_local);
+
+
+/**
+* This function with params dev_id and path will invalidate the
+* given "path" at index DEV_ID in DPT. All other pathes will
+* remains without changes.
+*
+* param[in] returns: 0 success
+*	   !0 error
+*/
+int sx_dpt_remove_dev_path(int sx_dev_id , enum ku_dpt_path_type path);
+
+/**
+* This function with params dev_id and path will invalidate all pathes
+* at index DEV_ID in DPT.
+*
+* param[in] returns: 0 success
+*	   !0 error
+*/
+int sx_dpt_remove_dev(int sx_dev_id, int restart_flow);
+
+/**
+* This function is used to change cmd dpt path : i2c , pci-e , sgmii
+* by changing the callbacks to approprite interface functions
+* To validate if the change is suceessed need to make some wr/rd
+* to check if the new interface is working
+*
+* param[in] returns: 0 success
+*	   !0 error
+*/
+int sx_dpt_set_cmd_path(int sx_dev_id, enum  ku_dpt_path_type cmd_path);
+
+/**
+ * This function is used to change emad dpt path : i2c , pci-e , sgmii
+ * by changing the callbacks to approprite interface functions
+ * To validate if the change is suceessed need to make some
+ * wr/rd to check if the new interface is working
+ *
+ * param[in] returns: 0 success
+ *	   !0 error
+ */
+int sx_dpt_set_emad_path(int sx_dev_id, enum  ku_dpt_path_type emad_path);
+
+/**
+ * This function is used to change mad dpt path : i2c , pci-e ,
+ * sgmii by changing the callbacks to approprite interface
+ * functions To validate if the change is suceessed need to make
+ * some wr/rd to check if the new interface is working
+ *
+ * param[in] returns: 0 success
+ *	   !0 error
+ */
+int sx_dpt_set_mad_path(int sx_dev_id, enum  ku_dpt_path_type mad_path);
+
+/**
+ * This function is used to change mad dpt path : i2c , pci-e ,
+ * sgmii by changing the callbacks to approprite interface
+ * functions To validate if the change is suceessed need to make
+ * some wr/rd to check if the new interface is working
+ *
+ * param[in] returns: 0 success
+ *	   !0 error
+ */
+int sx_dpt_set_cr_access_path(int sx_dev_id,
+	enum  ku_dpt_path_type cr_access_path);
+
+/**
+ * This functions will perform access through cmd_path defined in DPT : I2C,PCI,SGMII.
+ * Each writel in driver should be replaced to sx_dpt_write_u32.
+ *
+ * param[in] returns: 0 success
+ *	   !0 error
+ */
+int sx_dpt_i2c_writel(int i2c_dev_id, u32 reg, u32 value);
+int sx_dpt_i2c_write_buf(int i2c_dev_id, unsigned int i2c_offset,
+						unsigned char *buf, int size);
+int sx_dpt_i2c_read_buf(int i2c_dev_id, unsigned int i2c_offset,
+				unsigned char *buf, int size);
+
+
+/**
+ * This functions will perform access through cmd_path defined in DPT : I2C,PCI,SGMII.
+ * Each readl in driver should be replaced to sx_dpt_read_u32.
+ * param[in] returns: 0 success
+ *	   !0 error
+ */
+u32 sx_dpt_i2c_readl(int dev_id, u32 address, int *err);
+
+/**
+ * This functions will send emads according emad_path defined in DPT : I2C,PCI,SGMII.
+ */
+int sx_dpt_send_emad(int sx_dev_id, struct sk_buff *skb, struct isx_meta *meta);
+
+int sx_dpt_send_mad(int sx_dev_id, struct sk_buff *skb, struct isx_meta *meta);
+
+/**
+ * This functions will return sx_dev_id of recived emads
+ * according emad_path defined in DPT : I2C,PCI,SGMII
+ */
+int sx_dpt_get_sx_dev_id(struct sk_buff *skb, int *sx_dev_id);
+
+/**
+ * This functions will read VPD. or WR/RD some scratch register
+ */
+int sx_dpt_validate_new_cmd_path(int sx_dev_id, enum  ku_dpt_path_type cmd_path);
+
+/**
+ * This functions will read VPD. or WR/RD some scratch register by emad
+ */
+int sx_dpt_validate_new_emad_path(int sx_dev_id);
+
+/*
+ * This functions will read VPD. or WR/RD some scratch register by mad
+ */
+int sx_dpt_validate_new_mad_path(int sx_dev_id);
+void sx_dpt_dump(void);
+int sx_dpt_get_cmd_path(int sx_dev_id);
+int sx_dpt_get_i2c_info(int sx_dev_id, struct ku_dpt_i2c_info** i2c_info);
+int sx_dpt_get_i2c_dev_by_id(int sx_dev_id, int *i2c_dev);
+int sx_dpt_get_sx_dev_by_id(int sx_dev_id, struct sx_dev **dev);
+int sx_dpt_get_cmd_sx_dev_by_id(int sx_dev_id, struct sx_dev **dev);
+int sx_dpt_find_pci_dev(unsigned int sx_pci_dev_id,
+		int vendor, int device, struct pci_dev **sx_pci_dev);
+int sx_dpt_stub_i2c_write(int i2c_dev_id, int offset, int len, u8 *in_out_buf);
+int sx_dpt_stub_i2c_read(int i2c_dev_id, int offset, int len, u8 *in_out_buf);
+int sx_dpt_stub_i2c_write_dword(int i2c_dev_id, int offset, u32 val);
+int sx_dpt_stub_i2c_read_dword(int i2c_dev_id, int offset, u32 *val);
+int sx_dpt_stub_i2c_enforce(int i2c_dev_id);
+int sx_dpt_stub_i2c_release(int i2c_dev_id);
+int sx_dpt_get_emad_path(int sx_dev_id);
+int sx_dpt_cr_space_read(int dev_id, unsigned int address,
+		unsigned char *buf, int size);
+int sx_dpt_cr_space_write(int dev_id, unsigned int address,
+		unsigned char *buf, int size);
+int sx_dpt_move(int dst_dev_id, int src_dev_id);
+int sx_dpt_is_dev_pci_attached(int sx_dev_id);
+#endif
diff --git a/linux/drivers/hwmon/mellanox/sx_proc.c b/linux/drivers/hwmon/mellanox/sx_proc.c
new file mode 100644
index 0000000..1e564d6
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/sx_proc.c
@@ -0,0 +1,2461 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+#include <linux/cdev.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/wait.h>
+#include <net/sock.h>
+#include <asm/pgtable.h>
+#include <linux/seq_file.h>
+#include <linux/proc_fs.h>
+#include <linux/fs.h>
+#include <linux/ctype.h>
+#include <linux/string.h>
+#include <linux/kthread.h>
+#include <linux/mlx_sx/cmd.h>
+#include <linux/mlx_sx/device.h>
+#include <linux/mlx_sx/kernel_user.h>
+#include <linux/netdevice.h>
+#include <linux/vmalloc.h>
+#include "sx_proc.h"
+#include "cq.h"
+#include "fw.h"
+#include "sx.h"
+#include "alloc.h"
+#include "sx_dpt.h"
+
+#define DEF_SX_CORE_PROC_DIR						"mlx_sx"
+#define DEF_SX_CORE_PROC_CORE_FILE		 	"sx_core"
+
+#define PROC_DUMP(fmt, args...)   printk(KERN_ERR fmt, ## args)
+
+void __sx_proc_dump_swids(struct sx_dev *dev);
+void __sx_proc_dump_sdq(struct sx_dev *dev);
+void __sx_proc_dump_rdq(struct sx_dev *dev);
+void __sx_proc_dump_eq(struct sx_dev *dev);
+void __sx_proc_dump_rdq_single(struct sx_dev *dev, int rdq);
+void __sx_proc_set_dev_profile(struct sx_dev *dev);
+extern void sx_core_dump_synd_tbl(struct sx_dev *dev) ;
+//void __dump_stats(struct sx_dev* sx_dev);
+
+void __sx_proc_dump_reg(struct sx_dev *my_dev, unsigned long reg_id, unsigned long max_cnt);
+void __sx_proc_dump_emad(struct sx_dev *my_dev, unsigned long emad_type, unsigned long max_cnt, unsigned long direction);
+void __sx_proc_show_cq(struct sx_dev *dev, int cq);
+void __sx_proc_flush_rdq(struct sx_dev *my_dev, int idx);
+void __debug_dump_netdev(struct sx_dev *dev);
+void __debug_netdev_traps(struct sx_dev *dev, unsigned long trap_id, unsigned long is_add);
+void __set_net_dev_oper_state(char	*dev_name ,	char *oper_state);
+void __dump_net_dev_oper_state(void);
+static  ssize_t sx_proc_write(struct file *file, const char __user *buffer,
+                                size_t count, loff_t *pos);
+
+
+extern struct sx_globals sx_glb;
+
+extern int rx_debug;
+extern int rx_debug_pkt_type;
+extern int rx_debug_emad_type;
+extern int rx_dump;
+extern int rx_dump_cnt;
+extern int tx_debug;
+extern int tx_debug_pkt_type;
+extern int tx_debug_emad_type;
+extern int tx_dump;
+extern int tx_dump_cnt;
+
+extern int i2c_cmd_dump;
+extern int i2c_cmd_op;
+extern int i2c_cmd_reg_id;
+extern int i2c_cmd_dump_cnt;
+
+int sx_proc_registered;
+u8 dbg_buf[100];
+
+extern unsigned int credit_thread_vals[100];
+extern unsigned long long arr_count;
+extern atomic_t cq_backup_polling_enabled;
+extern int debug_cq_backup_poll_cqn;
+
+static const char *ku_pkt_type_str[] = {
+	"SX_PKT_TYPE_ETH_CTL_UC", /**< Eth control unicast */
+	"SX_PKT_TYPE_ETH_CTL_MC", /**< Eth control multicast */
+	"SX_PKT_TYPE_ETH_DATA", /**< Eth data */
+	"SX_PKT_TYPE_DROUTE_EMAD_CTL", /**< Directed route emad */
+	"SX_PKT_TYPE_EMAD_CTL", /**< Emad */
+	"SX_PKT_TYPE_FC_CTL_UC", /**< FC control unicast */
+	"SX_PKT_TYPE_FC_CTL_MC", /**< FC control multicast */
+	"SX_PKT_TYPE_FCOE_CTL_UC", /**< FC over Eth control unicast */
+	"SX_PKT_TYPE_FCOE_CTL_MC", /**< FC over Eth control multicast */
+	"SX_PKT_TYPE_IB_RAW_CTL", /**< IB raw control */
+	"SX_PKT_TYPE_IB_TRANSPORT_CTL", /**< IB transport control */
+	"SX_PKT_TYPE_IB_RAW_DATA", /**< IB raw data */
+	"SX_PKT_TYPE_IB_TRANSPORT_DATA", /**< IB transport data */
+	"SX_PKT_TYPE_EOIB_CTL", /**< Eth over IB control */
+	"SX_PKT_TYPE_FCOIB_CTL" , /**< FC over IB control */
+};
+
+void __dump_kdbs(void);
+
+char *sx_proc_str_get_ulong(char *buffer, unsigned long *val)
+{
+	const char delimiters[] = " .,;:!-";
+	char *running;
+	char *token;
+	running = buffer;
+
+	token = strsep(&running, delimiters);
+	if (token == NULL) {
+		*val = 0;
+		return NULL;
+	}
+
+	if (strstr(token, "0x"))
+		*val = simple_strtoul(token, NULL, 16);
+	else
+		*val = simple_strtoul(token, NULL, 10);
+
+	return running;
+}
+
+
+char *sx_proc_str_get_u32(char *buffer, u32 *val32)
+{
+	const char delimiters[] = " .,;:!-";
+	char *running;
+	char *token;
+
+	running = (char *)buffer;
+	token = strsep(&running, delimiters);
+	if (token == NULL) {
+		*val32 = 0;
+		return NULL;
+	}
+
+	if (strstr(token, "0x"))
+		*val32 = simple_strtol(token, NULL, 16);
+	else
+		*val32 = simple_strtol(token, NULL, 10);
+
+	return running;
+}
+
+
+char *sx_proc_str_get_str(char *buffer, char **str)
+{
+	const char delimiters[] = " ,;:!-";
+	char *running;
+	char *token;
+
+	running = buffer;
+	token = strsep(&running, delimiters);
+	if (token == NULL) {
+		*str = 0;
+		return NULL;
+	}
+
+	*str = token;
+	return running;
+}
+
+static int sx_proc_show(struct seq_file *m, void *v)
+{
+	struct sx_dev *my_dev = sx_glb.sx_dpt.dpt_info[DEFAULT_DEVICE_ID].sx_pcie_info.sx_dev;
+    struct sx_fw  *fw  = &sx_priv(my_dev)->fw;
+
+	seq_printf(m,"\nPROC Version: %08x\n", 0x2);
+
+	seq_printf(m,"fw_ver: %llu, fw_date: %02x:%02x %02x.%02x.%04x\n",
+                                  fw->fw_ver & 0xffffUL, fw->fw_hour,fw->fw_minutes, fw->fw_day,fw->fw_month,fw->fw_year);
+
+	seq_printf(m,"out_mb: offset:0x%x, size:0x%x ; in_mb offset:0x%x, size:0x%x\n",
+				  fw->local_out_mb_offset, fw->local_out_mb_size,
+				  fw->local_in_mb_offset, fw->local_in_mb_size);
+
+	return 0;
+}
+
+static int sx_proc_open(struct inode *inode, struct file *file)
+{
+        return single_open(file, sx_proc_show, NULL);
+}
+
+static void sx_proc_handle_access_reg(struct sx_dev *dev, char *p,
+		u32 dev_id, u32 reg_id, char *running) {
+	int err = 0;
+	u32 local_port;
+
+	switch (reg_id) {
+	case MGIR_REG_ID: {
+		struct ku_access_mgir_reg reg_data;
+
+		memset(&reg_data, 0, sizeof(reg_data));
+		reg_data.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data.op_tlv, reg_id, 1);
+		PROC_DUMP("Executing ACCESS_REG_MGIR Command\n");
+		err = sx_ACCESS_REG_MGIR(dev, &reg_data);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("Finished Executing Query ACCESS_REG_MGIR Command, PSID = %s\n",
+				reg_data.mgir_reg.fw_info.psid);
+		break;
+	}
+	case PSPA_REG_ID: {
+		struct ku_access_pspa_reg reg_data;
+		u32 swid_id;
+		memset(&reg_data, 0, sizeof(reg_data));
+		reg_data.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data.op_tlv, reg_id, 2);
+		/* local port + sub port are inputs */
+		running = sx_proc_str_get_u32(running, &swid_id);
+		reg_data.pspa_reg.swid = swid_id;
+		running = sx_proc_str_get_u32(running, &local_port);
+		reg_data.pspa_reg.local_port = local_port;
+		/*
+		reg_data.pspa_reg.swid = 0;
+		reg_data.pspa_reg.local_port = 1;
+		reg_data.pspa_reg.sub_port = 0;
+		*/
+		PROC_DUMP("Executing ACCESS_REG_PSPA Command, "
+				"swid_id:%d, lport:%d\n",
+				reg_data.pspa_reg.swid,
+				reg_data.pspa_reg.local_port);
+		err = sx_ACCESS_REG_PSPA(dev, &reg_data);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("Finished Executing Write ACCESS_REG_PSPA Command."
+			"Query not implemented yet\n");
+		break;
+	}
+	case OEPFT_REG_ID: {
+		struct ku_access_oepft_reg reg_data;
+		int i;
+
+		for (i = 0; i < 2; i++) {
+			PROC_DUMP("Executing Query ACCESS_REG_OEPFT Command, "
+					"sr = %d\n", i);
+			reg_data.oepft_reg.sr = 0;
+			memset(&reg_data, 0, sizeof(reg_data));
+			reg_data.dev_id = dev_id;
+			sx_cmd_set_op_tlv(&reg_data.op_tlv, reg_id, 1);
+			err = sx_ACCESS_REG_OEPFT(dev, &reg_data);
+			if (err)
+				goto print_err;
+
+			PROC_DUMP("Finished Executing Query ACCESS_REG_OEPFT "
+					"Command.\n");
+			PROC_DUMP("cpu_tclass = 0x%x\n",
+					reg_data.oepft_reg.cpu_tclass);
+			PROC_DUMP("flow_number = 0x%x\n",
+					reg_data.oepft_reg.flow_number);
+			PROC_DUMP("interface = 0x%x\n",
+					reg_data.oepft_reg.interface);
+			PROC_DUMP("mac = 0x%llx\n",
+					reg_data.oepft_reg.mac);
+		}
+		break;
+	}
+	case PTYS_REG_ID: {
+		struct ku_access_ptys_reg reg_data, reg_data2;
+		memset(&reg_data, 0, sizeof(reg_data));
+		reg_data.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data.op_tlv, reg_id, 2);
+		/* local port is the input */
+		running = sx_proc_str_get_u32(running, &local_port);
+		reg_data.ptys_reg.local_port = local_port;
+		running = sx_proc_str_get_u32(running,
+				&reg_data.ptys_reg.eth_proto_admin);
+		running = sx_proc_str_get_u32(running,
+				&reg_data.ptys_reg.eth_proto_oper);
+		reg_data.ptys_reg.proto_mask = (1<<2);
+		PROC_DUMP("Executing ACCESS_REG_PTYS Command: loc port:%d, "
+				"eth_proto_adm:0x%x, proto_oper:0x%x, "
+				"proto_mask:%d\n",
+				reg_data.ptys_reg.local_port,
+				reg_data.ptys_reg.eth_proto_admin,
+				reg_data.ptys_reg.eth_proto_oper,
+				reg_data.ptys_reg.proto_mask);
+		err = sx_ACCESS_REG_PTYS(dev, &reg_data);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("Finished Executing Write ACCESS_REG_PTYS Command. "
+			"Now executing Query:\n");
+		memset(&reg_data, 0, sizeof(reg_data));
+		memset(&reg_data2, 0, sizeof(reg_data2));
+		reg_data2.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data2.op_tlv, reg_id, 1);
+		reg_data2.ptys_reg.local_port = 1;
+		reg_data2.ptys_reg.proto_mask = (1<<2);
+		err = sx_ACCESS_REG_PTYS(dev, &reg_data2);
+		if (err)
+			goto print_err;
+		PROC_DUMP("fc_proto_capability = %x\n",
+				reg_data2.ptys_reg.fc_proto_capability);
+		PROC_DUMP("eth_proto_capability = %x\n",
+				reg_data2.ptys_reg.eth_proto_capability);
+		PROC_DUMP("ib_proto_capability = %x\n",
+				reg_data2.ptys_reg.ib_proto_capability);
+		PROC_DUMP("fc_proto_admin = %x\n",
+				reg_data2.ptys_reg.fc_proto_admin);
+		PROC_DUMP("eth_proto_admin = %x\n",
+				reg_data2.ptys_reg.eth_proto_admin);
+		PROC_DUMP("ib_proto_admin = %x\n",
+				reg_data2.ptys_reg.ib_proto_admin);
+		PROC_DUMP("fc_proto_oper = %x\n",
+				reg_data2.ptys_reg.fc_proto_oper);
+		PROC_DUMP("eth_proto_oper = %x\n",
+				reg_data2.ptys_reg.eth_proto_oper);
+		PROC_DUMP("ib_proto_oper = %x\n",
+				reg_data2.ptys_reg.ib_proto_oper);
+		break;
+	}
+	case PMLP_REG_ID: {
+		struct ku_access_pmlp_reg reg_data, reg_data2;
+		int i;
+		memset(&reg_data, 0, sizeof(reg_data));
+		reg_data.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data.op_tlv, reg_id, 2);
+		/* local port is the input */
+		reg_data.pmlp_reg.local_port = 1;
+		reg_data.pmlp_reg.width = 4;
+		for (i = 0; i < 4; i++) {
+			reg_data.pmlp_reg.lane[i] = i;
+			reg_data.pmlp_reg.module[i] = 3;
+		}
+		PROC_DUMP("Executing ACCESS_REG_PMLP Command\n");
+		err = sx_ACCESS_REG_PMLP(dev, &reg_data);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("Finished Executing Write ACCESS_REG_PMLP Command. "
+			"Now executing Query:\n");
+		memset(&reg_data2, 0, sizeof(reg_data2));
+		reg_data2.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data2.op_tlv, reg_id, 1);
+		reg_data2.pmlp_reg.local_port = 1;
+		err = sx_ACCESS_REG_PMLP(dev, &reg_data2);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("width = %u\n", reg_data2.pmlp_reg.width);
+		PROC_DUMP("lane0 = %u\n", reg_data2.pmlp_reg.lane[0]);
+		PROC_DUMP("module0 = %u\n", reg_data2.pmlp_reg.module[0]);
+		PROC_DUMP("lane1 = %u\n", reg_data2.pmlp_reg.lane[1]);
+		PROC_DUMP("module1 = %u\n", reg_data2.pmlp_reg.module[1]);
+		PROC_DUMP("lane2 = %u\n", reg_data2.pmlp_reg.lane[2]);
+		PROC_DUMP("module2 = %u\n", reg_data2.pmlp_reg.module[2]);
+		PROC_DUMP("lane3 = %u\n", reg_data2.pmlp_reg.lane[3]);
+		PROC_DUMP("module3 = %u\n", reg_data2.pmlp_reg.module[3]);
+		break;
+	}
+	case PLIB_REG_ID: {
+		struct ku_access_plib_reg reg_data;
+		memset(&reg_data, 0, sizeof(reg_data));
+		reg_data.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data.op_tlv, reg_id, 2);
+		/* local port is the input */
+		running = sx_proc_str_get_u32(running, &local_port);
+                reg_data.plib_reg.local_port = local_port;
+                running = sx_proc_str_get_u32(running, &local_port);
+		reg_data.plib_reg.ib_port = local_port;
+		PROC_DUMP("Executing ACCESS_REG_PLIB SET Command, local_port = %u, ib_port = %u\n",
+				reg_data.plib_reg.local_port, reg_data.plib_reg.ib_port);
+		err = sx_ACCESS_REG_PLIB(dev, &reg_data);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("Finished Executing ACCESS_REG_PLIB SET Command:\n");
+                sx_cmd_set_op_tlv(&reg_data.op_tlv, reg_id, 1);
+		reg_data.plib_reg.ib_port = 0;
+                PROC_DUMP("Executing ACCESS_REG_PLIB GET Command, local_port = %u\n", reg_data.plib_reg.local_port);
+                err = sx_ACCESS_REG_PLIB(dev, &reg_data);
+		PROC_DUMP("ib_port = %u\n", reg_data.plib_reg.ib_port);
+		break;
+	}
+	case PAOS_REG_ID: {
+		struct ku_access_paos_reg reg_data, reg_data2;
+		memset(&reg_data, 0, sizeof(reg_data));
+		reg_data.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data.op_tlv, reg_id, 2);
+		reg_data.paos_reg.local_port = 1;
+		reg_data.paos_reg.admin_status = 2;
+		reg_data.paos_reg.oper_status = 0; /* read only field */
+		reg_data.paos_reg.ase = 1;
+		reg_data.paos_reg.ee = 0;
+		reg_data.paos_reg.e = 0;
+		PROC_DUMP("Executing ACCESS_REG_PAOS Command\n");
+		err = sx_ACCESS_REG_PAOS(dev, &reg_data);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("Finished Executing Write ACCESS_REG_PAOS Command. "
+			"Now executing query:\n");
+		memset(&reg_data2, 0, sizeof(reg_data2));
+		reg_data2.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data2.op_tlv, reg_id, 1);
+		reg_data2.paos_reg.local_port = 1;
+		err = sx_ACCESS_REG_PAOS(dev, &reg_data2);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("admin_status = %u\n",
+			reg_data2.paos_reg.admin_status);
+		PROC_DUMP("oper_status = %u\n",
+				reg_data2.paos_reg.oper_status);
+		PROC_DUMP("ase = %u\n", reg_data2.paos_reg.ase);
+		PROC_DUMP("ee = %u\n", reg_data2.paos_reg.ee);
+		PROC_DUMP("e = %u\n", reg_data2.paos_reg.e);
+		break;
+	}
+	case PMPR_REG_ID: {
+		struct ku_access_pmpr_reg reg_data, reg_data2;
+		memset(&reg_data, 0, sizeof(reg_data));
+		reg_data.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data.op_tlv, reg_id, 2);
+		reg_data.pmpr_reg.module = 1;
+		reg_data.pmpr_reg.attenuation5g = 2;
+		PROC_DUMP("Executing ACCESS_REG_PMPR Command\n");
+		err = sx_ACCESS_REG_PMPR(dev, &reg_data);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("Finished Executing Write ACCESS_REG_PMPR Command. "
+			"Now executing query:\n");
+		memset(&reg_data2, 0, sizeof(reg_data2));
+		reg_data2.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data2.op_tlv, reg_id, 1);
+		reg_data2.pmpr_reg.module = 1;
+		err = sx_ACCESS_REG_PMPR(dev, &reg_data2);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("module = %u\n",
+			reg_data2.pmpr_reg.module);
+		PROC_DUMP("Attenuation5G = %u\n",
+				reg_data2.pmpr_reg.attenuation5g);
+		break;
+	}
+	case PMTU_REG_ID: {
+		struct ku_access_pmtu_reg reg_data, reg_data2;
+		memset(&reg_data, 0, sizeof(reg_data));
+		reg_data.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data.op_tlv, reg_id, 2);
+		reg_data.pmtu_reg.local_port = 1;
+		reg_data.pmtu_reg.admin_mtu = 638;
+		PROC_DUMP("Executing ACCESS_REG_PMTU Command\n");
+		err = sx_ACCESS_REG_PMTU(dev, &reg_data);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("Finished Executing Write ACCESS_REG_PMTU Command. "
+			"Now executing Query:\n");
+		memset(&reg_data2, 0, sizeof(reg_data2));
+		reg_data2.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data2.op_tlv, reg_id, 1);
+		reg_data2.pmtu_reg.local_port = 1;
+		err = sx_ACCESS_REG_PMTU(dev, &reg_data2);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("max_mtu = %u\n", reg_data2.pmtu_reg.max_mtu);
+		PROC_DUMP("admin_mtu = %u\n", reg_data2.pmtu_reg.admin_mtu);
+		PROC_DUMP("oper_mtu = %u\n", reg_data2.pmtu_reg.oper_mtu);
+		break;
+	}
+	case PELC_REG_ID: {
+		u32 operation; /* 1 = QUERY 2 = WRITE*/
+		u32 op = 0;
+		unsigned long admin, request;
+		struct ku_access_pelc_reg reg_data;
+		memset(&reg_data, 0, sizeof(reg_data));
+		reg_data.dev_id = dev_id;
+		running = sx_proc_str_get_u32(running, &operation);
+		sx_cmd_set_op_tlv(&reg_data.op_tlv, reg_id, operation);
+		/* local port and op are the input */
+		running = sx_proc_str_get_u32(running, &local_port);
+		running = sx_proc_str_get_u32(running, &op);
+		reg_data.pelc_reg.local_port = local_port;
+		reg_data.pelc_reg.op = op;
+		if (operation == 2) {
+			running = sx_proc_str_get_ulong(running, &admin);
+			running = sx_proc_str_get_ulong(running, &request);
+			reg_data.pelc_reg.admin = 0x10ULL;
+			reg_data.pelc_reg.request = 0x10ULL;
+			PROC_DUMP("Read admin = 0x%llx, request = 0x%llx\n", reg_data.pelc_reg.admin, reg_data.pelc_reg.request);
+			PROC_DUMP("Executing ACCESS_REG_PELC Set Command\n");
+	                err = sx_ACCESS_REG_PELC(dev, &reg_data);
+        	        if (err)
+	                        goto print_err;
+
+/*			sx_cmd_set_op_tlv(&reg_data.op_tlv, reg_id, 1); */
+		}
+/*		reg_data.pelc_reg.admin = 0;
+		reg_data.pelc_reg.request = 0; */
+		else {
+			PROC_DUMP("Executing ACCESS_REG_PELC Get Command\n");
+			err = sx_ACCESS_REG_PELC(dev, &reg_data);
+			if (err)
+				goto print_err;
+
+			PROC_DUMP("Finished Executing ACCESS_REG_PELC Get Command:\n");
+			PROC_DUMP("admin = %llx\n", reg_data.pelc_reg.admin);
+			PROC_DUMP("capability = %llx\n", reg_data.pelc_reg.capability);
+			PROC_DUMP("request = %llx\n", reg_data.pelc_reg.request);
+			PROC_DUMP("active = %llx\n", reg_data.pelc_reg.active);
+		}
+		break;
+	}
+	case PVLC_REG_ID: {
+		struct ku_access_pvlc_reg reg_data, reg_data2;
+		memset(&reg_data, 0, sizeof(reg_data));
+		reg_data.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data.op_tlv, reg_id, 2);
+		/* local port is the input */
+		reg_data.pvlc_reg.local_port = 1;
+		reg_data.pvlc_reg.vl_admin = 3;
+		PROC_DUMP("Executing ACCESS_REG_PVLC Command\n");
+		err = sx_ACCESS_REG_PVLC(dev, &reg_data);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("Finished Executing Write ACCESS_REG_PVLC Command. "
+				"Now executing Query:\n");
+		memset(&reg_data2, 0, sizeof(reg_data2));
+		reg_data2.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data2.op_tlv, reg_id, 1);
+		reg_data2.pvlc_reg.local_port = 1;
+		err = sx_ACCESS_REG_PVLC(dev, &reg_data2);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("vl_cap = %u\n", reg_data2.pvlc_reg.vl_cap);
+		PROC_DUMP("vl_admin = %u\n", reg_data2.pvlc_reg.vl_admin);
+		PROC_DUMP("vl_oper = %u\n", reg_data2.pvlc_reg.vl_operational);
+		break;
+	}
+	case HPKT_REG_ID: {
+		struct ku_access_hpkt_reg reg_data, reg_data2;
+		memset(&reg_data, 0, sizeof(reg_data));
+		reg_data.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data.op_tlv, reg_id, 2);
+		/* trap_id is the input */
+		reg_data.hpkt_reg.action = 1;
+		reg_data.hpkt_reg.trap_group = 2;
+		reg_data.hpkt_reg.trap_id = 5;
+		PROC_DUMP("Executing ACCESS_REG_HPKT Command\n");
+		err = sx_ACCESS_REG_HPKT(dev, &reg_data);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("Finished Executing Write ACCESS_REG_HPKT Command. "
+			"Now executing Query:\n");
+		memset(&reg_data2, 0, sizeof(reg_data2));
+		reg_data2.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data2.op_tlv, reg_id, 1);
+		reg_data2.hpkt_reg.trap_id = 0x10;
+		err = sx_ACCESS_REG_HPKT(dev, &reg_data2);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("action = %u\n", reg_data2.hpkt_reg.action);
+		PROC_DUMP("trap_group = %u\n", reg_data2.hpkt_reg.trap_group);
+		break;
+	}
+	case HCAP_REG_ID: {
+		struct ku_access_hcap_reg reg_data;
+		memset(&reg_data, 0, sizeof(reg_data));
+		reg_data.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data.op_tlv, reg_id, 1);
+		PROC_DUMP("Executing ACCESS_REG_HCAP Command\n");
+		err = sx_ACCESS_REG_HCAP(dev, &reg_data);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("Finished Executing ACCESS_REG_HCAP Command:\n");
+		PROC_DUMP("max_num_cpu_tclass = %u\n",
+				reg_data.hcap_reg.max_cpu_ingress_tclass);
+		PROC_DUMP("max_num_trap_groups = %u\n",
+				reg_data.hcap_reg.max_num_trap_groups);
+		PROC_DUMP("max_num_dr_paths = %u\n",
+				reg_data.hcap_reg.max_num_dr_paths);
+		break;
+	}
+	case HDRT_REG_ID: {
+		struct ku_access_hdrt_reg reg_data;
+		u32 dr_index;
+		int i;
+		memset(&reg_data, 0, sizeof(reg_data));
+		reg_data.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data.op_tlv, reg_id, 1);
+		/* dr_index port is the input */
+		sx_proc_str_get_u32(running, &dr_index);
+		reg_data.hdrt_reg.dr_index = (u8)dr_index;
+		PROC_DUMP("Executing ACCESS_REG_HDRT Command\n");
+		err = sx_ACCESS_REG_HDRT(dev, &reg_data);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("Finished Executing ACCESS_REG_HDRT Command:\n");
+		PROC_DUMP("hop_cnt = %u\n", reg_data.hdrt_reg.hop_cnt);
+		for (i = 0; i < 64; i++)
+			PROC_DUMP("path %d = %u\n", i,
+					reg_data.hdrt_reg.path[i]);
+		for (i = 0; i < 64; i++)
+			PROC_DUMP("rpath %d = %u\n", i,
+					reg_data.hdrt_reg.rpath[i]);
+		break;
+	}
+	case MFSC_REG_ID: {
+		struct ku_access_mfsc_reg reg_data;
+		u32 fan;
+		memset(&reg_data, 0, sizeof(reg_data));
+		reg_data.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data.op_tlv, reg_id, 1);
+		/* fan is the input */
+		running = sx_proc_str_get_u32(running, &fan);
+		reg_data.mfsc_reg.pwm = (u8)(fan & 0x7);
+		PROC_DUMP("Executing ACCESS_REG_MFSC Command , dev_id: %d \n", reg_data.dev_id);
+		err = sx_ACCESS_REG_MFSC(dev, &reg_data);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("Finished Executing ACCESS_REG_MFSC Command:\n");
+		PROC_DUMP("pwm_duty_cycle = %u\n",
+				reg_data.mfsc_reg.pwm_duty_cycle);
+		break;
+	}
+	case MFSM_REG_ID: {
+		struct ku_access_mfsm_reg reg_data;
+		u32 tacho;
+		memset(&reg_data, 0, sizeof(reg_data));
+		reg_data.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data.op_tlv, reg_id, 1);
+		/* tacho is the input */
+		running = sx_proc_str_get_u32(running, &tacho);
+		reg_data.mfsm_reg.tacho = (u8)(tacho & 0x7);
+		PROC_DUMP("Executing ACCESS_REG_MFSM Command, dev_id: %d\n", dev_id);
+		err = sx_ACCESS_REG_MFSM(dev, &reg_data);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("Finished Executing ACCESS_REG_MFSM Command:\n");
+		PROC_DUMP("n = %u\n", reg_data.mfsm_reg.n);
+		PROC_DUMP("rpm = %u\n", reg_data.mfsm_reg.rpm);
+		break;
+	}
+	case MFSL_REG_ID: {
+		struct ku_access_mfsl_reg reg_data;
+		u32 tacho;
+		memset(&reg_data, 0, sizeof(reg_data));
+		reg_data.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data.op_tlv, reg_id, 1);
+		/* tacho is the input */
+		running = sx_proc_str_get_u32(running, &tacho);
+		reg_data.mfsl_reg.fan = (u8)(tacho & 0x7);
+		PROC_DUMP("Executing ACCESS_REG_MFSL Command\n");
+		err = sx_ACCESS_REG_MFSL(dev, &reg_data);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("Finished Executing ACCESS_REG_MFSL Command:\n");
+		PROC_DUMP("ee = %u\n", reg_data.mfsl_reg.ee);
+		PROC_DUMP("ie = %u\n", reg_data.mfsl_reg.ie);
+		PROC_DUMP("tach_min = %u\n", reg_data.mfsl_reg.tach_min);
+		PROC_DUMP("tach_max = %u\n", reg_data.mfsl_reg.tach_max);
+		break;
+	}
+	case SPZR_REG_ID: {
+		struct ku_access_spzr_reg reg_data;
+		memset(&reg_data, 0, sizeof(reg_data));
+		reg_data.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data.op_tlv, reg_id, 1);
+		reg_data.spzr_reg.EnhSwP0 = 1;
+		reg_data.spzr_reg.EnhSwP0_mask = 1;
+		PROC_DUMP("Executing ACCESS_REG_SPZR Command\n");
+		err = sx_ACCESS_REG_SPZR(dev, &reg_data);
+		break;
+	}
+	case HTGT_REG_ID: {
+		struct ku_access_htgt_reg reg_data, reg_data2;
+		memset(&reg_data, 0, sizeof(reg_data));
+		reg_data.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data.op_tlv, reg_id, 2);
+		PROC_DUMP("Executing ACCESS_REG_HTGT Command\n");
+		reg_data.htgt_reg.swid = 254;
+		reg_data.htgt_reg.type = 0;
+		reg_data.htgt_reg.trap_group = 2;
+		reg_data.htgt_reg.path.local_path.rdq = 21;
+		reg_data.htgt_reg.path.local_path.cpu_tclass = 1;
+		err = sx_ACCESS_REG_HTGT(dev, &reg_data);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("Finished Executing Write ACCESS_REG_HTGT Command. "
+				"Now executing Query:\n");
+		memset(&reg_data2, 0, sizeof(reg_data2));
+		reg_data2.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data2.op_tlv, reg_id, 1);
+		reg_data2.htgt_reg.swid = 0;
+		reg_data2.htgt_reg.trap_group = 2;
+		err = sx_ACCESS_REG_HTGT(dev, &reg_data2);
+		if (err)
+			goto print_err;
+
+		PROC_DUMP("swid = %u\n", reg_data2.htgt_reg.swid);
+		PROC_DUMP("type = %u\n", reg_data2.htgt_reg.type);
+		PROC_DUMP("grp = %u\n", reg_data2.htgt_reg.trap_group);
+		PROC_DUMP("rdq = %u\n",
+				reg_data2.htgt_reg.path.local_path.rdq);
+		PROC_DUMP("cpu_tclass = %u\n",
+				reg_data2.htgt_reg.path.local_path.cpu_tclass);
+		break;
+	}
+	case MCIA_REG_ID:
+	case QSPTC_REG_ID:
+	case QSTCT_REG_ID:
+		printk(KERN_WARNING "%s() reg_id %u not implemented yet\n",
+			   __func__, reg_id);
+		break;
+
+	case MPSC_REG_ID: {
+		struct ku_access_mpsc_reg reg_data, reg_data2;
+		u32 rate;
+		u32 enable;
+		memset(&reg_data, 0, sizeof(reg_data));
+		reg_data.dev_id = dev_id;
+
+		running = sx_proc_str_get_u32(running, &local_port);
+		reg_data.mpsc_reg.local_port = local_port;
+		running = sx_proc_str_get_u32(running, &rate);
+		reg_data.mpsc_reg.rate = rate;
+		running = sx_proc_str_get_u32(running, &enable);
+		reg_data.mpsc_reg.enable = (u8)enable;
+
+		sx_cmd_set_op_tlv(&reg_data.op_tlv, reg_id, 2);
+		PROC_DUMP("Executing ACCESS_REG_MPSC Write Command\n");
+
+		err = sx_ACCESS_REG_MPSC(dev, &reg_data);
+		PROC_DUMP();
+
+		PROC_DUMP("Finished Executing Write ACCESS_REG_MPSC Command. "
+				"Now executing Query:\n");
+		memset(&reg_data2, 0, sizeof(reg_data2));
+		reg_data2.dev_id = dev_id;
+		sx_cmd_set_op_tlv(&reg_data2.op_tlv, reg_id, 1);
+
+		err = sx_ACCESS_REG_MPSC(dev, &reg_data2);
+		if (err)
+			goto print_err;
+		PROC_DUMP("Read rate = 0x%08u, count_drops = %lld\n", reg_data2.mpsc_reg.rate, reg_data2.mpsc_reg.count_drops);
+		break;
+
+	}
+
+	default:
+		printk(KERN_WARNING "%s() reg_id %u doesn't exist\n",
+			   __func__, reg_id);
+
+	}
+
+	return;
+
+print_err:
+	printk(KERN_WARNING "%s() execution of ACCESS_REG on reg_id %x Failed."
+			"error code = %d \n", __func__, reg_id, err);
+
+}
+
+static void check_netdev_name(char *name) {
+	/* whitespaces are not allowed in netdevice names */
+	while (*name) {
+		if (*name == '/' || isspace(*name)) {
+			*name = '\0';
+			break;
+		}
+
+		name++;
+	}
+
+}
+void __rem_port_netdev(struct sx_dev *my_dev, char *running)
+{
+    /* We only need the sysport */
+    int sysport;
+    union sx_event_data event_data;
+    running = sx_proc_str_get_u32(running, &sysport);
+    event_data.port_netdev_set.sysport = sysport;
+    PROC_DUMP("Remove port netdev, sysport: %d \n", sysport);
+    sx_core_dispatch_event(my_dev, SX_DEV_EVENT_CLOSE_PORT_NETDEV, &event_data);
+}
+
+static ssize_t sx_proc_write(struct file *file, const char __user *buffer,
+				size_t count, loff_t *pos)
+{
+	int cmd = 0;
+	char *p;
+	char *running;
+	char *cmd_str;
+	struct sx_dev *my_dev = sx_glb.
+			sx_dpt.dpt_info[DEFAULT_DEVICE_ID].
+			sx_pcie_info.sx_dev;
+
+	running = (char *)buffer;
+	running = sx_proc_str_get_str(running, &cmd_str);
+	p = strstr(cmd_str, "dump_dpt");
+	if (p != NULL) {
+		/* PROC_DUMP("  call to dump DPT table !!!\n"); */
+		sx_dpt_dump();
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "add_port_netdev");
+	if (p != NULL) {
+		int sysport;
+		char *name;
+		union sx_event_data event_data;
+		running = sx_proc_str_get_u32(running, &sysport);
+		running = sx_proc_str_get_str(running, &name);
+		check_netdev_name(name);
+		event_data.port_netdev_set.sysport = sysport;
+		event_data.port_netdev_set.name = name;
+		event_data.port_netdev_set.swid = 1;
+		PROC_DUMP("Add port netdev, sysport: 0x%x name: %s \n", sysport, name);
+		sx_core_dispatch_event(my_dev, SX_DEV_EVENT_OPEN_PORT_NETDEV, &event_data);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "rem_port_netdev");
+	if (p != NULL) {
+        __rem_port_netdev(my_dev, running);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "dpt_setp");
+	if (p != NULL) {
+		int dev_id, path_type;
+		running = sx_proc_str_get_u32(running, &dev_id);
+		running = sx_proc_str_get_u32(running, &path_type);
+		PROC_DUMP("Change dpt dev_id:%d , path_type: %d  \n",
+				dev_id, path_type);
+		sx_dpt_set_cmd_dbg(dev_id, path_type);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "dpt_set_cr");
+        if (p != NULL) {
+                int dev_id, path_type;
+                running = sx_proc_str_get_u32(running, &dev_id);
+                running = sx_proc_str_get_u32(running, &path_type);
+                PROC_DUMP("Change dpt dev_id:%d , CR path_type: %d  \n",
+                                dev_id, path_type);
+                sx_dpt_set_cr_access_path(dev_id, path_type);
+                cmd++;
+        }
+
+	p = strstr(cmd_str, "cr_space_write");
+        if (p != NULL) {
+                int dev_id;
+		unsigned int address;
+		unsigned int value;
+                running = sx_proc_str_get_u32(running, &dev_id);
+                running = sx_proc_str_get_u32(running, &address);
+		running = sx_proc_str_get_u32(running, &value);
+                PROC_DUMP("write dword to cr_space dev_id:%d , address: 0x%x , value: %d \n",
+                                dev_id, address, value);
+                sx_dpt_cr_space_write(dev_id, address, (unsigned char *)&value, 4);
+                cmd++;
+        }
+
+	p = strstr(cmd_str, "cr_space_read");
+        if (p != NULL) {
+                int dev_id;
+		unsigned int address;
+		unsigned int value = 0;
+                running = sx_proc_str_get_u32(running, &dev_id);
+                running = sx_proc_str_get_u32(running, &address);
+                PROC_DUMP("read dword from cr_space dev_id:%d , address: 0x%x\n",
+                                dev_id, address);
+                sx_dpt_cr_space_read(dev_id, address, (unsigned char *)&value, 4);
+		PROC_DUMP("cr_space read: dev_id:%d , address: 0x%x , value: %u \n",
+                                dev_id, address, value);
+                cmd++;
+        }
+
+	p = strstr(cmd_str, "q_fw");
+	if (p != NULL) {
+		struct ku_query_fw fw;
+		int dev_id;
+		running = sx_proc_str_get_u32(running, &dev_id);
+		if (dev_id == 0)
+			dev_id = my_dev->device_id;
+
+		if (sx_glb.sx_dpt.dpt_info[dev_id].sx_pcie_info.sx_dev != NULL)
+		    my_dev = sx_glb.sx_dpt.dpt_info[dev_id].sx_pcie_info.sx_dev;
+
+		fw.dev_id = dev_id;
+
+		PROC_DUMP("Executing QUERY_FW Command to dev %d !\n", dev_id);
+		sx_QUERY_FW(my_dev, &fw);
+		PROC_DUMP("fw_rev = 0x%llx\n", fw.fw_rev);
+		PROC_DUMP("debug_trace = 0x%x\n", fw.dt);
+		PROC_DUMP("core_clock = 0x%x\n", fw.core_clk);
+		PROC_DUMP("fw_hour = 0x%x\n", fw.fw_hour);
+		PROC_DUMP("fw_minutes = 0x%x\n", fw.fw_minutes);
+		PROC_DUMP("fw_seconds = 0x%x\n", fw.fw_seconds);
+		PROC_DUMP("fw_year = 0x%x\n", fw.fw_year);
+		PROC_DUMP("fw_month = 0x%x\n", fw.fw_month);
+		PROC_DUMP("fw_day = 0x%x\n", fw.fw_day);
+
+		PROC_DUMP("Executing QUERY_FW_2 Command\n");
+		sx_QUERY_FW_2(my_dev, dev_id);
+		PROC_DUMP("Finished Executing QUERY_FW_2 Command:\n");
+		PROC_DUMP("local_out_mb_offset = 0x%x\n",
+				sx_glb.sx_dpt.dpt_info[dev_id].out_mb_offset);
+		PROC_DUMP("local_out_mb_size = 0x%x\n",
+				sx_glb.sx_dpt.dpt_info[dev_id].out_mb_size);
+		PROC_DUMP("local_in_mb_offset = 0x%x\n",
+				sx_glb.sx_dpt.dpt_info[dev_id].in_mb_offset);
+		PROC_DUMP("local_in_mb_size = 0x%x\n",
+				sx_glb.sx_dpt.dpt_info[dev_id].in_mb_size);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "q_board_info");
+	if (p != NULL) {
+		struct sx_board board;
+
+		memset(&board, 0, sizeof(board));
+		PROC_DUMP("Executing QUERY_BOARDINFO Command\n");
+		sx_QUERY_BOARDINFO(my_dev, &board);
+		PROC_DUMP("Finished Executing QUERY_BOARDINFO Command:\n");
+		PROC_DUMP("vsd_vendor_id = 0x%x\n", board.vsd_vendor_id);
+		PROC_DUMP("board_id = %s\n", board.board_id);
+		PROC_DUMP("inta_pin = 0x%x\n", board.inta_pin);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "intr");
+	if (p != NULL) {
+		struct sx_board board;
+
+		memset(&board, 0, sizeof(board));
+		PROC_DUMP("Executing QUERY_BOARDINFO Command\n");
+		sx_cmd_use_events(my_dev);
+		sx_QUERY_BOARDINFO(my_dev, &board);
+		sx_cmd_use_polling(my_dev);
+		PROC_DUMP("Finished Executing QUERY_BOARDINFO Command:\n");
+		PROC_DUMP("vsd_vendor_id = 0x%x\n", board.vsd_vendor_id);
+		PROC_DUMP("board_id = %s\n", board.board_id);
+		PROC_DUMP("inta_pin = 0x%x\n", board.inta_pin);
+		cmd++;
+	}
+
+
+	p = strstr(cmd_str, "q_aq_cap");
+	if (p != NULL) {
+		struct sx_dev_cap *dev_cap = &my_dev->dev_cap;
+
+		PROC_DUMP("Executing QUERY_AQ_CAP Command\n");
+		sx_QUERY_AQ_CAP(my_dev);
+		PROC_DUMP("Finished Executing QUERY_AQ_CAP Command:\n");
+		PROC_DUMP("log_max_rdq_sz = %d\n", dev_cap->log_max_rdq_sz);
+		PROC_DUMP("log_max_sdq_sz = %d\n", dev_cap->log_max_sdq_sz);
+		PROC_DUMP("log_max_cq_sz = %d\n", dev_cap->log_max_cq_sz);
+		PROC_DUMP("log_max_eq_sz = %d\n", dev_cap->log_max_eq_sz);
+		PROC_DUMP("max_num_rdqs = %u\n", dev_cap->max_num_rdqs);
+		PROC_DUMP("max_num_sdqs = %u\n", dev_cap->max_num_sdqs);
+		PROC_DUMP("max_num_cqs = %u\n", dev_cap->max_num_cqs);
+		PROC_DUMP("max_num_eqs = %u\n", dev_cap->max_num_eqs);
+		PROC_DUMP("max_sg_sq = %u\n", dev_cap->max_sg_sq);
+		PROC_DUMP("max_sg_rq = %u\n", dev_cap->max_sg_rq);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "get_profile");
+	if (p != NULL) {
+		struct ku_profile *profile;
+
+		profile = vmalloc(sizeof(*profile));
+		if(profile == NULL){
+			printk("Error allocate mem for profile\n");
+			goto out;
+		}
+		memset(profile, 0, sizeof(*profile));
+		PROC_DUMP("Executing GET_PROFILE Command\n");
+		sx_GET_PROFILE(my_dev, profile);
+		PROC_DUMP("Finished Executing GET_PROFILE Command:\n");
+		PROC_DUMP("max_vepa_channels = %u\n",
+				profile->max_vepa_channels);
+		PROC_DUMP("max_lag = %u\n", profile->max_lag);
+		PROC_DUMP("max_port_per_lag = %u\n", profile->max_port_per_lag);
+		PROC_DUMP("max_mid = %u\n", profile->max_mid);
+		PROC_DUMP("max_pgt = %u\n", profile->max_pgt);
+		PROC_DUMP("max_system_port = %u\n", profile->max_system_port);
+		PROC_DUMP("max_active_vlans = %u\n", profile->max_active_vlans);
+		PROC_DUMP("max_regions = %u\n", profile->max_regions);
+		PROC_DUMP("max_flood_tables = %u\n", profile->max_flood_tables);
+		PROC_DUMP("max_per_vid_flood_tables = %u\n", profile->max_per_vid_flood_tables);
+		PROC_DUMP("flood mode = %u\n", profile->flood_mode);
+		PROC_DUMP("max_fid_offset_flood_tables = %u\n", profile->max_fid_offset_flood_tables);
+		PROC_DUMP("fid_offset_table_size = %u\n", profile->fid_offset_table_size);
+		PROC_DUMP("max_per_fid_flood_table = %u\n", profile->max_per_fid_flood_table);
+		PROC_DUMP("per_fid_table_size = %u\n", profile->per_fid_table_size);
+		PROC_DUMP("max_ib_mc = %u\n", profile->max_ib_mc);
+		PROC_DUMP("max_pkey = %u\n", profile->max_pkey);
+		PROC_DUMP("ar_sec = %u\n", profile->ar_sec);
+		PROC_DUMP("adaptive_routing_group_cap = %u\n", profile->adaptive_routing_group_cap);
+		PROC_DUMP("arn = %u\n", profile->arn);
+		PROC_DUMP("kvd_linear_size = %u\n", profile->kvd_linear_size);
+		PROC_DUMP("kvd_hash_single_size = %u\n", profile->kvd_hash_single_size);
+		PROC_DUMP("kvd_hash_double_size = %u\n", profile->kvd_hash_double_size);
+		PROC_DUMP("switch_0_type = %u\n",
+				profile->swid0_config_type.type);
+		PROC_DUMP("switch_1_type = %u\n",
+				profile->swid1_config_type.type);
+		PROC_DUMP("switch_2_type = %u\n",
+				profile->swid2_config_type.type);
+		PROC_DUMP("switch_3_type = %u\n",
+				profile->swid3_config_type.type);
+		PROC_DUMP("switch_4_type = %u\n",
+				profile->swid4_config_type.type);
+		PROC_DUMP("switch_5_type = %u\n",
+				profile->swid5_config_type.type);
+		PROC_DUMP("switch_6_type = %u\n",
+				profile->swid6_config_type.type);
+		PROC_DUMP("switch_7_type = %u\n",
+				profile->swid7_config_type.type);
+		vfree(profile);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "access_reg");
+	if (p != NULL) {
+		u32 reg_id = 0;
+		u32 dev_id = 0;
+		sx_cmd_use_polling(my_dev);
+		running = sx_proc_str_get_u32(running, &dev_id);
+		running = sx_proc_str_get_u32(running, &reg_id);
+		sx_proc_handle_access_reg(my_dev, p, dev_id, reg_id, running);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "get_pci");
+	if (p != NULL) {
+		unsigned int sx_pci_dev_id, vendor, device;
+		struct pci_dev *sx_pci_dev;
+		running = sx_proc_str_get_u32(running, &sx_pci_dev_id);
+		running = sx_proc_str_get_u32(running, &vendor);
+		running = sx_proc_str_get_u32(running, &device);
+		sx_dpt_find_pci_dev(sx_pci_dev_id, vendor, device, &sx_pci_dev);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "mem_wr");
+	if (p != NULL) {
+		unsigned long  width;
+		unsigned long  addr, val;
+		running = sx_proc_str_get_ulong(running, &width);
+		running = sx_proc_str_get_ulong(running, &addr);
+		running = sx_proc_str_get_ulong(running, &val);
+		printk("Write value %lx to addr: 0x%lx  \n", val, addr);
+		sx_proc_dbg_wr(width, addr, val);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "mem_rd ");
+	if (p != NULL) {
+		unsigned long width;
+		unsigned long val, addr;
+		running = sx_proc_str_get_ulong(running, &width);
+		running = sx_proc_str_get_ulong(running, &addr);
+		sx_proc_dbg_rd(width, addr, &val);
+		printk(KERN_INFO "Read value addr: 0x%lx  val: 0x%lx \n",
+			addr, val);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "sw_reset");
+	if (p != NULL) {
+		int err = 0;
+
+		if (!my_dev->pdev) {
+			PROC_DUMP("Executing sw_reset is only possible "
+					"when PCI is available\n");
+			goto out;
+		}
+
+		PROC_DUMP("Executing sw_reset\n");
+		err = sx_reset(my_dev);
+		PROC_DUMP("Finished Executing sw_reset. err = %d\n", err);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "enable_swid");
+	if (p != NULL) {
+		int err = 0;
+		u32 swid;
+		running = sx_proc_str_get_u32(running, &swid);
+		spin_lock(&my_dev->profile_lock);
+		if (!my_dev->profile_set || swid >= NUMBER_OF_SWIDS
+				|| my_dev->profile.swid_type[swid] ==
+				SX_KU_L2_TYPE_DONT_CARE ||
+				sx_bitmap_test(&sx_priv(my_dev)->swid_bitmap,
+						swid)) {
+			PROC_DUMP("Err in param, not executing enable_swid\n");
+			spin_unlock(&my_dev->profile_lock);
+			goto out;
+		}
+
+		spin_unlock(&my_dev->profile_lock);
+		PROC_DUMP("Executing enable_swid. dev_id = %u, "
+				"swid num = %u\n",
+				my_dev->device_id, swid);
+		err = sx_enable_swid(my_dev, my_dev->device_id, swid, 0x1c0 + swid, 0);
+		PROC_DUMP("Finished Executing enable_swid. err = %d\n", err);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "disable_swid");
+	if (p != NULL) {
+		u32 swid;
+		running = sx_proc_str_get_u32(running, &swid);
+		spin_lock(&my_dev->profile_lock);
+		if (!my_dev->profile_set || swid >= NUMBER_OF_SWIDS
+				|| my_dev->profile.swid_type[swid] ==
+				SX_KU_L2_TYPE_DONT_CARE ||
+				!sx_bitmap_test(&sx_priv(my_dev)->swid_bitmap,
+						swid)) {
+			PROC_DUMP("Err in param not executing disable_swid\n");
+			spin_unlock(&my_dev->profile_lock);
+			goto out;
+		}
+		spin_unlock(&my_dev->profile_lock);
+		PROC_DUMP("Executing disable_swid. swid num = %u\n", swid);
+		sx_disable_swid(my_dev, swid);
+		PROC_DUMP("Finished Executing disable_swid\n");
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "change_conf");
+	if (p != NULL) {
+		int err = 0;
+		PROC_DUMP("Executing change_configuration. \n");
+		err = sx_change_configuration(my_dev);
+		PROC_DUMP("Finished Executing change_configuration. "
+				"err = %d\n", err);
+		cmd++;
+	}	
+	
+	p = strstr(cmd_str, "pcidrv_restart");
+    if (p != NULL) {
+        int err = 0;
+        PROC_DUMP("Executing restart pci driver. \n");        
+        err = sx_restart_one_pci(my_dev->pdev);
+        PROC_DUMP("Finished restart pci driver. "
+                "err = %d\n", err);
+        cmd++;
+    }
+
+	p = strstr(cmd_str, "set_pci_prof");
+	if (p != NULL) {
+		struct sx_pci_profile *profile = &my_dev->profile;
+		int err = 0;
+		memset(profile, 0, sizeof(*profile));
+		profile->swid_type[0] = SX_KU_L2_TYPE_IB;
+		profile->tx_prof[0][0].stclass = 0;
+		profile->tx_prof[0][1].stclass = 1;
+		profile->tx_prof[0][2].stclass = 2;
+		profile->tx_prof[0][0].sdq = 0;
+		profile->tx_prof[0][1].sdq = 1;
+		profile->tx_prof[0][2].sdq = 2;
+		profile->rdq_count[0] = 3;
+		profile->rdq[0][0] = 0;
+		profile->rdq[0][1] = 1;
+		profile->rdq[0][2] = 2;
+		profile->rdq_properties[0].number_of_entries = 128;
+		profile->rdq_properties[0].entry_size = 2048;
+		profile->rdq_properties[1].number_of_entries = 128;
+		profile->rdq_properties[1].entry_size = 2048;
+		profile->rdq_properties[2].number_of_entries = 128;
+		profile->rdq_properties[2].entry_size = 2048;
+		profile->cpu_egress_tclass[0] = 0;
+		profile->cpu_egress_tclass[1] = 1;
+		profile->cpu_egress_tclass[2] = 2;
+		PROC_DUMP("Executing set profile. dev_id = %u \n",
+				my_dev->device_id);
+		err = sx_handle_set_profile(my_dev);
+		PROC_DUMP("Finished Executing set profile. "
+				"err = %d\n", err);
+		if (!err)
+			my_dev->profile_set = 1;
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "set_eth_prof");
+	if (p != NULL) {
+		struct sx_pci_profile profile;
+		int err = 0, j;
+		memset(&profile, 0, sizeof(profile));
+		profile.swid_type[0] = SX_KU_L2_TYPE_ETH;
+
+		for (j = 0; j <= 7; j++) {
+			profile.tx_prof[0][j].stclass = j;
+			profile.tx_prof[0][j].sdq = j;
+		}
+
+		profile.rdq_count[0] = 3;
+		profile.rdq[0][0] = 0;
+		profile.rdq[0][1] = 1;
+		profile.rdq[0][2] = 2;
+
+		profile.emad_tx_prof.stclass = 7;
+		profile.emad_tx_prof.sdq = 7;
+		profile.emad_rdq = 21;
+
+		profile.rdq_properties[0].number_of_entries = 128;
+		profile.rdq_properties[0].entry_size = 2048;
+		profile.rdq_properties[0].rdq_weight = 10;
+		profile.rdq_properties[1].number_of_entries = 128;
+		profile.rdq_properties[1].entry_size = 2048;
+		profile.rdq_properties[1].rdq_weight = 10;
+		profile.rdq_properties[2].number_of_entries = 128;
+		profile.rdq_properties[2].entry_size = 2048;
+		profile.rdq_properties[2].rdq_weight = 10;
+		profile.rdq_properties[21].number_of_entries = 128;
+		profile.rdq_properties[21].entry_size = 2048;
+		profile.rdq_properties[21].rdq_weight = 10;
+
+		profile.cpu_egress_tclass[0] = 0;
+		profile.cpu_egress_tclass[1] = 1;
+		profile.cpu_egress_tclass[2] = 1;
+		profile.cpu_egress_tclass[3] = 0;
+		profile.cpu_egress_tclass[4] = 2;
+		profile.cpu_egress_tclass[5] = 2;
+		profile.cpu_egress_tclass[6] = 0;
+		profile.cpu_egress_tclass[7] = 3;
+		profile.cpu_egress_tclass[8] = 3;
+		profile.cpu_egress_tclass[9] = 0;
+		profile.cpu_egress_tclass[10] = 4;
+		profile.cpu_egress_tclass[11] = 4;
+		profile.cpu_egress_tclass[12] = 0;
+		profile.cpu_egress_tclass[13] = 5;
+		profile.cpu_egress_tclass[14] = 5;
+		profile.cpu_egress_tclass[15] = 0;
+		profile.cpu_egress_tclass[16] = 6;
+		profile.cpu_egress_tclass[17] = 6;
+		profile.cpu_egress_tclass[18] = 0;
+		profile.cpu_egress_tclass[19] = 7;
+		profile.cpu_egress_tclass[20] = 7;
+		profile.cpu_egress_tclass[21] = 0;
+		profile.cpu_egress_tclass[22] = 8;
+		profile.cpu_egress_tclass[23] = 8;
+
+		profile.pci_profile = PCI_PROFILE_EN_SINGLE_SWID;
+		my_dev->profile.pci_profile = profile.pci_profile;
+
+		memcpy(&my_dev->profile, &profile, sizeof(profile));
+		PROC_DUMP("Executing set profile. dev_id = %u \n",
+				my_dev->device_id);
+		err = sx_handle_set_profile(my_dev);
+		PROC_DUMP("Finished Executing set profile. "
+				"err = %d\n", err);
+		if (!err)
+			my_dev->profile_set = 1;
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "dump_buf");
+	if (p != NULL) {
+		unsigned long val, addr, cnt;
+		int i;
+		running = sx_proc_str_get_ulong(running, &cnt);
+		sx_proc_str_get_ulong(running, &addr);
+		for (i = 0; i < cnt; i++) {
+			if (i == 0 || (i%4 == 0))
+				printk(KERN_INFO "\n0x%04x : ", i);
+
+			sx_proc_dbg_rd(1, addr+i, &val);
+			printk(KERN_INFO " 0x%02x", (u8)val);
+		}
+		printk("\n");
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "dump_swid");
+	if (p != NULL) {
+		PROC_DUMP("Dump active swids: \n");
+		__sx_proc_dump_swids(my_dev);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "dump_sdq");
+	if (p != NULL) {
+		PROC_DUMP("Dump active sdqs: \n");
+		__sx_proc_dump_sdq(my_dev);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "dump_rdq");
+	if (p != NULL) {
+		PROC_DUMP("Dump active rdqs: \n");
+		__sx_proc_dump_rdq(my_dev);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "dump_eq");
+    if (p != NULL) {
+        PROC_DUMP("Dump active eqs: \n");
+        __sx_proc_dump_eq(my_dev);
+        cmd++;
+    }
+
+	p = strstr(cmd_str, "show_rdq_post");
+	if (p != NULL) {
+		unsigned long rdq_idx;
+		running = sx_proc_str_get_ulong(running, &rdq_idx);
+
+		PROC_DUMP("Show rdqs data: \n");
+		__sx_proc_dump_rdq_single(my_dev,rdq_idx);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "dev_prof");
+	if (p != NULL) {
+		PROC_DUMP("Set dev profile: \n");
+		__sx_proc_set_dev_profile(my_dev);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "dump_synd");
+	if (p != NULL) {
+		PROC_DUMP("Dump syndromes: \n");
+		sx_core_dump_synd_tbl(my_dev) ;
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "dump_stats");
+	if (p != NULL) {
+		PROC_DUMP("Dump statistics: \n");
+		__dump_stats(my_dev);
+		cmd++;
+	}
+	p = strstr(cmd_str, "trace_reg");
+	if (p != NULL) {
+		unsigned long reg_id;
+		unsigned long max_cnt;
+		running = sx_proc_str_get_ulong(running, &reg_id);
+		running = sx_proc_str_get_ulong(running, &max_cnt);
+
+		PROC_DUMP("trace_reg id:0x%lx max_cnt:0x%lx :\n",
+				  reg_id, max_cnt);
+		__sx_proc_dump_reg(my_dev, reg_id, max_cnt);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "trace_pkt");
+	if (p != NULL) {
+		unsigned long emad_type;
+		unsigned long max_cnt;
+		unsigned long direction;
+		running = sx_proc_str_get_ulong(running, &emad_type);
+		running = sx_proc_str_get_ulong(running, &max_cnt);
+		running = sx_proc_str_get_ulong(running, &direction);
+
+		PROC_DUMP("trace_emad type:0x%lx max_cnt:0x%lx direction:%ld\n",
+				  emad_type, max_cnt, direction);
+		__sx_proc_dump_emad(my_dev, emad_type, max_cnt, direction);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "dump_netdev");
+	if (p != NULL) {
+		PROC_DUMP("dump netdev info.\n");
+		__debug_dump_netdev(my_dev);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "dbg_trap");
+	if (p != NULL) {
+		unsigned long trap_id;
+		unsigned long is_add;
+		running = sx_proc_str_get_ulong(running, &trap_id);
+		running = sx_proc_str_get_ulong(running, &is_add);
+
+		PROC_DUMP("debug netdev trap: trap_id: %ld (0-all 5 traps), is_add; %ld \n",
+								trap_id, is_add);
+		__debug_netdev_traps(my_dev, trap_id, is_add);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "oper_dev");
+	if (p != NULL) {
+		char	*dev_name;
+		char	*oper_state;
+		running = sx_proc_str_get_str(running, &dev_name);
+		running = sx_proc_str_get_str(running, &oper_state);
+
+		PROC_DUMP("debug set dev %s oper state: %s \n",
+								dev_name, oper_state);
+		__set_net_dev_oper_state(dev_name, oper_state);
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "dump_dev");
+	if (p != NULL) {
+		PROC_DUMP("Dump netdev :\n");
+		__dump_net_dev_oper_state();
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "dump_kdb");
+    if (p != NULL) {
+        PROC_DUMP("Dump kernel DBs:\n");
+        __dump_kdbs();
+        cmd++;
+    }
+
+
+
+	p = strstr(cmd_str, "set_rdq_rl");
+	if (p != NULL) {
+		unsigned long rl_time_interval;
+		unsigned long rdq;
+		unsigned long max_credit;
+		unsigned long use_limiter;
+		unsigned long interval_credit;
+		int cqn;
+		running = sx_proc_str_get_ulong(running, &rl_time_interval);
+		running = sx_proc_str_get_ulong(running, &rdq);
+		running = sx_proc_str_get_ulong(running, &use_limiter);
+		running = sx_proc_str_get_ulong(running, &max_credit);
+		running = sx_proc_str_get_ulong(running, &interval_credit);
+
+		PROC_DUMP("Set RDQ rate limiter: "
+				"rl_time_interval = %lu, rdq = %lu, "
+				"max_credit = %lu, use_limiter = %lu, "
+				"interval_credit = %lu\n",
+				rl_time_interval, rdq, max_credit,
+				use_limiter, interval_credit);
+		cqn = rdq + NUMBER_OF_SDQS;
+		if (!sx_bitmap_test(&sx_priv(my_dev)->cq_table.bitmap, cqn)) {
+			printk(KERN_WARNING PFX "Cannot set the rate limiter, RDQ %lu does not exist\n", rdq);
+			cmd++;
+			goto end;
+		}
+
+		if (use_limiter) {
+			sx_priv(my_dev)->cq_table.rl_time_interval = max((int)rl_time_interval, 50);
+			sx_priv(my_dev)->cq_table.cq_rl_params[cqn].interval_credit = interval_credit;
+			sx_priv(my_dev)->cq_table.cq_rl_params[cqn].max_cq_credit = max_credit;
+		}
+
+		sx_priv(my_dev)->cq_table.cq_rl_params[cqn].use_limiter = use_limiter;
+		if (!use_limiter) {
+			sx_priv(my_dev)->cq_table.cq_rl_params[cqn].curr_cq_credit = 0;
+			/* The CQ might not be armed if it ran out of credits */
+			sx_cq_arm(sx_priv(my_dev)->cq_table.cq[cqn]);
+		}
+
+
+		if (!sx_priv(my_dev)->cq_table.cq_credit_thread && use_limiter) {
+			char kth_name[20];
+
+			sprintf(kth_name,"cq_credit_thread");
+			sx_priv(my_dev)->cq_table.cq_credit_thread = kthread_create(sx_cq_credit_thread_handler,
+					(void*)sx_priv(my_dev), kth_name);
+			if (IS_ERR(sx_priv(my_dev)->cq_table.cq_credit_thread)) {
+				printk(KERN_ERR PFX "Failed creating the CQ credit thread\n");
+				return -ENOMEM;
+			} else
+				printk(KERN_INFO PFX "cq_credit_thread has been created\n");
+
+			/* start the CQ credit task */
+			wake_up_process(sx_priv(my_dev)->cq_table.cq_credit_thread);
+		}
+#if 0
+		cqn = rdq + NUMBER_OF_SDQS;
+		sx_priv(my_dev)->cq_table.rl_time_interval = max((int)rl_time_interval, 50);
+		sx_priv(my_dev)->cq_table.cq_rl_params[cqn].use_limiter = use_limiter;
+		sx_priv(my_dev)->cq_table.cq_rl_params[cqn].interval_credit = interval_credit;
+		sx_priv(my_dev)->cq_table.cq_rl_params[cqn].max_cq_credit = max_credit;
+		if (!use_limiter)
+			sx_cq_arm(sx_priv(my_dev)->cq_table.cq[cqn]);
+#endif
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "get_rdq_rl");
+	if (p != NULL) {
+		int i, max_idx = -1, cnt = 0, min_idx = -1;
+		unsigned int sum = 0;
+		unsigned int max_time_interval = 0;
+		unsigned int min_time_interval = 1000;
+		unsigned int limit = 1001;
+
+		if (arr_count < 1001)
+			limit = arr_count;
+
+		printk(KERN_INFO PFX "credit_thread_vals:");
+		for (i = 1; i < limit; i++) {
+			unsigned int interval = credit_thread_vals[i] - credit_thread_vals[i-1];
+			if (interval > 200)
+				continue;
+
+			if (i < 30)
+				printk(KERN_INFO PFX "%u ", interval);
+
+			sum += interval;
+			cnt++;
+			if (interval > max_time_interval) {
+				max_time_interval = interval;
+				max_idx = i;
+			} else if (interval < min_time_interval) {
+				min_time_interval = interval;
+				min_idx = i;
+			}
+		}
+
+		if (cnt)
+			printk(KERN_INFO PFX "sum = %u, average = %u, max_time = "
+					"%u, max_idx = %d, min_time = %u, min_idx = %d\n",
+					sum, sum/cnt, max_time_interval, max_idx,
+					min_time_interval, min_idx);
+		else
+			printk(KERN_INFO PFX "No statistics are available\n");
+
+		cmd++;
+	}
+
+	p = strstr(cmd_str, "show_cq");
+	if (p != NULL) {
+	    unsigned long cq_idx;
+	    running = sx_proc_str_get_ulong(running, &cq_idx);
+
+	    PROC_DUMP("Show CQ data: \n");
+	    __sx_proc_show_cq(my_dev,cq_idx);
+	    cmd++;
+	}
+
+    p = strstr(cmd_str, "flush_rdq");
+    if (p != NULL) {
+        unsigned long idx;
+        running = sx_proc_str_get_ulong(running, &idx);
+
+        PROC_DUMP("Flush RDQ %ld: \n",idx);
+        __sx_proc_flush_rdq(my_dev,idx);
+        cmd++;
+    }
+
+    p = strstr(cmd_str, "backup_poll");
+    if (p != NULL) {
+        unsigned long enable;
+        running = sx_proc_str_get_ulong(running, &enable);
+
+        if (enable == 1) {
+            PROC_DUMP("Enable CQ backup polling \n");
+            atomic_set(&cq_backup_polling_enabled,1);
+        } else if (enable == 0){
+            PROC_DUMP("Disable CQ backup polling \n");
+            atomic_set(&cq_backup_polling_enabled,0);
+        } else {
+            PROC_DUMP("CQ backup polling: %s\n",
+                      atomic_read(&cq_backup_polling_enabled)?"enabled":"disabled");
+        }
+        cmd++;
+    }
+
+    p = strstr(cmd_str, "debug_cq_bckp_poll_cqn");
+    if (p != NULL) {
+        unsigned long param1;
+        running = sx_proc_str_get_ulong(running, &param1);
+
+        PROC_DUMP("debug_cq_backup_poll_cqn set to %ld \n",param1);
+        debug_cq_backup_poll_cqn=param1;
+		cmd++;
+	}
+
+end:
+	if (cmd == 0) {
+		PROC_DUMP("Available Commands:\n");
+		PROC_DUMP("  add_port_netdev - Create a port netdevice");
+		PROC_DUMP("  rem_port_netdev - Destroy a port netdevice");
+		PROC_DUMP("  dump_dpt - dump DPT table\n");
+		PROC_DUMP("  dpt_setp - set DPT table entry path\n");
+		PROC_DUMP("  dpt_set_cr [dev_id] [path type] - set DPT path for cr_space\n");
+		PROC_DUMP("  cr_space_write [dev_id] [address] [value] - write dword to the cr_space\n");
+		PROC_DUMP("  cr_space_read [dev_id] [address] - read dword from the cr_space\n");
+		PROC_DUMP("  q_fw - query fw\n");
+		PROC_DUMP("  q_board_info - query board info\n");
+		PROC_DUMP("  q_aq_cap - query async queues capabilities\n");
+		PROC_DUMP("  get_profile - "
+				"runs CONFIG_PROFILE with get modifier\n");
+		PROC_DUMP("  access_reg [dev_id] [reg_id] [additional info]- runs "
+				"ACCESS_REG (get) for the register with the "
+				"given reg_id\n");
+		PROC_DUMP("  get_pci -  walk pci devices\n");
+		PROC_DUMP("  mem_wr [width 1 2 4] [address] [value] - "
+				"write memory\n");
+		PROC_DUMP("  mem_rd [width 1 2 4] [address] - read memory\n");
+		PROC_DUMP("  sw_reset - run SW reset\n");
+		PROC_DUMP("  enable_swid [swid_num] - enable the swid\n");
+		PROC_DUMP("  disable_swid [swid_num] - disable the swid\n");
+		PROC_DUMP("  change_conf [swid_num] - execute "
+				"change_configuration\n");
+		PROC_DUMP("  set_pci_prof - execute set_pci_profile\n");
+		PROC_DUMP("  dump_buf [size] [address] - dumps a buffer\n");
+		PROC_DUMP("  dump_swid - dumps active swids\n");
+		PROC_DUMP("  dump_sdq - dumps opened sdqs\n");
+		PROC_DUMP("  dump_rdq - dumps opened rdqs\n");
+		PROC_DUMP("  dump_eq - dumps opened eqs\n");
+		PROC_DUMP("  show_rdq_post <rdq> - dumps one rdq\n");
+		PROC_DUMP("  dev_prof - set dev_profile\n");
+		PROC_DUMP("  dump_synd - dump syndromes\n");
+		PROC_DUMP("  dump_stats - dump statistics\n");
+		PROC_DUMP("  trace_reg [reg_id] [max_cnt]\n");
+		PROC_DUMP("  trace_pkt [type] [max_cnt] [direction rx:1, tx:2, both:3]\n");
+		PROC_DUMP("  dump_netdev - dump netdev info\n");
+		PROC_DUMP("  dbg_trap [trap_id] [is_add] - debug trap add/rem (trap_id = 0 add/rem all traps).\n");
+		PROC_DUMP("  oper_dev - set oper state of netdev \n");
+		PROC_DUMP("  dump_dev - set oper state of netdev \n");
+		PROC_DUMP("  dump_kdb - dump kernel dbs \n");
+		PROC_DUMP("  set_rdq_rl - set RDQ rate limiter <time interval> <rdq> <use_limiter> <max_credits> <interval credit>\n");
+		PROC_DUMP("  get_rdq_rl - get RDQ rate limiter \n");
+        PROC_DUMP("  show_cq <CQ> - dump CQ \n");
+        PROC_DUMP("  flush_rdq <RDQ> - flush RDQ\n");
+        PROC_DUMP("  backup_poll <disable(0)/enable(1)/query> - Enable CQ backup polling mechanism\n");
+        PROC_DUMP("  debug_cq_bckp_poll_cqn <cqn> - trigger backup polling for CQN\n");
+
+	}
+
+out:
+	return count;
+}
+
+static const struct file_operations sx_proc_fops = {
+       .owner          = THIS_MODULE,
+       .open           = sx_proc_open,
+       .read           = seq_read,
+       .llseek         = seq_lseek,
+       .release        = single_release,
+	.write          = sx_proc_write,
+};
+
+int sx_core_init_proc_fs(void)
+{
+	struct proc_dir_entry *dir_proc_entry;
+	struct proc_dir_entry *proc_entry;
+	char *proc_dir_name = DEF_SX_CORE_PROC_DIR;
+	char *proc_file_name = DEF_SX_CORE_PROC_CORE_FILE;
+
+	sx_proc_registered = 0;
+
+	dir_proc_entry = proc_mkdir(proc_dir_name, NULL);
+	if (dir_proc_entry == NULL) {
+		printk(KERN_WARNING "create proc dir %s failed\n",
+			   proc_dir_name);
+		return -EINVAL;
+	}
+
+	proc_entry = proc_create(proc_file_name,
+                        S_IFREG | S_IRUGO | S_IWUGO , dir_proc_entry,&sx_proc_fops);
+	if (proc_entry == NULL) {
+		printk(KERN_WARNING "create proc %s failed\n",
+			   proc_file_name);
+		return -EINVAL;
+	}
+
+	/* So we can figure out which chip */
+	/* proc_entry->data = (void *) index; */
+	sx_proc_registered = 1;
+
+	printk(KERN_INFO "create proc %s successed\n", proc_file_name);
+
+	return 0;
+}
+
+void sx_core_close_proc_fs(void)
+{
+	char *proc_dir_name = DEF_SX_CORE_PROC_DIR;
+	char *proc_file_name = DEF_SX_CORE_PROC_CORE_FILE;
+	char buf[100];
+
+	if (sx_proc_registered) {
+		(void) snprintf(buf, sizeof(buf), "%s/%s",
+				proc_dir_name, proc_file_name);
+		remove_proc_entry(buf, NULL);
+		remove_proc_entry(proc_dir_name, NULL);
+		sx_proc_registered = 0;
+	}
+}
+
+void sx_proc_dbg_wr(int width, unsigned long  addr, unsigned long val)
+{
+	if (addr == 0)
+		printk(KERN_WARNING "%s() unsupported addr NULL \n", __func__);
+
+	switch (width) {
+	case 1:
+		*(u8 *)addr = (u8)val;
+		break;
+	case 2:
+		*(u16 *)addr = (u16)val;
+		break;
+	case 4:
+		*(u32 *)addr = (u32)val;
+		break;
+	case 8:
+		*(u64 *)addr = val;
+		break;
+	default:
+		printk(KERN_WARNING "%s() unsupported width %d \n",
+			   __func__, width);
+	}
+}
+
+void sx_proc_dbg_rd(int width, unsigned long addr, unsigned long *val)
+{
+	if (addr == 0)
+		printk(KERN_WARNING "%s() unsupported addr NULL \n", __func__);
+
+	switch (width) {
+	case 1:
+		*val = *(u8 *)addr;
+		break;
+	case 2:
+		*val = *(u16 *)addr;
+		break;
+	case 4:
+		*val = *(u32 *)addr;
+		break;
+	case 8:
+		*val = *(u64 *)addr;
+		break;
+	default:
+		printk(KERN_WARNING "%s() unsupported width %d \n",
+			   __func__, width);
+	}
+}
+
+
+void __sx_proc_dump_swids(struct sx_dev *dev)
+{
+	int i;
+
+	for (i = 0; i < NUMBER_OF_SWIDS; i++)
+		if (sx_bitmap_test(&sx_priv(dev)->swid_bitmap, i))
+			printk(KERN_INFO "Swid %i is ACTIVE.\n", i);
+		else
+			printk(KERN_INFO "Swid %i is DISABLED.\n", i);
+}
+
+void __sx_proc_dump_sdq(struct sx_dev *dev)
+{
+	struct sx_priv *priv = sx_priv(dev);
+	struct sx_dq_table *sdq_table = &priv->sdq_table;
+	int i;
+
+	if (sdq_table == NULL) {
+	    printk(KERN_INFO "sdq table is empty \n");
+	    return;
+	}
+
+	for (i = 0; i < dev->dev_cap.max_num_sdqs; i++) {
+		if (sdq_table->dq[i]) {
+			printk(KERN_INFO "[sdq %d]: dqn:%d, is_send:%d, "
+					"head:%d, tail:%d,"
+					" wqe_cnt:%d, wqe_shift:%d,is_flush:%d,"
+					" state:%d, ref_cnt: %d\n cqn:%d, "
+					"cq_con_idx:%d, cq_nent:%d, "
+					"cq_ref_cnt: %d \n",
+					i,
+					sdq_table->dq[i]->dqn,
+					sdq_table->dq[i]->is_send,
+					sdq_table->dq[i]->head,
+					sdq_table->dq[i]->tail,
+					sdq_table->dq[i]->wqe_cnt,
+					sdq_table->dq[i]->wqe_shift,
+				   sdq_table->dq[i]->is_flushing,
+					sdq_table->dq[i]->state,
+					atomic_read(
+						&sdq_table->dq[i]->refcount),
+
+				    sdq_table->dq[i]->cq->cqn,
+					sdq_table->dq[i]->cq->cons_index,
+					sdq_table->dq[i]->cq->nent,
+					atomic_read(
+					&sdq_table->dq[i]->cq->refcount)
+					);
+		}
+	}
+}
+
+void __sx_proc_dump_rdq(struct sx_dev *dev)
+{
+	struct sx_priv *priv = sx_priv(dev);
+	struct sx_dq_table *rdq_table = &priv->rdq_table;
+	struct sx_cq_table *cq_table = &priv->cq_table;
+	int i;
+	int cqn;
+
+	if (rdq_table == NULL) {
+        printk(KERN_INFO "rdq table is empty \n");
+        return;
+    }
+
+	if (cq_table == NULL) {
+        printk(KERN_INFO "cq table is empty \n");
+        return;
+    }
+
+	for (i = 0; i < dev->dev_cap.max_num_rdqs; i++) {
+		if (rdq_table->dq[i]) {
+			cqn = i + NUMBER_OF_SDQS;
+			printk(KERN_INFO "[rdq %d]: dqn:%d, is_send:%d, "
+					"head:%d, tail:%d, wqe_cnt:%d, "
+					"wqe_shift:%d,is_flush:%d, state:%d, "
+					"ref_cnt: %d e_sz:%d \n cqn:%d, cq_con_idx:%d, "
+					"cq_nent:%d, cq_ref_cnt: %d, use_limiter: %u,"
+					"interval_credit: %d, max_cq_credit: %d, "
+					"curr_cq_credit: %d, num_cq_stops = %d\n",
+					i,
+					rdq_table->dq[i]->dqn,
+					rdq_table->dq[i]->is_send,
+					rdq_table->dq[i]->head,
+					rdq_table->dq[i]->tail,
+					rdq_table->dq[i]->wqe_cnt,
+					rdq_table->dq[i]->wqe_shift,
+					rdq_table->dq[i]->is_flushing,
+					rdq_table->dq[i]->state,
+					atomic_read(&rdq_table->dq[i]->refcount),
+					rdq_table->dq[i]->dev->profile.rdq_properties[rdq_table->dq[i]->dqn].entry_size,
+					rdq_table->dq[i]->cq->cqn,
+					rdq_table->dq[i]->cq->cons_index,
+					rdq_table->dq[i]->cq->nent,
+					atomic_read(
+					&rdq_table->dq[i]->cq->refcount),
+					cq_table->cq_rl_params[cqn].use_limiter,
+					cq_table->cq_rl_params[cqn].interval_credit,
+					cq_table->cq_rl_params[cqn].max_cq_credit,
+					cq_table->cq_rl_params[cqn].curr_cq_credit,
+					cq_table->cq_rl_params[cqn].num_cq_stops);
+		}
+	}
+}
+
+void __sx_proc_dump_eq(struct sx_dev *dev)
+{
+    struct sx_priv *priv = sx_priv(dev);
+    struct sx_eq_table *eq_table = &priv->eq_table;
+    int i;
+
+    if (eq_table == NULL) {
+        printk(KERN_INFO "eq_table is empty \n");
+        return;
+    }
+
+    for (i = 0; i < SX_NUM_EQ; i++) {
+        printk("[eq %d]: eqn:%d, nent:%d, cons_index:%d \n",
+             i, eq_table->eq[i].eqn, eq_table->eq[i].nent,
+             eq_table->eq[i].cons_index );
+    }
+}
+
+void __sx_proc_dump_rdq_single(struct sx_dev *dev, int rdq)
+{
+	struct sx_priv *priv = sx_priv(dev);
+	struct sx_dq_table *rdq_table = &priv->rdq_table;
+	int i = rdq, j,k;
+
+	/* for (i = 0; i < dev->dev_cap.max_num_rdqs; i++) */
+	printk("Dump rdq index %d \n", rdq);
+
+	if (rdq_table == NULL) {
+        printk(KERN_INFO "rdq table is empty \n");
+        return;
+    }
+
+	if (rdq_table->dq[i]) {
+		printk("[rdq %d]: dqn:%d, is_send:%d, "
+				"head:%d, tail:%d, wqe_cnt:%d, "
+				"wqe_shift:%d,is_flush:%d, state:%d, "
+				"ref_cnt: %d e_sz:%d \n cqn:%d, cq_con_idx:%d, "
+				"cq_nent:%d, cq_ref_cnt: %d \n",
+				i,
+				rdq_table->dq[i]->dqn,
+				rdq_table->dq[i]->is_send,
+				rdq_table->dq[i]->head,
+				rdq_table->dq[i]->tail,
+				rdq_table->dq[i]->wqe_cnt,
+				rdq_table->dq[i]->wqe_shift,
+				rdq_table->dq[i]->is_flushing,
+				rdq_table->dq[i]->state,
+				atomic_read(
+					&rdq_table->dq[i]->refcount),
+				rdq_table->dq[i]->dev->profile.rdq_properties[rdq_table->dq[i]->dqn].entry_size,
+
+				rdq_table->dq[i]->cq->cqn,
+				rdq_table->dq[i]->cq->cons_index,
+				rdq_table->dq[i]->cq->nent,
+				atomic_read(
+				&rdq_table->dq[i]->cq->refcount)
+				);
+	}
+	else{
+		printk("rdq %d doesn't exist \n", i);
+	}
+
+#define DUMP_BYTES_NUM		16
+
+	for (j=0; j<rdq_table->dq[i]->wqe_cnt; j++) {
+		char* buf = rdq_table->dq[i]->sge[j].skb->data;
+
+		printk("==================\n");
+		printk("rdq %d , wqe %d , buf: %p, first %d bytes :\n",
+										i,j,buf, DUMP_BYTES_NUM);
+		for (k = 0; k < DUMP_BYTES_NUM; k++) {
+			if (k == 0 || (k%4 == 0))
+				printk("\n0x%04x : ", k);
+			printk(" 0x%02x", buf[k]);
+		}
+		printk("\n");
+	}
+}
+
+/* Current values are for IB single swid with AR enabled */
+void __sx_proc_set_dev_profile(struct sx_dev *dev)
+{
+	struct ku_profile single_part_eth_device_profile = {
+	.dev_id		   = 255,
+	.set_mask_0_63     = 0xf3ff,
+	.set_mask_64_127   = 0,
+	.max_vepa_channels = 0,
+	.max_lag           = 0,
+	.max_port_per_lag  = 0,
+	.max_mid           = 0,
+	.max_pgt           = 0,
+	.max_system_port   = 0,
+	.max_active_vlans  = 0,
+	.max_regions       = 0,
+	.max_flood_tables  = 0,
+	.max_per_vid_flood_tables = 0,
+	.flood_mode        = 0,
+	.max_fid_offset_flood_tables = 0,
+	.fid_offset_table_size = 0,
+	.max_per_fid_flood_table = 0,
+	.per_fid_table_size = 0,
+	.max_ib_mc         = 27,
+	.max_pkey          = 126,
+	.ar_sec		   = 1,
+	.adaptive_routing_group_cap = 2048,
+	.arn		   = 0,
+	.kvd_linear_size = 0,
+    .kvd_hash_single_size = 0,
+    .kvd_hash_double_size = 0,
+	.swid0_config_type = {
+			.mask = 1,
+			.type = KU_SWID_TYPE_INFINIBAND
+	},
+	.swid1_config_type = {
+			.mask = 1,
+			.type = KU_SWID_TYPE_DISABLED
+	},
+	.swid2_config_type = {
+			.mask = 1,
+			.type = KU_SWID_TYPE_DISABLED
+	},
+	.swid3_config_type = {
+			.mask = 1,
+			.type = KU_SWID_TYPE_DISABLED
+	},
+	.swid4_config_type = {
+			.mask = 1,
+			.type = KU_SWID_TYPE_DISABLED
+	},
+	.swid5_config_type = {
+			.mask = 1,
+			.type = KU_SWID_TYPE_DISABLED
+	},
+	.swid6_config_type = {
+			.mask = 1,
+			.type = KU_SWID_TYPE_DISABLED
+	},
+	.swid7_config_type = {
+			.mask = 1,
+			.type = KU_SWID_TYPE_DISABLED
+	}
+	};
+
+	sx_SET_PROFILE(dev, &single_part_eth_device_profile);
+}
+
+
+void __dump_stats(struct sx_dev* sx_dev)
+{
+	int swid, pkt_type, synd;
+
+	for (swid=0; swid<NUMBER_OF_SWIDS+1; swid++) {
+			printk("=========================\n");
+
+			if (NUMBER_OF_SWIDS == swid) {
+				printk("Packets stats for swid 254 \n");
+			}
+			else{
+				printk("Packets stats for swid %d \n",swid);
+			}
+
+			for (pkt_type=0; pkt_type<PKT_TYPE_NUM; pkt_type++) {
+				if (0 != sx_dev->stats.rx_by_pkt_type[swid][pkt_type]) {
+					printk("rx pkt of type [%s (%d)]: %llu \n",
+								sx_cqe_packet_type_str[pkt_type], pkt_type,
+								sx_dev->stats.rx_by_pkt_type[swid][pkt_type]);
+				}
+
+				if (0 != sx_dev->stats.tx_by_pkt_type[swid][pkt_type]) {
+					printk("tx pkt of type [%s (%d)]: %llu \n",
+						   ku_pkt_type_str[pkt_type], pkt_type,
+						   sx_dev->stats.tx_by_pkt_type[swid][pkt_type]);
+				}
+			} /* for (pkt_type=0; pkt_type<PKT_TYPE_NUM; pkt_type++) { */
+
+			for (synd=0; synd<NUM_HW_SYNDROMES; synd++) {
+				if (0 != sx_dev->stats.rx_by_synd[swid][synd]) {
+					printk("rx pkt on synd [%d]: %llu \n", synd, sx_dev->stats.rx_by_synd[swid][synd]);
+				}
+			} /* for (synd=0; synd<PKT_TYPE_NUM; synd++) { */
+
+	}
+
+	printk("=========================\n");
+	for (synd = 0; synd < NUM_HW_SYNDROMES; synd++) {
+		for (pkt_type=0; pkt_type<PKT_TYPE_NUM; pkt_type++) {
+			if (sx_dev->stats.rx_unconsumed_by_synd[synd][pkt_type] != 0) {
+				printk("rx unconsumed on synd [%d] type [%s]: %llu \n",
+						synd, sx_cqe_packet_type_str[pkt_type],
+						sx_dev->stats.rx_unconsumed_by_synd[synd][pkt_type]);
+			}
+		}
+	}
+	//rx_eventlist_drops_by_synd
+	printk("=========================\n");
+	for (synd = 0; synd < NUM_HW_SYNDROMES; synd++) {
+		if (sx_dev->stats.rx_eventlist_drops_by_synd[synd] != 0) {
+			printk("rx eventlist drops on synd [%d]: %llu \n",
+					synd, sx_dev->stats.rx_eventlist_drops_by_synd[synd]);
+		}
+	}
+
+	printk("=========================\n");
+	printk("eventlist_drops_counter: %llu \n"
+				"unconsumed_packets_counter: %llu \n"
+				"filtered_lag_packets_counter: %llu \n"
+				"filtered_port_packets_counter: %llu \n"
+				"loopback_packets_counter: %llu \n",
+					sx_dev->eventlist_drops_counter,
+					sx_dev->unconsumed_packets_counter,
+					sx_dev->filtered_lag_packets_counter,
+					sx_dev->filtered_port_packets_counter,
+					sx_dev->loopback_packets_counter);
+}
+
+void __sx_proc_dump_reg(struct sx_dev *my_dev, unsigned long reg_id, unsigned long max_cnt)
+{
+	if (0 != max_cnt) {
+		i2c_cmd_dump = SX_DBG_ENABLE;
+		i2c_cmd_op = 0x40;
+		i2c_cmd_reg_id = reg_id;
+		i2c_cmd_dump_cnt = max_cnt;
+	}
+	else{
+		i2c_cmd_dump = SX_DBG_DISABLE;
+		i2c_cmd_op = SX_DBG_CMD_OP_TYPE_ANY;
+		i2c_cmd_reg_id = SX_DBG_REG_TYPE_ANY;
+		i2c_cmd_dump_cnt = SX_DBG_COUNT_UNLIMITED;
+	}
+}
+
+void __sx_proc_dump_emad(struct sx_dev *my_dev, unsigned long emad_type, unsigned long max_cnt, unsigned long direction)
+{
+	if (SX_PROC_DUMP_PKT_DISABLE == direction ||
+		0 == max_cnt){
+		rx_debug = SX_DBG_DISABLE;
+		rx_debug_pkt_type = SX_DBG_PACKET_TYPE_ANY;
+		rx_debug_emad_type = SX_DBG_EMAD_TYPE_ANY;
+		rx_dump = SX_DBG_DISABLE;
+		rx_dump_cnt = SX_DBG_COUNT_UNLIMITED;
+
+		tx_debug = SX_DBG_DISABLE;
+		tx_debug_pkt_type = SX_DBG_PACKET_TYPE_ANY;
+		tx_debug_emad_type = SX_DBG_EMAD_TYPE_ANY;
+		tx_dump = SX_DBG_DISABLE;
+		tx_dump_cnt = SX_DBG_COUNT_UNLIMITED;
+		return;
+	}
+
+	if (direction & SX_PROC_DUMP_PKT_RX) {
+		rx_debug = SX_DBG_ENABLE;
+		rx_debug_pkt_type = 5; /* emad trap */
+		rx_debug_emad_type = emad_type;
+		rx_dump = SX_DBG_ENABLE;
+		rx_dump_cnt = max_cnt;
+	}
+
+	if (direction & SX_PROC_DUMP_PKT_TX) {
+		tx_debug = SX_DBG_ENABLE;
+		tx_debug_pkt_type = SX_PKT_TYPE_EMAD_CTL;
+		tx_debug_emad_type = emad_type;
+		tx_dump = SX_DBG_ENABLE;
+		tx_dump_cnt = max_cnt;
+	}
+}
+
+
+void __debug_dump_netdev(struct sx_dev *dev)
+{
+	union sx_event_data event_data;
+	sx_core_dispatch_event(dev, SX_DEV_EVENT_DEBUG_NETDEV, &event_data);
+}
+
+void __debug_netdev_traps(struct sx_dev *dev, unsigned long trap_id, unsigned long is_add)
+{
+	int cmd;
+	union sx_event_data event_data;
+
+	if (is_add == 1) {
+		cmd = SX_DEV_EVENT_ADD_SYND_NETDEV;
+	}
+	else{
+		cmd = SX_DEV_EVENT_REMOVE_SYND_NETDEV;
+	}
+
+	event_data.eth_l3_synd.swid = 0;
+	event_data.eth_l3_synd.hw_synd = trap_id;
+
+	sx_core_dispatch_event(dev, cmd, &event_data);
+}
+
+
+void __set_net_dev_oper_state(char	*dev_name ,	char *oper_state)
+{
+	struct net_device *dev;
+
+	dev = first_net_device(&init_net);
+	while (dev) {
+		/* printk(KERN_DEBUG "found [%s]\n", dev->name); */
+
+		if (dev_name != NULL &&
+			(!strcmp(dev->name, dev_name)) ){
+			printk(KERN_INFO "Device [%s] set oper state to %s \n",
+				   dev->name, oper_state);
+
+			if (strstr(oper_state, "up")) {
+				netif_dormant_off(dev);
+				netif_carrier_on(dev);
+				netif_start_queue(dev);
+			}
+			else if (strstr(oper_state, "down")) {
+				netif_dormant_on(dev);
+				netif_carrier_off(dev);
+				netif_stop_queue(dev);
+			}
+			else{
+				printk("Unsupported oper_state: %s \n", oper_state);
+			}
+		}
+
+		dev = next_net_device(dev);
+	}
+}
+
+void __dump_net_dev_oper_state(void)
+{
+	struct net_device *dev;
+
+	dev = first_net_device(&init_net);
+	while (dev) {
+		printk(KERN_DEBUG "Found dev [%s]: dormant:%d, carrier:%d  \n",
+				   dev->name, netif_dormant(dev), netif_carrier_ok(dev));
+
+		dev = next_net_device(dev);
+	}
+}
+
+void __dump_kdbs(void)
+{
+    struct sx_dev *my_dev = sx_glb.
+                sx_dpt.dpt_info[DEFAULT_DEVICE_ID].
+                sx_pcie_info.sx_dev;
+    struct sx_priv *priv = sx_priv(my_dev);
+    int i,j,is_valid = 0, is_printed=0;
+
+    /* IB only */
+    printk("============================\n");
+    printk("IB to LOCAL :\n");
+    for (i=0; i<MAX_IBPORT_NUM + 1; i++){
+        if (priv->ib_to_local_db[i] != 0){
+            printk("[ib%d : local %d] \n",
+                   i, priv->ib_to_local_db[i]);
+        }
+    }
+
+    /* ETH only */
+    printk("============================\n");
+    printk("ETH SYS to LOCAL :\n");
+    for (i=0; i<MAX_SYSPORT_NUM; i++){
+        if (priv->system_to_local_db[i] != 0){
+            printk("[system 0x%x : local %d : pvid %d] \n",
+                   i, priv->system_to_local_db[i], priv->pvid_sysport_db[i]);
+        }
+    }
+
+    printk("============================\n");
+    printk("ETH LAG MEMBER to LOCAL :\n");
+    for (i=0; i<MAX_LAG_NUM; i++){  /* lag */
+        for (j=0; j<MAX_LAG_MEMBERS_NUM; j++){  /* lag member id */
+            if (priv->lag_member_to_local_db[i][j] != 0){
+                if (is_printed == 0){
+                    printk("LAG 0x%x: ", i);
+                    is_printed = 1;
+                }
+
+                printk("[%d:local %d], ",
+                       j, priv->lag_member_to_local_db[i][j]);
+            }
+        }
+
+        if (is_printed == 1)
+            printk("\n");
+
+        is_printed = 0;
+    }
+
+    printk("============================\n");
+    printk("ETH SYSTEM_PORT_RP :\n");
+    for (i=0; i<MAX_PHYPORT_NUM; i++){  /* system port */
+        if (priv->local_is_rp[i] != 0){
+            printk("(local %d), ",i);
+        }
+    }
+    printk("\n");
+
+    printk("============================\n");
+    printk("ETH LAG_RP :\n");
+    for (i=0; i<MAX_LAG_NUM; i++){  /* system port */
+        if (priv->lag_is_rp[i] != 0){
+            printk("%d, ",i);
+        }
+    }
+    printk("\n");
+
+    /* common */
+    printk("============================\n");
+    printk("COMMON LOCAL TO SWID :\n");
+    printk("LOCAL\t| SWID\t|\n");
+    printk("------------------------------------------\n");
+    for (i=0; i<(MAX_PHYPORT_NUM+1); i++){  /* system port */
+        printk("%02d \t| %d\t|  \n",
+               i, priv->local_to_swid_db[i]);
+    }
+
+#if 0
+    printk("============================\n");
+    printk("COMMON SYSPORT TO PVID:\n");
+    for (i=0; i<(MAX_SYSPORT_NUM); i++){  /* system port */
+        printk("sysport 0x%x pvid %d\n",
+               i, priv->pvid_sysport_db[i]);
+    }
+
+    printk("============================\n");
+    printk("COMMON LAG TO PVID:\n");
+    for (i=0; i<(MAX_LAG_NUM); i++){  /* system port */
+        printk("LAG %d pvid %d\n",
+               i, priv->pvid_lag_db[i]);
+    }
+#endif
+
+#if 0
+    printk("============================\n");
+    printk("COMMON TRUNCATE_SZ:\n");
+    for (i=0; i<(NUMBER_OF_RDQS); i++){  /* system port */
+        printk("RDQ%d  tr_sz:%d\n",
+               i, priv->truncate_size_db[i]);
+    }
+
+    printk("============================\n");
+    printk("SYSPORT FILTER DB:\n");
+    for (i=0; i<(NUM_HW_SYNDROMES); i++){  /* system port */
+        for (j=0; j<(MAX_SYSTEM_PORTS_IN_FILTER); j++){  /* system port */
+            if (priv->sysport_filter_db[i][j] != 0){
+                printk("HW_SYND 0x%x SYSPORT 0x%x filter: %d \n",
+                               i, j, priv->sysport_filter_db[i][j]);
+            }
+        }
+    }
+
+    printk("============================\n");
+    printk("LAG FILTER DB:\n");
+    for (i=0; i<(NUM_HW_SYNDROMES); i++){  /* system port */
+        for (j=0; j<(MAX_LAG_PORTS_IN_FILTER); j++){  /* system port */
+            if (priv->lag_filter_db[i][j] != 0){
+                printk("HW_SYND 0x%x LAG_ID %d filter: %d \n",
+                       i, j, priv->lag_filter_db[i][j]);
+            }
+        }
+    }
+#endif
+    printk("============================\n");
+    printk("port_prio2tc:\n");
+    for (i=0; i<(MAX_PHYPORT_NUM + 1); i++){  /* system port */
+        for (j=0; j<(MAX_PRIO_NUM + 1); j++){  /* system port */
+           is_valid += priv->port_prio2tc[i][j];
+        }
+
+        if (is_valid){
+            printk("sysport 0x%x : ", i);
+            for (j=0; j<(MAX_PRIO_NUM + 1); j++){  /* system port */
+                 printk("p%d:tc%d, ",
+                      j, priv->port_prio2tc[i][j]);
+            }
+            printk("\n");
+        }
+        is_valid = 0;
+    }
+
+    printk("============================\n");
+    printk("lag_prio2tc:\n");
+    for (i=0; i<(MAX_LAG_NUM + 1); i++){  /* system port */
+        for (j=0; j<(MAX_PRIO_NUM + 1); j++){  /* system port */
+           is_valid += priv->lag_prio2tc[i][j];
+        }
+
+        if (is_valid){
+            printk("lag_id %d : ", i);
+            for (j=0; j<(MAX_PRIO_NUM + 1); j++){  /* system port */
+                 printk("p%d:tc%d, ",
+                      j, priv->port_prio2tc[i][j]);
+            }
+            printk("\n");
+        }
+        is_valid = 0;
+    }
+
+    printk("============================\n");
+    printk("port_vtag_mode:\n");
+    for (i=0; i<(MAX_PHYPORT_NUM + 1); i++){  /* system port */
+        for (j=0; j<(MAX_VLAN_NUM); j++){  /* system port */
+           is_valid += priv->port_vtag_mode[i][j];
+        }
+
+        if (is_valid){
+            printk("sys_port 0x%x tagged on vlans: ", i);
+            for (j=0; j<(MAX_VLAN_NUM); j++){  /* system port */
+                if (priv->port_vtag_mode[i][j])
+                    printk("%d, ",j);
+            }
+            printk("\n");
+        }
+        is_valid = 0;
+    }
+
+    printk("============================\n");
+    printk("lag_vtag_mode:\n");
+    for (i=0; i<(MAX_LAG_NUM + 1); i++){  /* system port */
+        for (j=0; j<(MAX_VLAN_NUM); j++){  /* system port */
+           is_valid += priv->lag_vtag_mode[i][j];
+        }
+
+        if (is_valid){
+            printk("lag 0x%x tagged on vlans: ", i);
+            for (j=0; j<(MAX_VLAN_NUM); j++){  /* system port */
+                if (priv->lag_vtag_mode[i][j])
+                    printk("%d, ",j);
+            }
+            printk("\n");
+        }
+        is_valid = 0;
+    }
+
+    printk("============================\n");
+    printk("prio_tag_sysports:\n");
+    for (i=0; i<(MAX_PHYPORT_NUM + 1); i++){  /* system port */
+        if (priv->port_prio_tagging_mode[i])
+            printk("0x%x, ", i);
+    }
+    printk("\n");
+
+
+    printk("============================\n");
+    printk("prio_tag_lags:\n");
+    for (i=0; i<(MAX_LAG_NUM + 1); i++){  /* system port */
+        if (priv->lag_prio_tagging_mode[i])
+            printk("lag%d, ", i);
+    }
+    printk("\n");
+
+    printk("============================\n");
+    printk("vid2ip:\n");
+    for (i=0; i<(MAX_VLAN_NUM - 1); i++){  /* vlan */
+        if (priv->icmp_vlan2ip_db[i])
+            printk("vlan%d: 0x%x, ", i, priv->icmp_vlan2ip_db[i]);
+    }
+    printk("\n");
+
+    printk("============================\n");
+    printk("port_rp_rif:\n");
+    printk("LOCAL\t| VLAN\t| RIF\t|\n");
+    printk("-------------------------\n");
+    for (i=0; i<(MAX_PHYPORT_NUM + 1); i++) {
+        for (j=0; j<MAX_VLAN_NUM; j++) {
+            if (priv->port_rp_rif_valid[i][j]) {
+                printk("%u\t| %u\t| %u\t|\n", i, j, priv->port_rp_rif[i][j]);
+            }
+        }
+    }
+
+    printk("============================\n");
+    printk("lag_rp_rif:\n");
+    printk("LID\t| VLAN\t| RIF\t|\n");
+    printk("-------------------------\n");
+    for (i=0; i<MAX_LAG_NUM; i++) {
+        for (j=0; j<MAX_VLAN_NUM; j++) {
+            if (priv->lag_rp_rif_valid[i][j]) {
+                printk("%u\t| %u\t| %u\t|\n", i, j, priv->lag_rp_rif[i][j]);
+            }
+        }
+    }
+    
+    printk("============================\n");
+    printk("port_vid_to_fid:\n");
+    printk("PORT\t| VLAN\t| FID\t|\n");
+    printk("-------------------------\n");
+    for (i=0; i<MAX_PHYPORT_NUM; i++) {
+        for (j=0; j<MAX_VLAN_NUM; j++) {
+            if (priv->port_vid_to_fid[i][j]) {
+                printk("%u\t| %u\t| %u\t|\n", i, j, priv->port_vid_to_fid[i][j]);
+            }
+        }
+    }    
+}
+
+
+void __sx_proc_show_cq(struct sx_dev *dev, int cqn)
+{
+    sx_cq_show_cq(dev,cqn);
+}
+
+void __sx_proc_flush_rdq(struct sx_dev *my_dev, int idx)
+{
+    sx_cq_flush_rdq(my_dev,idx);
+}
diff --git a/linux/drivers/hwmon/mellanox/sx_proc.h b/linux/drivers/hwmon/mellanox/sx_proc.h
new file mode 100644
index 0000000..de822e7
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/sx_proc.h
@@ -0,0 +1,43 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+#ifndef __SX_PROC_H__
+#define __SX_PROC_H__
+
+int 	sx_core_init_proc_fs(void);
+void 	sx_core_close_proc_fs(void);
+
+void sx_dpt_dump(void);
+void sx_dpt_set_cmd_dbg(int dev_id, int path);
+void sx_proc_dbg_wr(int width, unsigned long addr, unsigned long val);
+void sx_proc_dbg_rd(int width, unsigned long addr, unsigned long *val);
+void __sx_proc_dump_swids(struct sx_dev *dev);
+void __sx_proc_dump_sdq(struct sx_dev *dev);
+void __sx_proc_dump_rdq(struct sx_dev *dev);
+void __sx_proc_set_dev_profile(struct sx_dev *dev);
+void __dump_stats(struct sx_dev* sx_dev);
+
+#define SX_DBG_PACKET_TYPE_ANY		 	0xFFFF
+#define SX_DBG_EMAD_TYPE_ANY		 	 0xFFFF
+#define SX_DBG_CMD_OP_TYPE_ANY		   0xFFFF
+#define SX_DBG_REG_TYPE_ANY				    0xFFFF
+
+#define SX_DBG_COUNT_UNLIMITED			0xFFFF
+
+#define SX_PROC_DUMP_PKT_DISABLE		0
+#define SX_PROC_DUMP_PKT_RX					 1
+#define SX_PROC_DUMP_PKT_TX					 2
+
+#define SX_DBG_DISABLE								0
+#define SX_DBG_ENABLE								 1
+
+#endif
diff --git a/linux/drivers/hwmon/mellanox/sx_sgmii.h b/linux/drivers/hwmon/mellanox/sx_sgmii.h
new file mode 100644
index 0000000..cb251d8
--- /dev/null
+++ b/linux/drivers/hwmon/mellanox/sx_sgmii.h
@@ -0,0 +1,87 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+#ifndef SX_SGMII_H
+#define SX_SGMII_H
+
+/************************************************
+ * Includes
+ ***********************************************/
+
+#include <linux/types.h>
+
+/************************************************
+ *  Enums
+ ***********************************************/
+
+enum sx_sgmii_pkt_type_t {
+	SX_SGMII_PKT_TYPE_IB_RAW	= 0x00,
+	SX_SGMII_PKT_TYPE_IB_TRANSPORT	= 0x01,
+	SX_SGMII_PKT_TYPE_ETH		= 0x02
+};
+
+enum sgmii_oob_ctrl_hdr_lp_t {
+	SX_SGMII_OOB_PKT_CTRL_HDR_LP_FWD_TO_SWITCH 	= 0,
+	SX_SGMII_OOB_PKT_CTRL_HDR_LP_CR_SPACE 		= 2
+};
+
+/************************************************
+ * Structures
+ ***********************************************/
+struct sx_sgmii_ifc {
+	u8		initialized;
+	u64		base_smac;
+	u8		number_of_macs; /* number of MACs is the actual number of flows */
+	int		(*init)(void);
+	void	(*deinit)(void);
+	int		(*send)(struct sk_buff *skb);
+	int		(*build_encapsulation_header)(struct sk_buff *skb, u64 dmac,
+			u64 smac, u16 ethertype, u8 proto, u8 ver);
+	int		(*build_ctrl_segment)(struct sk_buff *skb, u8 lp, u8 sdqn, 
+			enum sx_sgmii_pkt_type_t pkt_type);
+	u32		(*cr_space_readl)(int dev_id, u32 address, int *err);
+	int		(*cr_space_writel)(int dev_id, u32 address, u32 value);
+	int		(*cr_space_read_buf)(int dev_id, u32 address,
+			unsigned char *buf, int size);
+	int		(*cr_space_write_buf)(int dev_id, u32 address,
+			unsigned char *buf, int size);
+};
+
+
+/************************************************
+ * Functions
+ ***********************************************/
+#ifdef CONFIG_SX_SGMII_PRESENT
+void sx_sgmii_init_cb(struct sx_sgmii_ifc *sgmii_ifc);
+#else
+static inline void sx_sgmii_init_cb(struct sx_sgmii_ifc *sgmii_ifc)
+{
+	sgmii_ifc->init = NULL;
+	sgmii_ifc->deinit = NULL;
+	sgmii_ifc->send = NULL;
+	sgmii_ifc->build_encapsulation_header = NULL;
+	sgmii_ifc->build_ctrl_segment = NULL;
+	sgmii_ifc->cr_space_readl = NULL;
+	sgmii_ifc->cr_space_writel = NULL;
+	sgmii_ifc->cr_space_read_buf = NULL;
+	sgmii_ifc->cr_space_write_buf = NULL;
+}
+#endif
+
+int sx_sgmii_cr_space_write_buf(int dev_id, u32 address, unsigned char *buf, int size);
+int sx_sgmii_cr_space_read_buf(int dev_id, u32 address, unsigned char *buf, int size);
+
+int sx_sgmii_build_cr_space_header_switchx(struct sk_buff *skb, u8 token, u8 rw, u32 address, u8 size);
+int sx_sgmii_build_cr_space_header_spectrum(struct sk_buff *skb, u8 token, u8 rw, u32 address, u8 size);
+
+#endif /* SX_SGMII_H */
diff --git a/linux/include/linux/mlx_sx/cmd.h b/linux/include/linux/mlx_sx/cmd.h
new file mode 100644
index 0000000..a511f0b
--- /dev/null
+++ b/linux/include/linux/mlx_sx/cmd.h
@@ -0,0 +1,221 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+#ifndef SX_CMD_H_
+#define SX_CMD_H_
+
+/************************************************
+ * Includes
+ ***********************************************/
+
+#include <linux/dma-mapping.h>
+
+#include <linux/mlx_sx/device.h>
+
+enum {
+	/* initialization and teardown commands */
+	SX_CMD_MAP_FA		= 0xfff,
+	SX_CMD_UNMAP_FA		= 0xffe,
+	SX_CMD_QUERY_FW		= 0x4,
+	SX_CMD_QUERY_BOARDINFO	= 0x6,
+	SX_CMD_QUERY_AQ_CAP	= 0x3,
+	SX_CMD_CONFIG_PROFILE	= 0x100,
+
+	/* General commands */
+	SX_CMD_ACCESS_REG	= 0x40,
+
+	/* port commands */
+	SX_CMD_CONF_PORT	= 0xc,
+	SX_CMD_INIT_PORT	= 0x9,
+	SX_CMD_CLOSE_PORT	= 0xa,
+
+	/* DQ commands */
+	SX_CMD_SW2HW_DQ		= 0x201,
+	SX_CMD_HW2SW_DQ		= 0x202,
+	SX_CMD_2ERR_DQ		= 0x1e,
+	SX_CMD_QUERY_DQ		= 0x22,
+
+	/* CQ commands */
+	SX_CMD_SW2HW_CQ		= 0x16,
+	SX_CMD_HW2SW_CQ		= 0x17,
+	SX_CMD_QUERY_CQ		= 0x18,
+
+	/* EQ commands */
+	SX_CMD_SW2HW_EQ		= 0x13,
+	SX_CMD_HW2SW_EQ		= 0x14,
+	SX_CMD_QUERY_EQ		= 0x15,
+
+	/* Infiniband commands */
+	SX_CMD_INIT_MAD_DEMUX	= 0x203,
+	SX_CMD_MAD_IFC		= 0x24,
+};
+#ifndef INCREASED_TIMEOUT
+enum {
+	SX_CMD_TIME_CLASS_A	= 10000,
+	SX_CMD_TIME_CLASS_B	= 10000,
+	SX_CMD_TIME_CLASS_C	= 10000,
+};
+#else
+enum {
+	SX_CMD_TIME_CLASS_A	= 10000,
+	SX_CMD_TIME_CLASS_B	= 10000,
+	SX_CMD_TIME_CLASS_C	= 10000,
+};
+#endif
+enum {
+	SX_MAILBOX_SIZE	=  4096
+};
+
+struct sx_dev;
+
+
+/************************************************
+ * Structs
+ ***********************************************/
+
+struct sx_cmd_mailbox {
+	void		*buf;
+	dma_addr_t	dma;
+	u64		imm_data;
+};
+
+struct sx_board {
+	u16  vsd_vendor_id;
+	char board_id[SX_BOARD_ID_LEN];
+	u8   inta_pin;
+};
+
+typedef void generic_reg_data;
+typedef int (*sx_ACCESS_REG_generic)(struct sx_dev *dev, generic_reg_data *reg_data);
+
+/************************************************
+ * Functions
+ ***********************************************/
+int __sx_cmd(struct sx_dev *dev, int sx_dev_id,
+		struct sx_cmd_mailbox *in_param,
+		struct sx_cmd_mailbox *out_param,
+		int out_is_imm, u32 in_modifier, u8 op_modifier,
+		u16 op, unsigned long timeout, int in_mb_size);
+
+/* Invoke a command with no output parameter */
+static inline int sx_cmd(struct sx_dev *dev, int sx_dev_id,
+		struct sx_cmd_mailbox *in_param,
+		u32 in_modifier, u8 op_modifier, u16 op,
+		unsigned long timeout, int in_mb_size)
+{
+	return __sx_cmd(dev, sx_dev_id, in_param, NULL, 0, in_modifier,
+			  op_modifier, op, timeout, in_mb_size);
+}
+
+/* Invoke a command with an output mailbox */
+static inline int sx_cmd_box(struct sx_dev *dev, int sx_dev_id,
+		struct sx_cmd_mailbox *in_param,
+		struct sx_cmd_mailbox *out_param, u32 in_modifier,
+		u8 op_modifier, u16 op, unsigned long timeout,
+		int in_mb_size)
+{
+	return __sx_cmd(dev, sx_dev_id, in_param, out_param, 0, in_modifier,
+			  op_modifier, op, timeout, in_mb_size);
+}
+
+/*
+ * Invoke a command with an immediate output parameter (and copy the
+ * output into the caller's out_param pointer after the command
+ * executes).
+ */
+static inline int sx_cmd_imm(struct sx_dev *dev,
+		int sx_dev_id,
+		struct sx_cmd_mailbox *in_param,
+		struct sx_cmd_mailbox *out_param, u32 in_modifier,
+		u8 op_modifier, u16 op, unsigned long timeout, int in_mb_size)
+{
+	return __sx_cmd(dev, sx_dev_id,  in_param, out_param, 1,
+			in_modifier, op_modifier, op, timeout, in_mb_size);
+}
+
+void sx_cmd_set_op_tlv(struct ku_operation_tlv *op_tlv, u32 reg_id, u8 method);
+struct sx_cmd_mailbox *sx_alloc_cmd_mailbox(struct sx_dev *dev, int sx_dev_id);
+void sx_free_cmd_mailbox(struct sx_dev *dev, struct sx_cmd_mailbox *mailbox);
+void sx_cmd_event(struct sx_dev *dev, u16 token, u8 status, u64 out_param);
+int sx_cmd_use_events(struct sx_dev *dev);
+void sx_cmd_use_polling(struct sx_dev *dev);
+
+int sx_ACCESS_REG_SPZR(struct sx_dev *dev, struct ku_access_spzr_reg *reg_data);
+int sx_GET_PROFILE(struct sx_dev *dev, struct ku_profile *profile);
+int sx_QUERY_FW(struct sx_dev *dev, struct ku_query_fw* query_fw);
+int sx_QUERY_AQ_CAP(struct sx_dev *dev);
+int sx_QUERY_BOARDINFO(struct sx_dev *dev, struct sx_board *adapter);
+int sx_ACCESS_REG_MGIR(struct sx_dev *dev, struct ku_access_mgir_reg *reg_data);
+int sx_ACCESS_REG_PLIB(struct sx_dev *dev, struct ku_access_plib_reg *reg_data);
+int sx_ACCESS_REG_PMLP(struct sx_dev *dev, struct ku_access_pmlp_reg *reg_data);
+int sx_ACCESS_REG_PTYS(struct sx_dev *dev, struct ku_access_ptys_reg *reg_data);
+int sx_ACCESS_REG_QSTCT(struct sx_dev *dev, struct ku_access_qstct_reg *reg_data);
+int sx_ACCESS_REG_QSPTC(struct sx_dev *dev, struct ku_access_qsptc_reg *reg_data);
+int sx_ACCESS_REG_PSPA(struct sx_dev *dev, struct ku_access_pspa_reg *reg_data);
+int sx_ACCESS_REG_PAOS(struct sx_dev *dev, struct ku_access_paos_reg *reg_data);
+int sx_ACCESS_REG_PPLM(struct sx_dev *dev, struct ku_access_pplm_reg *reg_data);
+int sx_ACCESS_REG_PLPC(struct sx_dev *dev, struct ku_access_plpc_reg *reg_data);
+int sx_ACCESS_REG_PMPC(struct sx_dev *dev, struct ku_access_pmpc_reg *reg_data);
+int sx_ACCESS_REG_MJTAG(struct sx_dev *dev, struct ku_access_mjtag_reg *reg_data);
+int sx_ACCESS_REG_PMPR(struct sx_dev *dev, struct ku_access_pmpr_reg *reg_data);
+int sx_ACCESS_REG_PMTU(struct sx_dev *dev, struct ku_access_pmtu_reg *reg_data);
+int sx_ACCESS_REG_PELC(struct sx_dev *dev, struct ku_access_pelc_reg *reg_data);
+int sx_ACCESS_REG_HTGT(struct sx_dev *dev, struct ku_access_htgt_reg *reg_data);
+int sx_ACCESS_REG_MFSC(struct sx_dev *dev, struct ku_access_mfsc_reg *reg_data);
+int sx_ACCESS_REG_MFSM(struct sx_dev *dev, struct ku_access_mfsm_reg *reg_data);
+int sx_ACCESS_REG_MFSL(struct sx_dev *dev, struct ku_access_mfsl_reg *reg_data);
+int sx_ACCESS_REG_PVLC(struct sx_dev *dev, struct ku_access_pvlc_reg *reg_data);
+int sx_ACCESS_REG_MCIA(struct sx_dev *dev, struct ku_access_mcia_reg *reg_data);
+int sx_ACCESS_REG_HPKT(struct sx_dev *dev, struct ku_access_hpkt_reg *reg_data);
+int sx_ACCESS_REG_HCAP(struct sx_dev *dev, struct ku_access_hcap_reg *reg_data);
+int sx_ACCESS_REG_HDRT(struct sx_dev *dev, struct ku_access_hdrt_reg *reg_data);
+int sx_ACCESS_REG_QPRT(struct sx_dev *dev, struct ku_access_qprt_reg *reg_data);
+int sx_ACCESS_REG_MFCR(struct sx_dev *dev, struct ku_access_mfcr_reg *reg_data);
+int sx_ACCESS_REG_FORE(struct sx_dev *dev, struct ku_access_fore_reg *reg_data);
+int sx_ACCESS_REG_MTCAP(struct sx_dev *dev, struct ku_access_mtcap_reg *reg_data);
+int sx_ACCESS_REG_MTMP(struct sx_dev *dev, struct ku_access_mtmp_reg *reg_data);
+int sx_ACCESS_REG_MTWE(struct sx_dev *dev, struct ku_access_mtwe_reg *reg_data);
+int sx_ACCESS_REG_MMDIO(struct sx_dev *dev, struct ku_access_mmdio_reg *reg_data);
+int sx_ACCESS_REG_MMIA(struct sx_dev *dev, struct ku_access_mmia_reg *reg_data);
+int sx_ACCESS_REG_MFPA(struct sx_dev *dev, struct ku_access_mfpa_reg *reg_data);
+int sx_ACCESS_REG_MFBE(struct sx_dev *dev, struct ku_access_mfbe_reg *reg_data);
+int sx_ACCESS_REG_MFBA(struct sx_dev *dev, struct ku_access_mfba_reg *reg_data);
+int sx_ACCESS_REG_QCAP(struct sx_dev *dev, struct ku_access_qcap_reg *reg_data);
+int sx_ACCESS_REG_RAW(struct sx_dev *dev, struct ku_access_raw_reg *reg_data);
+int sx_ACCESS_REG_RAW_BUFF(struct sx_dev *dev, struct ku_access_reg_raw_buff *raw_data);
+int sx_ACCESS_REG_PMAOS(struct sx_dev *dev, struct ku_access_pmaos_reg *reg_data);
+int sx_ACCESS_REG_MFM(struct sx_dev *dev, struct ku_access_mfm_reg *reg_data);
+int sx_ACCESS_REG_SPAD(struct sx_dev *dev, struct ku_access_spad_reg *reg_data);
+int sx_ACCESS_REG_SSPR(struct sx_dev *dev, struct ku_access_sspr_reg *reg_data);
+int sx_ACCESS_REG_PPAD(struct sx_dev *dev, struct ku_access_ppad_reg *reg_data);
+int sx_ACCESS_REG_SPMCR(struct sx_dev *dev, struct ku_access_spmcr_reg *reg_data);
+int sx_ACCESS_REG_PBMC(struct sx_dev *dev, struct ku_access_pbmc_reg *reg_data);
+int sx_ACCESS_REG_PPTB(struct sx_dev *dev, struct ku_access_pptb_reg *reg_data);
+int sx_ACCESS_REG_SMID(struct sx_dev *dev, struct ku_access_smid_reg *reg_data);
+int sx_ACCESS_REG_SPVID(struct sx_dev *dev, struct ku_access_spvid_reg *reg_data);
+int sx_ACCESS_REG_SFGC(struct sx_dev *dev, struct ku_access_sfgc_reg *reg_data);
+int sx_ACCESS_REG_SFD(struct sx_dev *dev, struct ku_access_sfd_reg *reg_data);
+int sx_ACCESS_REG_OEPFT(struct sx_dev *dev, struct ku_access_oepft_reg *reg_data);
+int sx_ACCESS_REG_PPSC(struct sx_dev *dev, struct ku_access_ppsc_reg *reg_data);
+int sx_ACCESS_REG_PLBF(struct sx_dev *dev, struct ku_access_plbf_reg *reg_data);
+int sx_ACCESS_REG_MHSR(struct sx_dev *dev,struct ku_access_mhsr_reg *reg_data);
+int sx_ACCESS_REG_SGCR(struct sx_dev *dev,struct ku_access_sgcr_reg *reg_data);
+int sx_ACCESS_REG_MSCI(struct sx_dev *dev, struct ku_access_msci_reg *reg_data);
+int sx_ACCESS_REG_MRSR(struct sx_dev *dev, struct ku_access_mrsr_reg *reg_data);
+int sx_ACCESS_REG_MPSC(struct sx_dev *dev, struct ku_access_mpsc_reg *reg_data);
+
+
+#endif /* SX_CMD_H_ */
+
+/************************************************
+ *                  EOF                         *
+ ***********************************************/
diff --git a/linux/include/linux/mlx_sx/device.h b/linux/include/linux/mlx_sx/device.h
new file mode 100644
index 0000000..f686ade
--- /dev/null
+++ b/linux/include/linux/mlx_sx/device.h
@@ -0,0 +1,308 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+#ifndef SX_DEVICE_H
+#define SX_DEVICE_H
+
+#include <linux/types.h>
+#include <linux/fs.h>
+#include <linux/device.h>
+#include <linux/cdev.h>
+#include <linux/mlx_sx/kernel_user.h>
+
+#ifndef SYSTEM_PCI
+#define NO_PCI
+#endif
+
+/* According to CQe */
+enum sx_packet_type {
+	PKT_TYPE_IB_Raw		= 0,
+	PKT_TYPE_IB_non_Raw	= 1,
+	PKT_TYPE_ETH		= 2,
+	PKT_TYPE_FC			= 3,
+	PKT_TYPE_FCoIB		= 4,
+	PKT_TYPE_FCoETH		= 5,
+	PKT_TYPE_ETHoIB		= 6,
+	PKT_TYPE_NUM
+};
+
+static const char *sx_cqe_packet_type_str[] = {
+	"PKT_TYPE_IB_Raw",
+	"PKT_TYPE_IB_non_Raw",
+	"PKT_TYPE_ETH",
+	"PKT_TYPE_FC",
+	"PKT_TYPE_FCoIB",
+	"PKT_TYPE_FCoETH",
+	"PKT_TYPE_ETHoIB"
+};
+
+static const int sx_cqe_packet_type_str_len =
+		sizeof(sx_cqe_packet_type_str)/sizeof(char *);
+
+enum l2_type {
+	L2_TYPE_DONT_CARE	= -1,
+	L2_TYPE_IB			= 0,
+	L2_TYPE_ETH			= 1,
+	L2_TYPE_FC			= 2
+};
+
+enum sx_event {
+	SX_EVENT_TYPE_COMP				= 0x00,
+	SX_EVENT_TYPE_CMD				= 0x0a,
+	SX_EVENT_TYPE_INTERNAL_ERROR	= 0x08
+};
+
+enum {
+	SX_DBELL_REGION_SIZE		= 0xc00
+};
+
+struct completion_info {
+	u8							swid;
+	u16 						sysport;
+	u16 						hw_synd;
+	u8							is_send;
+	enum sx_packet_type			pkt_type;
+	struct sk_buff				*skb;
+	union ku_filter_critireas	info;
+	u8							is_lag;
+	u8							lag_subport;
+	u8							is_tagged;
+	u16							vid;
+	void						*context;
+	struct sx_dev    		    *dev;
+    u32                         original_packet_size;
+    u16                         bridge_id;
+};
+
+typedef void (*cq_handler)(struct completion_info*, void *);
+
+struct listener_entry {
+	u8							swid;
+	enum l2_type				listener_type;
+	u8							is_default; /*is a default listener */
+	union ku_filter_critireas	critireas;  /*more filter critireas */
+	cq_handler					handler;    /*The completion handler*/
+	void						*context;   /*to pass to the handler*/
+	u64							rx_pkts;	/* rx pkts */
+	struct list_head			list;
+};
+
+struct sx_stats{
+	u64	rx_by_pkt_type[NUMBER_OF_SWIDS+1][PKT_TYPE_NUM];
+	u64	tx_by_pkt_type[NUMBER_OF_SWIDS+1][PKT_TYPE_NUM];
+	u64	rx_by_synd[NUMBER_OF_SWIDS+1][NUM_HW_SYNDROMES+1];
+	u64	tx_by_synd[NUMBER_OF_SWIDS+1][NUM_HW_SYNDROMES+1];
+	u64	rx_unconsumed_by_synd[NUM_HW_SYNDROMES+1][PKT_TYPE_NUM];
+	u64	rx_eventlist_drops_by_synd[NUM_HW_SYNDROMES+1];
+};
+
+struct sx_dev {
+	struct sx_dev_cap		dev_cap;
+	spinlock_t				profile_lock; /* the profile's lock */
+	struct sx_pci_profile	profile;
+	u8						profile_set;
+    u8 						dev_profile_set;
+    u8                      first_ib_swid;
+	unsigned long			flags;
+	struct pci_dev			*pdev;
+	u64						bar0_dbregs_offset;
+	u8						bar0_dbregs_bar;
+	void __iomem			*db_base;
+	char					board_id[SX_BOARD_ID_LEN];
+	u16						vsd_vendor_id;
+	struct device			dev; /* TBD - do we need it? */
+	u16						device_id;
+	struct list_head		list;
+	u64						fw_ver;
+	u8						dev_stuck;
+	u8						global_flushing;
+	struct cdev             cdev;
+
+	/* multi-dev support */
+	struct sx_stats         stats;
+	u64                     eventlist_drops_counter;
+    u64                     unconsumed_packets_counter;
+    u64                   filtered_lag_packets_counter;
+    u64                   filtered_port_packets_counter;
+	u64					loopback_packets_counter;
+	struct work_struct catas_work;
+	struct workqueue_struct *catas_wq;
+	int         			catas_poll_running;
+};
+
+enum {
+	PPBT_REG_ID = 0x3003,
+	QSPTC_REG_ID = 0x4009,
+	QSTCT_REG_ID = 0x400b,
+	PMLP_REG_ID = 0x5002,
+	PMTU_REG_ID = 0x5003,
+	PTYS_REG_ID = 0x5004,
+	PPAD_REG_ID = 0x5005,
+	PAOS_REG_ID = 0x5006,
+	PUDE_REG_ID = 0x5009,
+	PLIB_REG_ID = 0x500a,
+	PPTB_REG_ID = 0x500B,
+	PSPA_REG_ID = 0x500d,
+	PELC_REG_ID = 0x500e,
+	PVLC_REG_ID = 0x500f,
+	PMPR_REG_ID = 0x5013,
+	SPZR_REG_ID = 0x6002,
+	HCAP_REG_ID = 0x7001,
+	HTGT_REG_ID = 0x7002,
+	HPKT_REG_ID = 0x7003,
+	HDRT_REG_ID = 0x7004,
+	OEPFT_REG_ID = 0x7081,
+	MFCR_REG_ID = 0x9001,
+	MFSC_REG_ID = 0x9002,
+	MFSM_REG_ID = 0x9003,
+	MFSL_REG_ID = 0x9004,
+	MTCAP_REG_ID = 0x9009,
+	MTMP_REG_ID = 0x900a,
+	MFPA_REG_ID = 0x9010,
+	MFBA_REG_ID = 0x9011,
+	MFBE_REG_ID = 0x9012,
+	MCIA_REG_ID = 0x9014,
+	MGIR_REG_ID = 0x9020,
+	PMAOS_REG_ID = 0x5012,
+	MFM_REG_ID = 0x901d,
+        MJTAG_REG_ID = 0x901F,
+        PMPC_REG_ID = 0x501F,
+	MPSC_REG_ID = 0x9080,
+};
+
+enum {
+	TLV_TYPE_END_E,
+	TLV_TYPE_OPERATION_E,
+	TLV_TYPE_DR_E,
+	TLV_TYPE_REG_E,
+	TLV_TYPE_USER_DATA_E
+};
+
+enum {
+	EMAD_METHOD_QUERY = 1,
+	EMAD_METHOD_WRITE = 2,
+	EMAD_METHOD_SEND  = 3,
+	EMAD_METHOD_EVENT = 5,
+};
+
+enum {
+		PORT_OPER_STATUS_UP = 1,
+		PORT_OPER_STATUS_DOWN = 2,
+		PORT_OPER_STATUS_FAILURE = 4,
+};
+
+struct sx_eth_hdr {
+	__be64	dmac_smac1;
+	__be32	smac2;
+	__be16	ethertype;
+	u8		mlx_proto;
+	u8		ver;
+};
+
+struct emad_operation {
+	__be16  type_len;
+	u8      status;
+	u8      reserved1;
+	__be16  register_id;
+	u8      r_method;
+	u8      class;
+	__be64  tid;
+};
+
+struct sx_emad {
+	struct sx_eth_hdr eth_hdr;
+	struct emad_operation emad_op;
+};
+
+#define EMAD_TLV_TYPE_SHIFT (3)
+struct sxd_emad_tlv_reg {
+	u8     type;
+	u8     len;
+	__be16 reserved0;
+};
+
+struct sxd_emad_pude_reg {
+	struct sx_emad emad_header;
+	struct sxd_emad_tlv_reg tlv_header;
+	u8     swid;
+	u8     local_port;
+	u8     admin_status;
+	u8     oper_status;
+	__be32 reserved3[3];
+};
+
+#define SX_PORT_PHY_ID_OFFS     (8)
+#define SX_PORT_PHY_ID_MASK     (0x0000FF00)
+#define SX_PORT_PHY_ID_ISO(id)  ((id) & (SX_PORT_PHY_ID_MASK)) 
+#define SX_PORT_PHY_ID_GET(id)  (SX_PORT_PHY_ID_ISO(id) >> SX_PORT_PHY_ID_OFFS)
+
+#define SX_PORT_DEV_ID_OFFS  (16) 
+#define SX_PORT_DEV_ID_MASK  (0x0FFF0000)
+#define SX_PORT_DEV_ID_ISO(id)  ((id) & (SX_PORT_DEV_ID_MASK))
+#define SX_PORT_DEV_ID_GET(id)  (SX_PORT_DEV_ID_ISO(id) >> SX_PORT_DEV_ID_OFFS)
+
+#define SX_PORT_TYPE_ID_OFFS (28)
+#define SX_PORT_TYPE_ID_MASK (0xF0000000)
+#define SX_PORT_TYPE_ID_ISO(id) ((id) & (SX_PORT_TYPE_ID_MASK))
+#define SX_PORT_TYPE_ID_GET(id) (SX_PORT_TYPE_ID_ISO(id) >> SX_PORT_TYPE_ID_OFFS)
+
+#define SX_PORT_LAG_ID_OFFS  (8)
+#define SX_PORT_LAG_ID_MASK  (0x000FFF00)
+#define SX_PORT_LAG_ID_ISO(id)  ((id) & (SX_PORT_LAG_ID_MASK))
+#define SX_PORT_LAG_ID_GET(id)  (SX_PORT_LAG_ID_ISO(id) >> SX_PORT_LAG_ID_OFFS)
+
+#define CPU_PORT_PHY_ID              (0)
+#define UCROUTE_CPU_PORT_DEV_MASK    (0x0FC0)
+#define UCROUTE_CPU_DEV_BIT_OFFSET   (6)
+#define UCROUTE_DEV_ID_BIT_OFFSET    (10)
+#define UCROUTE_PHY_PORT_BITS_OFFSET (4)
+#define UCROUTE_CPU_PORT_PREFIX      (0xB000)
+
+u16 translate_user_port_to_sysport(struct sx_dev *dev, u32 log_port, int* is_lag);
+u32 translate_sysport_to_user_port(struct sx_dev *dev, u16 port, u8 is_lag);
+
+
+#define SX_TRAP_ID_PUDE  0x08
+
+
+#define NUM_OF_SYSPORT_BITS 16
+#define NUM_OF_LAG_BITS 12
+#define MAX_SYSPORT_NUM (1 << NUM_OF_SYSPORT_BITS)
+#define MAX_PHYPORT_NUM 64
+#define MAX_LAG_NUM MAX_PHYPORT_NUM
+#define MAX_LAG_MEMBERS_NUM 32
+#define MAX_IBPORT_NUM MAX_PHYPORT_NUM
+#define MAX_SYSTEM_PORTS_IN_FILTER 256
+#define MAX_LAG_PORTS_IN_FILTER 256
+#define MAX_PRIO_NUM 15
+#define MAX_VLAN_NUM 4096
+
+/* Bridge Netdev values */
+/* MIN_BRIDGE_ID = 4k */
+#define MIN_BRIDGE_ID 4096
+/* MAX_BRIDGE_ID = (15k - 1) */
+#define MAX_BRIDGE_ID 15359
+/* MAX_BRIDGE_NUM */
+#define MAX_BRIDGE_NUM (MAX_BRIDGE_ID - MIN_BRIDGE_ID + 1)
+
+/** This enum defines bitmask values for combinations of port types */
+enum sx_port_type {
+    SX_PORT_TYPE_NETWORK = 0,
+    SX_PORT_TYPE_LAG = 1,
+    SX_PORT_TYPE_VPORT = 2,
+    SX_PORT_TYPE_MULTICAST = 4,
+    SX_PORT_TYPE_MIN = SX_PORT_TYPE_NETWORK,
+    SX_PORT_TYPE_MAX = SX_PORT_TYPE_MULTICAST,
+};
+
+#endif /* SX_DEVICE_H */
diff --git a/linux/include/linux/mlx_sx/driver.h b/linux/include/linux/mlx_sx/driver.h
new file mode 100644
index 0000000..b726ea8
--- /dev/null
+++ b/linux/include/linux/mlx_sx/driver.h
@@ -0,0 +1,200 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+#ifndef SX_DRIVER_H
+#define SX_DRIVER_H
+
+#include <linux/device.h>
+#include <linux/mlx_sx/device.h>
+#include <linux/mlx_sx/kernel_user.h>
+
+struct sx_dev;
+
+enum sx_dev_event {
+	SX_DEV_EVENT_CATASTROPHIC_ERROR,
+	SX_DEV_EVENT_IB_SWID_UP,
+	SX_DEV_EVENT_ETH_SWID_UP,
+	SX_DEV_EVENT_IB_SWID_DOWN,
+	SX_DEV_EVENT_ETH_SWID_DOWN,
+	SX_DEV_EVENT_OPEN_PORT_NETDEV,
+	SX_DEV_EVENT_CLOSE_PORT_NETDEV,
+	SX_DEV_EVENT_PORT_UP,
+	SX_DEV_EVENT_PORT_DOWN,
+	SX_DEV_EVENT_PORT_REINIT,
+	SX_DEV_EVENT_TYPE_INTERNAL_ERROR,
+	SX_DEV_EVENT_TYPE_TCA_INIT,
+	SX_DEV_EVENT_MAD_IFC_ENABLE,
+	SX_DEV_EVENT_ADD_SYND_NETDEV,
+	SX_DEV_EVENT_REMOVE_SYND_NETDEV,
+	SX_DEV_EVENT_ADD_SYND_IPOIB,
+	SX_DEV_EVENT_REMOVE_SYND_IPOIB,
+	SX_DEV_EVENT_DEBUG_NETDEV,
+	SX_DEV_EVENT_NODE_DESC_UPDATE,
+	SX_DEV_EVENT_ADD_SYND_L2_NETDEV,
+	SX_DEV_EVENT_REMOVE_SYND_L2_NETDEV
+};
+
+#define SX_PAGE_SIZE		4096
+#define SX_PAGE_SHIFT		12
+
+#define ETHTYPE_ARP		0x0806
+#define ETHTYPE_VLAN		0x8100
+#define ETHTYPE_EMAD		0x8932
+#define ETHTYPE_DONT_CARE_VALUE 0
+#define QPN_DONT_CARE_VALUE 	0xffffffff
+#define QPN_MULTICAST_VALUE 	0xffffff
+#define DMAC_DONT_CARE_VALUE 	0
+#define TID_DONT_CARE_VALUE 	0
+#define SYSPORT_DONT_CARE_VALUE 0
+#define FWD_BY_FDB_TRAP_ID	0x01
+#define SWITCHIB_QP0_TRAP_ID		0xf0
+#define SWITCHIB_QP1_TRAP_ID		0xf1
+#define SWITCHIB_OTHER_QP_TRAP_ID	0xf2
+#define PACKET_SAMPLE_TRAP_ID	0x38
+#define ROUTER_QP0_TRAP_ID	0x5e
+#define FDB_TRAP_ID		0x06
+#define ARP_REQUEST_TRAP_ID	0x50
+#define ARP_RESPONSE_TRAP_ID	0x51
+#define ETH_L3_MTUERROR_TRAP_ID	0x52
+#define ETH_L3_TTLERROR_TRAP_ID	0x53
+#define ETH_L3_LBERROR_TRAP_ID  0x54
+#define MIN_IPTRAP_TRAP_ID	0x1C0 /* TODO define which one will be used */
+
+union sx_event_data {
+	struct {
+		int swid;
+		u16 dev_id;
+	} ib_swid_change;
+	struct {
+		int swid;
+		int synd;
+		u64 mac;
+	} eth_swid_up;
+	struct {
+		int swid;
+		int hw_synd;
+	} eth_l3_synd;
+	struct {
+		int swid;
+		int hw_synd;
+	} ipoib_synd;
+	struct {
+		int swid;
+	} eth_swid_down;
+	struct {
+		int swid;
+		u16 sysport;
+		u8 	is_lag;
+		u16 mid;
+		char *name;
+		u8  send_to_rp_as_data_supported;
+	} port_netdev_set;
+	struct {
+		int num_of_ib_swids;
+		u8  swid[NUMBER_OF_SWIDS];
+		u16 max_pkey;
+	} tca_init;
+	struct {
+		uint8_t swid;
+		uint8_t NodeDescription[64];
+	} node_desc_update;
+};
+
+struct sx_interface {
+	void *			(*add)	 (struct sx_dev *dev);
+	void			(*remove)(struct sx_dev *dev, void *context);
+	void			(*event) (struct sx_dev *dev, void *context,
+					enum sx_dev_event event,
+					union sx_event_data *event_data);
+	struct list_head	list;
+};
+
+struct sx_sgmii_ctrl_segment {
+	u8	reserved1;
+	u8	one;
+	__be16	type_sdq_lp;
+	__be32 reserved2[3];
+} __attribute__((packed));
+
+struct sx_ethernet_header {
+	uint8_t dmac[6];
+	uint8_t smac[6];
+	__be16 et;
+	uint8_t mlx_proto;
+	uint8_t ver;
+};
+
+typedef enum check_dup{
+    CHECK_DUP_DISABLED_E = 0,
+    CHECK_DUP_ENABLED_E = 1
+}check_dup_e;
+
+typedef enum is_rp {
+    IS_RP_DONT_CARE_E = 0,
+    IS_RP_FROM_RP_E = 1,
+    IS_RP_NOT_FROM_RP_E = 2,
+} is_rp_e;
+
+typedef enum is_bridge {
+    IS_BRIDGE_DONT_CARE_E = 0,
+    IS_BRIDGE_FROM_BRIDGE_E = 1,
+    IS_BRIDGE_NOT_FROM_BRIDGE_E = 2,
+} is_bridge_e;
+
+int sx_core_flush_synd_by_context(void * context);
+int sx_core_flush_synd_by_handler(cq_handler handler);
+int sx_register_interface(struct sx_interface *intf);
+void sx_unregister_interface(struct sx_interface *intf);
+int sx_core_add_synd(u8 swid, u16 hw_synd, enum l2_type type, u8 is_default,
+	union ku_filter_critireas crit, cq_handler handler, void *context, 
+	check_dup_e check_dup, struct sx_dev* sx_dev);
+int sx_core_remove_synd(u8 swid, u16 hw_synd, enum l2_type type, u8 is_default,
+		union ku_filter_critireas critireas, 
+		void *context, struct sx_dev* sx_dev);
+int sx_core_post_send(struct sx_dev *dev, struct sk_buff *skb,
+			struct isx_meta *meta);
+int __sx_core_post_send(struct sx_dev *dev, struct sk_buff *skb,
+			struct isx_meta *meta);
+void sx_skb_free(struct sk_buff *skb);
+
+int sx_core_get_prio2tc(struct sx_dev *dev,
+               uint16_t port_lag_id, uint8_t is_lag,
+               uint8_t pcp, uint8_t *tc);
+int sx_core_get_pvid(struct sx_dev *dev,
+                     uint16_t       port_lag_id,
+                     uint8_t        is_lag,
+                     uint16_t       *pvid);
+int sx_core_get_vlan_tagging(struct sx_dev *dev,
+               uint16_t port_lag_id, uint8_t is_lag,
+               uint16_t vlan, uint8_t *is_vlan_tagged);
+int sx_core_get_prio_tagging(struct sx_dev *dev,
+               uint16_t port_lag_id, uint8_t is_lag,
+               uint8_t *is_port_prio_tagged);
+int sx_core_get_rp_vlan(struct sx_dev *dev,
+                        struct completion_info *comp_info,
+                        uint16_t *vlan_id);
+int sx_core_get_swid(struct sx_dev *dev,
+                     struct completion_info *comp_info,
+                     uint8_t *swid);
+int sx_core_get_vlan2ip(struct sx_dev *dev,
+               uint16_t vid, uint32_t *ip_addr);
+int sx_core_get_rp_rif_id(struct sx_dev *dev, uint16_t port_lag_id,
+                          uint8_t is_lag, uint16_t vlan_id, uint16_t *rif_id);
+int sx_core_get_rp_mode(struct sx_dev *dev, u8 is_lag, u16 sysport_lag_id,
+                        u16 vlan_id, u8 *is_rp);
+
+int sx_core_get_fid_by_port_vid(struct sx_dev *dev, 
+                            struct completion_info *comp_info, uint16_t *fid);
+int sx_core_get_lag_mid(struct sx_dev *dev, u16 lag_id, u16 *mid);
+
+#endif /* SX_DRIVER_H */
diff --git a/linux/include/linux/mlx_sx/kernel_user.h b/linux/include/linux/mlx_sx/kernel_user.h
new file mode 100644
index 0000000..6ad9714
--- /dev/null
+++ b/linux/include/linux/mlx_sx/kernel_user.h
@@ -0,0 +1,6143 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2016 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+#ifndef KERNEL_USER_H_
+#define KERNEL_USER_H_
+
+#ifdef __KERNEL__
+#include <linux/uio.h>
+#else
+#include <sys/uio.h>
+#include <stdint.h>
+#endif
+
+/************************************************
+ *  Define
+ ***********************************************/
+
+/**
+ * DEFAULT_DEVICE_ID defines the default value for
+ * device ID for local devices.
+ */
+#define DEFAULT_DEVICE_ID   255
+#define LOCAL_DEVICE_ID_MIN 254
+#define LOCAL_DEVICE_ID_MAX DEFAULT_DEVICE_ID
+#define DEFAULT_DEVICE_ID_CHECK(dev_id) (dev_id >= LOCAL_DEVICE_ID_MIN)
+
+#define REDECN_NUM_HW_PROFILES 3
+/**
+ * SWID_NUM_DONT_CARE define the don't care value for swids
+ * when registering a listener.
+ */
+#define SWID_NUM_DONT_CARE 255
+/**
+ * NUMBER_OF_SWIDS define the number of possible swids
+ * in the system.
+ */
+#define NUMBER_OF_SWIDS 8
+/**
+ * Router Port Swid
+ */
+#define ROUTER_PORT_SWID 1
+/**
+ * NUMBER_OF_ETCLASSES define the number of possible etclasses
+ * in the system.
+ */
+#define NUMBER_OF_ETCLASSES 17
+/**
+ * NUMBER_OF_STCLASSES define the number of possible stclasses
+ * in the system.
+ */
+#define NUMBER_OF_STCLASSES 8
+/**
+ * NUMBER_OF_RDQS define the number of possible rdqs
+ * in the system.
+ */
+#define NUMBER_OF_RDQS 34
+/**
+ * NUMBER_OF_SDQS define the number of possible sdqs
+ * in the system.
+ */
+#define NUMBER_OF_SDQS 24
+/**
+ * NUMBER_OF_SERDESS define the number of possible serdeses
+ * in the system.
+ */
+#define NUMBER_OF_SERDESES 4
+/**
+ * SX_BOARD_ID_LEN define the length of the board id
+ * string.
+ */
+#define SX_BOARD_ID_LEN 64
+
+/**
+ * ETHER_ADDR_LENGTH define the length of the ethernet
+ * address in bytes
+ */
+#define ETHER_ADDR_LENGTH 6
+
+#define NUM_SW_SYNDROMES	64
+#define NUM_HW_SYNDROMES 	(512 + NUM_SW_SYNDROMES)
+
+/**
+ * SYND_NUM_DONT_CARE define the don't care value for synd
+ * when registering a listener.
+ */
+#define SYND_NUM_DONT_CARE	NUM_HW_SYNDROMES
+
+
+#define SXD_ACL_INFO_SIZE_BYTES         		16
+#define SXD_MAX_ACL_IN_GROUP           			16
+#define SXD_PACL_TCAM_REGIONS          			 4
+#define SXD_TCAM_REGION_INFO_SIZE_BYTES 		16
+#define SXD_FLEXIBLE_KEY_ID_SIZE_BYTES 			16
+#define SXD_FLEXIBLE_KEY_BLOCK_REG_SIZE_BYTES   12
+#define SXD_ACL_FLEX_KEY_BLOCK_SIZE_BYTES       96
+#define SXD_ACL_NUM_OF_ACTION_SLOTS              5
+#define SXD_ACL_NUM_OF_EXTRACTION_POINT         128
+#define SXD_ACL_NUM_OF_KEY_BLOCKS               6
+
+/**
+ * MAX_TRANSACTIONS_NUM define the max number of mjtag register
+ * stransactions.
+ */
+#define MAX_TRANSACTIONS_NUM 40
+
+
+/**
+ * DSCP_CODES_NUMBER defines the DSCP codes number
+ */
+#define DSCP_CODES_NUMBER  64
+#define EXP_CODES_NUMBER   8
+#define ECN_CODES_NUMBER   4
+#define COLOR_CODES_NUMBER 3
+
+#define SX_IFNAMSIZ 16
+
+/*
+ * MAX num of records to retreive from rauhtd
+ * */
+#define SXD_RAUHTD_MAX_REC_NUM 32
+
+/**
+ * Shared buffers statistics and pools
+ */
+#define SXD_EMAD_SBSR_PORT_MASK_SIZE 8
+#define SXD_EMAD_SBSR_TC_MASK_SIZE   2
+#define SXD_EMAD_SBSR_MAX_RET_SIZE   120
+
+#define SXD_ACL_KEY_BLOCK_NULL                    0x00
+#define SXD_ACL_KEY_BLOCK_QOS                     0x01
+#define SXD_ACL_KEY_BLOCK_L2_DMAC                 0x10
+#define SXD_ACL_KEY_BLOCK_L2_SMAC                 0x11
+#define SXD_ACL_KEY_BLOCK_L2_SMAC_EX              0x12
+#define SXD_ACL_KEY_BLOCK_ETHERNET_ETH_PAYLOAD0   0x13
+#define SXD_ACL_KEY_BLOCK_CUSTOM_ETH_PAYLOAD1     0x14
+#define SXD_ACL_KEY_BLOCK_ETHERTYPE_ETH_PAYLOAD2  0x17
+#define SXD_ACL_KEY_BLOCK_ETHERTYPE_ETH_PAYLOAD3  0x15
+#define SXD_ACL_KEY_BLOCK_VID_MCID                0x16
+#define SXD_ACL_KEY_BLOCK_IPV4_SIP                0x30
+#define SXD_ACL_KEY_BLOCK_IPV4_DIP                0x31
+#define SXD_ACL_KEY_BLOCK_IPV4                    0x32
+#define SXD_ACL_KEY_BLOCK_IPV4_EX                 0x33
+#define SXD_ACL_KEY_BLOCK_IPV4_5TUPLE             0x34
+#define SXD_ACL_KEY_BLOCK_IPV4_12TUPLE            0x35
+#define SXD_ACL_KEY_BLOCK_IPV4_12TUPLE_EX         0x36
+#define SXD_ACL_KEY_BLOCK_IPV4_CUSTOM             0x37
+#define SXD_ACL_KEY_BLOCK_IPV4_CUSTOM_EX          0x38
+#define SXD_ACL_KEY_BLOCK_IPV4_INNER_DIP          0x39
+#define SXD_ACL_KEY_BLOCK_IPV4_INNER_5TUPLE       0x3A
+#define SXD_ACL_KEY_BLOCK_IPV4_INNER_12TUPLE      0x3B
+#define SXD_ACL_KEY_BLOCK_IPV4_INNER_12TUPLE_EX   0x3C
+#define SXD_ACL_KEY_BLOCK_RPF                     0x3D
+#define SXD_ACL_KEY_BLOCK_ROCE                    0x40
+#define SXD_ACL_KEY_BLOCK_ROCE_EX                 0x41
+#define SXD_ACL_KEY_BLOCK_IPV6_DIP                0x60
+#define SXD_ACL_KEY_BLOCK_IPV6_DIP_EX             0x61
+#define SXD_ACL_KEY_BLOCK_IPV6_SIP                0x62
+#define SXD_ACL_KEY_BLOCK_IPV6_SIP_EX             0x63
+#define SXD_ACL_KEY_BLOCK_IPV6                    0x64
+#define SXD_ACL_KEY_BLOCK_IPV6_EX1                0x65
+#define SXD_ACL_KEY_BLOCK_IPV6_EX2                0x66
+#define SXD_ACL_KEY_BLOCK_IPV6_EX3                0x67
+#define SXD_ACL_KEY_BLOCK_IPV6_EX4                0x68
+#define SXD_ACL_KEY_BLOCK_INNER_IPV6              0x69
+#define SXD_ACL_KEY_BLOCK_INNER_IPV6_EX1          0x70
+#define SXD_ACL_KEY_BLOCK_INNER_IPV6_EX2          0x71
+#define SXD_ACL_KEY_BLOCK_INNER_IPV6_EX3          0x73
+#define SXD_ACL_KEY_BLOCK_TUNNEL                  0x80
+#define SXD_ACL_KEY_BLOCK_IPSEC                   0x81
+#define SXD_ACL_KEY_BLOCK_MPLS                    0x90
+#define SXD_ACL_KEY_BLOCK_MPLS_EX                 0x91
+#define SXD_ACL_KEY_BLOCK_FIBER_CHANNEL           0xA0
+#define SXD_ACL_KEY_BLOCK_FIBER_CHANNEL_EX        0xA1
+#define SXD_ACL_KEY_BLOCK_LOADBALANCING           0xA2
+#define SXD_ACL_KEY_BLOCK_LOADBALANCING_EX        0xA3
+#define SXD_ACL_KEY_BLOCK_PACKETTYPE              0xB0
+#define SXD_ACL_KEY_BLOCK_RX_LIST                 0xB1
+
+/************************************************
+ *  Enum
+ ***********************************************/
+/**
+ * ku_pkt_type enumerated type is used to note the possible
+ * packet types.
+ */
+enum ku_pkt_type {
+    SX_PKT_TYPE_ETH_CTL_UC, /**< Eth control unicast */
+    SX_PKT_TYPE_ETH_CTL_MC, /**< Eth control multicast */
+    SX_PKT_TYPE_ETH_DATA, /**< Eth data */
+    SX_PKT_TYPE_DROUTE_EMAD_CTL, /**< Directed route emad */
+    SX_PKT_TYPE_EMAD_CTL, /**< Emad */
+    SX_PKT_TYPE_FC_CTL_UC, /**< FC control unicast */
+    SX_PKT_TYPE_FC_CTL_MC, /**< FC control multicast */
+    SX_PKT_TYPE_FCOE_CTL_UC, /**< FC over Eth control unicast */
+    SX_PKT_TYPE_FCOE_CTL_MC, /**< FC over Eth control multicast */
+    SX_PKT_TYPE_IB_RAW_CTL, /**< IB raw control */
+    SX_PKT_TYPE_IB_TRANSPORT_CTL, /**< IB transport control */
+    SX_PKT_TYPE_IB_RAW_DATA, /**< IB raw data */
+    SX_PKT_TYPE_IB_TRANSPORT_DATA, /**< IB transport data */
+    SX_PKT_TYPE_EOIB_CTL, /**< Eth over IB control */
+    SX_PKT_TYPE_FCOIB_CTL, /**< FC over IB control */
+    SX_PKT_TYPE_LOOPBACK_CTL, /**< Loopback control */
+    SX_PKT_TYPE_MIN = SX_PKT_TYPE_ETH_CTL_UC, /**< Minimum enum value */
+    SX_PKT_TYPE_MAX = SX_PKT_TYPE_LOOPBACK_CTL /**< Maximum enum value */
+};
+
+/**
+ * ku_ctrl_cmd enumerated type is used to note the possible
+ * ioctl control commands.
+ */
+enum ku_ctrl_cmd {
+    CTRL_CMD_GET_CAPABILITIES, /**< Get system capabilities */
+    CTRL_CMD_SET_PCI_PROFILE, /**< Set the PCI profile */
+    CTRL_CMD_INVALID, /**< Invalid */
+    CTRL_CMD_GET_DEVICE_PROFILE, /**< Get the device profile */
+    CTRL_CMD_ADD_SYND, /**< Add a new syndrome */
+    CTRL_CMD_REMOVE_SYND, /**< Remove an existing syndrome */
+    CTRL_CMD_MULTI_PACKET_ENABLE, /**< Enable multi packets read operation */
+    CTRL_CMD_BLOCKING_ENABLE, /**< Enable blocking read operation */
+    CTRL_CMD_RESET, /**< System reset */
+    CTRL_CMD_PCI_DEVICE_RESTART, /**< PCI device restart */
+    CTRL_CMD_RAISE_EVENT, /**< Raise an event */
+    CTRL_CMD_ENABLE_SWID, /**< Enable a swid */
+    CTRL_CMD_DISABLE_SWID, /**< Disable a swid */
+    CTRL_CMD_GET_SYNDROME_STATUS, /**< Get syndrome statue */
+    CTRL_CMD_QUERY_FW, /**< Run query FW command */
+    CTRL_CMD_QUERY_BOARD_INFO, /**< Run query board info command */
+    CTRL_CMD_ADD_DEV_PATH, /**< Add a device path to the DPT */
+    CTRL_CMD_REMOVE_DEV_PATH, /**< Remove a device path from the DPT */
+    CTRL_CMD_REMOVE_DEV, /**< Remove a device from the DPT */
+    CTRL_CMD_SET_CMD_PATH, /**< Set a device's command path in the DPT */
+    CTRL_CMD_SET_EMAD_PATH, /**< Set a device's emad path in the DPT */
+    CTRL_CMD_SET_MAD_PATH, /**< Set a device's mad path in the DPT */
+    CTRL_CMD_SET_CR_ACCESS_PATH, /**< Set a device's CR access path in the DPT */
+    CTRL_CMD_GET_PCI_PROFILE, /**< Get the PCI profile */
+    CTRL_CMD_GET_SWID_2_RDQ, /**< Get swid to RDQ mapping */
+    CTRL_CMD_SET_DEFAULT_VID, /**< Update default vid of a port or lag */
+    CTRL_CMD_SET_VID_MEMBERSHIP, /**< Update vid membership of a port or lag */
+    CTRL_CMD_SET_PRIO_TAGGING, /**< Update prio tagging mode of a port or lag */
+    CTRL_CMD_SET_PRIO_TO_TC, /**< Update prio tagging mode of a port or lag */
+    CTRL_CMD_SET_DEVICE_PROFILE, /**< Set the device profile */
+    CTRL_CMD_CREATE_PORT_NETDEV, /**< Create a port network device */
+    CTRL_CMD_REMOVE_PORT_NETDEV, /**< Remove a port network device */
+    CTRL_CMD_SET_RDQ_RATE_LIMITER, /**< Set a rate limiter on one of the RDQs */
+    CTRL_CMD_SET_TRUNCATE_PARAMS, /**< Enable/Disable truncate on one of the RDQs */
+    CTRL_CMD_CR_SPACE_READ, /**< Read a buffer from CR space */
+    CTRL_CMD_CR_SPACE_WRITE, /**< Write a buffer to CR space */
+    CTRL_CMD_SET_LOCAL_PORT_TO_SWID, /**< Set local port to swid db */
+    CTRL_CMD_SET_IB_TO_LOCAL_PORT, /**< Set local port to swid db */
+    CTRL_CMD_SET_SYSTEM_TO_LOCAL_PORT, /**< Set local port to swid db */
+    CTRL_CMD_SET_PORT_RP_MODE,
+    CTRL_CMD_SET_LOCAL_PORT_TO_LAG, /**< Set local port to swid db */
+    CTRL_CMD_TRAP_FILTER_ADD, /**< Add a port or a LAG to a trap filter */
+    CTRL_CMD_TRAP_FILTER_REMOVE, /**< Remove a port or a LAG from a trap filter */
+    CTRL_CMD_TRAP_FILTER_REMOVE_ALL, /**< Remove all ports and LAGs from a trap filter */
+    CTRL_CMD_SET_SGMII_BASE_SMAC, /**< Set SGMII base MAC address */
+    CTRL_CMD_SET_VID_2_IP, /**< Update vid membership of a port or lag */
+    CTRL_CMD_SET_PORT_VID_TO_FID_MAP,
+    CTRL_CMD_MIN_VAL = CTRL_CMD_GET_CAPABILITIES, /**< Minimum enum value */
+    CTRL_CMD_MAX_VAL = CTRL_CMD_SET_PORT_VID_TO_FID_MAP, /**< Maximum enum value */
+};
+
+/**
+ * ku_ctrl_cmd_access_reg enumerated type is used to note the
+ * possible ioctl control commands and to differ between access
+ * register control commands and other commands .
+ */
+enum ku_ctrl_cmd_access_reg {
+    CTRL_CMD_ACCESS_REG_PSPA = CTRL_CMD_MAX_VAL + 1,  /**< Run access register PSPA command */
+    CTRL_CMD_ACCESS_REG_QSPTC, /**< Run access register QSPTC command */
+    CTRL_CMD_ACCESS_REG_QSTCT, /**< Run access register QSTCT command */
+    CTRL_CMD_ACCESS_REG_PTYS, /**< Run access register PTYS command */
+    CTRL_CMD_ACCESS_REG_PMLP, /**< Run access register PMLP command */
+    CTRL_CMD_ACCESS_REG_PLIB, /**< Run access register PLIB command */
+    CTRL_CMD_ACCESS_REG_SPZR, /**< Run access register SPZR command */
+    CTRL_CMD_ACCESS_REG_PAOS, /**< Run access register PAOS command */
+    CTRL_CMD_ACCESS_REG_PPLM, /**< Run access register PPLM command */
+    CTRL_CMD_ACCESS_REG_PLPC, /**< Run access register PLPC command */
+    CTRL_CMD_ACCESS_REG_PMPC, /**< Run access register PMPC command */
+    CTRL_CMD_ACCESS_REG_PMPR, /**< Run access register PMPR command */
+    CTRL_CMD_ACCESS_REG_PMTU, /**< Run access register PMTU command */
+    CTRL_CMD_ACCESS_REG_PPLR, /**< Run access register PPLR command */
+    CTRL_CMD_ACCESS_REG_PELC, /**< Run access register PELC command */
+    CTRL_CMD_ACCESS_REG_PFCA, /**< Run access register PFCA command */
+    CTRL_CMD_ACCESS_REG_PFCNT, /**< Run access register PFCNT command */
+    CTRL_CMD_ACCESS_REG_HTGT, /**< Run access register HTGT command */
+    CTRL_CMD_ACCESS_REG_MFSC, /**< Run access register MFSC command */
+    CTRL_CMD_ACCESS_REG_MFSM, /**< Run access register MFSM command */
+    CTRL_CMD_ACCESS_REG_MFSL, /**< Run access register MFSL command */
+    CTRL_CMD_ACCESS_REG_MJTAG, /**< Run access register MJTAG command */
+    CTRL_CMD_ACCESS_REG_PPSC, /**< Run access register PPSC command */
+    CTRL_CMD_ACCESS_REG_PVLC, /**< Run access register PVLC command */
+    CTRL_CMD_ACCESS_REG_MCIA, /**< Run access register MCIA command */
+    CTRL_CMD_ACCESS_REG_HPKT, /**< Run access register HPKT command */
+    CTRL_CMD_ACCESS_REG_HCAP, /**< Run access register HCAP command */
+    CTRL_CMD_ACCESS_REG_HDRT, /**< Run access register HDRT command */
+    CTRL_CMD_ACCESS_REG_QPRT, /**< Run access register QPRT command */
+    CTRL_CMD_ACCESS_REG_MFCR, /**< Run access register MFCR command */
+    CTRL_CMD_ACCESS_REG_FORE, /**< Run access register FORE command */
+    CTRL_CMD_ACCESS_REG_MTCAP, /**< Run access register MTCAP command */
+    CTRL_CMD_ACCESS_REG_MTMP, /**< Run access register MTMP command */
+    CTRL_CMD_ACCESS_REG_MTWE, /**< Run access register MTWE command */
+    CTRL_CMD_ACCESS_REG_PMAOS, /**< Run access register PMAOS command */
+    CTRL_CMD_ACCESS_REG_MMDIO, /**< Run access register MMDIO command */
+    CTRL_CMD_ACCESS_REG_MMIA, /**< Run access register MMIA command */
+    CTRL_CMD_ACCESS_REG_MFPA, /**< Run access register MFPA command */
+    CTRL_CMD_ACCESS_REG_MFBE, /**< Run access register MFBE command */
+    CTRL_CMD_ACCESS_REG_MFBA, /**< Run access register MFBA command */
+    CTRL_CMD_ACCESS_REG_QCAP, /**< Run access register QCAP command */
+    CTRL_CMD_ACCESS_REG_RAW, /**< Run access register command for a RAW register */
+    CTRL_CMD_ACCESS_REG_RAW_BUFF, /**< Run access register command with a RAW buffer */
+    CTRL_CMD_ACCESS_REG_MFM, /**< Run access register MFM command */
+    CTRL_CMD_ACCESS_REG_SPAD, /**< Run access register SPAD command */
+    CTRL_CMD_ACCESS_REG_SSPR, /**< Run access register SSPR command */
+    CTRL_CMD_ACCESS_REG_PPAD, /**< Run access register PPAD command */
+    CTRL_CMD_ACCESS_REG_SPMCR, /**< Run access register SPMCR command */
+    CTRL_CMD_ACCESS_REG_PBMC, /**< Run access register PBMC command */
+    CTRL_CMD_ACCESS_REG_PPTB, /**< Run access register PPTB command */
+    CTRL_CMD_ACCESS_REG_SMID, /**< Run access register SMID command */
+    CTRL_CMD_ACCESS_REG_SPMS, /**< Run access register SPMS command */
+    CTRL_CMD_ACCESS_REG_SPVID, /**< Run access register SPVID command */
+    CTRL_CMD_ACCESS_REG_SFGC, /**< Run access register SFGC command */
+    CTRL_CMD_ACCESS_REG_SFD, /**< Run access register SFD command */
+    CTRL_CMD_ACCESS_REG_QPBR, /**< Run access register QPBR command */
+    CTRL_CMD_ACCESS_REG_OEPFT, /**< Run access register OEPFT command */
+    CTRL_CMD_ACCESS_REG_PLBF, /**< Run access register PLBF command */
+    CTRL_CMD_ACCESS_REG_MGIR, /**< Run access register MGIR command */
+    CTRL_CMD_ACCESS_REG_MHSR, /**< Run access register MHSR command */
+    CTRL_CMD_ACCESS_REG_SGCR, /**< Run access register SGCR command */
+    CTRL_CMD_ACCESS_REG_MSCI, /**< Run access register MSCI command */
+    CTRL_CMD_ACCESS_REG_MRSR, /**< Run access register MRSR command */
+    CTRL_CMD_ACCESS_REG_SBPR, /**< Run access register SBPR command */
+    CTRL_CMD_ACCESS_REG_SBSR, /**< Run access register SBSR command */
+    CTRL_CMD_ACCESS_REG_SBCM, /**< Run access register SBCM command */
+    CTRL_CMD_ACCESS_REG_SBPM, /**< Run access register SBPM command */
+    CTRL_CMD_ACCESS_REG_SBMM, /**< Run access register SBMM command */
+    CTRL_CMD_ACCESS_REG_CWGCR, /**< Run access register CWGCR command */
+    CTRL_CMD_ACCESS_REG_CWTP, /**< Run access register CWTP command */
+    CTRL_CMD_ACCESS_REG_CWTPM, /**< Run access register CWTPM command */
+    CTRL_CMD_ACCESS_REG_CWPP, /**< Run access register CWPP command */
+    CTRL_CMD_ACCESS_REG_CWPPM, /**< Run access register CWPPM command */
+    CTRL_CMD_ACCESS_REG_CPQE, /**< Run access register CPQE command */
+    CTRL_CMD_ACCESS_REG_MPSC, /**< Run access register MPSC command */
+
+    CTRL_CMD_ACCESS_REG_MIN = CTRL_CMD_ACCESS_REG_PSPA, /**< Minimum enum value */
+    CTRL_CMD_ACCESS_REG_MAX = CTRL_CMD_ACCESS_REG_MPSC  /**< Maximum enum value */
+};
+
+/**
+ * ku_l2_type enumerated type is used to note the possible
+ * L2 types.
+ */
+enum ku_l2_type {
+    SX_KU_L2_TYPE_DONT_CARE, /**< Don't care value for listeners */
+    SX_KU_L2_TYPE_IB, /**< IB */
+    SX_KU_L2_TYPE_ETH, /**< Eth */
+    SX_KU_L2_TYPE_FC, /**< FC */
+    SX_KU_L2_TYPE_ROUTER_PORT, /**< RP */
+    SX_KU_L2_MIN = SX_KU_L2_TYPE_DONT_CARE, /**< Minimum enum value */
+    SX_KU_L2_MAX = SX_KU_L2_TYPE_ROUTER_PORT /**< Maximum enum value */
+};
+
+/**
+ * ku_swid_type enumerated type is used to note the possible
+ * swid types.
+ */
+enum ku_swid_type {
+    KU_SWID_TYPE_DISABLED = 0, /**< Disabled */
+    KU_SWID_TYPE_INFINIBAND = 1, /**< IB */
+    KU_SWID_TYPE_ETHERNET = 2, /**< Eth */
+    KU_SWID_TYPE_ROUTER_PORT = 8, /**< RP */
+    KU_SWID_TYPE_MIN = KU_SWID_TYPE_DISABLED, /**< Minimum enum value */
+    KU_SWID_TYPE_MAX = KU_SWID_TYPE_ROUTER_PORT /**< Maximum enum value */
+};
+
+/**
+ * ku_command_ifc_ret_status enumerated type is used to note the possible
+ * return values from command IFC.
+ */
+enum ku_command_ifc_ret_status {
+    COMMAND_IFC_RET_STATUS_OK = 0X00,             /**< OK */
+    COMMAND_IFC_RET_STATUS_INTERNAL_ERROR = 0X01, /**< Internal error */
+    COMMAND_IFC_RET_STATUS_BAD_OP = 0X02,         /**< Bad operation */
+    COMMAND_IFC_RET_STATUS_BAD_PARAM = 0X03,      /**< Bad parameter */
+    COMMAND_IFC_RET_STATUS_BAD_SYS_STATE = 0X04,  /**< Bad system state */
+    COMMAND_IFC_RET_STATUS_BAD_RESOURCE = 0X05,   /**< Bad resource */
+    COMMAND_IFC_RET_STATUS_RESOURCE_BUSY = 0X06,  /**< Resource busy */
+    COMMAND_IFC_RET_STATUS_EXCEED_LIM = 0X08,     /**< Exceeds limitation */
+    COMMAND_IFC_RET_STATUS_BAD_RES_STATE = 0X09,  /**< Bad res state */
+    COMMAND_IFC_RET_STATUS_BAD_INDEX = 0X0A,      /**< Bad index */
+    COMMAND_IFC_RET_STATUS_BAD_NVMEM = 0X0B,      /**< Bad NVMEM */
+    COMMAND_IFC_RET_STATUS_BAD_PKT = 0X30,        /**< Bad packet */
+    COMMAND_IFC_RET_STATUS_MIN = COMMAND_IFC_RET_STATUS_OK, /**< Minimum enum value */
+    COMMAND_IFC_RET_STATUS_MAX = COMMAND_IFC_RET_STATUS_BAD_PKT /**< Maximum enum value */
+};
+
+/**
+ * ku_dpt_path_type enumerated type is used to note the possible
+ * DPT path types.
+ */
+enum ku_dpt_path_type {
+    DPT_PATH_INVALID, /**< Invalid */
+    DPT_PATH_I2C, /**< I2C */
+    DPT_PATH_SGMII, /**< SGMII */
+    DPT_PATH_PCI_E, /**< PCI express */
+    DPT_PATH_MIN = DPT_PATH_INVALID, /**< Minimum enum value */
+    DPT_PATH_MAX = DPT_PATH_PCI_E /**< Maximum enum value */
+};
+
+/**
+ * pci_profile_e enumerated type is used to note the possible
+ * PCI profile types.
+ */
+enum pci_profile_e {
+    PCI_PROFILE_IB_SINGLE_SWID, /**< Single IB swid */
+    PCI_PROFILE_IB_NAR_SINGLE_SWID, /**< Single IB swid with Adaptive Routing enabled */
+    PCI_PROFILE_IB_MULTI_SWID, /**< Mutliple IB swids */
+    PCI_PROFILE_EN_SINGLE_SWID, /**< Single eth swid */
+    PCI_PROFILE_EN_MULTI_SWID, /**< Multiple Eth swids */
+    PCI_PROFILE_VPI_SINGLE_SWID, /**< VPI single swid */
+    PCI_PROFILE_VPI_MULTI_SWID, /**< VPI multiple swids */
+};
+
+/**
+ * hpkt_action enumerated type is used to note the possible
+ * actions in HPKT register.
+ */
+enum hpkt_action {
+    HPKT_ACTION_IGNORE, /**< Ignore */
+    HPKT_ACTION_TRAP_2_CPU, /**< Trap to CPU */
+    HPKT_ACTION_MIRROR_2_CPU, /**< Mirror to CPU */
+    HPKT_ACTION_DISCARD /**< Discard */
+};
+
+/**
+ * htgt_path enumerated type is used to note the possible
+ * paths in HTGT register.
+ */
+enum htgt_path {
+    HTGT_LOCAL_PATH = 0, /**< Local path */
+    HTGT_STACKING_PATH = 1, /**< Stacking path */
+    HTGT_DR_PATH = 2, /**< Directed route path */
+    HTGT_ETH_PATH = 3 /**< Ethernet path */
+};
+
+
+/**
+ * sxd_port_eth_proto_t enumerated type is used to store
+ * Ethernet protocol.
+ */
+typedef enum sxd_port_eth_proto {
+    SXD_PORT_ETH_PROTOCOL_1000_BASE_CX_SGMII = (1 << 0),
+    SXD_PORT_ETH_PROTOCOL_1000_BASE_KX = (1 << 1),
+    SXD_PORT_ETH_PROTOCOL_10G_BASE_CX4_XAUI = (1 << 2),
+    SXD_PORT_ETH_PROTOCOL_10G_BASE_KX4 = (1 << 3),
+    SXD_PORT_ETH_PROTOCOL_10G_BASE_KR4 = (1 << 4),
+    SXD_PORT_ETH_PROTOCOL_20G_BASE_KR2 = (1 << 5),
+    SXD_PORT_ETH_PROTOCOL_40G_BASE_CR4 = (1 << 6),
+    SXD_PORT_ETH_PROTOCOL_40G_BASE_KR4 = (1 << 7),
+    SXD_PORT_ETH_PROTOCOL_56G_BASE_KR4 = (1 << 8),
+    SXD_PORT_ETH_PROTOCOL_56G_BASE_KX4 = (1 << 9),
+} sxd_port_eth_proto_t;
+
+/**
+ * sxd_port_fc_proto_t enumerated type is used to store Fibre
+ * Channel protocol.
+ */
+typedef enum sxd_port_fc_proto {
+    SXD_PORT_FC_PROTOCOL_1GFC = (1 << 0),
+    SXD_PORT_FC_PROTOCOL_2GFC = (1 << 1),
+    SXD_PORT_FC_PROTOCOL_4GFC = (1 << 2),
+    SXD_PORT_FC_PROTOCOL_8GFC = (1 << 3),
+} sxd_port_fc_proto_t;
+
+/**
+ * sxd_port_ib_proto_t enumerated type is used to store
+ * InfiniBand protocol.
+ */
+typedef enum sxd_port_ib_proto {
+    SXD_PORT_IB_PROTOCOL_TBD = (1 << 0),
+} sxd_port_ib_proto_t;
+
+/**
+ * sxd_prcr_op_type enumerated type is used copy or move
+ * rules in tcam.
+ */
+typedef enum sxd_prcr_op_type {
+    SXD_PRCR_OP_RULES_MOVE = 0,
+    SXD_PRCR_OP_RULES_COPY = 1
+} sxd_prcr_op_type_t;
+
+
+/**
+ * Counter set type
+ */
+typedef enum sxd_counter_set_type {
+    SXD_COUNTER_SET_TYPE_NO_COUNT = 0x0,
+    SXD_COUNTER_SET_TYPE_PACKET = 0x1,
+    SXD_COUNTER_SET_TYPE_BYTE = 0x2,
+    SXD_COUNTER_SET_TYPE_PACKET_AND_BYTE = 0x3,
+    SXD_COUNTER_SET_TYPE_BYTE_STATISTICAL = 0x4,
+    SXD_COUNTER_SET_TYPE_PACKET_AND_BYTE_STATISTICAL = 0x5,
+    SXD_COUNTER_SET_TYPE_RIF = 0x8,
+    SXD_COUNTER_SET_TYPE_RIF_BASIC = 0x9,
+    SXD_COUNTER_SET_TYPE_RIF_ENHANCED = 0xA,
+    SXD_COUNTER_SET_TYPE_RIF_MIXED_1 = 0xB,
+    SXD_COUNTER_SET_TYPE_RIF_MIXED_2 = 0xC,
+} sxd_counter_set_type_t;
+
+/**
+ * Counter Set.
+ */
+typedef struct sxd_counter_set {
+    sxd_counter_set_type_t type;
+    uint32_t               index;
+} sxd_counter_set_t;
+
+/************************************************
+ *  Structs
+ ***********************************************/
+
+struct sx_ether_addr {
+    uint8_t ether_addr_octet[ETHER_ADDR_LENGTH];
+};
+
+/**
+ * ku_dpt_i2c_info structure is used to store I2C
+ * info in the DPT.
+ */
+struct ku_dpt_i2c_info {
+    int sx_i2c_dev; /**< sx_i2c_dev - I2C info */
+};
+
+/**
+ * ku_dpt_pcie_info structure is used to store PCI express
+ * info in the DPT.
+ */
+struct ku_dpt_pcie_info {
+    unsigned int pci_id; /**< pci_id - PCI ID */
+    void        *sx_dev; /**< sx_dev - SX device pointer */
+};
+
+/**
+ * ku_dpt_sgmii_info structure is used to store SGMII
+ * info in the DPT.
+ */
+struct ku_dpt_sgmii_info {
+    uint64_t dmac; /**< dmac - MAC address of the destination device */
+};
+
+/**
+ * ku_dpt_sgmii_info union is used to store the path
+ * info in the DPT.
+ */
+union ku_dpt_path_info {
+    struct ku_dpt_i2c_info   sx_i2c_info; /**< sx_i2c_info - I2C info */
+    struct ku_dpt_pcie_info  sx_pcie_info; /**< sx_pcie_info - PCI info */
+    struct ku_dpt_sgmii_info sx_sgmii_info; /**< sx_sgmii_info - SGMII info */
+};
+
+/**
+ * ku_dpt_path_add structure is used to store the add path
+ * parameters.
+ */
+struct ku_dpt_path_add {
+    uint8_t                dev_id; /**< dev_id - device ID */
+    enum  ku_dpt_path_type path_type; /**< path_type - the path type */
+    union ku_dpt_path_info path_info; /**< path_info - the path info */
+    uint8_t                is_local; /**< is_local - is it the local device */
+};
+
+/**
+ * ku_dpt_path_modify structure is used to store the modify path
+ * parameters.
+ */
+struct ku_dpt_path_modify {
+    uint8_t                dev_id; /**< dev_id - device ID */
+    enum  ku_dpt_path_type path_type; /**< path_type - the path type */
+};
+
+/**
+ * ku_swid_2_rdq_query structure is used to store the swid to RDQ
+ * query parameters.
+ */
+struct ku_swid_2_rdq_query {
+    int swid; /**< swid - the swid */
+    int rdq; /**< rdq - the RDQ */
+};
+
+/**
+ * sx_dev_cap structure is used to store the device capabilities
+ * info.
+ */
+struct sx_dev_cap {
+    int     log_max_rdq_sz; /**< log_max_rdq_sz - log max RDQ size */
+    int     log_max_sdq_sz; /**< log_max_sdq_sz - log max SDQ size */
+    int     log_max_cq_sz; /**< log_max_cq_sz - log max CQ size */
+    int     log_max_eq_sz; /**< log_max_eq_sz - log max EQ size */
+    uint8_t max_num_rdqs; /**< max_num_rdqs - maximum numer of RDQs */
+    uint8_t max_num_sdqs; /**< max_num_sdqs - maximum numer of SDQs */
+    uint8_t max_num_cqs; /**< max_num_cqs - maximum numer of CQs */
+    uint8_t max_num_eqs; /**< max_num_eqs - maximum numer of EQs */
+    uint8_t max_num_cpu_egress_tcs; /**< max_num_cpu_egress_tcs - maximum numer of CPU egress tclasses */
+    uint8_t max_num_cpu_ingress_tcs; /**< max_num_cpu_ingress_tcs - maximum numer of CPU ingress tclasses */
+    uint8_t max_sg_sq; /**< max_sg_sq - maximum numer scatter gather entries in SDQs */
+    uint8_t max_sg_rq; /**< max_sg_rq - maximum numer scatter gather entries in RDQs */
+    uint8_t dev_id; /**< dev_id - device ID */
+};
+
+/**
+ * ku_read structure is used to store the read
+ * info.
+ */
+struct ku_read {
+    uint64_t length;    /**< length - packet size (if 0 - no more packets) */
+    uint16_t system_port; /**< system_port - system port on which the packet was received */
+    uint16_t trap_id;  /**< trap_id - TrapID(=syndrome id) that captured the packet */
+    uint8_t  is_lag; /**< is_lag - was the packet received from a port which is a LAG */
+    uint8_t  lag_subport; /**< lag_subport - For LAGs this field describe the port index within the LAG */
+    uint8_t  swid; /**< swid - swid */
+    uint32_t original_packet_size; /**<the original size of packet,
+                                    *        if packet wasn't truncated packet_size=original_packet_size*/
+};
+
+/**
+ * loopback_data structure is used to store the data of a sent loopback packet.
+ */
+struct loopback_data {
+    uint16_t trap_id;    /**< trap_id - the trap ID in case of a loopback packet */
+    uint8_t  is_lag;    /**< is the system port member of a lag. */
+    uint8_t  lag_subport;    /**< lag sub-port when applicable */
+};
+
+/**
+ * isx_meta structure is used to store the ISX meta
+ * data of a sent packet.
+ */
+struct isx_meta {
+    uint8_t              etclass; /**< etclass - egress tclass */
+    uint8_t              swid; /**< swid - swid */
+    uint16_t             system_port_mid; /**< system_port_mid - system port or multicast ID */
+    uint8_t              rdq; /**< rdq - RDQ */
+    uint8_t              to_cpu; /**< to_cpu - shoud the packet go to the CPU */
+    uint8_t              lp; /**< lp - should the packet be processed locally */
+    enum ku_pkt_type     type; /**< type - packet type */
+    uint8_t              dev_id; /**< dev_id - device ID */
+    struct loopback_data loopback_data;    /**< loopback_data - loopback packets data */
+    uint8_t              rx_is_router;
+    uint8_t              fid_valid;
+    uint16_t             fid;
+};
+
+/**
+ * ku_raise_trap structure is used to store the raise trap ioctl info
+ */
+struct ku_raise_trap {
+    uint16_t trap_id;    /**< trap_id - the trap ID */
+    uint32_t buffer_size;    /**< buffer_size - the buffer size */
+    void    *buffer_p;     /**< buffer_p - a pointer to the buffer */
+    uint8_t  swid;    /**< swid - switch partition ID */
+    uint16_t sysport;    /**< sysport - system port or LAG ID */
+    uint8_t  is_lag;    /**< is the source logical port member of a lag. */
+    uint8_t  lag_subport;    /**< source lag port when applicable */
+};
+
+/**
+ * ku_write structure is used to store the write
+ * info.
+ */
+struct ku_write {
+    struct isx_meta meta; /**< meta - the ISX meta data */
+    unsigned long   vec_entries; /**< vec_entries - iovec entries number */
+    struct iovec   *iov;   /**< iov - an array of iovec, each one point to one of a packet buffer */
+};
+
+/**
+ * ku_filter_critireas union is used to store the filter critireas
+ * info.
+ */
+union ku_filter_critireas {
+    struct {
+        uint16_t ethtype; /**< ethtype - filter Eth pkts according to ethtype or 0 for ALL */
+        uint64_t dmac;   /**< dmac - filter Eth pkts according to dmac or 0 for ALL */
+        uint32_t emad_tid; /**< emad_tid - filter emads according to tid */
+        uint8_t  from_rp; /**< from_rp - is packet received from router port */
+        uint8_t  from_bridge; /**< from_bridge - is packet received from bridge */
+    } eth; /**< eth - ETH filter critireas */
+    struct  {
+        uint16_t TBD; /**< TBD - TBD */
+    } fc; /**< FC - FC filter critireas */
+    struct {
+        uint32_t qpn; /**< qpn - filter IB pkt according to destination qpn */
+    } ib; /**< ib - IB filter critireas */
+    struct {
+        uint16_t sysport;
+    } dont_care;
+};
+
+/**
+ * ku_l2_tunnel_params structure is used to store L2 tunnel channel parameters.
+ */
+struct ku_l2_tunnel_params {
+    uint64_t dmac; /**< dmac - destination MAC address */
+    uint16_t vid; /** vid - VLAN ID */
+    uint8_t  prio; /** prio - priority */
+};
+
+/**
+ * ku_user_channel_type enumerated type is used to note the possible user channels types.
+ */
+enum ku_user_channel_type {
+    SX_KU_USER_CHANNEL_TYPE_FD,
+    SX_KU_USER_CHANNEL_TYPE_L3_NETDEV,
+    SX_KU_USER_CHANNEL_TYPE_L2_NETDEV,
+    SX_KU_USER_CHANNEL_TYPE_L2_TUNNEL
+};
+
+/**
+ * ku_synd_ioctl structure is used to store the syndrome
+ * info.
+ */
+struct ku_synd_ioctl {
+    uint16_t                   syndrome_num; /**< syndrome_num - syndrome num (0-511, 512=Don't care) */
+    uint8_t                    swid; /**< swid - swid (0-7, or 255=Don't care) */
+    enum  ku_l2_type           type; /**< type - L2 type (ib, eth, fc, dont-care */
+    uint8_t                    is_default; /**< is_default - is default listener (0=false, 1=true) */
+    union ku_filter_critireas  critireas;  /**< critireas - additional filter critireas  */
+    enum ku_user_channel_type  channel_type;  /**< channel_type - channel type */
+    struct ku_l2_tunnel_params l2_tunnel_params;  /**< l2_tunnel_params - L2 tunnel parameters when channel type is L2 tunnel */
+};
+
+/**
+ * tx_resources structure is used to store the tx resources
+ * info.
+ */
+struct tx_resources {
+    uint8_t stclass; /**< stclass - stacking tclass */
+    uint8_t sdq; /**< sdq - SDQ */
+};
+
+/**
+ * rdq_properties structure is used to store the RDQ properties
+ */
+struct rdq_properties {
+    uint8_t  number_of_entries; /**< number_of_entries - number of entries */
+    uint16_t entry_size; /**< entry_size - entry size */
+    uint16_t rdq_weight; /**< rdq_weight - rdq weight */
+};
+
+
+typedef enum sxd_chip_types {
+    SXD_CHIP_TYPE_UNKNOWN = 0,
+    SXD_CHIP_TYPE_SWITCHX_A2 = 1,
+    SXD_CHIP_TYPE_SWITCHX_A1 = 3,
+    SXD_CHIP_TYPE_SWITCHX_A0 = 4,
+    SXD_CHIP_TYPE_SWITCH_IB = 5,
+    SXD_CHIP_TYPE_SPECTRUM = 6,
+	SXD_CHIP_TYPE_SWITCH_IB2 = 7,
+    SXD_CHIP_TYPES_MAX
+} sxd_chip_types_t;
+
+
+enum sxd_chip_rev {
+    SXD_CHIP_REV_UNKNOWN = 0,
+    SXD_CHIP_REV_A0 = 1 << 1,
+    SXD_CHIP_REV_A1 = 1 << 2,
+    SXD_CHIP_REV_A2 = 1 << 3,
+};
+struct sxd_chip_ver {
+    enum sxd_chip_types chip_type;
+    enum sxd_chip_rev   chip_rev;
+};
+
+/**
+ * sx_pci_profile structure is used to store the PCI profile info
+ */
+struct sx_pci_profile {
+    enum pci_profile_e    pci_profile;   /**< pci_profile - PCI profile type */
+    struct tx_resources   tx_prof[NUMBER_OF_SWIDS][NUMBER_OF_ETCLASSES];   /**< tx_prof - tx profile per swid and etclass */
+    struct tx_resources   emad_tx_prof;   /**< emad_tx_prof - tx profile for emads */
+    enum ku_l2_type       swid_type[NUMBER_OF_SWIDS];   /**< swid_type - L2 type for each swid */
+    uint8_t               ipoib_router_port_enable[NUMBER_OF_SWIDS];   /**< ipoib_router_port_enable - For IB swids IPoIB Router Port Enable */
+    uint16_t              max_pkey;   /**< max_pkey - Maximum per port pkey table size (for pkey enforcement) */
+    uint8_t               rdq_count[NUMBER_OF_SWIDS];   /**< rdq_count - RDQ count for each swid */
+    uint8_t               rdq[NUMBER_OF_SWIDS][NUMBER_OF_RDQS];   /**< rdq - RDQ per swid and index */
+    uint8_t               emad_rdq;   /**< emad_rdq - emad RDQ */
+    struct rdq_properties rdq_properties[NUMBER_OF_RDQS];   /**< rdq_properties - properties of each RDQ */
+    uint8_t               cpu_egress_tclass[NUMBER_OF_SDQS];   /**< cpu_egress_tclass - CPU egress tclass per SDQ */
+    uint8_t               dev_id;   /**< dev_id - device ID */
+};
+
+/**
+ * ku_set_rdq_rate_limiter structure is used to store the per RDQ rate limiter info
+ */
+struct ku_set_rdq_rate_limiter {
+    unsigned int time_interval;    /**< time_interval - Time interval in milliseconds between each credit addition phase (shared for all RDQs) */
+    int          rdq;    /**< rdq - RDQ */
+    uint8_t      use_limiter;    /**< use_limiter - Should a rate limiter be used for this RDQ */
+    int          max_credit;    /**< max_credit - The Maximum credit for the RDQ */
+    int          interval_credit;    /**< interval_credit - The credit added in each interval */
+};
+
+/**
+ * ku_set_truncate_params structure is used to store the per RDQ truncate parameters
+ */
+struct ku_set_truncate_params {
+    int      rdq;    /**< rdq - the RDQ */
+    uint8_t  truncate_enable;    /**< truncate_enable - Should packets received on this RDQ be truncated */
+    uint16_t truncate_size;    /**< truncate_size - The Maximum size of the truncated packets */
+};
+
+/**
+ * ku_cr_space_read structure is used to store the CR space read parameters
+ */
+struct ku_cr_space_read {
+    uint8_t      dev_id;    /**< dev_id - Device ID */
+    unsigned int address;    /**< address - Address to read from */
+    uint8_t     *data; /**< data - Buffer to read into */
+    int          size;    /**< size - size in bytes to read */
+};
+
+/**
+ * ku_cr_space_write structure is used to store the CR space write parameters
+ */
+struct ku_cr_space_write {
+    uint8_t      dev_id;    /**< dev_id - Device ID */
+    unsigned int address;    /**< address - Address to write to */
+    uint8_t     *data; /**< data - Data to write */
+    int          size;    /**< size - size in bytes to write */
+};
+
+/**
+ * ku_port_netdev structure is used to store the port netdevice parameters
+ */
+struct ku_port_netdev {
+    char     name[SX_IFNAMSIZ];    /**< name - interface name, will be the name of the net device */
+    uint16_t sysport;    /**< sysport - system port or LAG ID */
+    uint8_t  is_lag;    /**< is_lag - a boolean idicating if the port is a LAG port */
+    uint8_t  swid;    /**< swid - swid (0-7) */
+};
+
+/**
+ * ku_sgmii_smac structure is used to store the SGMII SMAC parameters
+ */
+struct ku_sgmii_smac {
+    uint64_t base_smac;
+    uint8_t  number_of_macs;
+};
+
+/**
+ * ku_get_pci_profile structure is used to store the PCI profile info
+ */
+struct ku_get_pci_profile {
+    uint8_t            dev_id;  /**< dev_id - device ID */
+    enum pci_profile_e pci_profile;  /**< pci_profile - the PCI profile */
+};
+
+/**
+ * ku_synd_query_ioctl structure is used to store the query syndrome ioctl info
+ */
+struct ku_synd_query_ioctl {
+    uint16_t syndrome_num;    /**< syndrome_num - syndrome num (0-511, 512=Don't care)*/
+    uint8_t  is_registered;    /**< is_registered - anyone is registered on syndrome_num? */
+};
+
+/**
+ * ku_query_fw structure is used to store the query FW info
+ */
+struct ku_query_fw {
+    uint64_t fw_rev;    /**< fw_rev - Firmware Revision - Major, Minor, Subminor */
+    uint16_t core_clk;    /**< core_clk - Internal Clock Frequency (in MHz) */
+    uint8_t  dt;    /**< dt - If set, Debug Trace is supported */
+    uint8_t  fw_hour;    /**< fw_hour - Firmware timestamp - hour (displayed as a hexadecimal number) */
+    uint8_t  fw_minutes;    /**< fw_minutes - Firmware timestamp - minutes (displayed as a hexadecimal number) */
+    uint8_t  fw_seconds;    /**< fw_seconds - Firmware timestamp - seconds (displayed as a hexadecimal number) */
+    uint16_t fw_year;    /**< fw_year - Firmware timestamp - year (displayed as a hexadecimal number) */
+    uint8_t  fw_month;    /**< fw_month - Firmware timestamp - month (displayed as a hexadecimal number) */
+    uint8_t  fw_day;    /**< fw_day - Firmware timestamp - day (displayed as a hexadecimal number) */
+    uint8_t  dev_id;    /**< dev_id - device id */
+};
+
+/**
+ * ku_query_board_info structure is used to store the query board info parameters
+ */
+struct ku_query_board_info {
+    uint16_t vsd_vendor_id;    /**< vsd_vendor_id - PCISIG Vendor ID */
+    char     board_id[SX_BOARD_ID_LEN];    /**< board_id - The board id string */
+    uint8_t  dev_id;    /**< dev_id - device id */
+};
+
+/**
+ * ku_operation_tlv structure is used to store the operation TLV parameters
+ */
+struct ku_operation_tlv {
+    uint8_t  type;    /**< type - operation */
+    uint16_t length;    /**< length - Length of TLV on DWORDs (4) */
+    uint8_t  dr;    /**< dr - Direct route */
+    uint8_t  status;    /**< status - Returned status. Must be 0 on Query/Write methods requests and Send/Traps. */
+    uint16_t register_id;    /**< register_id - Register ID within class */
+    uint8_t  r;    /**< r - response (1) / request (0) */
+    uint8_t  method;    /**< method - Method */
+    uint8_t  op_class;    /**< op_class - Class of operation */
+    uint64_t tid;    /**< tid - Transaction ID */
+};
+
+/**
+ * ku_ptys_reg structure is used to store the PTYS register parameters
+ */
+struct ku_ptys_reg {
+    uint8_t              local_port; /**< local_port - local port number */
+    uint8_t              proto_mask; /**< proto_mask - protocol mask */
+    sxd_port_eth_proto_t eth_proto_capability; /**< eth_proto_capability - Etherenet port speed/protocols supported (bitmask) */
+    sxd_port_fc_proto_t  fc_proto_capability; /**< fc_proto_capability - FC port speed/protocols supported (bitmask) */
+    sxd_port_ib_proto_t  ib_proto_capability; /**< ib_proto_capability - IB port speed/protocols supported (bitmask) */
+    uint32_t             eth_proto_admin; /**< eth_proto_admin - Ethernet port speed/protocols bitmask */
+    uint32_t             fc_proto_admin; /**< fc_proto_admin - FC port speed/protocols bitmask */
+    uint32_t             ib_proto_admin; /**< ib_proto_admin - IB port speed/protocols bitmask */
+    uint32_t             eth_proto_oper; /**< eth_proto_oper - Ethernet port speed/protocols bitmask */
+    uint32_t             fc_proto_oper; /**< fc_proto_oper - FC port speed/protocols bitmask */
+    uint32_t             ib_proto_oper; /**< ib_proto_oper - IB port speed/protocols bitmask */
+};
+
+/**
+ * mhsr_health_mode enumerated type is used to note the MHSR health state.
+ */
+enum mhsr_health_mode {
+    MHSR_HEALTH_MODE_FAILURE = 0,
+    MHSR_HEALTH_MODE_RESERVED = 1,
+    MHSR_HEALTH_MODE_NORMAL_OPERATION = 2,
+    MHSR_HEALTH_MODE_DEFAULT_STATE = 3,      /*boot/init*/
+    MHSR_HEALTH_MODE_MIN = MHSR_HEALTH_MODE_FAILURE,
+    MHSR_HEALTH_MODE_MAX = MHSR_HEALTH_MODE_DEFAULT_STATE,
+};
+
+/**
+ * ku_mhsr_reg structure is used to store the MHSR register parameters
+ */
+struct ku_mhsr_reg {
+    enum mhsr_health_mode health; /**< Software health state */
+};
+
+/**
+ * sfgc_flooding_type enumerated type is used to note the SFGC flooding type.
+ */
+enum sfgc_flooding_type {
+    SFGC_FLOODING_TYPE_BROADCAST = 0,
+    SFGC_FLOODING_TYPE_UNICAST = 1,
+    SFGC_FLOODING_TYPE_MULTICAST_IPV4 = 2,
+    SFGC_FLOODING_TYPE_MULTICAST_IPV6 = 3,
+    SFGC_FLOODING_TYPE_MULTICAST_NON_IP = 5,
+    SFGC_FLOODING_TYPE_MULTICAST_IPV4_LINK_LOCAL = 6,
+    SFGC_FLOODING_TYPE_MULTICAST_IPV6_ALL_HOST = 7,
+};
+
+/**
+ * sfgc_fid_bridge_type enumerated type is used to note the SFGC bridge type.
+ */
+enum sfgc_fid_bridge_type {
+    SFGC_BRIDGE_TYPE_FID = 0,
+    SFGC_BRIDGE_TYPE_VFID = 1,
+    SFGC_BRIDGE_TYPE_MAX,
+};
+
+/**
+ * profile_flood_mode_type enumerated type is used to note flood mode in profile
+ */
+enum profile_flood_mode_type {
+    PROFILE_FLOOD_MODE_TYPE_SINGLE_ENTRY = 0,
+    PROFILE_FLOOD_MODE_TYPE_USE_FID = 1,
+    PROFILE_FLOOD_MODE_TYPE_USE_VID = 2,
+    PROFILE_FLOOD_MODE_TYPE_MIXED_MODE = 3,
+    PROFILE_FLOOD_MODE_TYPE_MAX,
+};
+
+/**
+ * sfgc_flooding_type enumerated type is used to note the SFGC flooding type.
+ */
+enum ku_flood_table_type {
+    SFGC_TABLE_TYPE_ANY = 0,
+    SFGC_TABLE_TYPE_PER_VID = 1,
+    SFGC_TABLE_TYPE_SINGLE_ENTRY = 2,
+    SFGC_TABLE_TYPE_FID_OFFSET = 3,
+    SFGC_TABLE_TYPE_FID = 4,
+};
+
+/**
+ * fgc_fid_flooding_mode enumerated type is used to
+ * note the SFGC fid flooding mode.
+ */
+enum sfgc_fid_flooding_mode {
+    SFGC_FID_FLOODING_MODE_NONE = 0,
+    SFGC_FID_FLOODING_MODE_PGI_OFFEST = 1,
+    SFGC_FID_FLOODING_MODE_PGI_MID_OFFSET = 2,
+};
+
+/**
+ * ku_sftr_reg structure is used to store the SFTR register
+ * parameters
+ *
+ */
+struct ku_sftr_reg {
+    uint8_t                  swid; /** swid - Switch partition ID */
+    uint8_t                  flood_table; /**F-table index per type per switch*/
+    uint16_t                 index;
+    enum ku_flood_table_type table_type;
+    uint16_t                 range; /**Range of entries to update*/
+    uint16_t                 ports_bitmap[0x000000FF + 1];
+    uint16_t                 mask_bitmap[0x000000FF + 1];
+};
+
+
+/**
+ * ku_sfgc_reg structure is used to store the SFGC register parameters
+ */
+struct ku_sfgc_reg {
+    enum sfgc_flooding_type   type;
+    enum sfgc_fid_bridge_type bridge_type;
+    enum ku_flood_table_type  table_type;
+    uint8_t                   flood_table;
+    uint16_t                  mid;
+};
+
+/**
+ * fgc_fid_flooding_mode enumerated type is used to
+ * note the SFGC fid flooding mode.
+ */
+enum svfa_bridge_type {
+    SVFA_BRIDGE_TYPE_802_1Q = 0,
+    SVFA_BRIDGE_TYPE_VFID = 1,
+};
+
+/**
+ * ku_svfa_reg structure is used to store the SVFA register parameters
+ */
+struct ku_svfa_reg {
+    uint8_t  swid; /** swid - Switch partition ID */
+    uint8_t  local_port; /** local_port - Local port number */
+    uint8_t  bridge_type; /** bridge_type - Bridge Type */
+    uint8_t  v; /** v - Valid */
+    uint16_t fid; /** fid - FDB ID */
+    uint16_t vid; /** vid - VLAN ID */
+    sxd_counter_set_t counter_set;
+    uint8_t trap_action;
+    uint16_t trap_id;
+};
+
+/**
+ * ku_svpe_reg structure is used to store the SVPE register parameters
+ */
+struct ku_svpe_reg {
+    uint8_t local_port; /** local_port - Local port number */
+    uint8_t vp_en;      /** vp_en - Virtual Port Enable */
+};
+
+/**
+ * svpe_virtual_port_enable enumerated type is used to note
+ * the virtual port enable.
+ */
+enum svpe_virtual_port_enable {
+    SVPE_VIRTUAL_PORT_802_1Q = 0,
+    SVPE_VIRTUAL_PORT_802_1D = 1,
+};
+
+/**
+ * ku_sfmr_reg structure is used to store the SFMR register parameters
+ */
+struct ku_sfmr_reg {
+    uint8_t  op;               /** op - Operation */
+    uint16_t fid;              /** fid - Filtering Identifier */
+    uint8_t  vtep_id;          /** vtep_id - VTEP Index */
+    uint16_t fid_offset;       /** fid_offset - FID Offset */
+    uint32_t vtfp;              /** vtfp - Tunnel Flood Pointer Valid */
+    uint32_t tunnel_flood_ptr; /** tunnel_flood_ptr - Head end replication */
+    uint32_t vv;                /** vv - VNI Valid */
+    uint32_t vni;              /** vni - VXLAN network identifier */
+};
+
+/**
+ * sx_fdb_flush_type enumerated type is used to note FDB
+ * flush type.
+ */
+enum fdb_flush_type {
+    FDB_FLUSH_TYPE_SWID = 0,
+    FDB_FLUSH_TYPE_FID = 1,
+    FDB_FLUSH_TYPE_PORT = 2,
+    FDB_FLUSH_TYPE_PORT_FID = 3,
+    FDB_FLUSH_TYPE_LAG = 4,
+    FDB_FLUSH_TYPE_LAG_FID = 5
+};
+
+/**
+ * ku_sfdf_reg structure is used to store the SFDF register parameters
+ */
+struct ku_sfdf_reg {
+    uint8_t             swid; /** swid - Switch partition ID */
+    enum fdb_flush_type flush_type; /** flush_type - Flush type */
+    uint16_t            fid; /** fid - FDB ID */
+    union {
+        uint16_t system_port;
+        uint16_t lag_id;
+    } lag_port; /** lag_port - System port or lag id*/
+};
+
+/**
+ * ku_slecr_reg structure is used to store the SLECR register parameters
+ */
+struct ku_slecr_reg {
+    uint8_t swid; /** swid - Switch partition ID */
+    uint8_t independent_learning; /** independent_learning - Independent Learning */
+    uint8_t roaming_enable; /** roaming_enable - Roaming Enable */
+};
+
+/**
+ * ku_spmlr_reg structure is used to store the SPMLR register parameters
+ */
+struct ku_spmlr_reg {
+    uint8_t local_port; /**< local_port - local port number */
+    uint8_t sub_port; /** sub_port - Virtual port within the physical port */
+    uint8_t learn_enable; /** learn_enable - Enable learning on this port */
+};
+
+/**
+ * ku_spfsr_reg structure is used to store the SPFSR register parameters
+ */
+struct ku_spfsr_reg {
+    uint8_t local_port; /** local_port - local port number */
+    uint8_t security;   /** security   - Enable security checks on this port */
+};
+
+/**
+ * ku_smid_reg structure is used to store the SMID register parameters
+ */
+struct ku_smid_reg {
+    uint8_t  swid; /** swid - Switch partition ID */
+    uint16_t mid; /** mid - Multicast Identifier */
+    uint16_t ports_bitmap[0x000000FF + 1]; /** ports_bitmap - Local port <i> sub port <j> membership */
+    uint16_t mask_bitmap[0x000000FF + 1]; /** mask_bitmap - Local port <i> sub port <j> mask */
+};
+
+/**
+ * ku_smpu_reg structure is used to store the SMPU register parameters
+ */
+struct ku_smpu_reg {
+    uint8_t  swid; /** swid - Switch partition ID */
+    uint8_t  local_port; /** local_port - Local port to send the packet */
+    uint8_t  op; /** Operation - 00 - Set bit, 01 - Clear bit */
+    uint8_t  size; /** size - Number of valid MIDs to be updated. */
+    uint16_t mid[255]; /** MID list to where the relevant
+                        *   local_port bit indication is to be set / cleared. */
+};
+
+/**
+ * ku_svmlr_reg structure is used to store the SVMLR register parameters
+ */
+struct ku_svmlr_reg {
+    uint8_t  swid; /** swid - Switch partition ID */
+    uint16_t vid; /** vid - VLAN ID */
+    uint8_t  learn_enable; /** learn_enable - Enable learning on this VLAN */
+};
+
+/**
+ * SPVMLR_MAX_RECORDS define maximum records supported by one SPVMLR reg access
+ */
+#define SPVMLR_MAX_RECORDS  255
+
+/**
+ * ku_spvmlr_reg structure is used to store the SPVMLR register parameters
+ */
+typedef struct spvmlr_vlan_data {
+    uint16_t vid;           /** vid - VLAN ID */
+    uint8_t  learn_enable; /** learn_enable - Enable learning on this VLAN */
+} spvmlr_vlan_data_t;
+
+struct ku_spvmlr_reg {
+    uint8_t  local_port;   /** local_port - Local port to send the packet */
+    uint8_t num_rec;       /** number of VLAN learm_mode records    */
+    spvmlr_vlan_data_t vlan_data[SPVMLR_MAX_RECORDS];
+};
+
+/**
+ * sxd_flow_counter_op enumerated type is used to note the
+ * PFCA operation.
+ */
+enum sxd_flow_counter_op {
+    SXD_PFCA_OP_NOP = 0,
+    SXD_PFCA_OP_ALLOCATE = 1,
+    SXD_PFCA_OP_TEST = 2,
+    SXD_PFCA_OP_FREE = 3,
+};
+
+
+/**
+ * spgt_operation enumerated type is used to note the SPGT operation.
+ */
+enum spgt_operation {
+    SPGT_OPERATION_ADD = 0,
+    SPGT_OPERATION_DELETE = 1,
+    SPGT_OPERATION_EDIT = 2,
+};
+
+/**
+ * ku_spgt_reg structure is used to store the SPGT register parameters
+ */
+struct ku_spgt_reg {
+    enum spgt_operation operation;
+    uint16_t            pgi;
+    uint16_t            ports_bitmap[0x000000FF + 1];
+    uint16_t            mask_bitmap[0x000000FF + 1];
+};
+
+/**
+ * sfn_type enumerated type is used to note the SFN data type.
+ */
+enum sfn_type {
+    SFN_TYPE_LEARNT_MAC = 5,
+    SFN_TYPE_LEARNT_MAC_LAG = 6,
+    SFN_TYPE_AGED_MAC = 7,
+    SFN_TYPE_AGED_MAC_LAG = 8,
+};
+
+/**
+ * sfn_learnt_mac_data structure is used to store learnt MAC data.
+ */
+struct sfn_learnt_mac_data {
+    struct sx_ether_addr mac; /**< mac - Base MAC address */
+    uint8_t              sub_port;
+    uint16_t             fid;
+    uint16_t             system_port;
+};
+
+/**
+ * sfn_learnt_mac_lag_data structure is used to store learnt MAC LAG data.
+ */
+struct sfn_learnt_mac_lag_data {
+    struct sx_ether_addr mac; /**< mac - Base MAC address */
+    uint8_t              sub_port;
+    uint16_t             fid;
+    uint16_t             lag_id;
+};
+
+/**
+ * sfn_aged_mac_data structure is used to store aged out MAC data.
+ */
+struct sfn_aged_mac_data {
+    struct sx_ether_addr mac; /**< mac - Base MAC address */
+    uint8_t              sub_port;
+    uint16_t             fid;
+    uint16_t             system_port;
+};
+
+/**
+ * sfn_aged_mac_lag_data structure is used to store aged out MAC LAG data.
+ */
+struct sfn_aged_mac_lag_data {
+    struct sx_ether_addr mac; /**< mac - Base MAC address */
+    uint8_t              sub_port;
+    uint16_t             fid;
+    uint16_t             lag_id;
+};
+
+/**
+ * sfn_record_data structure is used to store one SFN record data.
+ */
+struct sfn_record_data {
+    enum sfn_type sfn_type;
+    union {
+        struct sfn_learnt_mac_data     lrnt_mac;
+        struct sfn_learnt_mac_lag_data lrnt_mac_lag;
+        struct sfn_aged_mac_data       aged_mac;
+        struct sfn_aged_mac_lag_data   aged_mac_lag;
+    } sx_sfn_type;
+};
+
+/**
+ * SFN_MAX_RECORDS define maximum records supported by one SFN
+ */
+#define SFN_MAX_RECORDS 64
+
+/**
+ * ku_sfn_reg structure is used to store the SFN register parameters
+ */
+struct ku_sfn_reg {
+    uint8_t                swid;
+    uint8_t                num_records;
+    struct sfn_record_data records[SFN_MAX_RECORDS];
+};
+
+/**
+ * SFD_MAX_RECORDS define maximum records supported by one SFD
+ */
+#define SFD_MAX_RECORDS SFN_MAX_RECORDS
+
+/**
+ * sfd_type_t enumerated type is used to note the SFD data type.
+ */
+enum sfd_type {
+    SFD_TYPE_UNICAST = 0,
+    SFD_TYPE_UNICAST_LAG = 1,
+    SFD_TYPE_MULTICAST = 2,
+};
+
+/**
+ * sfd_operation enumerated type is used to note the SFD operation.
+ */
+enum sfd_operation {
+    SFD_OPERATION_DUMP_FDB = 0,
+    SFD_OPERATION_QUERY = 1,
+    SFD_OPERATION_TEST = 0,
+    SFD_OPERATION_ADD = 1,
+    SFD_OPERATION_DELETE = 2,
+};
+
+/**
+ * sfd_policy enumerated type is used to note the SFD data replace policy.
+ */
+enum sfd_policy {
+    SFD_POLICY_STATIC = 0,
+    SFD_POLICY_DYNAMIC_REMOTE = 1,
+    SFD_POLICY_DYNAMIC_AGEABLE = 3,
+    SFD_POLICY_INVALID = -1,
+};
+
+/**
+ * sfd_action enumerated type is used to note the SFD frame
+ * action.
+ */
+enum sfd_action {
+    SFD_ACTION_FORWARD_ONLY = 0,
+    SFD_ACTION_FORWARD_AND_TRAP = 1,
+    SFD_ACTION_TRAP_ONLY = 2,
+    SFD_ACTION_FORWARD_TO_IP_ROUTER = 3,
+    SFD_ACTION_FORWARD_TO_FCF = 4,
+    SFD_ACTION_DISCARD = 15,
+    SFD_ACTION_INVALID = -1,
+};
+
+/**
+ * sfd_unicast_data structure is used to store
+ * unicast data.
+ */
+struct sfd_unicast_data {
+    enum sfd_policy      policy;
+    struct sx_ether_addr mac; /**< mac - Base MAC address */
+    uint8_t              sub_port;
+    union {
+        uint16_t fid;
+        uint16_t vid;
+    } fid_vid_type;
+    enum sfd_action action;
+    uint16_t        system_port;
+};
+
+/**
+ * sfd_unicast_lag_data structure is used to store
+ * unicast LAG data.
+ */
+struct sfd_unicast_lag_data {
+    enum sfd_policy      policy;
+    struct sx_ether_addr mac; /**< mac - Base MAC address */
+    uint8_t              sub_port;
+    union {
+        uint16_t fid;
+        uint16_t vid;
+    } fid_vid_type;
+    enum sfd_action action;
+    uint16_t        lag_id;
+};
+
+/**
+ * sfd_multicast_data structure is used to store multicast data.
+ */
+struct sfd_multicast_data {
+    struct sx_ether_addr mac; /**< mac - Base MAC address */
+    uint16_t             pgi;
+    uint16_t             vid;
+    uint8_t              action;
+    uint16_t             mid;
+};
+
+/**
+ * ku_sfd_reg structure is used to store the SFD register parameters
+ */
+struct ku_sfd_reg {
+    uint8_t            swid;
+    enum sfd_operation operation;
+    uint32_t           record_locator;
+    enum sfd_type      sfd_type[SFD_MAX_RECORDS];
+    uint8_t            num_records;
+    union {
+        struct sfd_unicast_data     uc;
+        struct sfd_unicast_lag_data uc_lag;
+        struct sfd_multicast_data   mc;
+    } sfd_data_type[SFD_MAX_RECORDS];
+};
+
+/**
+ * ku_sfdat_reg structure is used to store the SFDAT register parameters
+ */
+struct ku_sfdat_reg {
+    uint8_t  swid; /**< swid - Switch partition ID */
+    uint32_t age_time; /**< age_time - Ageing time in seconds */
+};
+
+/**
+ * ku_spaft_reg structure is used to store the SPAFT register parameters
+ */
+struct ku_spaft_reg {
+    uint8_t local_port; /**< local_port - Chip local port ID */
+    uint8_t sub_port; /**< sub_port - VEPA channel on Local Port */
+    uint8_t allow_untagged; /**< allow_untagged - When set, untagged frames on the ingress are admitted */
+    uint8_t allow_priotagged; /**< allow_priotagged - When set, priority tagged frames on the ingress are admitted */
+    uint8_t allow_tagged; /**< allow_tagged - When set, tagged frames on the ingress are admitted */
+};
+
+/**
+ * spvm_vlan_data structure is used to store SPVM vlan data
+ * vlan data.
+ */
+struct spvm_vlan_data {
+    uint8_t  ingress_membership; /**< ingress_membership - Ingress membership in VLAN ID vid<i> */
+    uint8_t  egress_membership; /**< egress_membership - Egress membership in VLAN ID vid<i> */
+    uint8_t  untagged_membership; /**< untagged_membership - Untagged - port is an untagged member - egress transmission uses untagged frames on VID<i> */
+    uint16_t vid; /** VLAN ID to be added/removed from port or for querying */
+};
+
+/**
+ * ku_spvm_reg structure is used to store the SPVM register parameters
+ */
+struct ku_spvm_reg {
+    uint8_t               prio_tagged; /** untagged packets are being transmitted untagged OR priority-tagged */
+    uint8_t               local_port; /**< local_port - Chip local port ID */
+    uint8_t               sub_port; /**< sub_port - VEPA channel on Local Port */
+    uint8_t               num_vlans; /**< num_vlans - Number of records to update. Each record contains: i, e, u, vid */
+    struct spvm_vlan_data vlan_data[256]; /**< vlan_data - VLAN data */
+};
+
+/**
+ * ku_spvid_reg structure is used to store the SPVID register parameters
+ */
+struct ku_spvid_reg {
+    uint8_t  local_port; /**< local_port - Chip local port ID */
+    uint8_t  sub_port; /**< sub_port - VEPA channel on Local Port */
+    uint16_t port_default_vid; /**< port_default_vid - Port default VID (default PVID is 001h) */
+};
+
+/**
+ * ku_spvtr_reg structure is used to store the SPVTR register parameters
+ */
+struct ku_spvtr_reg {
+    uint8_t sub_port; /**< sub_port - VEPA channel on Local Port */
+    uint8_t local_port; /**< local_port - Chip local port ID */
+    uint8_t ipprio_enable; /**< ipprio_enable - set to 1 to configure ipprio_mode */
+    uint8_t ipvid_enable;  /**< ipvid_enable - set to 1 to configure ipvid_mode */
+    uint8_t epvid_enable;  /**< epvid_enable - set to 1 to configure epvid_mode */
+    uint8_t ipprio_mode; /**< ipprio_mode - Ingress Port Priority Mode */
+    uint8_t epvid_mode; /**< epvid_mode - Egress Port VLAN-ID Mode */
+    uint8_t ipvid_mode; /**< ipvid_mode - Ingress Port VLAN-ID Mode */
+};
+
+/**
+ * ku_pifr_reg structure is used to store the PIFR register parameters
+ */
+struct ku_pifr_reg {
+    uint8_t local_port; /**< local_port - Chip local port ID */
+    uint8_t ports_bitmap[0x000000FF + 1]; /**< 1 = filter packets coming from port[i], 0 = don't filter */
+    uint8_t mask_bitmap[0x000000FF + 1];  /**< 1 = set port[i], 0 = don't set */
+};
+
+/**
+ * ku_sspr_reg structure is used to store the SSPR register parameters
+ */
+struct ku_sspr_reg {
+    uint8_t  is_master; /**< is_master - if set then this record describes the Master System Port */
+    uint8_t  local_port; /**< local_port - Chip local port ID */
+    uint8_t  sub_port; /**< sub_port - VEPA channel on Local Port */
+    uint16_t system_port; /**< system_port - Multi SwitchX environment port ID */
+};
+
+/**
+ * ku_scar_reg structure is used to store the SCAR register parameters
+ */
+struct ku_scar_reg {
+    uint8_t log2_fdb_size; /**< log2_fdb_size - Capability: Log (base2) of the maximum FDB entries that are supported by the device */
+};
+
+/**
+ * ku_sgcr_reg structure is used to store the SGCR register parameters
+ */
+struct ku_sgcr_reg {
+    uint8_t llb; /**< llb - Link Local Broadcast  */
+};
+enum spms_mstp_state {
+    SPMS_MSTP_STATE_DISCARDING = 1,
+    SPMS_MSTP_STATE_LEARNING = 2,
+    SPMS_MSTP_STATE_FORWARDING = 3,
+};
+
+/**
+ * ku_spms_reg structure is used to store the SPMS register parameters
+ */
+struct ku_spms_reg {
+    uint8_t              local_port; /**< local_port - local port number */
+    enum spms_mstp_state state[4096]; /**< state - MSTP/RSTP State on VID<i> */
+};
+struct mpat_encap_local_eth {
+    uint8_t tclass;
+};
+struct mpat_encap_remote_eth_vlan {
+    uint8_t  tclass;
+    uint16_t vid;
+    uint8_t  pcp;
+    uint8_t  dei;
+    uint8_t  vlan_ethertype_id;
+};
+struct mpat_encap_remote_eth_l2 {
+    uint8_t  swid;
+    uint8_t  tclass;
+    uint16_t vid;
+    uint8_t  pcp;
+    uint8_t  tp;
+    uint8_t  mac[6];
+    uint8_t  dei;
+    uint8_t  vlan_ethertype_id;
+    uint8_t  version;
+    uint8_t  dpa;
+};
+struct mpat_encap_local_ib {
+    uint8_t vl;
+};
+struct mpat_encap_remote_ib {
+    uint8_t vl;
+    uint8_t slid;
+    uint8_t dlid;
+    uint8_t sl;
+};
+struct mpat_encap_remote_eth_l3 {
+    uint8_t  swid;
+    uint8_t  tclass;
+    uint16_t vid;
+    uint8_t  pcp;
+    uint8_t  tp;
+    uint8_t  mac[6];
+    uint8_t  dei;
+    uint8_t  vlan_ethertype_id;
+    uint8_t  version;
+    uint8_t  dpa;
+    uint8_t  protocol;
+    uint8_t  smac[6];
+    uint32_t dip[4];
+    uint32_t sip[4];
+    uint8_t  dscp;
+    uint8_t  ecn;
+    uint8_t  ttl;
+};
+
+/**
+ * SPAN session type format
+ */
+union mpat_encapsulation {
+    struct mpat_encap_local_eth       local_eth;
+    struct mpat_encap_remote_eth_vlan remote_eth_vlan;
+    struct mpat_encap_remote_eth_l2   remote_eth_l2;
+    struct mpat_encap_local_ib        local_ib;
+    struct mpat_encap_remote_ib       remote_ib;
+    struct mpat_encap_remote_eth_l3   remote_eth_l3;
+};
+
+/**
+ * ku_mpat_reg structure is used to store the MPAT register
+ * parameters
+ */
+struct ku_mpat_reg {
+    uint8_t                  pa_id; /**< pa_id - port analyzer id */
+    uint8_t                  mngr_type; /**<  mngr_type - Manager Type*/
+    uint16_t                 system_port; /**< system_port  */
+    uint8_t                  e; /**<  e - indicating the Port Analyzer it enabled */
+    uint8_t                  c; /**<  c - clear counters */
+    uint8_t                  qos; /**< qos - quality of service mode */
+    uint8_t                  be; /**< be- Best Effort traffic handlings */
+    uint8_t                  tr; /**<  tr - truncate the packet to truncate size */
+    uint8_t                  stclass; /**< stclass - Stacking TClass */
+    uint8_t                  span_type; /**<  SPAN Type */
+    uint16_t                 truncation_size; /**< truncation_size - granularity 4 */
+    union mpat_encapsulation encap; /**<  Remote SPAN encapsulation */
+    uint64_t                 buffer_drop; /**< packet drops due to buffer size */
+    uint64_t                 be_drop; /**< packet drops due to best effort */
+    uint64_t                 wred_drop; /**< packet drops due to WRED */
+};
+
+/**
+ * ku_mpar_reg structure is used to store the MPAR register
+ * parameters
+ */
+struct ku_mpar_reg {
+    uint8_t mngr_type; /**<  mngr_type - Manager Type*/
+    uint8_t local_port; /**< local_port   */
+    uint8_t sub_port; /**< sub_port   */
+    uint8_t i_e;    /**<  Ingress / Egress */
+    uint8_t enable; /**<  indicating the Port Analyzer enable*/
+    uint8_t pa_id;  /**< pa_id - port analyzer id */
+};
+
+/**
+ * ku_slcor_reg structure is used to store the SLCOR register parameters
+ */
+struct ku_slcor_reg {
+    uint8_t  collector; /**< collector - Collector configuration */
+    uint8_t  local_port; /**< local_port - local port number */
+    uint16_t lag_id; /**< lag_id - LAG Identifier. Index into the LAG Descriptor table */
+    uint8_t  port_index; /**< port index - port index in the LAG list.*/
+};
+
+/**
+ * ku_slcr_reg structure is used to store the SLCR register parameters
+ */
+struct ku_slcr_reg {
+    uint8_t  sh; /**< sh - Symmetric Hash */
+    uint8_t  hash_type; /**< hash_type - Hash Type */
+    uint32_t hash_configuration; /**< hash_configuration - LAG Hashing Configuration */
+    uint32_t seed; /**< seed - LAG seed value */
+};
+enum sldr_operation {
+    SLDR_OPERATION_CREATE_LAG = 0,
+    SLDR_OPERATION_DESTROY_LAG = 1,
+    SLDR_OPERATION_ADD_PORT_LIST = 2,
+    SLDR_OPERATION_DEL_PORT_LIST = 3,
+    SLDR_OPERATION_LAG_REDIRECT = 4,
+};
+
+/**
+ * ku_sldr_reg structure is used to store the SLDR register parameters
+ */
+struct ku_sldr_reg {
+    enum sldr_operation operation; /**< operation - Operation */
+    uint16_t            lag_id; /**< lag_id - LAG Identifier, the lag_id is the index into the LAG Descriptor table */
+    uint16_t            dst_lag; /**< dst_lag - In Redirection, the destenation LAG which lag_id points to */
+    uint32_t            num_ports; /**< num_ports - The number of member ports of the LAG */
+    uint16_t            ports[16]; /**< ports - Final destination of the packet */
+};
+
+/**
+ * ku_qprt_reg structure is used to store the QPRT register parameters
+ */
+struct ku_qprt_reg {
+    uint8_t local_port; /**< local_port - local port number */
+    uint8_t dei; /**< dei - DEI */
+    uint8_t prio; /**< prio - PCP */
+    uint8_t color; /**< color */
+    uint8_t rprio; /**< rprio - Regenerated priority for received priority=<prio> */
+};
+
+/**
+ * ku_qpdp_reg structure is used to store the QPDP register parameters
+ */
+struct ku_qpdp_reg {
+    uint8_t local_port; /**< local_port - local port number */
+    uint8_t sub_port; /**< sub_port - Virtual port within the physical port. */
+    uint8_t color; /**< color - 0-green, 1-yellow, 2-red */
+    uint8_t default_priority; /**< default_priority - Default Port priority (default 0) */
+};
+
+/**
+ * ku_qsptc_reg structure is used to store the QSPTC register parameters
+ */
+struct ku_qsptc_reg {
+    uint8_t local_iport; /**< local_iport - local ingress port number (must be stucking port) */
+    uint8_t local_eport; /**< local_eport - local egress port number (must be stucking port) */
+    uint8_t itclass; /**< itclass - Received tclass */
+    uint8_t tclass; /**< tclass - Regenerated stacking traffic class for received packet on stacking port */
+};
+
+/**
+ * ku_qtct_reg structure is used to store the QTCT register parameters
+ */
+struct ku_qtct_reg {
+    uint8_t local_port; /**< local_port - local port number */
+    uint8_t sub_port; /**< sub_port - Virtual port within the physical port. */
+    uint8_t priority; /**< priority - Priority */
+    uint8_t traffic_class; /**< traffic_class - Traffic class used for priority=<prio> */
+};
+
+/**
+ * ku_cnct_reg structure is used to store the CNCT register parameters
+ */
+struct ku_cnct_reg {
+    uint8_t local_port; /**< local_port - local port number */
+    uint8_t prio; /**< priority - Priority */
+    uint8_t enable_congestion_notif_valid; /**< enable write operation into enable_congestion_notif */
+    uint8_t enable_congestion_notif; /**< disable/enable congestion notification for this egress port */
+    uint8_t keep_cn_tags_valid; /**< enable write operation into keep_cn_tags */
+    uint8_t keep_cn_tags; /**< keep/remove CNTags on frames egressing to this port */
+};
+
+
+/**
+ * ku_cpid_reg structure is used to store the CPID register parameters
+ */
+struct ku_cpid_reg {
+    uint8_t  local_port; /**< local_port - local port number */
+    uint8_t  prio; /**< priority - Priority */
+    uint64_t cpid; /**< IEEE 802.1Qau Congestion Point Identifier */
+};
+
+/**
+ * sxd_cpcs_operation enumerated type is used to note the
+ * CPCS operation type.
+ */
+enum sxd_cpcs_operation {
+    SXD_CPCS_OP_SET = 0,
+    SXD_CPCS_OP_GET = 1,
+};
+
+
+/**
+ * ku_cpcs_reg structure is used to store the CPCS register parameters
+ */
+struct ku_cpcs_reg {
+    enum sxd_cpcs_operation operation;
+    uint8_t                 traffic_class; /**< Traffic Class */
+    uint32_t                set_point; /**< The set point for the queue, */
+    int32_t                 cp_weight; /**< The weight (cpW) of the congestion point */
+    uint32_t                cp_sample_base; /**< The minimum number of octets to enqueue in the CPs queue between CNM PDU transmissions */
+    uint32_t                cp_min_header_octets; /**< The minimum number of octets that the CP is to return in the Encapsulated MSDU field ( */
+};
+
+
+/**
+ * ku_cnmc_reg structure is used to store the CNMC register parameters
+ */
+struct ku_cnmc_reg {
+    uint8_t prio; /**< priority - Priority */
+};
+
+/**
+ * ets_tc_conf structure is used to store the QETCR register per tc parameters
+ */
+struct ets_tc_conf {
+    uint8_t group_update; /**< group_update - Group Update */
+    uint8_t bw_update; /**< bw_update - Bandwidth Allocation Update */
+    uint8_t rate_update; /**< rate_update - Rate Limit Update */
+    uint8_t group; /**< group - TCG assigned to traffic class tc */
+    uint8_t bw_allocation; /**< bw_allocation - The percentage of bandwidth guaranteed to traffic class tc within its TCG */
+    uint8_t max_bw_units; /**< max_bw_units - The maximal bandwidth allowed for the use Ttraffic class tc */
+    uint8_t max_bw_value; /**< max_bw_value - The maximal bandwidth allowed for the use Ttraffic class tc */
+};
+
+/**
+ * ets_global_shaper_conf structure is used to store the QETCR register global shaper parameters
+ */
+struct ets_global_shaper_conf {
+    uint8_t rate_update; /**< rate_update - Rate Limit Update */
+    uint8_t max_bw_units; /**< max_bw_units - The maximal bandwidth units for the use of Global Shaper */
+    uint8_t max_bw_value; /**< max_bw_value - The maximal bandwidth value for the use of Global Shaper */
+};
+
+/**
+ * ku_qegcs_reg structure is used to store the QEGCS register parameters
+ */
+struct ku_qegcs_reg {
+    uint8_t local_port; /**< local_port - local port number */
+    uint8_t group_0_7_arbiter;
+    uint8_t group_15_arbiter;
+    uint8_t global_arbiter;
+};
+
+/**
+ * ku_qetcr_reg structure is used to store the QETCR register parameters
+ */
+struct ku_qetcr_reg {
+    uint8_t                       local_port; /**< local_port - local port number */
+    struct ets_tc_conf            tc_conf[8]; /**< tc_conf - Per-tclass configuration */
+    struct ets_global_shaper_conf global_shaper; /**< global_shaper - Global Shaper configuration */
+};
+
+/**
+ * ku_qpfcr_reg structure is used to store the QPFCR register parameters
+ */
+struct ku_qpfcr_reg {
+    uint8_t local_port;
+    uint8_t traffic_class;
+    uint8_t traffic_class_group;
+    uint8_t min_threshold;
+    uint8_t max_threshold;
+};
+
+/**
+ * ku_qdpm_reg structure is used to store the QDPM register parameters
+ */
+struct ku_qdpm_reg {
+    uint8_t dscp_update[DSCP_CODES_NUMBER];    /**< dscp_update - whether to update this DSCP mapping in HW */
+    uint8_t color[DSCP_CODES_NUMBER]; /**< color mapping per DSCP value */
+    uint8_t priority[DSCP_CODES_NUMBER];       /**< priority mapping per DSCP value - Priority */
+};
+
+/**
+ * ku_qpcr_reg sturcture is used to store the QPCR register parameters
+ */
+struct ku_qpcr_reg {
+    uint8_t port; /**< port - Policer port number */
+    uint8_t global; /**< global -   1 - Global policer configuration
+                     *  0 - Per port policer configuration */
+    uint16_t pid; /**< policer_id - Policer ID */
+    uint8_t clear_counter; /**<clear_counter - 1-clear, 0-don't clear */
+    uint8_t add_counter; /**>add_counter - 1 - add violate_count to the counter*/
+    uint8_t color_aware; /**< color_aware - 1-color-aware, 0-no color-aware */
+    uint8_t use_bytes; /**< use_bytes -     1 - Meter based on bytes/sec
+                        0 - Meter based on packets/sec */
+    uint8_t ir_units; /**< cir and eir units. Supported devices: SwitchEN
+                       *    0 - 10^6
+                       *    1 - 10^3 */
+    uint8_t type; /**< 00 - single-rate three color marking
+                   *   01 - single-rate dual color
+                   *   10 - dual-rate three color */
+    uint8_t  mode; /** < Operation Mode 00 - Policer 01 - Packet Sampling*/
+    uint8_t  committed_burst_size; /**< committed_burst_size - Committed Burst Size */
+    uint8_t  extended_burst_size; /**< extended_burst_size - Extended Burst Size */
+    uint32_t committed_information_rate; /**< committed_information_rate - Committed Information Rate */
+    uint32_t excess_information_rate; /**< excess_information_rate - Excess Information Rate */
+    uint8_t  exceed_action; /**< exceed_action - Action for exceed packets */
+    uint8_t  violate_action; /**< violate_action - Action for violate packets */
+    uint64_t violate_count; /**< violate_count - count the num of times violate_actions happened */
+};
+
+/**
+ * ku_qpbr_reg sturcture is used to store the QPBR register parameters
+ */
+struct ku_qpbr_reg {
+    uint8_t operation; /**< policer_operation - Operation */
+    uint8_t port; /**< port - Policer port number */
+    uint8_t global; /**< global -   1 - Global policer configuration
+                     *   0 - Per port policer configuration */
+    uint16_t pid; /**< pid - Policer ID */
+    uint8_t  unicast; /**< unicast - Meter ingress unicast packets */
+    uint8_t  multicast; /**< multicast - Meter ingress multicast packets */
+    uint8_t  broadcast; /**< broadcast - Meter ingress broadcast packets */
+    uint8_t  unknown_unicast; /**< unknown_unicast - Meter ingress unknown unicast packets */
+    uint8_t  unregistered_multicast; /**< unregistered_multicast - Meter ingress unregistered multicast packets */
+};
+
+/**
+ * ku_plbf_reg structure is used to store the PLBF register
+ * parameters
+ */
+struct ku_plbf_reg {
+    uint8_t port; /**< local_port - local port number */
+    uint8_t lbf_mode; /**< lbf_mode - Port loopback filtering state  */
+};
+
+/**
+ * ku_qpts_reg structure is used to store the QPTS register parameters
+ */
+struct ku_qpts_reg {
+    uint8_t port; /**< port - Port number */
+    uint8_t trust_level; /**< trust_level -
+                          *  0 - Trust Port
+                          *  1 - Trust User Priority - this is the default value / trust L2
+                          *  2 - Trust DSCP / trust L3
+                          *  3 - Trust Both
+                          *  Otherwise - reserved */
+};
+
+/**
+ * ku_qstct_reg structure is used to store the QSTCT register parameters
+ */
+struct ku_qstct_reg {
+    uint8_t swid; /**< swid - Switch partition ID */
+    uint8_t prio; /**< prio - Ingress Priority */
+    uint8_t utclass; /**< utclass - Stacking traffic class used for unicast packets with priority prio */
+    uint8_t mtclass; /**< mtclass - Stacking traffic class used for multicast packets with priority prio */
+};
+
+/**
+ * ku_qpdpm_reg structure is used to store the QPDPM register parameters
+ */
+struct ku_qpdpm_reg {
+    uint8_t local_port; /**< port - Port number */
+    uint8_t dscp_update[DSCP_CODES_NUMBER];    /**< dscp_update - whether to update this DSCP mapping in HW */
+    uint8_t color[DSCP_CODES_NUMBER]; /**< color mapping per DSCP value */
+    uint8_t priority[DSCP_CODES_NUMBER];       /**< priority mapping per DSCP value - Priority */
+};
+
+/**
+ * ku_qepm_reg structure is used to store the QEPM register parameters
+ */
+struct ku_qepm_reg {
+    uint8_t local_port; /**< port - Port number */
+    uint8_t exp_update[EXP_CODES_NUMBER];    /**< exp_update - whether to update this EXP mapping in HW */
+    uint8_t ecn[EXP_CODES_NUMBER]; /**< the new packet ecn value */
+    uint8_t color[EXP_CODES_NUMBER]; /**< color mapping per EXP value */
+    uint8_t priority[EXP_CODES_NUMBER];       /**< priority mapping per EXP value - Priority */
+};
+
+/**
+ * ku_qeec_reg structure is used to store the QEEC register parameters
+ */
+struct ku_qeec_reg {
+    uint8_t  local_port; /**< port - Port number */
+    uint8_t  port_rate; /**< port_rate - Port rate */
+    uint8_t  element_hierarchy; /**< 0-port, 1-group, 2-sub-group, 3-TC */
+    uint8_t  element_index; /** index in the hierarchy */
+    uint8_t  next_element_index; /** element index of the lower level element is connected to */
+    uint8_t  min_shaper_enable; /**< min_shaper_enable - 0-disable, 1- enable */
+    uint8_t  packet_mode; /**<  0-bytes mode, 1-packet mode */
+    uint32_t min_shaper; /** min shaper configuration */
+    uint8_t  max_shaper_enable; /**< max_shaper_enable - 0-disable, 1-enable */
+    uint32_t max_shaper; /** max shaper configuration */
+    uint8_t  phantom_queue_enable; /**< phantom_queue_enable - 0-disable, 1-enable */
+    uint32_t phantom_queue; /** phantom queue configuration */
+    uint8_t  dwrr_enable; /**< dwrr_enable - 0-disable, 1-enable */
+    uint8_t  dwrr; /** dwrr - 0-strict priority, 1-DWRR */
+    uint8_t  dwrr_weight; /** dwrr weight on the link going down from the element */
+};
+
+/**
+ * ku_qpdcp_reg structure is used to store the QPDCP register parameters
+ */
+struct ku_qpdpc_reg {
+    uint8_t local_port; /**local port number */
+    uint8_t sub_port; /** virtual port within the physical port */
+    uint8_t dei; /** default port DEI */
+    uint8_t pcp; /** default port PCP */
+};
+
+/**
+ * ku_qtctm_reg structure is used to store the QTCTM register parameters
+ */
+struct ku_qtctm_reg {
+    uint8_t local_port; /**local port number */
+    uint8_t mc_aware; /** 0-non mc aware, 1-mc aware */
+};
+
+/**
+ * ku_qspip_reg structure is used to store the QSPIP register parameters
+ */
+struct ku_qspip_reg {
+    uint8_t switch_prio; /**switch priority */
+    uint8_t ieee_prio; /** ieee priority */
+};
+
+/**
+ * ku_qrwe_reg structure is used to store the QRWE register parameters
+ */
+struct ku_qrwe_reg {
+    uint8_t local_port; /**local port number */
+    uint8_t exp_rewrite; /**rewrite EXP enable */
+    uint8_t dscp_rewrite; /**rewrite DSCP enable */
+    uint8_t pcp_rewrite; /**rewrite PCP enable */
+};
+
+/**
+ * qpem_color structure is used to store the QPEM register color parameter
+ */
+struct qpem_color {
+    uint8_t enable_exp; /**< enable EXP mapping update */
+    uint8_t exp; /**< EXP field in the outer level of the MPLS packet */
+};
+
+/**
+ * qpem_ecn structure is used to store the QPEM register ecn parameter
+ */
+struct qpem_ecn {
+    struct qpem_color color[3]; /**< EXP mapping for color */
+};
+
+/**
+ * qpem_switch_prio structure is used to store the QPEM register switch prio parameter
+ */
+struct qpem_switch_prio {
+    struct qpem_ecn ecn[4]; /**< EXP mapping for ECN */
+};
+
+/**
+ * ku_qpem_reg structure is used to store the QPEM register parameters
+ */
+struct ku_qpem_reg {
+    uint8_t                 local_port; /**local port number */
+    struct qpem_switch_prio switch_prio[16]; /** switch prio mapping */
+};
+
+/**
+ * qpdsm_color structure is used to store the QPDSM register color parameter
+ */
+struct qpdsm_color {
+    uint8_t enable_dscp; /**< enable DSCP mapping update */
+    uint8_t dscp; /**< DSCP */
+};
+
+/**
+ * qpdsm_switch_prio structure is used to store the QPDSM register switch prio parameter
+ */
+struct qpdsm_switch_prio {
+    struct qpdsm_color color[3]; /**< DSCP mapping for ECN */
+};
+
+/**
+ * ku_qpdsm_reg structure is used to store the QPDSM register parameters
+ */
+struct ku_qpdsm_reg {
+    uint8_t                  local_port; /**local port number */
+    struct qpdsm_switch_prio switch_prio[16]; /** switch priority to dscp mapping */
+};
+
+/**
+ * qppm_color structure is used to store the QPPM register color parameter
+ */
+struct qppm_color {
+    uint8_t enable_pcp; /**< enable PCP mapping update */
+    uint8_t dei; /**< DEI */
+    uint8_t pcp; /**< PCP */
+};
+
+/**
+ * qppm_switch_prio structure is used to store the QPPM register switch prio parameter
+ */
+struct qppm_switch_prio {
+    struct qppm_color color[3]; /**< Switch Prio to PCP, DEI mapping */
+};
+
+/**
+ * ku_qppm_reg structure is used to store the QPPM register parameters
+ */
+struct ku_qppm_reg {
+    uint8_t                 local_port; /**local port number */
+    struct qppm_switch_prio switch_prio[16]; /** switch priority to pcp mapping */
+};
+
+/**
+ * sxd_pvgt_operation_t enumerated type is used to note the
+ * PVGT operation type.
+ */
+typedef enum sxd_pvgt_operation {
+    SXD_PVGT_OP_ADD_VLAN_E = 0,
+    SXD_PVGT_OP_REMOVE_VLAN_E = 1,
+}sxd_pvgt_operation_t;
+
+/**
+ * ku_pvgt_reg structure is used to store the PVGT register parameters
+ */
+typedef struct ku_pvgt_reg {
+    uint8_t                 swid;
+    sxd_pvgt_operation_t op;
+    uint16_t                vid;
+    uint16_t                vlan_group;
+}ku_pvgt_reg_t;
+
+/**
+ * ku_msci_reg structure is used to store the MSCI register parameters
+ */
+struct ku_msci_reg {
+    uint8_t  index;
+    uint32_t version;
+};
+
+/**
+ * sxd_mrsr_command enumerated type is used to note the MRSR command type
+ */
+enum sxd_mrsr_command {
+    SXD_MRSR_CMD_DO_NOTHING = 0,
+    SXD_MRSR_CMD_SW_RESET = 1,
+    SXD_MRSR_CMD_ENCLOSURE_RESET = 3,
+    SXD_MRSR_CMD_SHUTDOWN = 4
+};
+
+/**
+ * ku_mrsr_reg structure is used to store the MRSR register parameters
+ */
+struct ku_mrsr_reg {
+    enum sxd_mrsr_command command;
+};
+
+/**
+ * ku_pprr_reg structure is used to store the PPRR register parameters
+ */
+typedef struct ku_pprr_reg {
+    uint8_t  ipv4;
+    uint8_t  ipv6;
+    uint8_t  src;
+    uint8_t  dst;
+    uint8_t  tcp;
+    uint8_t  udp;
+    uint8_t  inner_outer;
+	uint8_t  flags;
+    uint8_t inner;
+    uint8_t outer;
+    uint8_t  ip_lenght;
+    uint8_t  reg_index;
+    uint16_t port_range_min;
+    uint16_t port_range_max;
+}ku_pprr_reg_t;
+
+/**
+ * sxd_pagt_operation_t enumerated type is used to note the
+ * PAGT operation type.
+ */
+typedef enum sxd_pagt_operation {
+    SXD_PAGT_OP_CREATE_E = 0,
+    SXD_PAGT_OP_DEALLOCATE_E = 1,
+}sxd_pagt_operation_t;
+
+/**
+ * ku_pagt_reg structure is used to store the PAGT register parameters
+ */
+typedef struct ku_pagt_reg {
+    uint8_t  egress;
+    uint8_t  size;
+    uint16_t acl_group_id;
+    uint16_t acl_ids[SXD_MAX_ACL_IN_GROUP];
+}ku_pagt_reg_t;
+
+/**
+ * sxd_pvbt_operation enumerated type is used to note the
+ * PVBT operation type.
+ */
+typedef enum sxd_pvbt_operation {
+    SXD_PVBT_OP_BIND_E = 0,
+    SXD_PVBT_OP_UNBIND_E = 1,
+}sxd_pvbt_operation_t;
+
+/**
+ * ku_pvbt_reg structure is used to store the PVBT register parameters
+ */
+typedef struct ku_pvbt_reg {
+    sxd_pvbt_operation_t operation;
+    uint8_t                 swid;
+    uint8_t                 egress;
+    uint16_t                vlan_group;
+    uint8_t                 group;
+    uint16_t                acl_id_grp_id;
+}ku_pvbt_reg_t;
+
+
+/**
+ * sxd_ppbt_operation enumerated type is used to note the
+ * PPBT operation type.
+ */
+typedef enum sxd_ppbt_operation {
+    SXD_PPBT_OP_BIND_E = 0,
+    SXD_PPBT_OP_UNBIND_E = 1,
+}sxd_ppbt_operation_t;
+
+typedef enum sxd_flex_acl_action_type {
+    SXD_ACTION_TYPE_NULL_E = 0,
+    SXD_ACTION_TYPE_MAC_E = 1,
+    SXD_ACTION_TYPE_VLAN_E = 2,
+    SXD_ACTION_TYPE_TRAP_E = 3,
+    SXD_ACTION_TYPE_TRAP_W_COOKIE_E = 4,
+    SXD_ACTION_TYPE_PORT_FILTER_E = 5,
+    SXD_ACTION_TYPE_QOS_E = 6,
+    SXD_ACTION_TYPE_FORWARD_E = 7,
+    SXD_ACTION_TYPE_POLICING_COUNTING_E = 8,
+    SXD_ACTION_TYPE_META_DATA_E = 9,
+    SXD_ACTION_TYPE_UC_ROUTER_AND_MPLS_E = 10,
+    SXD_ACTION_TYPE_VXLAN_E = 11,
+    SXD_ACTION_TYPE_MPLS_E = 12,
+    SXD_ACTION_TYPE_HASH_E= 13,
+    SXD_ACTION_TYPE_VIRTUAL_FORWARDING_E = 14,
+    SXD_ACTION_TYPE_IGNORE_E = 15,
+	SXD_ACTION_TYPE_MC_E = 16,
+    SXD_ACTION_TYPE_LAST_E = 17
+}sxd_flex_acl_action_type_t;
+
+/**
+ * ku_ppbt_reg structure is used to store the PPBT register parameters
+ */
+typedef struct ku_ppbt_reg {
+    sxd_ppbt_operation_t operation;
+    uint8_t                 egress;
+    uint8_t                 port;
+    uint8_t                 sub_port;
+    uint8_t                 group;
+    uint16_t                acl_id_grp_id;
+}ku_ppbt_reg_t;
+
+/**
+ * sxd_acl_ptce_action_type enumerated type is used to
+ * note the PTCE action type.
+ */
+typedef enum sxd_acl_ptce_action_type {
+    SXD_PTCE_ACTION_TYPE_DEFAULT_E = 0,
+    SXD_PTCE_ACTION_TYPE_EXTENDED_E= 1,
+}sxd_acl_ptce_action_type_t;
+
+/**
+ * sxd_acl_ptce_key_type enumerated type is used to note
+ * the PTCE key type.
+ */
+typedef enum sxd_acl_ptce_key_type {
+    SXD_PTCE_KEY_TYPE_IPV4_FULL_E = 0,
+    SXD_PTCE_KEY_TYPE_IPV6_FULL_E = 1,
+    SXD_PTCE_KEY_TYPE_MAC_FULL_E = 2,
+    SXD_PTCE_KEY_TYPE_MAC_IPV4_FULL_E = 3,
+    SXD_PTCE_KEY_TYPE_MAC_SHORT_E = 5,
+    SXD_PTCE_KEY_TYPE_FCOE_FULL_E = 32
+}sxd_acl_ptce_key_type_t;
+
+/**
+ * sxd_ptce_trap_action enumerated type is used to note
+ * the PTCE action trap action type.
+ */
+typedef enum sxd_ptce_trap_action {
+    SXD_PTCE_TRAP_ACTION_PERMIT_E = 0,
+    SXD_PTCE_TRAP_ACTION_SOFT_DROP_E = 1,
+    SXD_PTCE_TRAP_ACTION_TRAP_E = 2,
+    SXD_PTCE_TRAP_ACTION_SOFT_DROP_TRAP_E = 3,
+    SXD_PTCE_TRAP_ACTION_DENY_E = 4,
+}sxd_ptce_trap_action_t;
+
+/**
+ * sxd_ptce_vlan_action enumerated type is used to note
+ * the PTCE action vlan action type.
+ */
+typedef enum sxd_ptce_vlan_action {
+    SXD_PTCE_VLAN_ACTION_NOP_E = 0,
+    SXD_PTCE_VLAN_ACTION_PUSH_VID_KEEP_PRIO_E = 0x8,
+    SXD_PTCE_VLAN_ACTION_PUSH_VLAN_E = 0xC,
+    SXD_PTCE_VLAN_ACTION_REPLACE_VID_KEEP_PRIO_E = 0x18,
+    SXD_PTCE_VLAN_ACTION_REPLACE_VID_PRIO_E = 0x1c,
+}sxd_ptce_vlan_action_t;
+
+/**
+ * sxd_ptce_ipv4_full_key structure is used to store PTCE
+ * IPv4 Full key.
+ */
+typedef struct sxd_ptce_ipv4_full_key {
+    uint32_t dst_ip;
+    uint32_t src_ip;
+    uint16_t src_l4_port;
+    uint16_t dst_l4_port;
+    uint8_t  ttl;
+    uint8_t  tcp_flags;
+    uint8_t  ip_proto;
+    uint8_t  ip_tos;
+    uint16_t flags;
+    uint8_t  ipv6_ext;
+    uint16_t dst_sys_port;
+    uint16_t src_sys_port;
+    uint16_t l4_port_range;
+    uint32_t flow_label;
+}sxd_ptce_ipv4_full_key_t;
+
+/**
+ * sxd_ptce_ipv6_full_key structure is used to store PTCE
+ * IPv6 Full key.
+ */
+typedef struct sxd_ptce_ipv6_full_key {
+    uint32_t dst_ip[4];
+    uint32_t src_ip[4];
+    uint16_t src_l4_port;
+    uint16_t dst_l4_port;
+    uint8_t  ttl;
+    uint8_t  tcp_flags;
+    uint8_t  ip_proto;
+    uint8_t  ip_tos;
+    uint16_t flags;
+    uint8_t  ipv6_ext;
+    uint16_t dst_sys_port;
+    uint16_t src_sys_port;
+    uint16_t l4_port_range;
+    uint32_t flow_label;
+}sxd_ptce_ipv6_full_key_t;
+
+/**
+ * sxd_ptce_operation enumerated type is used to note the
+ * op type.
+ */
+typedef enum sxd_ptce_operation {
+    SXD_PTCE_OP_WRITE = 0,
+    SXD_PTCE_OP_READ = 0,
+    SXD_PTCE_OP_CLEAR_ON_READ = 1,
+    SXD_PTCE_OP_UPDATE = 1,
+    SXD_PTCE_OP_CLEAR_ACTIVITY = 2
+} sxd_ptce_operation_t;
+
+
+/**
+ * sxd_ptce_mac_full_key structure is used to store PTCE
+ * MAC Full key.
+ */
+typedef struct sxd_ptce_mac_full_key {
+    uint8_t  dmac[6];
+    uint8_t  smac[6];
+    uint16_t ethertype;
+    uint16_t vid;
+    uint8_t  cfi;
+    uint8_t  prio;
+    uint8_t  vlan_tagged;
+    uint8_t  vlan_valid;
+    uint8_t  dmac_type;
+    uint32_t slag;
+    uint16_t src_sys_port;
+    uint16_t dst_sys_port;
+}sxd_ptce_mac_full_key_t;
+
+/**
+ * sxd_ptce_mac_short_key structure is used to store PTCE
+ * MAC short key.
+ */
+typedef struct sxd_ptce_mac_short_key {
+    uint8_t  dmac[6];
+    uint8_t  smac[6];
+    uint16_t vid;
+    uint8_t  cfi;
+    uint8_t  prio;
+    uint8_t  vlan_tagged;
+    uint8_t  vlan_valid;
+    uint8_t  dmac_type;
+    uint32_t slag;
+    uint16_t src_sys_port;
+}sxd_ptce_mac_short_key_t;
+
+/**
+ * sxd_ptce_mac_full_key structure is used to store PTCE
+ * MAC IPv4 Full key.
+ */
+typedef struct sxd_ptce_mac_ipv4_full_key {
+    uint8_t  dmac[6];
+    uint8_t  smac[6];
+    uint16_t ethertype;
+    uint16_t vid;
+    uint8_t  prio;
+    uint16_t mac_flags;
+    uint8_t  vlan_type;
+    uint8_t  vlan_valid;
+    uint16_t src_sys_port;
+    uint32_t dst_ip;
+    uint32_t src_ip;
+    uint16_t src_l4_port;
+    uint16_t dst_l4_port;
+    uint8_t  ip_flags;
+    uint8_t  ip_proto;
+    uint8_t  ip_tos;
+    uint8_t  slag;
+}sxd_ptce_mac_ipv4_full_key_t;
+
+/**
+ * sxd_ptce_fcoe_full_key structure is used to store PTCE
+ * FCoE Full key.
+ */
+typedef struct sxd_ptce_fcoe_full_key {
+    uint8_t  dmac[6];
+    uint8_t  smac[6];
+    uint16_t vid;
+    uint8_t  prio;
+    uint8_t  vlan_type;
+    uint8_t  slag;
+    uint8_t  vlan_valid;
+    uint16_t src_sys_port;
+    uint8_t  d_id[3];
+    uint8_t  s_id[3];
+    uint16_t ox_id;
+    uint16_t rx_id;
+    uint8_t  is_fc;
+    uint8_t  r_ctl;
+    uint8_t  type;
+}sxd_ptce_fcoe_full_key_t;
+
+/**
+ * sxd_ptce_default_action structure is used to store
+ * PTCE Default action set.
+ */
+typedef struct sxd_ptce_default_action {
+    sxd_ptce_trap_action_t 		trap;
+    uint8_t                   	trap_group;
+    uint16_t                  	trap_id;
+    uint8_t                   	mirror;
+    uint8_t                  	mirror_dst;
+    sxd_ptce_vlan_action_t 		vlan_prio_tclass_op;
+    uint16_t                 	vid;
+    uint8_t                   	prio;
+    uint8_t                   	etclass;
+    uint8_t                   	stclass;
+    uint32_t                  	flow_counter;
+    uint8_t                   	policer_port;
+    uint8_t                   	g_policer;
+    uint8_t                   	pid;
+    uint8_t                   	nr;
+    uint8_t                   	no_learning;
+}sxd_ptce_default_action_t;
+
+
+/**
+ * sxd_ptce_extended_action_t structure is used to store PTCE
+ * Extended action set.
+ */
+typedef struct sxd_ptce_extended_action{
+    uint8_t  pbs_en;
+    uint16_t pbs_index;
+} sxd_ptce_extended_action_t;
+
+/**
+ * ku_ptce_reg structure is used to store the PTCE register parameters
+ */
+typedef struct ku_ptce_reg {
+    sxd_acl_ptce_key_type_t    		key_type;
+    sxd_acl_ptce_action_type_t 		action_set_type;
+    uint8_t                       	valid;
+    uint8_t                       	activity;
+    sxd_ptce_operation_t          	op;
+    uint16_t                      	offset;
+    uint8_t                       	tcam_region_info[SXD_ACL_INFO_SIZE_BYTES];
+    union {
+        sxd_ptce_ipv4_full_key_t    	ipv4;
+        sxd_ptce_ipv6_full_key_t     	ipv6;
+        sxd_ptce_mac_full_key_t      	mac_full;
+        sxd_ptce_mac_short_key_t     	mac_short;
+        sxd_ptce_mac_ipv4_full_key_t 	mac_ipv4_full;
+        sxd_ptce_fcoe_full_key_t     	fcoe_full;
+    } sxd_ptce_key;
+    union {
+        sxd_ptce_ipv4_full_key_t     ipv4;
+        sxd_ptce_ipv6_full_key_t     ipv6;
+        sxd_ptce_mac_full_key_t      mac_full;
+        sxd_ptce_mac_short_key_t     mac_short;
+        sxd_ptce_mac_ipv4_full_key_t mac_ipv4_full;
+        sxd_ptce_fcoe_full_key_t     fcoe_full;
+    } sxd_ptce_mask;
+    struct {
+         sxd_ptce_default_action_t    	default_action;
+         sxd_ptce_extended_action_t 	extended_action;
+    } sxd_ptce_action_set;
+    uint8_t  terminate;
+    uint8_t  asbind;
+    uint8_t  next_is_group;
+    uint16_t next_acl_id_grp_id;
+}ku_ptce_reg_t;
+
+/**
+ *  * ku_ptce2_reg structure is used to store the PTCE register parameters
+ *   */
+
+typedef enum sxd_flex_defer{
+	SXD_FLEX_DEFER_ACTION_APPLIED_IMMEDITELY_E    = 0,
+	SXD_FLEX_DEFER_ACTION_WRITTEN_TO_ACTION_SET_E
+}sxd_flex_defer_t;
+
+typedef enum sxd_flex_ttl_cmd_ {
+    SXD_FLEX_TTL_CMD_DO_NOTHING_E     = 0,
+    SXD_FLEX_TTL_CMD_SET_TTL_VALUE_E  = 1,
+    SXD_FLEX_TTL_CMD_DECREMENT_E      = 2,
+}sxd_flex_ttl_cmd_t;
+
+typedef enum sxd_flex_mac_cmd {
+    SXD_FLEX_MAC_CMD_TYPE_DO_NOTHING_E = 0,
+    SXD_FLEX_MAC_CMD_TYPE_SET_SMAC_E,
+    SXD_FLEX_MAC_CMD_TYPE_SET_DMAC_E
+} sxd_flex_mac_cmd_t;
+
+typedef struct sxd_mac_flex_action {
+	sxd_flex_defer_t     defer;
+    sxd_flex_ttl_cmd_t   ttl_cmd;
+    uint8_t                 ttl_value;
+    sxd_flex_mac_cmd_t   mac_cmd;
+    uint8_t                 mac[6];
+} sxd_mac_flex_action_t;
+
+typedef enum sxd_flex_vlan_tag_cmd {
+    SXD_FLEX_VLAN_TAG_CMD_TYPE_DO_NOTHING_E = 0,
+    SXD_FLEX_VLAN_TAG_CMD_TYPE_PUSH_OUTER_E,
+    SXD_FLEX_VLAN_TAG_CMD_TYPE_POP_OUTER_E
+} sxd_flex_vlan_tag_cmd_t;
+
+typedef enum sxd_flex_vid_cmd {
+    SXD_FLEX_VID_CMD_TYPE_DO_NOTHING_E                = 0,
+    SXD_FLEX_VID_CMD_TYPE_SET_OUTER_E,
+    SXD_FLEX_VID_CMD_TYPE_SET_INNER_E,
+    SXD_FLEX_VID_CMD_TYPE_COPY_FROM_OUTER_TO_INNER_E,
+    SXD_FLEX_VID_CMD_TYPE_COPY_FROM_INNER_TO_OUTER_E,
+    SXD_FLEX_VID_CMD_TYPE_SWAP_INNER_OUTER_E
+}sxd_flex_vid_cmd_t;
+
+typedef enum sxd_flex_vlan_ethertype_cmd {
+    SXD_FLEX_VLAN_ETHERTYPE_CMD_TYPE_DO_NOTHING_E                 = 0,
+    SXD_FLEX_VLAN_ETHERTYPE_CMD_TYPE_SET_OUTER_E,
+    SXD_FLEX_VLAN_ETHERTYPE_CMD_TYPE_SET_INNER_E,
+    SXD_FLEX_VLAN_ETHERTYPE_CMD_TYPE_COPY_FROM_OUTER_TO_INNER_E,
+    SXD_FLEX_VLAN_ETHERTYPE_CMD_TYPE_COPY_FROM_INNER_TO_OUTER_E,
+    SXD_FLEX_VLANN_ETHERTYPE_CMD_TYPE_SWAP_INNER_OUTER_E
+}sxd_flex_vlan_ethertype_cmd_t;
+
+typedef enum sxd_flex_vlan_prio_cmd {
+    SXD_FLEX_VLAN_PRIO_CMD_TYPE_DO_NOTHING_E                  = 0,
+    SXD_FLEX_VLAN_PRIO_CMD_TYPE_SET_OUTER_E,
+    SXD_FLEX_VLAN_PRIO_CMD_TYPE_SET_INNER_E,
+    SXD_FLEX_VLAN_PRIO_CMD_TYPE_COPY_FROM_OUTER_TO_INNER_E,
+    SXD_FLEX_VLAN_PRIO_CMD_TYPE_COPY_FROM_INNER_TO_OUTER_E,
+    SXD_FLEX_VLAN_PRIO_CMD_TYPE_SWAP_INNER_OUTER_E
+}sxd_flex_vlan_prio_cmd_t;
+
+typedef enum sxd_flex_dei_cmd {
+    SXD_FLEX_DEI_CMD_TYPE_DO_NOTHING_E                    = 0,
+    SXD_FLEX_DEI_CMD_TYPE_SET_OUTER_E,
+    SXD_FLEX_DEI_CMD_TYPE_SET_INNER_E,
+    SXD_FLEX_DEI_CMD_TYPE_COPY_FROM_OUTER_TO_INNER_E,
+    SXD_FLEX_DEI_CMD_TYPE_COPY_FROM_INNER_TO_OUTER_E,
+    SXD_FLEX_DEI_CMD_TYPE_SWAP_INNER_OUTER_E
+}sxd_flex_dei_cmd_t;
+typedef enum sxd_flex_ether_type {
+	SXD_FLEX_ETHER_TYPE_0_E = 0,
+	SXD_FLEX_ETHER_TYPE_1_E ,
+	SXD_FLEX_ETHER_TYPE_2_E
+}sxd_flex_ether_type_t;
+
+typedef struct sxd_vlan_flex_action {
+    sxd_flex_defer_t                 	defer;
+    sxd_flex_vlan_tag_cmd_t             v_tag_cmd;
+    sxd_flex_vid_cmd_t               	vid_cmd;
+    uint16_t                            vid_val;
+    sxd_flex_vlan_ethertype_cmd_t    	ethertype_cmd;
+    sxd_flex_ether_type_t            	ethertype_val;
+    uint8_t                             pcp_val;
+    sxd_flex_vlan_prio_cmd_t         	pcp_cmd;
+    sxd_flex_dei_cmd_t               	dei_cmd;
+    uint8_t                             dei_val;
+}sxd_vlan_flex_action_t;
+
+typedef enum sxd_flex_trap_forward_action_val {
+    SXD_FLEX_TRAP_FORWARD_ACTION_TYPE_DO_NORTHING_E                   = 0,
+    SXD_FLEX_TRAP_FORWARD_ACTION_TYPE_FORWARD_DO_NORTHING_CLEAR_SOFT_DROP_E,
+    SXD_FLEX_TRAP_FORWARD_ACTION_TYPE_SOPT_DROP_ERROR_E,
+    SXD_FLEX_TRAP_FORWARD_ACTION_TYPE_DISCARD_HARD_DROP_E,
+    SXD_FLEX_TRAP_FORWARD_ACTION_TYPE_DISCARD_HARD_DROP_ERROR_E,
+} sxd_flex_trap_forward_action_val_t;
+
+typedef enum sxd_flex_trap_action_val {
+    SXD_FLEX_TRAP_ACTION_TYPE_DO_NORTHING_E                        = 0,
+    SXD_FLEX_TRAP_ACTION_TYPE_SOFT_DISCARD_CLEAR_TRAP_E,
+    SXD_FLEX_TRAP_ACTION_TYPE_TRAP_E,
+    SXD_FLEX_TRAP_ACTION_TYPE_DISCARD_NO_TRAP_E
+} sxd_flex_trap_action_val_t;
+
+typedef struct sxd_trap_flex_action {
+    sxd_flex_defer_t                     	defer;
+    sxd_flex_trap_forward_action_val_t      forward_action;
+    sxd_flex_trap_action_val_t              trap_action;
+    uint16_t                                trap_id;
+    uint8_t                                 mirror_agent;
+    uint8_t                                 mirror_enable;
+
+}sxd_trap_flex_action_t;
+
+typedef struct sxd_trap_w_user_defined_flex_action {
+    sxd_flex_defer_t                     	defer;
+    sxd_flex_trap_forward_action_val_t      forward_action;
+    sxd_flex_trap_action_val_t              trap_action;
+    uint16_t                                trap_id;
+    uint8_t                                 mirror_agent;
+    uint8_t                                 mirror_enable;
+    uint32_t                                user_def_val;
+}sxd_trap_w_user_defined_flex_action_t;
+
+typedef struct sxd_port_filter_flex_action {
+    uint32_t        egress_port_list_0_31;
+    uint32_t        egress_port_list_32_63;
+}sxd_port_filter_flex_action_t;
+
+typedef enum sxd_flex_dscp_cmd {
+    SXD_FLEX_DSCP_CMD_TYPE_DO_NOTHING_E       = 0,
+    SXD_FLEX_DSCP_CMD_TYPE_SET_3_LSB_BITS_E   = 1,
+    SXD_FLEX_DSCP_CMD_TYPE_SET_3_MSB_BITS_E   = 2,
+    SXD_FLEX_DSCP_CMD_TYPE_SET_DSCP_6_BITS_E  = 3,
+}sxd_flex_dscp_cmd_t;
+
+typedef enum sxd_flex_ecn_cmd {
+    SXD_FLEX_ECN_CMD_TYPE_DO_NOTHING_E                    = 0,
+    SXD_FLEX_ECN_CMD_TYPE_SET_OUTER_ECN_E ,
+    SXD_FLEX_ECN_CMD_TYPE_SET_INNER_ECN_E ,
+    SXD_FLEX_ECN_CMD_TYPE_COPY_ENC_OUTER_TO_INNER_E,
+    SXD_FLEX_ECN_CMD_TYPE_COPY_ENC_INNER_TO_OUTER_E,
+    SXD_FLEX_ECN_CMD_TYPE_SWAP_INNER_AND_OUTER_E,
+}sxd_flex_ecn_cmd_t;
+
+typedef enum sxd_flex_switch_prio_cmd {
+    SXD_FLEX_SWITCH_PRIO_CMD_TYPE_DO_NOTHING_E            = 0,
+    SXD_FLEX_SWITCH_PRIO_CMD_TYPE_SET_SWITCH_PRIORITY_E   = 1,
+}sxd_flex_switch_prio_cmd_t;
+
+typedef enum sxd_flex_color_cmd {
+    SXD_FLEX_COLOR_CMD_TYPE_DO_NOTHING_E  = 0,
+    SXD_FLEX_COLOR_CMD_TYPE_SET_COLOR_E   = 1,
+}sxd_flex_color_cmd_t;
+
+typedef enum sxd_flex_color_type {
+    SXD_FLEX_COLOR_CMD_TYPE_GREEN_E   = 0,
+    SXD_FLEX_COLOR_CMD_TYPE_YELLOW_E,
+    SXD_FLEX_COLOR_CMD_TYPE_RED_E
+}sxd_flex_color_type_t;
+
+typedef enum sxd_flex_rewrite_cmd {
+    SXD_FLEX_REWRITE_CMD_TYPE_PRESERVE_VALUE_REWRITE_ENABLE_BIT_E = 0,
+    SXD_FLEX_REWRITE_CMD_TYPE_SET_VALUE_REWRITE_ENABLE_BIT_E,
+    SXD_FLEX_REWRITE_CMD_TYPE_CLEAR_VALUE_REWRITE_ENABLE_BIT_E,
+}sxd_flex_rewrite_cmd_t;
+
+typedef enum sxd_flex_traffic_class_cmd{
+	SXD_FLEX_TRAFFIC_CLASS_CMD_TYPE_DO_NOTHING_E          = 0,
+	SXD_FLEX_TRAFFIC_CLASS_CMD_TYPE_SET_TRAFFIC_CLASS_E
+}sxd_flex_traffic_class_cmd_t;
+
+typedef enum sxd_ptce2_next_type {
+    SXD_FLEX_NEXT_POINTER_RECORD_E = 0,
+    SXD_FLEX_GOTO_RECORD_E         = 1,
+}sxd_ptce2_next_type_t;
+
+typedef enum sxd_ptce2_binding_cmd {
+    SXD_FLEX_BINDING_NONE_E   = 0,
+    SXD_FLEX_BINDING_JUMP_E   = 1,
+    SXD_FLEX_BINDING_CALL_E   = 2,
+    SXD_FLEX_BINDING_BREAK_E  = 3,
+}sxd_ptce2_binding_cmd_t;
+
+typedef struct sxd_qos_flex_action {
+	sxd_flex_defer_t                 	defer;
+    sxd_flex_ecn_cmd_t              	ecn_cmd;
+    uint8_t                             ecn_val;
+    sxd_flex_color_cmd_t             	color_cmd;
+    uint8_t                             color_val;
+    sxd_flex_dscp_cmd_t              	dscp_cmd;
+    uint8_t                             dscp_val;
+    sxd_flex_switch_prio_cmd_t       	switch_prio_cmd;
+    uint8_t                             switch_prio_val;
+    sxd_flex_rewrite_cmd_t           	rewrite_dscp_cmd;
+    sxd_flex_rewrite_cmd_t           	rewrite_pcp_cmd;
+    sxd_flex_traffic_class_cmd_t     	traffic_class_cmd;
+    uint8_t                             tc;
+}sxd_qos_flex_action_t;
+typedef enum sxd_forward_flex_action_type{
+    SXD_FORWARD_FLEX_ACTION_TYPE_PBS_E   = 0,
+    SXD_FORWARD_FLEX_ACTION_TYPE_OUTPUT_E
+}sxd_forward_flex_action_type_t;
+
+typedef enum sxd_forward_output_record_defer{
+    SXD_FORWARD_FLEX_ACTION_OUTPUT_DEFER_TYPE_APLLAY_E    = 0,
+    SXD_FORWARD_FLEX_ACTION_OUTPUT_DEFER_TYPE_LIKE_PBS_E
+}sxd_forward_output_record_defer_t;
+
+
+typedef struct sxd_forward_output_record_flex_action{
+	sxd_forward_output_record_defer_t       defer;
+	uint32_t                                pbs_ptr;
+	uint8_t                                 in_port;
+}sxd_forward_output_record_flex_action_t;
+
+
+typedef struct sxd_forward_flex_action {
+	sxd_forward_flex_action_type_t           type;
+	union {
+		uint32_t        pbs_ptr;
+		sxd_forward_output_record_flex_action_t output ;
+
+	}                                           record;
+}sxd_forward_flex_action_t;
+
+typedef enum sxd_policing_monitoring_flex_action_type{
+    SXD_POLIICING_MONITORING_FLEX_ACTION_COUNTER_E = 0,
+    SXD_POLIICING_MONITORING_FLEX_ACTION_POLICER_E
+}sxd_policing_monitoring_flex_action_type_t;
+
+typedef struct sxd_policing_monitoring_flex_action {
+	sxd_policing_monitoring_flex_action_type_t   c_p;
+	sxd_counter_set_t                               counter_set;
+	uint16_t                                        pid;
+}sxd_policing_monitoring_flex_action_t;
+
+typedef struct sxd_metadata_flex_action {
+	uint16_t        meta_data;
+	uint16_t        mask;
+}sxd_metadata_flex_action_t;
+
+typedef enum sxd_uc_router_flex_action_type{
+	 SXD_UC_ROUTER_FLEX_ACTION_TYPE_IP_REMOTE_E = 0,
+	 SXD_UC_ROUTER_FLEX_ACTION_TYPE_IP_LOCAL_E,
+	 SXD_UC_ROUTER_FLEX_ACTION_TYPE_TUNNL_TERMINIATION_E,
+	 SXD_UC_ROUTER_FLEX_ACTION_TYPE_MPLS_ILM_E,
+	 SXD_UC_ROUTER_FLEX_ACTION_TYPE_MPLS_NHLFE_E
+}sxd_uc_router_flex_action_type_t;
+
+typedef struct sxd_uc_router_flex_action_ip_remote {
+	uint32_t        adjacency_index;
+	uint16_t        ecmp_size;
+}sxd_uc_router_flex_action_ip_remote_t;
+
+typedef struct sxd_uc_router_flex_action_ip_local{
+	uint16_t    local_erif;
+}sxd_uc_router_flex_action_ip_local_t;
+
+typedef struct sxd_uc_router_flex_action_tunnul_termination{
+	uint32_t        tunnul_ptr;
+}sxd_uc_router_flex_action_tunnul_termination_t;
+
+typedef struct sxd_uc_router_flex_action_mpls_ilm{
+	uint32_t        ilm_ptr;
+}sxd_uc_router_flex_action_mpls_ilm_t;
+
+typedef struct sxd_uc_router_flex_action_mpls_nhlfe{
+	uint32_t        nhlfe_ptr;
+	uint16_t        ecmp_size;
+} sxd_uc_router_flex_action_mpls_nhlfe_t;
+
+typedef struct sxd_uc_router_flex_action {
+	sxd_uc_router_flex_action_type_t                     type;
+	union {
+		sxd_uc_router_flex_action_ip_remote_t 			ip_remote;
+		sxd_uc_router_flex_action_ip_local_t 			ip_local;
+		sxd_uc_router_flex_action_tunnul_termination_t tunnul_termination;
+		sxd_uc_router_flex_action_mpls_ilm_t 			mpls_ilm;
+		sxd_uc_router_flex_action_mpls_nhlfe_t         mpls_nhlfe;
+	} structs;
+}sxd_uc_router_flex_action_t;
+typedef enum sxd_vni_flex_action_type {
+	SXD_VNI_FLEX_ACTION_TYPE_NONE_E = 0 ,
+	SXD_VNI_FLEX_ACTION_TYPE_SET_E
+}sxd_vni_flex_action_type_t;
+
+typedef struct sxd_vni_flex_action {
+	enum sxd_vni_flex_action_type   set_vni;
+	uint32_t                        vni;
+} sxd_vni_flex_action_t;
+
+
+typedef enum sxd_mpls_action_ttl_cmd_type{
+	SXD_MPLS_FLEX_ACTION_TTL_CMD_TYPE_DO_NOTING_E         = 0,
+	SXD_MPLS_FLEX_ACTION_TTL_CMD_TYPE_SET_TTL_E ,
+	SXD_MPLS_FLEX_ACTION_TTL_CMD_TYPE_DECREMENT_BY_TTL_E
+}sxd_mpls_action_ttl_cmd_type_t;
+
+typedef enum sxd_mpls_action_exp_cmd_type{
+	SXD_MPLS_FLEX_ACTION_EXP_CMD_TYPE_DO_NOTING_E = 0,
+	SXD_MPLS_FLEX_ACTION_EXP_CMD_TYPE_SET_EXP_E
+}sxd_mpls_action_exp_cmd_type_t;
+
+typedef enum sxd_mpls_action_exp_rw_type {
+	SXD_MPLS_FLEX_ACTION_EXP_RW_TYPE_PRESERVE_VALUE_REWRITE_BIT_E = 0,
+	SXD_MPLS_FLEX_ACTION_EXP_CMD_TYPE_SET_REWRITE_BIT_E,
+	SXD_MPLS_FLEX_ACTION_EXP_CMD_TYPE_CLEAR_REWRITE_BIT_E
+}sxd_mpls_action_exp_rw_type_t;
+
+typedef struct sxd_mpls_flex_action{
+	sxd_mpls_action_ttl_cmd_type_t   	ttl_cmd;
+	uint8_t                             ttl;
+	uint8_t                             ttl_code;
+	sxd_mpls_action_exp_cmd_type_t   	exp_cmd;
+	uint8_t                             exp;
+	sxd_mpls_action_exp_rw_type_t    	exp_rw;
+}sxd_mpls_flex_action_t;
+
+typedef enum sxd_hash_flex_action_type{
+	SXD_HASH_FLEX_ACTION_TYPE_LAG               = 0,
+	SXD_HASH_FLEX_ACTION_TYPE_ECMP
+}sxd_hash_flex_action_type_t;
+
+typedef enum sxd_hash_flex_action_cmd{
+	SXD_HASH_FLEX_ACTION_CMD_NONE_E               = 0,
+	SXD_HASH_FLEX_ACTION_CMD_SET_HASH_VALUE_E,
+	SXD_HASH_FLEX_ACTION_CMD_XOR_E,
+	SXD_HASH_FLEX_ACTION_CMD_RANDOM_E,
+	SXD_HASH_FLEX_ACTION_CMD_COPY_E,
+	SXD_HASH_FLEX_ACTION_CMD_SWAP_LEG_AND_ECMP_E,
+	SXD_HASH_FLEX_ACTION_CMD_ACORDING_HASH_FIELDS_E
+} sxd_hash_flex_action_cmd_t;
+typedef enum sxd_hash_flex_action_hash_fields{
+	SXD_HASH_FLEX_ACTION_HASH_FIELD_SMAC_31_0_E           = 0,
+	SXD_HASH_FLEX_ACTION_HASH_FIELD_DMAC_31_0_E           = 1,
+	SXD_HASH_FLEX_ACTION_HASH_FIELD_DMAC_SMAC_47_32_E     = 2,
+	SXD_HASH_FLEX_ACTION_HASH_FIELD_SMAC_DMAC_47_32_E     = 3,
+	SXD_HASH_FLEX_ACTION_HASH_FIELD_SIP_31_0_E            = 4,
+	SXD_HASH_FLEX_ACTION_HASH_FIELD_SIP_63_32_E           = 5,
+	SXD_HASH_FLEX_ACTION_HASH_FIELD_SIP_95_64_E          = 6,
+	SXD_HASH_FLEX_ACTION_HASH_FIELD_SIP_127_96_E          = 7,
+	SXD_HASH_FLEX_ACTION_HASH_FIELD_DIP_31_0_E            = 8,
+	SXD_HASH_FLEX_ACTION_HASH_FIELD_DIP_63_32_E           = 9,
+	SXD_HASH_FLEX_ACTION_HASH_FIELD_DIP_95_64_E           = 10,
+	SXD_HASH_FLEX_ACTION_HASH_FIELD_DIP_127_96_E          = 11,
+	SXD_HASH_FLEX_ACTION_HASH_FIELD_SPI_E                 = 17,
+	SXD_HASH_FLEX_ACTION_HASH_FIELD_INNER_SIP_31_0_E      = 18,
+	SXD_HASH_FLEX_ACTION_HASH_FIELD_INNER_SIP_63_32_E     = 19,
+	SXD_HASH_FLEX_ACTION_HASH_FIELD_INNER_SIP_95_64_E     = 20,
+	SXD_HASH_FLEX_ACTION_HASH_FIELD_INNER_SIP_127_96_E    = 21,
+	SXD_HASH_FLEX_ACTION_HASH_FIELD_INNER_DIP_31_0_E      = 22,
+	SXD_HASH_FLEX_ACTION_HASH_FIELD_INNER_DIP_63_32_E     = 23,
+	SXD_HASH_FLEX_ACTION_HASH_FIELD_INNER_DIP_95_64_E     = 24,
+	SXD_HASH_FLEX_ACTION_HASH_FIELD_INNER_DIP_127_96_E    = 25
+} sxd_hash_flex_action_hash_fields_t;
+
+typedef struct sxd_hash_flex_action{
+	sxd_hash_flex_action_type_t              type;
+	sxd_hash_flex_action_cmd_t               hash_cmd;
+	sxd_hash_flex_action_hash_fields_t       hash_fields;
+	uint16_t                                    hash_value;
+	uint32_t                                    hash_mask;
+}sxd_hash_flex_action_t;
+typedef enum sxd_virtual_forward_flex_action_vr_cmd_type{
+	SXD_VIRTUAL_FORWARD_FLEX_ACTION_VR_CMD_TYPE_DO_NOTING_E           =0 ,
+	SXD_VIRTUAL_FORWARD_FLEX_ACTION_VR_CMD_TYPE_SET_VR_TO_PACKET_E
+} sxd_virtual_forward_flex_action_vr_cmd_type_t;
+
+
+typedef enum sxd_virtual_forward_flex_action_fid_cmd_type{
+	SXD_VIRTUAL_FORWARD_FLEX_ACTION_FID_CMD_TYPE_DO_NOTING_E          =0 ,
+	SXD_VIRTUAL_FORWARD_FLEX_ACTION_FID_CMD_TYPE_SET_FID_TO_PACKET_E
+}sxd_virtual_forward_flex_action_fid_cmd_type_t;
+
+typedef struct sxd_virtual_forward_flax_action{
+	sxd_virtual_forward_flex_action_vr_cmd_type_t        vr_cmd;
+	uint16_t                                                virtual_router;
+	sxd_virtual_forward_flex_action_fid_cmd_type_t       fid_cmd;
+	uint16_t                                                fid;
+}sxd_virtual_forward_flax_action_t;
+typedef enum sxd_ignore_flex_action_ignore_stp_type{
+	SXD_IGNORE_FLEX_ACTION_IGNORE_STP_TYPE_REGULAR_FLOW_E = 0 ,
+	SXD_IGNORE_FLEX_ACTION_IGNORE_STP_TYPE_IGNORE_STP_E
+}sxd_ignore_flex_action_ignore_stp_type_t;
+typedef enum sxd_ignore_flex_action_ignore_vl_filter_type{
+	SXD_IGNORE_FLEX_ACTION_IGNORE_VL_TYPE_REGULAR_FLOW_E = 0 ,
+	SXD_IGNORE_FLEX_ACTION_IGNORE_VL_TYPE_IGNORE_VLAN_E
+} sxd_ignore_flex_action_ignore_vl_filter_type_t;
+typedef enum sxd_ignore_flex_action_disable_learning_type{
+	SXD_IGNORE_FLEX_ACTION_IGNORE_DISABLE_LEARNING_TYPE_REGULAR_FLOW_E            = 0 ,
+	SXD_IGNORE_FLEX_ACTION_IGNORE_DISABLE_LEARNING_TYPE_DISABLE_LEARNING_E
+} sxd_ignore_flex_action_disable_learning_type_t;
+typedef enum sxd_ignore_flex_action_disable_ovl_learning_type{
+	SXD_IGNORE_FLEX_ACTION_IGNORE_DISABLE_OVL_LEARNING_TYPE_REGULAR_FLOW_E        = 0 ,
+	SXD_IGNORE_FLEX_ACTION_IGNORE_DISABLE_OVL_LEARNING_TYPE_DISABLE_LEARNING_E
+} sxd_ignore_flex_action_disable_ovl_learning_type_t;
+
+typedef struct sxd_ignore_flex_action {
+	 sxd_ignore_flex_action_ignore_stp_type_t                 ignore_stp;
+	 sxd_ignore_flex_action_ignore_vl_filter_type_t           ignore_vl_filter;
+	 sxd_ignore_flex_action_disable_learning_type_t          disable_learning;
+	 sxd_ignore_flex_action_disable_ovl_learning_type_t      disable_ovl_learning;
+} sxd_ignore_flex_action_t;
+
+typedef enum sxd_group_or_acl_binding_type{
+	SXD_GROUP_OR_ACL_BINDING_TYPE_ACL_E   = 0,
+	SXD_GROUP_OR_ACL_BINDING_TYPE_GROUP_E
+} sxd_group_or_acl_binding_type_t;
+
+typedef enum sxd_mc_flex_action_rpf_action_type {
+	SXD_MC_FLEX_ACTION_RPF_ACTION_NOP_E = 0,
+	SXD_MC_FLEX_ACTION_RPF_ACTION_RPF_TRAP_E = 1,
+	SXD_MC_FLEX_ACTION_RPF_ACTION_RPF_DISCARD_ERR_E = 3,
+	SXD_MC_FLEX_ACTION_RPF_ACTION_ASSERT_TRAP_E = 6
+} sxd_mc_flex_action_rpf_action_type_t;
+
+typedef enum sxd_mc_flex_action_eir_type{
+	SXD_MC_FLEX_ACTION_EIR_TYPE_IRIF_E = 0,
+	SXD_MC_FLEX_ACTION_EIR_TYPE_IRIF_LIST_E
+} sxd_mc_flex_action_eir_type_t;
+typedef struct sxd_mc_flex_action {
+	sxd_mc_flex_action_rpf_action_type_t   	rpf_action;
+	sxd_mc_flex_action_eir_type_t          	eir_type;
+	uint16_t       					   			expected_irif ;
+	uint32_t 									expected_irif_list_index;
+	uint16_t 									min_mtu;
+	uint8_t 									vrmid;
+	uint32_t 									rigr_rmid_index;
+
+} sxd_mc_flex_action_t;
+
+typedef struct sxd_action_slot {
+  sxd_flex_acl_action_type_t                             type;
+  union {
+      sxd_mac_flex_action_t                            action_mac;
+      sxd_vlan_flex_action_t                           action_vlan;
+      sxd_trap_flex_action_t                           action_trap;
+      sxd_trap_w_user_defined_flex_action_t            action_trap_w_user_defined;
+      sxd_port_filter_flex_action_t                    action_port_filter;
+      sxd_qos_flex_action_t                            action_qos;
+      sxd_forward_flex_action_t                        action_forward;
+      sxd_policing_monitoring_flex_action_t            action_policing_monitoring;
+      sxd_metadata_flex_action_t                       action_metadata;
+      sxd_uc_router_flex_action_t                      action_uc_router;
+      sxd_vni_flex_action_t                            action_vni;
+      sxd_mpls_flex_action_t                           action_mpls;
+      sxd_hash_flex_action_t                           action_hash;
+      sxd_virtual_forward_flax_action_t           	   action_virtual_forward;
+      sxd_ignore_flex_action_t                         action_ignore;
+      sxd_mc_flex_action_t                       	   action_mc;
+  }                                                    fields;
+}sxd_action_slot_t;
+
+
+
+typedef struct sxd_ptce2_goto_set_action {
+	uint16_t                       	 	next_binding;
+	uint8_t                         	commit;
+    enum sxd_group_or_acl_binding_type 	group_binding;
+    enum sxd_ptce2_binding_cmd     	 	binding_cmd;
+    uint8_t 							clear;
+}sxd_ptce2_goto_set_action_t;
+
+typedef struct sxd_flex_action_set {
+    struct sxd_action_slot                  action_slots[SXD_ACL_NUM_OF_ACTION_SLOTS];
+    enum sxd_ptce2_next_type                next_type;
+    union {
+        uint32_t                            next_action_set_ptr;
+        struct sxd_ptce2_goto_set_action    goto_set_action;
+   } next_goto_record;
+} sxd_flex_action_set_t;
+
+typedef enum sxd_egress_or_igress_type{
+	SXD_EGRESS_OR_IGRESS_TYPE_IGRESS = 0,
+	SXD_EGRESS_OR_IGRESS_TYPE_EGRESS
+}sxd_egress_or_igress_type_t;
+
+typedef  struct ku_ptce2_reg {
+	uint8_t                             valid;
+    uint8_t                             activity;
+    sxd_ptce_operation_t                op;
+    uint16_t                            offset;
+    uint8_t                             tcam_region_info[SXD_ACL_INFO_SIZE_BYTES];
+    uint8_t                             flex_key_blocks[SXD_ACL_FLEX_KEY_BLOCK_SIZE_BYTES];
+    uint8_t                             flex_mask_blocks[SXD_ACL_FLEX_KEY_BLOCK_SIZE_BYTES];
+    sxd_flex_action_set_t               action_set;
+
+ }ku_ptce2_reg_t;
+
+typedef enum ku_prbt_reg_op_type{
+  KU_PRBT_REG_OP_TYPE_BIND_ACL      = 0,
+  KU_PRBT_REG_OP_TYPE_UNBIND_ACL
+}ku_prbt_reg_op_type_t;
+
+typedef struct ku_prbt_reg {
+    sxd_group_or_acl_binding_type_t      group_binding;
+    sxd_egress_or_igress_type_t          egress_indication;
+    uint16_t                                acl_id_grp_id;
+    uint16_t                                rif;
+    ku_prbt_reg_op_type_t                op;
+}ku_prbt_reg_t;
+
+typedef struct ku_pefa_reg {
+    uint32_t                    index;
+    sxd_flex_action_set_t       action_set;
+}ku_pefa_reg_t;
+
+typedef struct sxd_flex_extraction_point{
+	 uint8_t     enable;
+	 uint8_t     offset;
+}sxd_flex_extraction_point_t;
+
+typedef  struct ku_pecb_reg {
+    uint8_t     cbset;
+    sxd_flex_extraction_point_t  extraction_points[SXD_ACL_NUM_OF_EXTRACTION_POINT];
+}ku_pecb_reg_t;
+
+typedef struct sxd_pemb_recorde_multicast_egress{
+	 uint8_t     group_id;
+	 uint8_t     valid;
+	 uint32_t    egress_port_list_63_32;
+	 uint32_t    egress_port_list_31_0;
+}sxd_pemb_recorde_multicast_egress_t;
+
+typedef enum ku_pemb_entry_type{
+  KU_PEMB_ENTRY_TYPE_SPREADING_E      		= 0x1,
+  KU_PEMB_ENTRY_TYPE_EGRESS_ACL_MULTICAST_E   = 0x2,
+  KU_PEMB_ENTRY_TYPE_RESERVED_E      			= 0x3
+}ku_pemb_entry_type_t;
+
+typedef struct ku_pemb_reg {
+	ku_pemb_entry_type_t   type;
+    union {
+    		sxd_pemb_recorde_multicast_egress_t multicast_egress;
+    	} record;
+}ku_pemb_reg_t;
+
+
+/**
+ * ku_prcr_reg structure is used to store the PRCR register parameters
+ */
+typedef struct ku_prcr_reg {
+    uint8_t  op;
+    uint16_t offset;
+    uint16_t size;
+    uint8_t  tcam_region_info[SXD_ACL_INFO_SIZE_BYTES];
+    uint16_t dest_offset;
+    uint8_t  dest_tcam_region_info[SXD_ACL_INFO_SIZE_BYTES];
+}ku_prcr_reg_t;
+
+/**
+ * sxd_acl_paclr_acl_type enumerated type is used to note
+ * the PTAR operation.
+ */
+typedef enum sxd_acl_pacl_acl_type {
+    SXD_PACL_ACL_TYPE_ALL = 0,
+    SXD_PACL_ACL_TYPE_L3 = 1,
+}sxd_acl_pacl_acl_type_t;
+
+/**
+ * ku_pacl_reg structure is used to store the PACL register parameters
+ */
+typedef struct ku_pacl_reg {
+    uint8_t                    egress;
+    uint8_t                    valid;
+    sxd_acl_pacl_acl_type_t acl_type;
+    uint16_t                   acl_id;
+    uint8_t                    tcam_region_info[SXD_PACL_TCAM_REGIONS][SXD_ACL_INFO_SIZE_BYTES];
+}ku_pacl_reg_t;
+
+/**
+ * sxd_acl_ptar_op enumerated type is used to note the
+ * PTAR operation.
+ */
+typedef enum sxd_acl_ptar_op {
+    SXD_PTAR_OP_ALLOCATE_E = 0,
+    SXD_PTAR_OP_RESIZE_E = 1,
+    SXD_PTAR_OP_DEALLOCATE_E = 2,
+    SXD_PTAR_OP_TEST_ALLOCATE_E = 3,
+} sxd_acl_ptar_op_t;
+
+
+
+/**
+ * sxd_acl_ptar_action_type enumerated type is used to
+ * note the PTAR action type.
+ */
+typedef enum sxd_acl_ptar_action_type {
+    SXD_PTAR_DEFAULT_ACTION_E = 0,
+    SXD_PTAR_EXTENDED_ACTION_E = 1,
+}sxd_acl_ptar_action_type_t;
+
+
+/**
+ * sxd_acl_ptar_action_type enumerated type is used to
+ * note the PTAR action type.
+ */
+typedef enum sxd_acl_ptar_key_type {
+    SXD_PTAR_KEY_IPv4_FULL_E = 0,
+    SXD_PTAR_KEY_IPv6_FULL_E= 1,
+    SXD_PTAR_KEY_MAC_FULL_E = 2,
+    SXD_PTAR_KEY_MAC_IPv4_FULL_E = 3,
+    SXD_PTAR_KEY_MAC_SHORT_E = 4,
+	SXD_PTAR_KEY_CONFIGURABLE_FULL_E = 16,
+    SXD_PTAR_KEY_FCOE_FULL_E = 32,
+	SXD_PTAR_KEY_FLEX_KEY_E = 0x50
+}sxd_acl_ptar_key_type_t;
+
+/**
+ * sxd_ptar_optimization_type_t enumerated type is used to note the TCAM
+ * optimization mode.
+ */
+typedef enum sxd_ptar_optimization_type {
+    SXD_PTAR_TCAM_NO_OPTIMIZATION_E = 0,
+    SXD_PTAR_TCAM_SOFT_OPTIMIZATION_E = 1,
+    SXD_PTAR_TCAM_HARD_OPTIMIZATION_E = 2,
+    SXD_PTAR_TCAM_RESERVED_E = 3,
+} sxd_ptar_optimization_type_t;
+
+typedef enum sxd_acl_ptar_direction {
+    SXD_PTAR_ACL_DIRECTION_INGRESS_E= 0,
+    SXD_PTAR_ACL_DIRECTION_EGRESS_E = 1
+}sxd_acl_ptar_direction_t;
+
+/**
+ * ku_ptar_reg structure is used to store the PTAR register parameters
+ */
+typedef struct ku_ptar_reg {
+    sxd_acl_ptar_op_t         op;
+    sxd_acl_ptar_action_type_t action_type;
+    sxd_acl_ptar_key_type_t    key_type;
+    uint16_t                      region_size;
+    uint16_t                      region_id;
+    uint8_t						  packet_rate;
+    sxd_ptar_optimization_type_t  op_type;
+    uint8_t                       tcam_region_info[SXD_TCAM_REGION_INFO_SIZE_BYTES];
+    uint8_t                       flexible_key_id[SXD_FLEXIBLE_KEY_BLOCK_REG_SIZE_BYTES];
+    sxd_acl_ptar_direction_t  direction;
+}ku_ptar_reg_t;
+
+/**
+ * sxd_acl_ffar_op enumerated type is used to note the
+ * FFAR operation.
+ */
+enum sxd_acl_ffar_op {
+    SXD_FFAR_OP_ALLOCATE = 0,
+    SXD_FFAR_OP_RESIZE = 1,
+    SXD_FFAR_OP_DEALLOCATE = 2,
+    SXD_FFAR_OP_TEST_ALLOCATE = 3,
+};
+
+
+/**
+ * sxd_fc_optimization_type_t enumerated type is used to note the TCAM
+ * optimization mode.
+ */
+typedef enum sxd_fc_optimization_type {
+    SXD_FC_TCAM_NO_OPTIMIZATION = 0,
+    SXD_FC_TCAM_SOFT_OPTIMIZATION = 1,
+    SXD_FC_TCAM_HARD_OPTIMIZATION = 2,
+    SXD_FC_TCAM_RESERVED = 3,
+} sxd_fc_optimization_type_t;
+
+/**
+ * ku_ffar_reg structure is used to store the FFAR register parameters
+ */
+struct ku_ffar_reg {
+    enum sxd_acl_ffar_op       op;
+    uint16_t                   region_size;
+    sxd_fc_optimization_type_t op_type;
+};
+
+/**
+ * ku_pgcr_reg structure is used to store the PGCR register
+ * parameters
+ */
+typedef struct ku_pgcr_reg {
+	uint16_t pbs_table_size;
+	uint16_t max_eacl;
+	uint16_t max_iacl;
+	uint16_t parsing_depth;
+}ku_pgcr_reg_t;
+
+/**
+ * sxd_acl_ppbs_type enumerated type is used to note the PPBS
+ * entry type.
+ */
+typedef enum sxd_acl_ppbs_type {
+    SXD_PPBS_ENTRY_UNICAST_E = 0,
+    SXD_PPBS_ENTRY_LAG_E = 1,
+    SXD_PPBS_ENTRY_MULTICAST_E = 2,
+	SXD_PPBS_ENTRY_TUNNEL_UNICAST_E = 0xC,
+	SXD_PPBS_ENTRY_TUNNEL_MULTICAST_E = 0xF,
+}sxd_acl_ppbs_type_t;
+
+
+/**
+ * sxd_acl_ppbs_uni_action enumerated type is used to note the
+ * PPBS unicast entry action field types.
+ */
+typedef enum sxd_acl_ppbs_uni_action {
+    SXD_PPBS_UNICAST_ACTION_FWD_E = 0,
+    SXD_PPBS_UNICAST_ACTION_FWD_TO_ROUTER_E = 0x3,
+    SXD_PPBS_UNICAST_ACTION_FWD_TO_FCF_E = 0x4,
+    SXD_PPBS_UNICAST_ACTION_DROP_E = 0xF
+}sxd_acl_ppbs_uni_action_t;
+
+/**
+ * sxd_ppbs_uni_record structure is used to store PPBS unicast
+ * record.
+ */
+typedef struct sxd_ppbs_uni_record {
+	uint8_t                      v_fid;
+    uint8_t                      sub_port;
+    uint16_t                     fid;
+    sxd_acl_ppbs_uni_action_t action;
+    uint16_t                     system_port;
+}sxd_ppbs_uni_record_t;
+
+/**
+ * sxd_ppbs_lag_record structure is used to store PPBS LAG
+ * record.
+ */
+typedef struct sxd_ppbs_lag_record {
+    uint8_t  sub_port;
+    uint8_t  update_vid;
+    uint16_t vid;
+    uint16_t lag_id;
+}sxd_ppbs_lag_record_t;
+
+
+/**
+ * sxd_acl_ppbs_uni_action enumerated type is used to note the
+ * PPBS unicast entry action field types.
+ */
+typedef enum sxd_acl_ppbs_mcast_action {
+    SXD_PPBS_MCAST_ACTION_FWD_E = 0,
+}sxd_acl_ppbs_mcast_action_t;
+
+
+
+
+
+/**
+ * sxd_ppbs_lag_record structure is used to store PPBS LAG
+ * record.
+ */
+
+
+typedef struct sxd_ppbs_mcast_record {
+	uint8_t                        v_fid;
+	uint16_t                       pgi;
+    sxd_acl_ppbs_mcast_action_t action;
+	uint16_t                       fid;
+    uint16_t                       mid;
+}sxd_ppbs_mcast_record_t;
+
+typedef struct sxd_ppbs_uni_tunnel_cast_record {
+	uint32_t                       udip;
+	uint8_t                       protocol;
+
+}sxd_ppbs_uni_tunnel_cast_record_t;
+
+typedef struct sxd_ppbs_multi_tunnel_cast_record {
+	uint16_t                   		underlay_mc_ptr_msb;
+	uint8_t						underlay_mc_ptr_lsb;
+	uint8_t                         v_fid;
+	uint16_t                        fid;
+	uint16_t                        mid;
+
+}sxd_ppbs_multi_tunnel_cast_record_t;
+
+/**
+ * ku_ppbs_reg structure is used to store the PPBS register
+ * parameters
+ */
+typedef struct ku_ppbs_reg {
+    uint8_t                swid;
+    sxd_acl_ppbs_type_t type;
+    uint32_t               index;
+    union {
+        sxd_ppbs_uni_record_t   unicast;
+        sxd_ppbs_lag_record_t   lag;
+        sxd_ppbs_mcast_record_t mcast;
+        sxd_ppbs_uni_tunnel_cast_record_t tunnel_unicast;
+        sxd_ppbs_multi_tunnel_cast_record_t tunnel_mcast;
+    } pbs_record;
+}ku_ppbs_reg_t;
+
+/**
+ * ku_puet_reg structure is used to store the PUET register
+ * parameters
+ */
+typedef struct ku_puet_reg {
+    uint8_t  index;
+    uint16_t ethertype;
+}ku_puet_reg_t;
+
+/**
+ * sxd_router_arp_operation_t enumerated type is used to note the ARP
+ * operation.
+ */
+typedef enum sxd_router_arp_operation {
+    SXD_ROUTER_ARP_OPERATION_ALLOCATE = 0,
+    SXD_ROUTER_ARP_OPERATION_WRITE = 1,
+    SXD_ROUTER_ARP_OPERATION_DEALLOCATE = 2,
+} sxd_router_arp_operation_t;
+
+/**
+ * Adjacency Index.
+ */
+typedef uint16_t sxd_adj_index_t;
+
+/**
+ * Adjacency Index MSB.
+ */
+typedef uint8_t sxd_adj_index_msb_t;
+
+/**
+ * ARP ID.
+ */
+typedef uint32_t sxd_arp_id_t;
+
+/**
+ * ARP Info.
+ */
+typedef uint32_t sxd_arp_info_t;
+
+/**
+ * Ethernet Adjacency Parameters Layout
+ */
+typedef struct ku_struct_eth_adj_parameters {
+    uint8_t destination_mac[6];
+} ku_eth_adj_parameters_t;
+
+/**
+ * IPoIB Unicast Adjacency Parameters Layout
+ */
+typedef struct ku_pkey_uni_without_grh_parameters {
+    uint8_t  sl;
+    uint16_t dlid;
+    uint32_t dqpn;
+    uint8_t  my_lid;
+} ku_pkey_uni_without_grh_parameters_t;
+
+/**
+ * IPoIB Multicast Adjacency Parameters Layout
+ */
+typedef struct ku_pkey_multi_parameters {
+    uint8_t  sl;
+    uint16_t dlid;
+    uint8_t  hoplimit;
+    uint8_t  tclass;
+} ku_pkey_multi_parameters_t;
+
+/**
+ * MPLS Adjacency Parameters Layout
+ */
+typedef struct ku_mpls_adj_parameters {
+    uint32_t nhlfe_ptr;
+    uint16_t ecmp_size;
+} ku_mpls_adj_parameters_t;
+
+/**
+ * Underlay Destination IP Types
+ */
+typedef enum sxd_udip_type {
+    SXD_UDIP_TYPE_IPV4 = 0,
+    SXD_UDIP_TYPE_IPV6 = 1,
+    SXD_UDIP_TYPE_AUTO_TUNNEL_IPV6_TO_IPV4 = 2,
+    SXD_UDIP_TYPE_AUTO_TUNNEL_ISATAP = 3,
+} sxd_udip_type_e;
+
+/**
+ * L3 Tunnel Encap Adjacency Parameters Layout
+ */
+typedef struct ku_l3_tunnel_encap_adj_parameters {
+    sxd_udip_type_e udip_type;
+    uint32_t        ipv4_udip;
+    uint32_t        ipv6_ptr;
+} ku_l3_tunnel_encap_adj_parameters_t;
+
+/*
+ * RATR - adjacency_parameters
+ */
+typedef union ku_adj_parameters {
+    ku_eth_adj_parameters_t              eth_adj_parameters;
+    ku_pkey_uni_without_grh_parameters_t pkey_uni_without_grh_parameters;
+    ku_pkey_multi_parameters_t           pkey_multi_parameters;
+    ku_mpls_adj_parameters_t             mpls_adj_parameters;
+    ku_l3_tunnel_encap_adj_parameters_t  l3_tunnel_encap_adj_parameters;
+} ku_adj_parameters_t;
+
+/**
+ * sxd_router_adjacency_table_type enumerated type is used to note the Adjacency Entry Type
+ * operation.
+ */
+typedef enum sxd_router_adjacency_table_type {
+    ETHERNET = 0,
+    PKEY_UNI_WITHOUT_GRH = 1,
+    PKEY_UNI_WITH_GRH = 2,
+    PKEY_MULTI = 3,
+    MPLS = 4,
+    L3_TUNNEL_ENCAP = 5,
+} sxd_router_adjacency_table_type_t;
+
+/**
+ * sxd_router_adjacency_table enumerated is used to identify the table type
+ */
+
+typedef enum sxd_router_adjacency_table {
+    ETHERNET_UNICAST_ADJACENCY = 0,
+    IPoIB_UNICAST_AND_MULTICAST_ADJACENCY = 1,
+    IPoIB_ALL_ROUTERS_ADJACENCY = 2,
+    IPoIB_IP_BROADCAST_ADJACENCY = 3,
+} sxd_router_adjacency_table_t;
+
+/**
+ * sxd_router_adjacency_validate type is used to note if an
+ * adjecency entry should be added or subtracted.
+ */
+typedef enum sxd_router_adjacency_validate {
+    SXD_ROUTE_ADJECENCY_DELETE = 0,
+    SXD_ROUTE_ADJECENCY_WRITE = 1,
+} sxd_router_adjacency_validate_t;
+
+/**
+ * Router Interface Group.
+ */
+typedef uint16_t sxd_rif_group_t;
+
+/**
+ * Router Interface.
+ */
+typedef uint16_t sxd_rif_t;
+
+/**
+ * sxd_router_route_action_t enumerated type is used to note the route action.
+ */
+typedef enum sxd_router_action_type {
+    SXD_ROUTER_ROUTE_ACTION_PERMIT = 0,
+    SXD_ROUTER_ROUTE_ACTION_SOFT_DROP = 1,
+    SXD_ROUTER_ROUTE_ACTION_TRAP = 2,
+    SXD_ROUTER_ROUTE_ACTION_SOFT_DROP_TRAP = 3,
+    SXD_ROUTER_ROUTE_ACTION_DENY = 4,
+} sxd_router_route_action_t;
+
+/**
+ * sxd_router_en_route_action_t enumerated type is used to note the route action in RouterEN.
+ */
+typedef enum sxd_router_en_action {
+    SXD_ROUTER_EN_ACTION_NOP = 0,
+    SXD_ROUTER_EN_ACTION_TRAP = 1,
+    SXD_ROUTER_EN_ACTION_MIRROR_TO_CPU = 2,
+    SXD_ROUTER_EN_ACTION_MIRROR = 3,
+    SXD_ROUTER_EN_ACTION_DISCARD_ERROR = 4,
+    SXD_ROUTER_EN_ACTION_MIN = SXD_ROUTER_EN_ACTION_NOP,
+    SXD_ROUTER_EN_ACTION_MAX = SXD_ROUTER_EN_ACTION_DISCARD_ERROR,
+} sxd_router_en_action_t;
+
+/**
+ * ku_ratr_reg structure is used to store the RATR register parameters
+ */
+struct ku_ratr_reg {
+    sxd_router_arp_operation_t        operation;
+    sxd_router_adjacency_validate_t   valid;
+    uint16_t                          size;
+    sxd_router_adjacency_table_type_t type;
+    uint8_t                           table;
+    sxd_adj_index_t                   adjacency_index;
+    sxd_rif_t                         egress_rif;
+    ku_adj_parameters_t               adj_parameters;
+    sxd_router_en_action_t            trap_action;
+    sxd_adj_index_msb_t               adjacency_index_msb;
+    uint16_t                          trap_id;
+    sxd_counter_set_t                 counter_set;
+};
+
+/**
+ * Virtual Router ID.
+ */
+typedef uint16_t sxd_vrid_t;
+
+/**
+ * FCF instance ID.
+ */
+typedef uint8_t sxd_fcf_id_t;
+
+#define SX_FC_ADDR_LEN 3
+
+typedef struct sxd_fc_addr {
+    uint8_t fc_addr[SX_FC_ADDR_LEN];
+} sxd_fc_addr_t;
+
+/**
+ * sxd_router_route_type_t enumerated type is used to note the
+ * route type.
+ */
+typedef enum sxd_router_route_type {
+    SXD_ROUTER_ROUTE_TYPE_IPV4 = 0,
+    SXD_ROUTER_ROUTE_TYPE_IPV6 = 1,
+} sxd_router_route_type_t;
+
+typedef enum sxd_router_tcam_write_operation {
+    /*On write register*/
+    SXD_ROUTER_TCAM_WRITE = 0,
+    SXD_ROUTER_TCAM_UPDATE = 1,
+    SXD_ROUTER_TCAM_CLEAR_ACTIVITY = 2,
+    /*On read register*/
+    SXD_ROUTER_TCAM_READ = 0,
+    SXD_ROUTER_ACTIVITY_CLEAR_ON_READ = 1,
+} sxd_router_tcam_write_operation_t;
+
+typedef enum sxd_kvd_hash_operation {
+    /*On write register*/
+    SXD_KVD_HASH_OPERATION_ADD = 0,
+    SXD_KVD_HASH_OPERATION_UPDATE,
+    SXD_KVD_HASH_OPERATION_CLEAR_ACTIVITY,
+    SXD_KVD_HASH_OPERATION_DELETE,
+    SXD_KVD_HASH_OPERATION_DELETE_ALL,
+    /*On read register*/
+    SXD_KVD_HASH_OPERATION_READ = 0,
+    SXD_KVD_HASH_OPERATION_READ_CLEAR = 1,
+} sxd_kvd_hash_operation_t;
+
+typedef enum sxd_fcf_tcam_write_operation {
+    SXD_FCF_TCAM_WRITE = 0,
+    SXD_FCF_TCAM_UPDATE = 1,
+    SXD_FCF_TCAM_CLEAR_ACTIVITY = 2,
+} sxd_fcf_tcam_write_operation_t;
+
+/**
+ * sxd_router_qos_t enumerated type is used to note the route
+ * quality of service .
+ */
+typedef enum {
+    SXD_ROUTER_QOS_MAP_PRIO_FROM_DSCP = 0,
+    SXD_ROUTER_QOS_PRESERVE_PRIO = 1,
+} sxd_router_qos_t;
+
+/*
+ * Egress Port Type for Fibre Channel protocols
+ */
+typedef enum sxd_ept {
+    SXD_FCF_FCOE_VF_PORT = 0,
+    SXD_FCF_FCOE_VE_PORT = 1,
+    SXD_FCF_FC_VF_PORT = 2,
+    SXD_FCF_FC_VE_PORT = 3,
+    SXD_FCF_FCOIB_VF_PORT = 4,
+    SXD_FCF_FCOIB_VE_PORT = 5,
+} sxd_ept_t;
+
+typedef enum sxd_counter_opcode {
+    SXD_COUNTER_OPCODE_NOP = 0,
+    SXD_COUNTER_OPCODE_ADD_COUNTERS = 1,
+    SXD_COUNTER_OPCODE_FLUSH_COUNTERS = 2,
+    SXD_COUNTER_OPCODE_CLEAR_COUNTERS = 8,
+} sxd_counter_opcode_t;
+
+/**
+ * ku_mgpc_reg structure is used to store the MGPC register
+ * parameters
+ */
+struct ku_mgpc_reg {
+    sxd_counter_set_t counter_set;
+    uint8_t           counter_opcode;         /** counter opcode */
+    uint64_t          byte_counter;         /**<  byte counter*/
+    uint64_t          packet_counter;         /**< packet counter*/
+};
+
+/**
+ * ku_ruft_reg structure is used to store the RUFT register parameters
+ */
+struct ku_ruft_reg {
+    uint8_t                           valid;
+    sxd_router_route_type_t           route_type;
+    sxd_router_tcam_write_operation_t operation;
+    uint8_t                           activity;
+    uint16_t                          offset;
+    sxd_vrid_t                        router;
+    uint32_t                          destination_ip[4];
+    uint32_t                          destination_ip_mask[4];
+    uint32_t                          ecmp_hash;
+    uint32_t                          ecmp_hash_mask;
+    sxd_router_route_action_t         route_action;
+    uint8_t                           trap_group;
+    uint16_t                          trap_id;
+    uint8_t                           qos;
+    uint32_t                          ecmp_size;
+    uint8_t                           table;
+    sxd_adj_index_t                   adjacency_index;
+};
+
+/**
+ * ku_fftr_reg structure is used to store the FFTR register parameters
+ */
+struct ku_fftr_reg {
+    uint8_t                        valid;
+    sxd_fcf_tcam_write_operation_t operation;
+    uint16_t                       offset;
+    sxd_fcf_id_t                   fcf;
+    sxd_fc_addr_t                  did;
+    sxd_fc_addr_t                  did_mask;
+    sxd_fc_addr_t                  sid;
+    sxd_fc_addr_t                  sid_mask;
+    sxd_ept_t                      ept;
+    uint16_t                       ve_port_index;  /* Should be 0 for vf_ports */
+};
+
+/**
+ * ku_ruht_reg structure is used to store the RUHT register parameters
+ */
+struct ku_ruht_reg {
+    uint16_t                          offset;
+    uint8_t                           offset_enable;
+    uint8_t                           valid;
+    sxd_router_route_type_t           route_type;
+    sxd_router_tcam_write_operation_t operation;
+    uint8_t                           activity;
+    sxd_vrid_t                        router;
+    uint32_t                          destination_ip[4];
+    uint32_t                          ecmp_hash;
+    uint32_t                          ecmp_hash_mask;
+    sxd_router_route_action_t         route_action;
+    uint8_t                           trap_group;
+    uint16_t                          trap_id;
+    uint8_t                           qos;
+    uint8_t                           table;
+    sxd_adj_index_t                   adjacency_index;
+};
+
+/**
+ * ku_ruht_reg structure is used to store the RAUHT register parameters
+ */
+struct ku_rauht_reg {
+    sxd_router_route_type_t  route_type;
+    sxd_kvd_hash_operation_t operation;
+    uint8_t                  activity;
+    sxd_rif_t                rif;
+    uint32_t                 destination_ip[4];
+    sxd_router_en_action_t   trap_action;
+    uint16_t                 trap_id;
+    sxd_counter_set_t        counter_set;
+    struct sx_ether_addr     mac_addr;
+};
+
+/**
+ * Filter feilds for RAUHTD
+ */
+typedef enum sxd_rauhtd_filter_fields {
+    SXD_RAUHTD_FILTER_ACTIVE = 1 << 0,
+        SXD_RAUHTD_FILTER_RIF = 1 << 3,
+} sxd_rauhtd_filter_fields_t;
+
+/**
+ * operation for RAUHTD
+ */
+typedef enum sxd_rauhtd_operation {
+    SXD_RAUHTD_OP_DUMP_ENTRIES = 0,
+    SXD_RAUHTD_OP_CLEAR_ACTIVITY = 1,
+} sxd_rauhtd_operation_t;
+
+/**
+ * sxd_rauhtd_ipv6_entry_t structure is used to store a single IPv6 RAUHTD entry
+ */
+typedef struct sxd_rauhtd_ipv6_entry {
+    sxd_router_route_type_t type;
+    uint8_t                 a;
+    sxd_rif_t               rif;
+    uint32_t                dip[4];
+} sxd_rauhtd_ipv6_entry_t;
+
+/**
+ * sxd_rauhtd_ipv4_single_entry_t structure is used to store a single IPv4 RAUHTD entry
+ */
+typedef struct sxd_rauhtd_ipv4_single_entry {
+    uint8_t   a;
+    sxd_rif_t rif;
+    uint32_t  dip;
+} sxd_rauhtd_ipv4_single_entry_t;
+
+/**
+ * sxd_rauhtd_ipv4_entry_t structure is used to store the IPv4 RAUHTD record
+ */
+typedef struct sxd_rauhtd_ipv4_entry {
+    uint8_t                        num_entries;
+    sxd_router_route_type_t        type;
+    sxd_rauhtd_ipv4_single_entry_t entry[4];
+} sxd_rauhtd_ipv4_entry_t;
+
+/**
+ * sxd_rauhtd_dump_record_t is used to store the RAUHTD dump entries.
+ */
+typedef union sxd_rauhtd_dump_record {
+    sxd_rauhtd_ipv4_entry_t ipv4_entry;
+    sxd_rauhtd_ipv6_entry_t ipv6_entry;
+} sxd_rauhtd_dump_record_t;
+
+/**
+ * ku_rauhtd_reg structure is used to store the RAUHTD register parameters
+ */
+struct ku_rauhtd_reg {
+    uint8_t                  filter_fields;
+    sxd_rauhtd_operation_t   op;
+    uint8_t                  num_of_rec;
+    uint8_t                  entry_a;    /* Boolean - entry active == TRUE */
+    sxd_router_route_type_t  entry_type;
+    sxd_rif_t                entry_rif;
+    sxd_rauhtd_dump_record_t dump_record[SXD_RAUHTD_MAX_REC_NUM];
+};
+
+/**
+ * sxd_acl_ptar_action_type enumerated type is used to
+ * note the PTAR action type.
+ */
+enum sxd_rmft_ttl_cmd {
+    SXD_RMFT_TTL_CMD_DEC = 0,
+    SXD_RMFT_TTL_CMD_SET = 1,
+};
+
+/**
+ * ku_rmft_reg structure is used to store the RMFT register parameters
+ */
+struct ku_rmft_reg {
+    uint8_t                           valid;
+    sxd_router_route_type_t           route_type;
+    sxd_router_tcam_write_operation_t operation;
+    uint8_t                           activity;
+    uint16_t                          offset;
+    sxd_vrid_t                        router;
+    uint32_t                          destination_ip[4];
+    uint32_t                          destination_ip_mask[4];
+    uint32_t                          source_ip[4];
+    uint32_t                          source_ip_mask[4];
+    uint32_t                          ecmp_hash;
+    uint32_t                          ecmp_hash_mask;
+    sxd_router_route_action_t         route_action;
+    uint8_t                           trap_group;
+    uint16_t                          trap_id;
+    uint8_t                           qos;
+    uint8_t                           ttl_cmd;
+    uint8_t                           ttl_value;
+    uint8_t                           rpf;
+    uint8_t                           assert;
+    sxd_rif_t                         expected_ingress_rif;
+};
+
+/**
+ * sxd_router_ecmp_hash_type_t enumarated type is used to store router ECMP hash
+ * type.
+ */
+typedef enum sxd_router_ecmp_hash_type {
+    SXD_ROUTER_ECMP_HASH_TYPE_CRC = 0,
+    SXD_ROUTER_ECMP_HASH_TYPE_XOR = 1,
+    SXD_ROUTER_ECMP_HASH_TYPE_RANDOM = 2,
+} sxd_router_ecmp_hash_type_t;
+
+/**
+ * ku_recr_reg structure is used to store the RECR register parameters
+ */
+struct ku_recr_reg {
+    uint8_t                     symmetric_hash;
+    sxd_router_ecmp_hash_type_t hash_type;
+    uint32_t                    hash_configuration;
+    uint32_t                    seed;
+};
+
+/**
+ * ku_recr_v2_reg structure is used to store the RECRv2 register parameters
+ */
+struct ku_recr_v2_reg {
+    uint8_t                     per_port_configuration;
+    uint8_t                     local_port;
+    uint8_t                     symmetric_hash;
+    sxd_router_ecmp_hash_type_t hash_type;
+    uint32_t                    seed;
+    uint32_t                    general_fields;
+    uint16_t                    outer_header_enables;
+    uint32_t                    outer_header_field_enables[5];
+    uint16_t                    inner_header_enables;
+    uint64_t                    inner_header_field_enables;
+};
+
+/**
+ * sxd_router_tcam_type_t enumerated type is used to note the TCAM type.
+ */
+typedef enum sxd_router_tcam_type {
+    SXD_ROUTER_TCAM_TYPE_IPV4_UNICAST = 0,
+    SXD_ROUTER_TCAM_TYPE_IPV4_MULTICAST = 1,
+    SXD_ROUTER_TCAM_TYPE_IPV6_UNICAST = 2,
+    SXD_ROUTER_TCAM_TYPE_IPV6_MULTICAST = 3,
+    SXD_ROUTER_TCAM_TYPE_IPV4_HOST_TABLE = 4,
+    SXD_ROUTER_TCAM_TYPE_IPV6_HOST_TABLE = 5,
+    SXD_ROUTER_TCAM_TYPE_INVALID
+} sxd_router_tcam_type_t;
+
+/**
+ * sxd_router_tcam_operation_t enumerated type is used to note the TCAM
+ * operation.
+ */
+typedef enum sxd_router_tcam_operation {
+    SXD_ROUTER_TCAM_OPERATION_ALLOCATE = 0,
+    SXD_ROUTER_TCAM_OPERATION_RESIZE = 1,
+    SXD_ROUTER_TCAM_OPERATION_DEALLOCATE = 2,
+    SXD_ROUTER_TCAM_OPERATION_TEST = 3,
+} sxd_router_tcam_operation_t;
+
+/**
+ * sxd_router_optimization_type_t enumerated type is used to note the TCAM
+ * optimization mode.
+ */
+typedef enum sxd_router_optimization_type_ {
+    SXD_ROUTER_TCAM_NO_OPTIMIZATION = 0,
+    SXD_ROUTER_TCAM_SOFT_OPTIMIZATION = 1,
+    SXD_ROUTER_TCAM_HARD_OPTIMIZATION = 2,
+    SXD_ROUTER_TCAM_RESERVED = 3,
+} sxd_router_optimization_type_t;
+
+
+/**
+ * ku_rtar_reg structure is used to store the RTAR register parameters
+ */
+struct ku_rtar_reg {
+    sxd_router_tcam_operation_t    operation;
+    sxd_router_tcam_type_t         type;
+    uint16_t                       tcam_size;
+    sxd_router_optimization_type_t op_type;
+};
+
+/**
+ * Router Interface List - Encoding of this field depends on the type field.
+ */
+
+typedef struct ku_eth_pkey_adjacency_list {
+    uint16_t rif_table;
+    uint16_t adjacency_index;
+} ku_eth_pkey_adjacency_list_t;
+
+typedef struct ku_eth_only_rif_list {
+    uint32_t rif_list[128];
+} ku_eth_only_rif_list_t;
+
+typedef struct ku_eth_pkey_rif_list {
+    uint8_t                      reserved1[3];
+    uint8_t                      size;
+    ku_eth_pkey_adjacency_list_t adj_list[127];
+} ku_eth_pkey_rif_list_t;
+
+typedef union ku_rif_list {
+    ku_eth_only_rif_list_t eth_only_rif_list;
+    ku_eth_pkey_rif_list_t eth_pkey_rif_list;
+} ku_rif_list_t;
+
+/**
+ * sxd_rigr_op_t enumerated type is used to indicates the encoding of the router_interface_list field
+ */
+typedef enum sxd_rigr_op {
+    RIGR_OP_ADD_RIF = 1,
+    RIGR_OP_REMOVE_RIF = 2,
+    RIGR_OP_REMOVE_ALL_RIFS = 3,
+} sxd_rigr_op_t;
+
+/**
+ * sxd_rigr_encoding_t enumerated type is used to indicates the
+ * encoding of the router_interface_list field
+ */
+typedef enum sxd_rigr_encoding {
+    ETH_ONLY = 0,
+    ETH_AND_PKEY = 1,
+} sxd_rigr_encoding_t;
+
+/**
+ * ku_rigr_reg structure is used to store the RIGTR register parameters
+ */
+struct ku_rigr_reg {
+    sxd_router_route_type_t types;
+    uint8_t                 op;
+    sxd_rigr_encoding_t     enc;
+    uint16_t                offset;
+    ku_rif_list_t           rif_list;
+};
+
+/**
+ * ku_router_vlan_interface_properties_t struct is used to store the router interface properties when the router interface type is vlan.
+ */
+
+typedef struct ku_router_vlan_interface_properties {
+    uint8_t  swid;
+    uint16_t vlan_id;
+    uint8_t  router_interface_mac[6];
+    uint8_t  vrrp_id_ipv6;
+    uint8_t  vrrp_id_ipv4;
+} ku_router_vlan_interface_properties_t;
+
+
+/**
+ * ku_router_fid_interface_properties_t struct is used to store the router interface properties when the router interface type is fid.
+ */
+
+typedef struct ku_router_fid_interface_properties {
+    uint8_t  swid;
+    uint16_t fid;
+    uint8_t  router_interface_mac[6];
+    uint8_t  vrrp_id_ipv6;
+    uint8_t  vrrp_id_ipv4;
+} ku_router_fid_interface_properties_t;
+
+
+/**
+ * ku_router_sub_port_interface_properties_t struct is used to store the router interface properties when the router interface type is sub_port.
+ */
+typedef struct ku_router_sub_port_interface_properties {
+    uint8_t  lag;
+    uint16_t system_port;
+    uint8_t  router_interface_mac[6];
+    uint16_t vlan_id;
+    uint8_t  vrrp_id_ipv6;
+    uint8_t  vrrp_id_ipv4;
+} ku_router_sub_port_interface_properties_t;
+
+/**
+ * ku_router_tunnel_interface_properties_t struct is used to store the router interface properties when the router interface type is tunnel.
+ */
+typedef struct ku_router_l3_tunnel_interface_properties {
+    uint8_t  protocol;
+    uint8_t  type;
+    uint8_t  options;
+    uint8_t  uvr;
+    uint32_t usip[4];
+    uint32_t gre_key;
+} ku_router_l3_tunnel_interface_properties_t;
+
+/**
+ * ku_router_vlan_interface_properties_t struct is used to store the router interface properties when the router interface type is pkey.
+ */
+typedef struct ku_router_pkey_interface_properties {
+    uint8_t  swid;
+    uint16_t pkey;
+    uint8_t  scope;
+    uint32_t qkey;
+    uint32_t qpn;
+} ku_router_pkey_interface_properties_t;
+
+/**
+ * rif_properties is used to store the router interface properties depending on the router interface type.
+ */
+typedef union ku_rif_properties {
+    ku_router_vlan_interface_properties_t      vlan_interface;
+    ku_router_fid_interface_properties_t       fid_interface;
+    ku_router_sub_port_interface_properties_t  sub_port_interface;
+    ku_router_l3_tunnel_interface_properties_t l3_tunnel_interface;
+    ku_router_pkey_interface_properties_t      pkey_interface;
+} ku_rif_properties_t;
+
+/**
+ * sxd_router_interface_type_t enumerated type is used to indicates the router interface type
+ */
+typedef enum sxd_router_interface_type {
+    VLAN_INTERFACE = 0,
+    FID_INTERFACE = 1,
+    SUB_PORT_INTERFACE = 2,
+    L3_TUNNEL_INTERFACE = 3,
+    PKEY_INTERFACE = 4,
+    INTERFACE_MIN = VLAN_INTERFACE,
+    INTERFACE_MAX = PKEY_INTERFACE
+} sxd_router_interface_type_t;
+
+/**
+ * sxd_router_interface_op_t enumerated type is used to indicates the router interface op
+ */
+typedef enum sxd_router_interface_op {
+    INTERFACE_CREATE_OR_EDIT = 0,
+    INTERFACE_DELETE = 1
+} sxd_router_interface_op_t;
+
+/**
+ * ku_ritr_reg structure is used to store the RITR register parameters
+ */
+struct ku_ritr_reg {
+    uint8_t                     enable;
+    uint8_t                     valid;
+    uint8_t                     ipv4_enable;
+    uint8_t                     ipv6_enable;
+    uint8_t                     ipv4_mc;
+    uint8_t                     ipv6_mc;
+    sxd_router_interface_type_t type;
+    uint8_t                     type_ib;
+    uint8_t                     op;
+    uint8_t                     mpls;
+    sxd_rif_t                   router_interface;
+    uint8_t                     ipv4_forward_enable;
+    uint8_t                     ipv6_forward_enable;
+    uint8_t                     ipv4_forward_mc;
+    uint8_t                     ipv6_forward_mc;
+    uint8_t                     lb_en;
+    uint8_t                     urpf_en_ipv4;
+    uint8_t                     urpf_en_ipv6;
+    uint8_t                     urpf_strict;
+    uint8_t                     urpf_ad;
+    uint8_t                     mpls_forward;
+    sxd_vrid_t                  router;
+    ku_rif_properties_t         rif_properties;
+    uint8_t                     ttl_threshold;
+    uint16_t                    mtu;
+    sxd_counter_set_t           ingress_counter_set;
+    sxd_counter_set_t           egress_counter_set;
+};
+
+/**
+ * ku_fitr_reg structure is used to store the FITR register parameters
+ */
+struct ku_fitr_reg {
+    uint8_t      valid;
+    sxd_fcf_id_t fcf;
+    uint8_t      fc_map[3];
+    uint8_t      prio; /**< prio - static VLAN prio for FCF egress traffic */
+    uint16_t     vid; /**< vid - VLAN Identifier */
+};
+
+/**
+ * sxd_router_interface_type_t enumerated type is used to indicates the router interface type
+ */
+typedef enum sxd_rgcr_rpf_mode {
+    SXD_RGCR_RPF_MODE_ENABLE = 0,
+    SXD_RGCR_RPF_MODE_DISABLE = 1,
+} sxd_rgcr_rpf_mode_t;
+
+typedef enum sxd_rgcr_pcp_rw_mode {
+    SXD_RGCR_PCP_RW_MODE_PRESERVE = 0,
+    SXD_RGCR_PCP_RW_MODE_DISABLE = 2,
+    SXD_RGCR_PCP_RW_MODE_ENABLE = 3,
+} sxd_rgcr_pcp_rw_mode_t;
+
+typedef enum sxd_rgcr_usp_mode {
+    SXD_RGCR_USP_MODE_PRESERVE = 0,
+    SXD_RGCR_USP_MODE_RECALCULATE = 1,
+} sxd_rgcr_usp_mode_t;
+
+typedef enum sxd_rgcr_op_type {
+    SXD_RGCR_OP_TYPE_OPTIMIZATION_NONE = 0,
+    SXD_RGCR_OP_TYPE_OPTIMIZATION_SOFT = 1,
+    SXD_RGCR_OP_TYPE_OPTIMIZATION_HARD = 2,
+    SXD_RGCR_OP_TYPE_RESERVED = 3,
+} sxd_rgcr_op_type_t;
+
+typedef enum sxd_rgcr_activity_dis {
+    SXD_RGCR_ACTIVITY_ENABLED = 0,
+    SXD_RGCR_ACTIVITY_DISABLED = 1,
+} sxd_rgcr_activity_dis_t;
+
+/**
+ * ku_rgcr_reg structure is used to store the RGCR register parameters
+ */
+struct ku_rgcr_reg {
+    uint8_t                 ipv4_enable;
+    uint8_t                 ipv6_enable;
+    uint8_t                 rif_counter_set_type;
+    uint16_t                max_vlan_router_interfaces;
+    uint16_t                max_port_router_interfaces;
+    uint16_t                max_pkey_router_interfaces;
+    uint16_t                max_router_interfaces;
+    uint16_t                max_virtual_routers;
+    sxd_rgcr_usp_mode_t     usp;
+    sxd_rgcr_pcp_rw_mode_t  pcp_rw;
+    uint8_t                 ipb;
+    uint8_t                 allr;
+    uint8_t                 mcsi;
+    sxd_rgcr_rpf_mode_t     rpf;
+    sxd_rgcr_op_type_t      ipv6_op_type;
+    uint8_t                 ipv6_packet_rate;
+    sxd_rgcr_op_type_t      ipv4_op_type;
+    uint8_t                 ipv4_packet_rate;
+    sxd_rgcr_activity_dis_t activity_dis_uc_route_entry;
+    sxd_rgcr_activity_dis_t activity_dis_host_entry;
+    sxd_rgcr_activity_dis_t activity_dis_adjacency_entry;
+    uint32_t                expected_irif_list_index_base;
+};
+
+/**
+ * ku_fgcr_reg structure is used to store the FGCR register parameters
+ */
+struct ku_fgcr_reg {
+    uint8_t  fcf_enable;
+    uint16_t max_fcf_instances;
+    uint16_t max_ve_ports;
+    uint8_t  fcf_mac[6];
+};
+
+/**
+ * ku_fvet_reg structure is used to store the FVET register parameters
+ */
+struct ku_fvet_reg {
+    uint8_t  valid;
+    uint16_t ve_port_id;
+    uint8_t  dmac[6];
+};
+
+/**
+ * ku_fipl_reg structure is used to store the FIPL register parameters
+ */
+struct ku_fipl_reg {
+    uint8_t ipl[0x000000FF + 1];
+    uint8_t ipl_mask[0x000000FF + 1];
+};
+
+/**
+ * ku_rdpm_reg structure is used to store the RDPM register parameters
+ */
+struct ku_rdpm_reg {
+    uint8_t dscp_update[DSCP_CODES_NUMBER];    /**< dscp_update - whether to update this DSCP mapping in HW */
+    uint8_t color[DSCP_CODES_NUMBER];          /**< color mapping per DSCP value */
+    uint8_t priority[DSCP_CODES_NUMBER];       /**< priority mapping per DSCP value - Priority */
+};
+
+/**
+ * rrcr operation
+ */
+typedef enum sxd_rrcr_opcode {
+    SXD_RRCR_OPCODE_MOVE = 0,
+    SXD_RRCR_OPCODE_COPY = 1,
+} sxd_rrcr_opcode_t;
+
+/**
+ * ku_rrcr_reg structure is used to router rules move/copy
+ * support
+ */
+struct ku_rrcr_reg {
+    sxd_rrcr_opcode_t op;   /**< opcode - 0 - move, 1 -copy */
+    uint16_t          offset; /**< offset - source offset */
+    uint16_t          size; /**< size - num of entries to move/copy */
+    uint8_t           key_type; /**< key_type - defines  the region (same coding as RTAR) */
+    uint16_t          dest_offset; /**< dest_offset - dest_source offset */
+};
+
+/**
+ * sxd_router_counter_operation_t enumerated type is used to note the Counter
+ * Set operation.
+ */
+typedef enum sxd_router_counter_operation {
+    SXD_ROUTER_COUNTER_OPERATION_NOP = 0,
+    SXD_ROUTER_COUNTER_OPERATION_ALLOCATE = 1,
+    SXD_ROUTER_COUNTER_OPERATION_TEST = 2,
+    SXD_ROUTER_COUNTER_OPERATION_FREE = 3,
+} sxd_router_counter_operation_t;
+
+/**
+ * ku_rica_reg structure is used to store the RICA register parameters
+ */
+struct ku_rica_reg {
+    sxd_router_counter_operation_t operation; /**< operation - Counter Operation */
+    uint8_t                        index; /**< index - Counter Set ID */
+    sxd_counter_set_t              ingress_counter_set; /**< ingress_counter_set - Opaque ID */
+    sxd_counter_set_t              egress_counter_set; /**< egress_counter_set - Opaque ID */
+};
+
+/**
+ * Router maximum counter set.
+ */
+#define SXD_ROUTER_COUNTER_SET_MAX (31)
+
+/**
+ * ku_ricnt_reg structure is used to store the RICNT register parameters
+ */
+struct ku_ricnt_reg {
+    uint8_t           clr;                     /**< clr - Clear Counters */
+    uint8_t           flush;                   /**< Flush - Flush Counters */
+    uint8_t           add;                     /**< Add - Add given counters to index */
+    uint8_t           gl;                      /**< gl - Global */
+    sxd_counter_set_t cntr_handle;             /**< index - Counter set index */
+    uint64_t          cntr[SXD_ROUTER_COUNTER_SET_MAX]; /**< cntr - Counter Set */
+};
+
+/*
+ * RALTA operation
+ */
+typedef enum sxd_ralta_operation {
+    SXD_RALTA_OPERATION_ALLOCATE = 0,
+    SXD_RALTA_OPERATION_DEALLOCATE = 1,
+} sxd_ralta_operation_t;
+
+#define SXD_SHSPM_TREE_DEFAULT_IPV4 0
+#define SXD_SHSPM_TREE_DEFAULT_IPV6 1
+#define SXD_SHSPM_TREE_USER_FIRST   2
+
+/**
+ * ku_ralta_reg structure is used to store the RALTA register parameters
+ */
+struct ku_ralta_reg {
+    sxd_ralta_operation_t   operation;
+    sxd_router_route_type_t protocol;
+    uint8_t                 tree_id;
+};
+
+/*
+ * Maximum LPM/SHSPM bins (= IPv6 maximum prefix length)
+ */
+#define SXD_RALST_MAX_BIN 128
+
+/*
+ * Tree terminator magic number. Denotes no-child
+ */
+#define SXD_RALST_NO_CHILD 0xFF
+
+/**
+ * Stores child bins of a bin in a SHSPM tree
+ */
+typedef struct sxd_ralst_children {
+    uint8_t left_child;
+    uint8_t right_child;
+} sxd_ralst_children_t;
+
+/**
+ * ku_ralst_reg structure is used to store the RALST register parameters
+ */
+struct ku_ralst_reg {
+    uint8_t              root_bin;
+    uint8_t              tree_id;
+    sxd_ralst_children_t structure[SXD_RALST_MAX_BIN];
+};
+
+/**
+ * ku_raltb_reg structure is used to store the RALTB register parameters
+ */
+struct ku_raltb_reg {
+    sxd_vrid_t              router;
+    sxd_router_route_type_t protocol;
+    uint8_t                 tree_id;
+};
+
+/**
+ * sxd_ralue_format_t enumerated type is used to note the
+ * format for RALUE register.
+ */
+typedef enum sxd_ralue_action_type {
+    SXD_RALUE_ACTION_TYPE_REMOTE = 0,
+    SXD_RALUE_ACTION_TYPE_LOCAL = 1,
+    SXD_RALUE_ACTION_TYPE_IP2ME = 2,
+} sxd_ralue_action_type_t;
+
+/**
+ * sxd_ralue_type_t enumerated type is used to note the
+ * type of RALUE register.
+ */
+typedef enum sxd_ralue_type {
+    SXD_RALUE_TYPE_MARKER = 1,
+    SXD_RALUE_TYPE_ROUTE = 2,
+    SXD_RALUE_TYPE_ROUTE_MARKER = 3,
+} sxd_ralue_type_t;
+
+/**
+ * ku_ralue_action_remote structure is used to store the RALUE action remote field parameters
+ */
+struct ku_ralue_action_remote {
+    sxd_router_en_action_t trap_action;
+    uint16_t               trap_id;
+    uint32_t               adjacency_index;
+    uint16_t               ecmp_size;
+};
+
+/**
+ * ku_ralue_action_local structure is used to store the RALUE action local field parameters
+ */
+struct ku_ralue_action_local {
+    sxd_router_en_action_t trap_action;
+    uint16_t               trap_id;
+    sxd_rif_t              egress_rif;
+};
+
+/**
+ * ku_ralue_action_ip2me structure is used to store the RALUE action ip2me field parameters
+ */
+struct ku_ralue_action_ip2me {
+    uint8_t  valid;
+    uint32_t tunnel_ptr;
+};
+
+/**
+ * ku_ralue_action structure is used to store the RALUE action field parameters
+ */
+union ku_ralue_action {
+    struct ku_ralue_action_remote remote;
+    struct ku_ralue_action_local  local;
+    struct ku_ralue_action_ip2me  ip2me;
+};
+
+/**
+ * ku_ralue_reg structure is used to store the RALUE register parameters
+ */
+struct ku_ralue_reg {
+    sxd_router_route_type_t  protocol;
+    sxd_kvd_hash_operation_t operation;
+    uint8_t                  activity;
+    sxd_vrid_t               router;
+    uint8_t                  update_entry_type;
+    uint8_t                  update_bmp_len;
+    uint8_t                  update_action_fields;
+    uint8_t                  prefix_len;
+    uint32_t                 destination_ip[4];
+    sxd_ralue_type_t         entry_type;
+    uint8_t                  bmp_len;
+    sxd_ralue_action_type_t  action_type;
+    union ku_ralue_action    action;
+    sxd_counter_set_t        counter_set;
+};
+
+/**
+ * ku_raleu_reg structure is used to store the RALEU register parameters
+ */
+struct ku_raleu_reg {
+    sxd_router_route_type_t protocol;
+    sxd_vrid_t              router;
+    uint32_t                old_adjacency_index;
+    uint16_t                old_ecmp_size;
+    uint32_t                new_adjacency_index;
+    uint16_t                new_ecmp_size;
+};
+
+/**
+ * ku_ralbu_reg structure is used to store the RALBU register parameters
+ */
+struct ku_ralbu_reg {
+    sxd_router_route_type_t protocol;
+    sxd_vrid_t              router;
+    uint8_t                 old_bmp;
+    uint8_t                 bin;
+    uint8_t                 new_bmp;
+    uint8_t                 prefix_len;
+    uint32_t                destination_ip[4];
+};
+
+typedef struct ib_addr1 {
+    u_int8_t addr_octet[16];
+} __attribute__ ((__packed__)) sxd_gid_t;
+
+/**
+ * ku_rtca_reg structure is used to store the RTCA register parameters
+ */
+struct ku_rtca_reg {
+    uint8_t   swid;   /**< swid - Switch partition ID */
+    uint8_t   lmc;    /**< lmc  - Lid Mask Count*/
+    uint16_t  lid;    /**< lid  - Base LID for the port*/
+    sxd_gid_t gid;     /**< gid  - Global ID*/
+};
+
+/**
+ * sxd_tca_log_pstate_type_t enumerated type is used to indicates the TCA logical port state type
+ */
+typedef enum sxd_tca_log_pstate_type {
+    SXD_TCA_LOG_PSTATE_DOWN = 0,
+    SXD_TCA_LOG_PSTATE_INIT = 1,
+    SXD_TCA_LOG_PSTATE_ARM = 2,
+    SXD_TCA_LOG_PSTATE_ACTIVE = 3,
+} sxd_tca_log_pstate_type_t;
+
+/**
+ * sxd_tca_ phy_pstate_type_t enumerated type is used to indicates the TCA physical port state type
+ */
+typedef enum sxd_tca_phy_pstate_type {
+    SXD_TCA_PHY_PSTATE_DOWN = 0,
+    SXD_TCA_PHY_PSTATE_POLLING = 1,
+    SXD_TCA_PHY_PSTATE_UP = 2,
+} sxd_tca_phy_pstate_type_t;
+
+/**
+ * ku_rtps_reg structure is used to store the RTPS register parameters
+ */
+struct ku_rtps_reg {
+    uint8_t                   swid;     /**< swid - Switch partition ID */
+    sxd_tca_log_pstate_type_t tca_log_pstate;      /**< TCA logical port state*/
+    sxd_tca_phy_pstate_type_t tca_phy_pstate;      /**< TCA physical port state*/
+    sxd_tca_log_pstate_type_t switch_log_pstate;      /**< Switch logical port state*/
+    sxd_tca_phy_pstate_type_t switch_phy_pstate;      /**< Switch physical port state*/
+};
+
+
+/**
+ * ku_rcap_reg structure is used to store the RCAP register parameters
+ */
+struct ku_rcap_reg {
+    uint8_t rif;          /**< rif - Router Interface */
+    uint8_t vir_router;   /**< vir_router  - virtual router*/
+};
+
+/**
+ * ku_pspa_reg structure is used to store the PSPA register parameters
+ */
+struct ku_pspa_reg {
+    uint8_t swid; /**< swid - Switch partition ID */
+    uint8_t local_port; /**< local_port - local port number */
+    uint8_t sub_port; /**< sub_port - sub port number */
+};
+
+/**
+ * ku_pmlp_reg structure is used to store the PMLP register parameters
+ */
+struct ku_pmlp_reg {
+    uint8_t use_different_rx_tx; /** < use_different_rx_tx - use different rx and tx lanes */
+    uint8_t local_port; /**< local_port - local port number */
+    uint8_t width;  /**< width - width */
+    uint8_t lane[NUMBER_OF_SERDESES]; /**< lane - Lane - up to 4 serdeses in a module can be mapped to a local port */
+    uint8_t rx_lane[NUMBER_OF_SERDESES]; /**< lane - Lane - up to 4 serdeses in a module can be mapped to a local port */
+    uint8_t module[NUMBER_OF_SERDESES]; /**< module - Module number */
+};
+
+#define SXD_MGIR_HW_DEV_ID_SX          0xc738
+#define SXD_MGIR_HW_DEV_ID_SWITCH_IB   0xcb20
+#define SXD_MGIR_HW_DEV_ID_SPECTRUM    0xcb84
+#define SXD_MGIR_HW_DEV_ID_SWITCH_IB2  0xcf08
+#define SXD_MGIR_HW_REV_ID_SX_A0       0xA0
+#define SXD_MGIR_HW_REV_ID_SX_A1       0xA1
+#define SXD_MGIR_HW_REV_ID_SX_A2       0xA2
+#define SXD_MGIR_HW_REV_ID_SWITCHIB_A0 0xA0
+#define SXD_MGIR_HW_REV_ID_SLAVE_DEV   0xFF
+
+struct ku_mgir_hw_info {
+    uint16_t device_hw_revision;
+    uint16_t device_id;
+    uint8_t  dvfs;
+    uint32_t uptime;
+};
+struct ku_mgir_fw_info {
+    uint8_t  major;
+    uint8_t  minor;
+    uint8_t  sub_minor;
+    uint32_t build_id;
+    uint8_t  month;
+    uint8_t  day;
+    uint16_t year;
+    uint16_t hour;
+    uint8_t  psid[16];
+    uint32_t ini_file_version;
+    uint32_t extended_major;
+    uint32_t extended_minor;
+    uint32_t extended_sub_minor;
+};
+struct ku_mgir_sw_info {
+    uint8_t major;
+    uint8_t minor;
+    uint8_t sub_minor;
+};
+
+/**
+ * ku_mgir_reg structure is used to store the MGIR register parameters
+ */
+struct ku_mgir_reg {
+    struct ku_mgir_hw_info hw_info; /**< hw_info - HW information */
+    struct ku_mgir_fw_info fw_info; /**< fw_info - FW information */
+    struct ku_mgir_sw_info sw_info; /**< sw_info - SW information */
+};
+
+/**
+ * ku_plib_reg structure is used to store the PLIB register parameters
+ */
+struct ku_plib_reg {
+    uint8_t local_port; /**< local_port - local port number */
+    uint8_t ib_port; /**< ib_port -  IB Port remapping for local_port */
+};
+
+/**
+ * ku_spzr_reg structure is used to store the SPZR register parameters
+ */
+struct ku_spzr_reg {
+    uint8_t  swid; /**< swid - SWitch partition ID */
+    uint8_t  ndm; /**< ndm - Node description mask. Set to 1 to write the NodeDescription field */
+    uint8_t  EnhSwP0_mask; /**< EnhSwP0_mask - Enhanced Switch Port 0 mask */
+    uint8_t  cm; /**< cm - Set PortInfo:CapabilityMask to PortInfo:CapabilityMask specified */
+    uint8_t  vk; /**< vk - Set the internal GSA V_Key */
+    uint8_t  mp; /**< mp - Change PKey table size to max_pkey */
+    uint8_t  sig; /**< sig - Set System Image GUID to system_image_guid specified */
+    uint8_t  ng; /**< ng - Set node GUID to node_guid specified */
+    uint8_t  g0; /**< g0 - Set port GUID0 to GUID0 specified */
+    uint8_t  EnhSwP0; /**< EnhSwP0 - When set, it enables Enhanced Switch Port 0. Reported in NodeInfo */
+    uint32_t capability_mask; /**< capability_mask - Sets the PortInfoCapabilityMask: Specifies the supported capabilities of this node */
+    uint64_t system_image_guid_h_l; /**< system_image_guid_h_l - System Image GUID, takes effect only if the sig bit is set */
+    uint64_t guid0_h_l; /**< guid0_h_l - EUI-64 GUID assigned by the manufacturer */
+    uint64_t node_guid_h_l; /**< node_guid_h_l - Node GUID must be the same for both ports */
+    uint32_t v_key_h; /**< v_key_h - The internal GSA V_Key (high) */
+    uint32_t v_key_l; /**< v_key_l - The internal GSA V_Key (low) */
+    uint16_t max_pkey; /**< max_pkey - max_pkey is derived from the profile - no set. Maximum pkeys for the port */
+    uint8_t  NodeDescription[64]; /**< NodeDescription - Text string that describes the node */
+};
+
+/**
+ * ku_oepft_reg structure is used to store the OEPFT register parameters
+ */
+struct ku_oepft_reg {
+    uint8_t  sr; /**< sr - Send/Receive */
+    uint32_t flow_number; /**< flow_number - Flow number (SDQ/RDQ) */
+    uint8_t  cpu_tclass; /**< cpu_tclass - Send Flow: CPU Egress TClass*/
+    uint8_t  interface; /**< interface - interface 0=SGMII link 0, 1=SGMII link 1 */
+    uint64_t mac; /**< mac - Destination MAC address for the returned packets */
+};
+
+/**
+ * ku_paos_reg structure is used to store the PAOS register parameters
+ */
+struct ku_paos_reg {
+    uint8_t swid; /**< swid - Switch partition ID of local_port, valid on set operation, for IB GW ports only */
+    uint8_t local_port; /**< local_port - local port number */
+    uint8_t admin_status; /**< admin_status - Port administrative state (the desired state of the interface) */
+    uint8_t oper_status; /**< oper_status - Port operational state */
+    uint8_t ase; /**< ase - Admin State Update Enable */
+    uint8_t ee; /**< ee - Event Update Enable */
+    uint8_t e; /**< e - Event Generation on operational state change */
+};
+
+/**
+ * ku_plpc_reg structure is used to store the PLPC register parameters
+ */
+struct ku_plpc_reg {
+    uint16_t profile_id; /**< profile_id - Profile ID (bitmask)  */
+    uint8_t  proto_mask; /**< proto_mask - Protocol Mask. Indicates which of the protocol data is valid */
+    uint16_t lane_speed; /**< lane_speed - Per lane speed (bitmask) */
+    uint8_t  lpbf; /**< lpbf - Link Performance Based FEC */
+    uint8_t  fec_mode_policy; /**< fec_mode_policy - FEC decision policy */
+    uint8_t  retransmission_capability; /**< retransmission capability (bitmask) */
+    uint32_t fec_mode_capability; /**< fec_mode_capability - FEC capability (bitmask) */
+    uint8_t  retransmission_support_admin; /**< retransmission_support_admin - Retransmission support admin (bitmask) */
+    uint32_t fec_mode_support_admin; /**< fec_mode_support_admin - FEC support admin (bitmask) */
+    uint8_t  retransmission_request_admin; /**< retransmission_request_admin - Retransmission request admin (bitmask) */
+    uint32_t fec_mode_request_admin; /**< fec_mode_request_admin - FEC request admin (bitmask) */
+};
+
+/**
+ * ku_pplm_reg structure is used to store the PPLM register parameters
+ */
+struct ku_pplm_reg {
+    uint8_t  local_port; /**< local_port - Local port number */
+    uint8_t  port_profile_mode; /**< port_profile_mode - Port profile modes (bitmask) */
+    uint8_t  static_port_profile; /**< static_port_profile - Valid when Media based port profile is cleared, used to set the static port's profile index. */
+    uint8_t  active_port_profile; /**< active_port_profile - The port's active Profile ID */
+    uint8_t  retransmission_active; /**< retransmission_active - Active Retransmission */
+    uint32_t fec_mode_active; /**< fec_mode_active - Acive FEC (bitmask) */
+    uint8_t  fec_override_cap_100g; /**< 100GE Ethernet FEC override capability bitmask */
+    uint8_t  fec_override_cap_50g; /**< 50GE Ethernet FEC override capability bitmask */
+    uint8_t  fec_override_cap_25g; /**< 25GE Ethernet FEC override capability bitmask */
+    uint8_t  fec_override_cap_10g_40g; /**< 10/40GE Ethernet FEC override capability bitmask */
+    uint8_t  fec_override_admin_100g; /**< 100GE Ethernet FEC override admin */
+    uint8_t  fec_override_admin_50g; /**< 50GE Ethernet FEC override admin */
+    uint8_t  fec_override_admin_25g; /**< 25GE Ethernet FEC override capability admin */
+    uint8_t  fec_override_admin_10g_40g; /**< 10/40GE Ethernet FEC override capability admin */
+};
+
+/**
+ * jtag_transaction_set structure is used to store the JTAG
+ * Transaction Set Byte Layout
+ */
+struct jtag_transaction_set {
+    uint8_t tms;
+    uint8_t tdi;
+    uint8_t tdo;
+};
+
+/**
+ * ku_mjtag_reg structure is used to store the MJTAG register
+ * parameters
+ */
+struct ku_mjtag_reg {
+    uint8_t                     cmd; /**< Command  */
+    uint8_t                     seq_num; /** < Command Sequest Number */
+    uint8_t                     size; /**< Size of operation */
+    struct jtag_transaction_set jtag_transaction_sets[MAX_TRANSACTIONS_NUM];
+};
+
+/**
+ * ku_pmpr_reg structure is used to store the PMPR register parameters
+ */
+struct ku_pmpr_reg {
+    uint8_t module; /**< module number */
+    uint8_t attenuation5g; /**< Attenuation5G */
+    uint8_t attenuation7g; /**< Attenuation7G */
+    uint8_t attenuation12g; /**< Attenuation12G */
+};
+
+/**
+ * ku_pmaos_reg structure is used to store the PMAOS register parameters
+ */
+struct ku_pmaos_reg {
+    uint8_t module; /**< module - Module number */
+    uint8_t admin_status; /**< admin_status - Port administrative state (the desired state of the interface) */
+    uint8_t oper_status; /**< oper_status - Port operational state */
+    uint8_t ase; /**< ase - Admin State Update Enable */
+    uint8_t ee; /**< ee - Event Update Enable */
+    uint8_t e; /**< e - Event Generation on operational state change */
+};
+
+/**
+ * ku_pmtu_reg structure is used to store the PMTU register parameters
+ */
+struct ku_pmtu_reg {
+    uint8_t  local_port; /**< local_port - local port number */
+    uint16_t max_mtu; /**< max_mtu - Maximum MTU supported on the port (Read Only) */
+    uint16_t admin_mtu; /**< admin_mtu - Administratively configured MTU on the port */
+    uint16_t oper_mtu; /**< oper_mtu - Operational MTU */
+};
+
+/**
+ * ku_sbcm_reg structure is used to store the SBCM register parameters
+ */
+struct ku_sbcm_reg {
+    uint8_t  local_port; /**< local_port - Local port number */
+    uint8_t  pg_buff; /**< pg_buff - Port PG */
+    uint8_t  dir; /**< dir - Direction */
+    uint32_t buff_occupancy; /**< buff_occupancy - Current buffer occupancy */
+    uint32_t max_buff_occupancy; /**< max_buff_occupancy - Maximum value of buffer occupancy monitored */
+    uint32_t clr; /**< clr - Clear max buffer occupancy - when set the max value is cleared */
+    uint32_t min_buff; /**< min_buff - Minimum buffer size for the limiter */
+    uint32_t max_buff; /**< max_buff - Maximum buffer size for the limiter in cells or "alpha" */
+    uint8_t  pool; /**< pool - Association of the port-priority to a pool*/
+};
+
+/**
+ * ku_sbpm_reg structure is used to store the SBPM register parameters
+ */
+struct ku_sbpm_reg {
+    uint8_t  local_port; /**< local_port - Local port number */
+    uint8_t  pool; /**< pool - Association of the port-priority to a pool*/
+    uint8_t  dir; /**< dir - Direction */
+    uint32_t buff_occupancy; /**< buff_occupancy - Current buffer occupancy */
+    uint32_t max_buff_occupancy; /**< max_buff_occupancy - Maximum value of buffer occupancy monitored */
+    uint32_t clr; /**< clr - Clear max buffer occupancy - when set the max value is cleared */
+    uint32_t min_buff; /**< min_buff - Minimum buffer size for the limiter */
+    uint32_t max_buff; /**< max_buff - Maximum buffer size for the limiter in cells or "alpha" */
+};
+
+/**
+ * ku_sbmm_reg structure is used to store the SBMM register parameters
+ */
+struct ku_sbmm_reg {
+    uint8_t  prio; /**< prio - Switch Priority*/
+    uint32_t buff_occupancy; /**< buff_occupancy - Current buffer occupancy */
+    uint32_t max_buff_occupancy; /**< max_buff_occupancy - Maximum value of buffer occupancy monitored */
+    uint32_t clr; /**< clr - Clear max buffer occupancy - when set the max value is cleared */
+    uint32_t min_buff; /**< min_buff - Minimum buffer size for the limiter */
+    uint32_t max_buff; /**< max_buff - Maximum buffer size for the limiter in cells or "alpha" */
+    uint8_t  pool; /**< pool - Association of the switch priority to a pool*/
+};
+
+/**
+ * ku_pplr_reg structure is used to store the PPLR register
+ * parameters
+ */
+struct ku_pplr_reg {
+    uint8_t local_port; /**< local_port - Local port number */
+    uint8_t el; /**< el - Egress Loopback Enable */
+    uint8_t il; /**< il - Ingress Loopback Enable */
+};
+
+/**
+ * ku_mfcr_reg structure is used to store the MFCR register parameters
+ */
+struct ku_mfcr_reg {
+    uint8_t  pwm_frequency; /**< pwm_frequency - Controls the frequency of the PWM signal */
+    uint8_t  pwm_active; /**< pwm_active - Indicates which of the PWM control is active (bit per PWM) */
+    uint16_t tacho_active; /**< tacho_active - Indicates which of the tachometer is active (bit per tachometer)*/
+};
+
+/**
+ * ku_mfsc_reg structure is used to store the MFSC register parameters
+ */
+struct ku_mfsc_reg {
+    uint8_t pwm; /**< pwm - Pwm to control / monitor */
+    uint8_t pwm_duty_cycle; /**< pwm_duty_cycle - Controls the duty cycle of the PWM */
+};
+
+/**
+ * ku_mfsm_reg structure is used to store the MFSM register parameters
+ */
+struct ku_mfsm_reg {
+    uint8_t  tacho; /**< tacho - Fan tachometer index */
+    uint8_t  n; /**< n - Defines the number of tacho pulses duration in which the SwitchX counts TBD mSec periods */
+    uint16_t rpm; /**< rpm - Fan speed */
+};
+
+/**
+ * ku_mfsl_reg structure is used to store the MFSL register parameters
+ */
+struct ku_mfsl_reg {
+    uint8_t  fan; /**< fan - Fan tachometer index */
+    uint8_t  ee; /**< ee - Event Enable */
+    uint8_t  ie; /**< ie - Interrupt Enable */
+    uint16_t tach_min; /**< tach_min - Tachometer minimum value */
+    uint16_t tach_max; /**< tach_max - Tachometer maximum value*/
+};
+
+/**
+ * ku_fore_reg structure is used to store the FORE register parameters
+ */
+struct ku_fore_reg {
+    uint16_t fan_under_limit; /**< fan_under_limit - Fan speed is below the low limit defined in MFSL register */
+    uint16_t fan_over_limit; /**< fan_over_limit - Fan speed is above the high limit defined in MFSL register */
+};
+
+/**
+ * ku_mtcap_reg structure is used to store the MTCAP register parameters
+ */
+struct ku_mtcap_reg {
+    uint8_t sensor_count; /**< sensor_count - Number of sensors supported by the device */
+};
+
+/**
+ * ku_mtmp_reg structure is used to store the MTMP register parameters
+ */
+struct ku_mtmp_reg {
+    uint8_t  sensor_index; /**< sensor_index - Sensors index to access */
+    uint16_t temperature; /**< temperature - Temperature reading from the sensor. Reading in 0.125 Celsius degrees */
+    uint8_t  mte; /**< mte - Max Temperature Enable - enables measuring the max temperature on a sensor */
+    uint8_t  mtr; /**< mtr - Max Temperature Reset - clears the value of the max temperature register */
+    uint16_t max_temperature; /**< max_temperature - The highest measured temperature from the sensor */
+    uint8_t  tee; /**< tee - Temperature Event Enable */
+    uint16_t temperature_threshold; /**< temperature_threshold - If the sensor temperature measurement is above the threshold (and events are enabled), an event will be generated */
+};
+
+/**
+ * ku_mmdio_reg structure is used to store the MMDIO register parameters
+ */
+struct ku_mmdio_reg {
+    uint8_t  mdio_index; /**< mdio_index - MDIO index */
+    uint8_t  operation; /**< operation - operation */
+    uint32_t address; /**< address - Address. If clause XX is used, only the lower 16 bits are valid */
+    uint32_t data; /**< data - Data. If clause XX is used, only the lower 16 bits are valid */
+};
+
+/**
+ * ku_mmia_reg structure is used to store the MMIA register parameters
+ */
+struct ku_mmia_reg {
+    uint8_t  operation; /**< operation - operation */
+    uint32_t data; /**< data - data */
+};
+
+/**
+ * ku_mfpa_reg structure is used to store the MFPA register parameters
+ */
+struct ku_mfpa_reg {
+    uint8_t  p; /**< p - Parallel */
+    uint8_t  fs; /**< fs - Flash Select */
+    uint32_t boot_address; /**< boot_address - Boot address points to the FW image in the flash */
+    uint8_t  flash_num; /**< flash_num - Number of Flash Devices connected */
+    uint32_t jedec_id; /**< jedec_id - Flash JEDEC ID */
+    uint16_t block_allignment; /**< block_allignment - Required allignment for block access */
+    uint16_t sector_size; /**< sector_size - Flash Sector Size */
+    uint8_t  capability_mask; /**< capability_mask - Capability Mask
+                               *  Bit 0: Parallel Flash Support
+                               *  Else: Reserved */
+};
+
+/**
+ * ku_mfbe_reg structure is used to store the MFBE register parameters
+ */
+struct ku_mfbe_reg {
+    uint8_t  p; /**< p - Parallel */
+    uint8_t  fs; /**< fs - Flash Select */
+    uint32_t address; /**< address - address in bytes */
+};
+
+/**
+ * ku_mfba_reg structure is used to store the MFBA register parameters
+ */
+struct ku_mfba_reg {
+    uint8_t  p; /**< p - Parallel */
+    uint8_t  fs; /**< fs - Flash Select */
+    uint16_t size; /**< size - Transaction size */
+    uint32_t address; /**< address - address in bytes */
+    uint8_t  data[192]; /**< data - Data */
+};
+
+/**
+ * ku_qcap_reg structure is used to store the QCAP register parameters
+ */
+struct ku_qcap_reg {
+    uint8_t max_policers_per_port; /**< max_policers_per_port - Maximum number of policers available per port */
+    uint8_t max_policers_global; /**< max_policers_global - Maximum number of global policers */
+};
+
+/**
+ * ku_raw_reg structure is used to store the RAW register parameters
+ */
+struct ku_raw_reg {
+    uint8_t *buff;     /**< buff - the register buffer */
+    uint16_t size;    /**< size - the buffer size */
+};
+
+/**
+ * ku_mtwe_reg structure is used to store the MTWE register parameters
+ */
+struct ku_mtwe_reg {
+    uint8_t sensor_warning; /**< sensor_warning - Bit vector indicating which of the sensor reading is above thereshold */
+};
+
+/**
+ * ku_pelc_reg structure is used to store the PELC register parameters
+ */
+struct ku_pelc_reg {
+    uint8_t  op; /**< Operation - 0 - FEC control 1 - LLR control*/
+    uint8_t  local_port; /**< local_port - Local port number */
+    uint8_t  op_admin; /**< op_admin - Operation specific administratively enabled properties */
+    uint8_t  op_capability; /**< op_capability - Operation specific properties capabilities */
+    uint8_t  op_request; /**< op_request - Operation specific properties request */
+    uint8_t  op_active; /**< op_active - Operation specific properties active */
+    uint64_t admin; /**< admin - Link administratively enabled */
+    uint64_t capability; /**< capability - Port Extended Property supported */
+    uint64_t request;
+    uint64_t active; /**< active - Link operational mode */
+};
+
+/**
+ * ku_spad_reg structure is used to store the SPAD register parameters
+ */
+struct ku_spad_reg {
+    uint64_t base_mac; /**< base_mac - Base MAC address */
+};
+
+/**
+ * ku_pvlc_reg structure is used to store the PVLC register parameters
+ */
+struct ku_pvlc_reg {
+    uint8_t local_port; /**< local_port - Local port number */
+    uint8_t vl_cap; /**< vl_cap - Virtual Lanes supported on this port */
+    uint8_t vl_admin; /**< vl_admin - Virtual Lanes enabled by the local admin on this port */
+    uint8_t vl_operational; /**< vl_operational - Virtual Lanes Operational as configured by the Subnet Manager on this port */
+};
+
+/**
+ * ku_mcia_reg structure is used to store the MCIA register parameters
+ */
+struct ku_mcia_reg {
+    uint8_t  l; /**< l - Lock bit. Setting this bit will lock the access to the specific cable */
+    uint8_t  module; /**< module - module number */
+    uint8_t  status; /**< status - module status */
+    uint8_t  i2c_device_address; /**< i2c_device_address - I2C device address */
+    uint8_t  page_number; /**< page_number - Page number */
+    uint16_t device_address; /**< device_address - Device address */
+    uint16_t size; /**< size - Number of bytes to read/write (up to 48 bytes) */
+    uint32_t dword_0; /**< dword_0 - Bytes to read/write */
+    uint32_t dword_1; /**< dword_1 - Bytes to read/write */
+    uint32_t dword_2; /**< dword_2 - Bytes to read/write */
+    uint32_t dword_3; /**< dword_3 - Bytes to read/write */
+    uint32_t dword_4; /**< dword_4 - Bytes to read/write */
+    uint32_t dword_5; /**< dword_5 - Bytes to read/write */
+    uint32_t dword_6; /**< dword_6 - Bytes to read/write */
+    uint32_t dword_7; /**< dword_7 - Bytes to read/write */
+    uint32_t dword_8; /**< dword_8 - Bytes to read/write */
+    uint32_t dword_9; /**< dword_9 - Bytes to read/write */
+    uint32_t dword_10; /**< dword_10 - Bytes to read/write */
+    uint32_t dword_11; /**< dword_11 - Bytes to read/write */
+};
+
+/**
+ * ku_pptb_reg structure is used to store the PPTB register parameters
+ */
+struct ku_pptb_reg {
+    uint8_t local_port; /**< local_port - Local port number */
+    uint8_t cm; /**< cm - ctrl_buff mask, enables update the ctrl_buff field */
+    uint8_t um; /**< um - untagged_buff mask, enables update the untagged_buff field */
+    uint8_t pm; /**< pm - pioX_buff mask, enables update the pioX_buff field */
+    uint8_t prio_0_buff; /**< prio_0_buff - Mapping of Prio 0 to one of the allocated receive port buffers. */
+    uint8_t prio_1_buff; /**< prio_1_buff - Mapping of Prio 1 to one of the allocated receive port buffers. */
+    uint8_t prio_2_buff; /**< prio_2_buff - Mapping of Prio 2 to one of the allocated receive port buffers. */
+    uint8_t prio_3_buff; /**< prio_3_buff - Mapping of Prio 3 to one of the allocated receive port buffers. */
+    uint8_t prio_4_buff; /**< prio_4_buff - Mapping of Prio 4 to one of the allocated receive port buffers. */
+    uint8_t prio_5_buff; /**< prio_5_buff - Mapping of Prio 5 to one of the allocated receive port buffers. */
+    uint8_t prio_6_buff; /**< prio_6_buff - Mapping of Prio 6 to one of the allocated receive port buffers. */
+    uint8_t prio_7_buff; /**< prio_7_buff - Mapping of Prio 7 to one of the allocated receive port buffers. */
+    uint8_t prio_8_buff; /**< prio_0_buff - Mapping of Prio 0 to one of the allocated receive port buffers. */
+    uint8_t prio_9_buff; /**< prio_1_buff - Mapping of Prio 1 to one of the allocated receive port buffers. */
+    uint8_t prio_10_buff; /**< prio_2_buff - Mapping of Prio 2 to one of the allocated receive port buffers. */
+    uint8_t prio_11_buff; /**< prio_3_buff - Mapping of Prio 3 to one of the allocated receive port buffers. */
+    uint8_t prio_12_buff; /**< prio_4_buff - Mapping of Prio 4 to one of the allocated receive port buffers. */
+    uint8_t prio_13_buff; /**< prio_5_buff - Mapping of Prio 5 to one of the allocated receive port buffers. */
+    uint8_t prio_14_buff; /**< prio_6_buff - Mapping of Prio 6 to one of the allocated receive port buffers. */
+    uint8_t prio_15_buff; /**< prio_7_buff - Mapping of Prio 7 to one of the allocated receive port buffers. */
+    uint8_t untagged_buff; /**< untagged_buff - Mapping of untagged frames to one of the allocated receive port buffers. */
+    uint8_t ctrl_buff; /**< ctrl_buff - Mapping of control frames to one of the allocated receive port buffers. */
+    uint8_t prio_buff_msb; /**< prio_buff_msb - Prio to buff mask. */
+    uint8_t mapping_mode; /**< mm - mapping mode */
+};
+
+/**
+ * ku_pfcc_reg structure is used to store the PFCC register parameters
+ */
+struct ku_pfcc_reg {
+    uint8_t local_port; /**< local_port - Local port number */
+    uint8_t prio_mask_tx; /**< prio_mask_tx - Bit per prio inticating if TX flow control policy should be updated based on the below */
+    uint8_t prio_mask_rx; /**< prio_mask_rx - Bit per prio inticating if RX flow control policy should be updated based on the below */
+    uint8_t pause_policy_tx; /**< pause_policy_tx - Pause policy on TX */
+    uint8_t prio_policy_tx; /**< prio_policy_tx - Priority based Flow Control policy on TX */
+    uint8_t cb_policy_tx; /**< cb_policy_tx - Credit Based Flow control policy in TX */
+    uint8_t pause_policy_rx; /**< pause_policy_rx - Pause policy on RX */
+    uint8_t prio_policy_rx; /**< prio_policy_rx - Priority based Flow Control policy on RX */
+    uint8_t cb_policy_rx; /**< cb_policy_rx - Credit Based Flow control policy in RX */
+};
+
+/**
+ * ku_pcap_reg structure is used to store the PCAP register parameters
+ */
+struct ku_pcap_reg {
+    uint8_t  local_port; /**< local_port - Local port number */
+    uint32_t port_capability_mask[4]; /**< port_capability_mask - Sets the PortInfoCapabilityMask */
+};
+
+/**
+ * ku_pude_reg structure is used to store the PUDE register parameters
+ */
+struct ku_pude_reg {
+    uint8_t local_port; /**< local_port - Local port number */
+    uint8_t oper_status; /**< oper_status - Port operational state */
+};
+
+/**
+ * ku_pmpe_reg structure is used to store the PMPE register parameters
+ */
+struct ku_pmpe_reg {
+    uint8_t module_id; /**< module_id - Port module number */
+    uint8_t oper_status; /**< oper_status - Port operational state */
+};
+
+
+/**
+ * ku_pmpc_reg structure is used to store the PMPC register parameters
+ */
+struct ku_pmpc_reg {
+    uint32_t module_state_updated_bitmap[8]; /**< module_state_updated - A bit vector. each bit represent a module plugged/unplugged*/
+};
+
+
+/**
+ * ku_mpsc_reg sturcture is used to store the MPSC register parameters
+ */
+struct ku_mpsc_reg {
+    uint8_t local_port;   /**< local_port - local port number */
+    uint8_t clear_count;  /**< clear_count - clear counter */
+    uint8_t enable;       /**< enable - enable sampling on local_port */
+    uint32_t rate;        /**< rate - sampling rate (total pkt/sampled pkt) */
+    uint64_t count_drops; /**< count_drops - count of frames after sampling */
+};
+
+
+/**
+ * ku_ppcnt_ib_port_counters structure is used to store the PPCNT register Infiniband
+ * port counters parameters
+ */
+struct ku_ppcnt_ib_port_counters {
+    uint16_t symbol_error_counter;
+    uint8_t  link_error_recovery_counter;
+    uint8_t  link_downed_counter;
+    uint16_t port_rcv_errors;
+    uint16_t port_rcv_remote_physical_errors;
+    uint16_t port_rcv_switch_relay_errors;
+    uint16_t port_xmit_discards;
+    uint8_t  port_xmit_constraint_errors;
+    uint8_t  port_rcv_constraint_errors;
+    uint8_t  local_link_integrity_errors;
+    uint8_t  excessive_buffer_overrun_errors;
+    uint16_t vl_15_dropped;
+    uint32_t port_xmit_data;
+    uint32_t port_rcv_data;
+    uint32_t port_xmit_pkts;
+    uint32_t port_rcv_pkts;
+    uint32_t port_xmit_wait;
+};
+
+
+/**
+ * ku_ppcnt_ib_port_counters_extended structure is used to store the PPCNT register Infiniband
+ * port counters extended parameters
+ */
+struct ku_ppcnt_ib_port_counters_extended {
+    uint64_t port_xmit_data;
+    uint64_t port_rcv_data;
+    uint64_t port_xmit_pkts;
+    uint64_t port_rcv_pkts;
+    uint64_t port_unicast_xmit_pkts;
+    uint64_t port_unicast_rcv_pkts;
+    uint64_t port_multicast_xmit_pkts;
+    uint64_t port_multicast_rcv_pkts;
+};
+
+/**
+ * ku_ppcnt_ib_port_rcv_err_details structure is used to store the PPCNT register Infiniband
+ * port receive error details parameters
+ */
+struct ku_ppcnt_ib_port_rcv_err_details {
+    uint16_t port_local_physical_errors;
+    uint16_t port_malformed_packet_errors;
+    uint16_t port_buffer_overrun_errors;
+    uint16_t port_dlid_mapping_errors;
+    uint16_t port_vl_mapping_errors;
+    uint16_t port_looping_errors;
+};
+
+/**
+ * ku_ppcnt_ib_port_xmit_discard_details structure is used to store the PPCNT register Infiniband
+ * port xmit discard details parameters
+ */
+struct ku_ppcnt_ib_port_xmit_discard_details {
+    uint16_t port_inactive_discards;
+    uint16_t port_neighbor_mtu_discards;
+    uint16_t port_sw_lifetime_limit_discards;
+    uint16_t port_sw_hoq_lifetime_limit_discards;
+};
+
+/**
+ * ku_ppcnt_ib_port_flow_ctl_counters structure is used to store the PPCNT register Infiniband
+ * port flow control counters parameters
+ */
+struct ku_ppcnt_ib_port_flow_ctl_counters {
+    uint32_t port_xmit_flow_pkts;
+    uint32_t port_rcv_flow_pkts;
+};
+
+/**
+ * ku_ppcnt_ib_port_vl_xmit_wait_counters structure is used to store the PPCNT register Infiniband
+ * port VL xmit wait counters parameters
+ */
+struct ku_ppcnt_ib_port_vl_xmit_wait_counters {
+    uint16_t port_vl_xmit_wait_0;
+    uint16_t port_vl_xmit_wait_1;
+    uint16_t port_vl_xmit_wait_2;
+    uint16_t port_vl_xmit_wait_3;
+    uint16_t port_vl_xmit_wait_4;
+    uint16_t port_vl_xmit_wait_5;
+    uint16_t port_vl_xmit_wait_6;
+    uint16_t port_vl_xmit_wait_7;
+    uint16_t port_vl_xmit_wait_8;
+    uint16_t port_vl_xmit_wait_9;
+    uint16_t port_vl_xmit_wait_10;
+    uint16_t port_vl_xmit_wait_11;
+    uint16_t port_vl_xmit_wait_12;
+    uint16_t port_vl_xmit_wait_13;
+    uint16_t port_vl_xmit_wait_14;
+    uint16_t port_vl_xmit_wait_15;
+};
+
+/**
+ * ku_ppcnt_ib_port_sw_port_vl_congestion structure is used to store the PPCNT register Infiniband
+ * port SW port vl congestion parameters
+ */
+struct ku_ppcnt_ib_port_sw_port_vl_congestion {
+    uint16_t sw_port_vl_congestion_0;
+    uint16_t sw_port_vl_congestion_1;
+    uint16_t sw_port_vl_congestion_2;
+    uint16_t sw_port_vl_congestion_3;
+    uint16_t sw_port_vl_congestion_4;
+    uint16_t sw_port_vl_congestion_5;
+    uint16_t sw_port_vl_congestion_6;
+    uint16_t sw_port_vl_congestion_7;
+    uint16_t sw_port_vl_congestion_8;
+    uint16_t sw_port_vl_congestion_9;
+    uint16_t sw_port_vl_congestion_10;
+    uint16_t sw_port_vl_congestion_11;
+    uint16_t sw_port_vl_congestion_12;
+    uint16_t sw_port_vl_congestion_13;
+    uint16_t sw_port_vl_congestion_14;
+    uint16_t sw_port_vl_congestion_15;
+};
+
+/**
+ * ku_ppcnt_reg structure is used to store the PPCNT register parameters
+ */
+struct ku_ppcnt_reg {
+    uint8_t swid; /**< swid - Switch Partition ID to associate port with */
+    uint8_t local_port; /**< local_port - Local port number */
+    uint8_t cntr_grp; /**< cntr_grp - Performance counter group */
+    uint8_t clr; /**< clr - Clear Counters */
+    uint8_t cntr_prio; /**< cntr_prio - Priority for counter set that support per priority. */
+    union {
+        uint64_t                                      cntr_list[31]; /**< cntr_list - Counter set */
+        struct ku_ppcnt_ib_port_counters              ib_port_counters;
+        struct ku_ppcnt_ib_port_counters_extended     ib_port_counters_extended;
+        struct ku_ppcnt_ib_port_rcv_err_details       ib_port_receive_error_details;
+        struct ku_ppcnt_ib_port_xmit_discard_details  ib_port_xmit_discard_details;
+        struct ku_ppcnt_ib_port_flow_ctl_counters     ib_port_flow_ctl_counters;
+        struct ku_ppcnt_ib_port_vl_xmit_wait_counters ib_port_vl_xmit_wait_counters;
+        struct ku_ppcnt_ib_port_sw_port_vl_congestion ib_port_sw_port_vl_congestion;
+    } cntrs;
+    uint32_t cntr_num; /**< cntr_num - Counter num */
+};
+
+/**
+ * ku_pfca_reg structure is used to store the PFCA register parameters
+ */
+struct ku_pfca_reg {
+    uint8_t  op; /**< op - Operation */
+    uint8_t  type; /**< type - Counter type */
+    uint8_t  index; /**< index - Counter set index */
+    uint32_t flow_counter_handle; /**< flow_counter_handle - Opaque object that represents the flow counter */
+};
+
+/**
+ * ku_pfcnt_reg structure is used to store the PFCNT register parameters
+ */
+struct ku_pfcnt_reg {
+    uint32_t flow_counter_handle; /**< flow_counter_handle - Handle to the flow counter to read */
+    uint8_t  clr; /**< clr - Counter - Setting the clr bit will reset the counter value for relevant flow counter. */
+    uint64_t flow_counter; /**< flow_counter - flow counter value */
+};
+
+
+/**
+ * ku_pbrl_reg structure is used to store the PBRL register parameters
+ */
+struct ku_pbrl_reg {
+    uint8_t  lossy; /**< lossy - The field indicates if the buffer is lossy. 0 - Lossless 1 - Lossy */
+    uint8_t  epsb; /**<  epsb - Eligible for Port Shared buffer */
+    uint16_t size; /**< size - The part of the packet buffer array is allocated for the specific buffer */
+    uint16_t xof_threshold; /**< xof_threshold - Once the amount of data in the buffer goes above this value, SwitchX
+                             *  starts sending Pause frames for all Prios associated with the buffer.
+                             *  Units represent 64 bytes chunks. */
+    uint16_t xon_threshold; /**< xon_threshold - When the amount of data in the buffer goes below this value,
+                             *  SwitchX stops sending Pause frames for the Prios associated with the
+                             *  buffer. Units represent 64 bytes chunks. */
+};
+
+/**
+ * ku_pbmc_reg structure is used to store the PBMC register parameters
+ */
+struct ku_pbmc_reg {
+    uint8_t            local_port; /**< local_port - Local port number */
+    uint16_t           xof_timer_value; /**< xof_timer_value - When SwitchX generates a Pause frame, it uses this value as the Pause timer. */
+    uint16_t           xof_refresh; /**< xof_refresh - The time before a new Pause frame should be sent to refresh the Pause state. Using the same units as xof_timer_value above. */
+    uint16_t           port_buffer_size; /**< port_buffer_size - Total packet buffer array available for the port. The sum of buffer array allocated to bufferX must not exceed port_buffer_size. */
+    struct ku_pbrl_reg buffer[10]; /**< buffer - Configuring per-buffer parameters */
+    struct ku_pbrl_reg port_shared_buffer; /**< port_shared_buffer - Configuring port shared buffer parameters. Using the same layout as in BufferX */
+};
+
+/**
+ * ku_sbpr_reg structure is used to store the SBPR register parameters
+ */
+struct ku_sbpr_reg {
+    uint8_t  direction; /**< Direction - Ingress/ Egress */
+    uint8_t  pool_id; /**< pool_id - pool number 1-16 */
+    uint32_t size; /**< size - pool size in buffers cells*/
+    uint8_t  mode; /**< mode - Absolute/ Relative*/
+    uint32_t current_occupancy; /**< current_buff occupancy*/
+    uint32_t clear; /**< clear - Clear on Read MAX buff occupancy*/
+    uint32_t max_occupancy; /**< Max buff occupancy*/
+};
+
+/**
+ * sxd_emad_shared_buffer_status structure is used to store shared buffer status for SBSR register
+ */
+struct shared_buffer_status {
+    uint32_t buff_occupancy; /**< buff_occupancy - Current buffer occupancy */
+    uint32_t max_buff_occupancy; /**< max_buff_occupancy - Maximum value of buffer occupancy monitored */
+};
+
+/**
+ * ku_sbsr_reg structure is used to store the SBSR register parameters
+ */
+struct ku_sbsr_reg {
+    uint8_t                     clr; /**< Direction - Ingress/ Egress */
+    uint32_t                    ingress_port_mask[SXD_EMAD_SBSR_PORT_MASK_SIZE]; /*< ingress port mask - 256 bits */
+    uint32_t                    pg_buff_mask;
+    uint32_t                    egress_port_mask[SXD_EMAD_SBSR_PORT_MASK_SIZE];
+    uint32_t                    tclass_mask[SXD_EMAD_SBSR_TC_MASK_SIZE];
+    struct shared_buffer_status sbstatus[SXD_EMAD_SBSR_MAX_RET_SIZE];
+};
+
+/**
+ * ku_ppad_reg structure is used to store the PPAD register parameters
+ */
+struct ku_ppad_reg {
+    uint8_t mac[6]; /**< mac - Base MAC address */
+};
+
+/**
+ * ku_ppsc_reg structure is used to store the PPSC register
+ * parameters
+ */
+struct ku_ppsc_reg {
+    uint8_t local_port; /**< local_port - local port number */
+    uint8_t wrps_admin; /**< wrps_admin - Width Reduction Power Save Admin state */
+    uint8_t wrps_status; /**< wrps_status - link actual width */
+    uint8_t up_threshold; /**< up_threshold - Link Width Up Threshold - the amount of data queued on the link before the link goes back to full width */
+    uint8_t down_threshold; /**< down_threshold - Link Width Down Threshold - the amount of quiet time on the link before the link width is moved to single lane */
+    uint8_t srps_admin; /**< srps_admin - Speed Reduction Power Save Admin state */
+    uint8_t srps_status; /**< srps_status - link actual speed */
+};
+
+/**
+ * ku_spmcr_reg structure is used to store the SPMCR register parameters
+ */
+struct ku_spmcr_reg {
+    uint8_t  swid; /**< swid - Switch partition ID */
+    uint8_t  local_port; /**< local_port - Local port number */
+    uint8_t  max_sub_port; /**< max_sub_port - Maximum number of VEPA channels for the port */
+    uint16_t base_stag_vid; /**< base_stag_vid - Base S-Tag to be used on the S Component to represent the first VEPA channel
+                             *  (subport). A subport is represented by S-Tag.VID = base_stag_vid+sub_port. */
+};
+
+/**
+ * ku_hpkt_reg structure is used to store the HPKT register parameters
+ */
+struct ku_hpkt_reg {
+    uint8_t  ack; /**< ack - Acknowledgment required. Only relevant for EVENT trap_id */
+    uint8_t  action; /**< action - Action to perform on trap_id */
+    uint8_t  trap_group; /**< trap_group - Trap Group configured for trap_id */
+    uint16_t trap_id; /**< trap_id - Trap ID to configure */
+    uint8_t  control; /**< control - control type of trap ID */
+};
+
+/**
+ * ku_hcap_reg structure is used to store the HCAP register parameters
+ */
+struct ku_hcap_reg {
+    uint8_t max_cpu_egress_tclass; /**< max_cpu_egress_tclass - Maximum number of CPU egress traffic classes supported */
+    uint8_t max_cpu_ingress_tclass; /**< max_cpu_ingress_tclass - Maximum number of CPU ingress traffic classes supported */
+    uint8_t max_num_trap_groups; /**< max_num_trap_groups - Maximum number of Trap Groups supported */
+    uint8_t max_num_dr_paths; /**< max_num_dr_paths - Maximum number of Direct Route paths in the DR Path table */
+};
+
+/**
+ * ku_hespr_reg structure is used to store the HESPR register parameters
+ */
+struct ku_hespr_reg {
+    uint8_t stacking_tclass; /**< stacking_tclass - Stacking traffic class for Stacking_EN EMAD response */
+    uint8_t cpu_tclass; /**< cpu_tclass - CPU traffic class for Stacking_EN EMAD response */
+    uint8_t rdq; /**< rdq - CPU Receive Descriptor Queue for Stacking_EN EMAD response */
+};
+
+/**
+ * ku_hdrt_reg structure is used to store the HDRT register parameters
+ */
+struct ku_hdrt_reg {
+    uint8_t dr_index; /**< dr_index - Index into the Direct Route table */
+    uint8_t hop_cnt; /**< hop_cnt - Hop Count is used to contain the number of valid elements in path and rpath */
+    uint8_t path[64]; /**< path - Destination port number on forward path */
+    uint8_t rpath[64]; /**< rpath - Destination port number on reverse path */
+};
+
+/**
+ * htgt_local_path structure is used to store the HTGT register local path parameters
+ */
+struct htgt_local_path {
+    uint8_t cpu_tclass; /**< cpu_tclass - CPU traffic class for Trap Group grp */
+    uint8_t rdq; /**< rdq - CPU Receive Descriptor Queue for Trap Group grp */
+};
+
+/**
+ * htgt_stacking_path structure is used to store the HTGT register stacking path parameters
+ */
+struct htgt_stacking_path {
+    uint8_t  stacking_tclass; /**< stacking_tclass - Stacking traffic class for Trap Group grp */
+    uint8_t  cpu_tclass; /**< cpu_tclass - CPU traffic class for Trap Group grp */
+    uint8_t  rdq; /**< rdq - CPU Receive Descriptor Queue for Trap Group grp */
+    uint16_t cpu_sys_port; /**< cpu_sys_port - Designated CPU system port for Trap Group grp */
+};
+
+/**
+ * htgt_dr_path structure is used to store the HTGT register dr path parameters
+ */
+struct htgt_dr_path {
+    uint8_t dr_ptr; /**< dr_ptr - A pointer to the Direct Route Path Table */
+};
+
+/**
+ * htgt_eth_path structure is used to store the HTGT register eth path parameters
+ */
+struct htgt_eth_path {
+    uint64_t mac; /**< mac - MAC Address */
+    uint16_t vid; /**< vid - VLAN Identifier */
+};
+
+/**
+ * htgt_reg_path union is used to store the HTGT register possible paths
+ */
+union htgt_reg_path {
+    struct htgt_local_path    local_path; /**< local_path - local path */
+    struct htgt_stacking_path stacking_path; /**< stacking_path - stacking path */
+    struct htgt_dr_path       dr_path; /**< dr_path - dr path */
+    struct htgt_eth_path      eth_path; /**< eth_path - eth path */
+};
+
+/**
+ * ku_htgt_reg structure is used to store the HTGT register parameters
+ */
+struct ku_htgt_reg {
+    uint8_t             swid; /**< swid - Switch partition id */
+    uint8_t             type; /**< type -  CPU Path Type */
+    uint8_t             trap_group; /**< grp -  Trap Group */
+    uint8_t             pide; /**< pide - Policer Enable */
+    uint8_t             pid; /**< pid - Global Policer ID for Trap Group grp */
+    union htgt_reg_path path; /**< path - Path to designated CPU */
+    uint8_t             mirror_action; /**< mirror_action - trap mirror action */
+    uint8_t             mirror_agent; /**< mirror_agent - mirroring agent */
+    uint8_t             priority; /**< priority - trap group priority */
+};
+
+/**
+ * ku_mfm_reg structure is used to store the MFM register parameters
+ */
+struct ku_mfm_reg {
+    uint8_t  index; /**< index - Index to the Fabric Memory table */
+    uint64_t memory; /**< memory - Holds software-written data (Default value: 0x0) */
+    uint64_t memory_mask; /**< memory_mask - On write commands, indicates which of the bits from memory field are updated */
+};
+
+/**
+ * ku_access_ptys_reg structure is used to store the access register PTYS command parameters
+ */
+struct ku_access_ptys_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_ptys_reg      ptys_reg; /**< ptys_reg - ptys register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_qsptc_reg structure is used to store the access register QSPTC command parameters
+ */
+struct ku_access_qsptc_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_qsptc_reg     qsptc_reg; /**< qsptc_reg - qsptc register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_qstct_reg structure is used to store the access register QSTCT command parameters
+ */
+struct ku_access_qstct_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_qstct_reg     qstct_reg; /**< qstct_reg - qstct_reg register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_pspa_reg structure is used to store the access register PSPA command parameters
+ */
+struct ku_access_pspa_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_pspa_reg      pspa_reg; /**< pspa_reg - pspa register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_pmlp_reg structure is used to store the access register PMLP command parameters
+ */
+struct ku_access_pmlp_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_pmlp_reg      pmlp_reg; /**< pmlp_reg - pmlp register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_mgir_reg structure is used to store the access register MGIR command parameters
+ */
+struct ku_access_mgir_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_mgir_reg      mgir_reg; /**< mgir_reg - mgir register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_plib_reg structure is used to store the access register PLIB command parameters
+ */
+struct ku_access_plib_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_plib_reg      plib_reg; /**< plib_reg - plib register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_spzr_reg structure is used to store the access register SPZR command parameters
+ */
+struct ku_access_spzr_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_spzr_reg      spzr_reg; /**< spzr_reg - spzr register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_oepft_reg structure is used to store the access register OEPFT command parameters
+ */
+struct ku_access_oepft_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_oepft_reg     oepft_reg; /**< oepft_reg - oepft register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_paos_reg structure is used to store the access register PAOS command parameters
+ */
+struct ku_access_paos_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_paos_reg      paos_reg; /**< paos_reg - paos register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_pplm_reg structure is used to store the access register PPLM command parameters
+ */
+struct ku_access_pplm_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_pplm_reg      pplm_reg; /**< pplm_reg - pplm register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_plpc_reg structure is used to store the access register PLPC command parameters
+ */
+struct ku_access_plpc_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_plpc_reg      plpc_reg; /**< plpc_reg - plpc register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_pmpc_reg structure is used to store the access register PMPC command parameters
+ */
+struct ku_access_pmpc_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_pmpc_reg      pmpc_reg; /**< pmpc_reg - pmpc register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_pmpr_reg structure is used to store the access register PMPR command parameters
+ */
+struct ku_access_pmpr_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_pmpr_reg      pmpr_reg; /**< pmpr_reg - pmpr register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_pmaos_reg structure is used to store the access register PMAOS command parameters
+ */
+struct ku_access_pmaos_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_pmaos_reg     pmaos_reg; /**< pmaos_reg - pmaos register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_pmtu_reg structure is used to store the access register PMTU command parameters
+ */
+struct ku_access_pmtu_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_pmtu_reg      pmtu_reg; /**< ku_pmtu_reg - pmtu register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_pplr_reg structure is used to store the access
+ * register PPLR command parameters
+ */
+struct ku_access_pplr_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_pplr_reg      pplr_reg; /**< ku_pplr_reg - pplr register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_mfsc_reg structure is used to store the access register MFSC command parameters
+ */
+struct ku_access_mfsc_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_mfsc_reg      mfsc_reg; /**< mfsc_reg - mfsc register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_mfsm_reg structure is used to store the access register MFSM command parameters
+ */
+struct ku_access_mfsm_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_mfsm_reg      mfsm_reg; /**< mfsm_reg - mfsm register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_mfsl_reg structure is used to store the access register MFSL command parameters
+ */
+struct ku_access_mfsl_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_mfsl_reg      mfsl_reg; /**< mfsl_reg - mfsl register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_mjtag_reg structure is used to store the access register MJTAG command parameters
+ */
+struct ku_access_mjtag_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_mjtag_reg     mjtag_reg; /**< mjtag_reg - mjtag register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_ppsc_reg structure is used to store the access
+ * register PPSC command parameters
+ */
+struct ku_access_ppsc_reg {
+    struct ku_operation_tlv op_tlv;     /**< op_tlv - operation tlv struct */
+    struct ku_ppsc_reg      ppsc_reg;   /**< ppsc_reg- ppsc register tlv */
+    uint8_t                 dev_id;     /**< dev_id - device id */
+};
+
+/**
+ * ku_access_pelc_reg structure is used to store the access register PELC command parameters
+ */
+struct ku_access_pelc_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_pelc_reg      pelc_reg; /**< pelc_reg - pelc register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_spad_reg structure is used to store the access register SPAD command parameters
+ */
+struct ku_access_spad_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_spad_reg      spad_reg; /**< spad_reg - spad register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_pvlc_reg structure is used to store the access register PVLC command parameters
+ */
+struct ku_access_pvlc_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_pvlc_reg      pvlc_reg; /**< pvlc_reg - pvlc register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_mcia_reg structure is used to store the access register MCIA command parameters
+ */
+struct ku_access_mcia_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_mcia_reg      mcia_reg; /**< mcia_reg - mcia register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_fore_reg structure is used to store the access register FORE command parameters
+ */
+struct ku_access_fore_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_fore_reg      fore_reg; /**< fore_reg - fore register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_mtcap_reg structure is used to store the access register MTCAP command parameters
+ */
+struct ku_access_mtcap_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_mtcap_reg     mtcap_reg; /**< mtcap_reg - mtcap register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_mtmp_reg structure is used to store the access register MTMP command parameters
+ */
+struct ku_access_mtmp_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_mtmp_reg      mtmp_reg; /**< mtmp_reg - mtmp register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_mtwe_reg structure is used to store the access register MTWE command parameters
+ */
+struct ku_access_mtwe_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_mtwe_reg      mtwe_reg; /**< mtwe_reg - mtwe register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_mmdio_reg structure is used to store the access register MMDIO command parameters
+ */
+struct ku_access_mmdio_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_mmdio_reg     mmdio_reg; /**< mmdio_reg - mmdio register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_mmia_reg structure is used to store the access register MMIA command parameters
+ */
+struct ku_access_mmia_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_mmia_reg      mmia_reg; /**< mmia_reg - mmia register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_mfpa_reg structure is used to store the access register MFPA command parameters
+ */
+struct ku_access_mfpa_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_mfpa_reg      mfpa_reg; /**< mfpa_reg - mfpa register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_mfbe_reg structure is used to store the access register MFBE command parameters
+ */
+struct ku_access_mfbe_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_mfbe_reg      mfbe_reg; /**< mfbe_reg - mfbe register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_mfba_reg structure is used to store the access register MFBA command parameters
+ */
+struct ku_access_mfba_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_mfba_reg      mfba_reg; /**< mfba_reg - mfba register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_qcap_reg structure is used to store the access register QCAP command parameters
+ */
+struct ku_access_qcap_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_qcap_reg      qcap_reg; /**< qcap_reg - qcap register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_raw_reg structure is used to store the access register command parameters for a RAW register
+ */
+struct ku_access_raw_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_raw_reg       raw_reg; /**< raw_reg - raw_reg register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_reg_raw_buff structure is used to store access register RAW parameters
+ */
+struct ku_access_reg_raw_buff {
+    struct ku_raw_reg raw_buff;   /**< raw_buff - raw_reg register tlv */
+    uint8_t           dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_hpkt_reg structure is used to store the access register HPKT command parameters
+ */
+struct ku_access_hpkt_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_hpkt_reg      hpkt_reg; /**< hpkt_reg - hpkt register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_hcap_reg structure is used to store the access register HCAP command parameters
+ */
+struct ku_access_hcap_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_hcap_reg      hcap_reg; /**< hcap_reg - hcap register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_hdrt_reg structure is used to store the access register HDRT command parameters
+ */
+struct ku_access_hdrt_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_hdrt_reg      hdrt_reg; /**< hdrt_reg - hdrt register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_htgt_reg structure is used to store the access register HTGT command parameters
+ */
+struct ku_access_htgt_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_htgt_reg      htgt_reg; /**< htgt_reg - htgt register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_qprt_reg structure is used to store the access register QPRT command parameters
+ */
+struct ku_access_qprt_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_qprt_reg      qprt_reg; /**< qprt_reg - qprt register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_mfcr_reg structure is used to store the access register MFCR command parameters
+ */
+struct ku_access_mfcr_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_mfcr_reg      mfcr_reg; /**< mfcr_reg - mfcr register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_mfm_reg structure is used to store the access register MFM command parameters
+ */
+struct ku_access_mfm_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_mfm_reg       mfm_reg; /**< mfm_reg - hpkt register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_sspr_reg structure is used to store the access
+ * register MFM command parameters
+ */
+struct ku_access_sspr_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_sspr_reg      sspr_reg; /**< mfm_reg - hpkt register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+
+/**
+ * ku_access_sfd_reg structure is used to store the access
+ * register MFM command parameters
+ */
+struct ku_access_sfd_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_sfd_reg       sfd_reg; /**< mfm_reg - hpkt register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_ppad_reg structure is used to store the access
+ * register ppad command parameters
+ */
+struct ku_access_ppad_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_ppad_reg      ppad_reg; /**< mfm_reg - hpkt register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_spmcr_reg structure is used to store the access
+ * register spmcr command parameters
+ */
+struct ku_access_spmcr_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_spmcr_reg     spmcr_reg; /**< mfm_reg - hpkt register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_pbmc_reg structure is used to store the access
+ * register pbmc command parameters
+ */
+struct ku_access_pbmc_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_pbmc_reg      pbmc_reg; /**< mfm_reg - hpkt register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_sbpr_reg structure is used to store the access
+ * register sbpr command parameters
+ */
+struct ku_access_sbpr_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_sbpr_reg      sbpr_reg; /**< mfm_reg - hpkt register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_sbsr_reg structure is used to store the access
+ * register sbsr command parameters
+ */
+struct ku_access_sbsr_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_sbsr_reg      sbsr_reg; /**< mfm_reg - sbsr register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_sbcm_reg structure is used to store the access
+ * register sbcm command parameters
+ */
+struct ku_access_sbcm_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_sbcm_reg      sbcm_reg; /**< mfm_reg - hpkt register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_sbpm_reg structure is used to store the access
+ * register sbpm command parameters
+ */
+struct ku_access_sbpm_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_sbpm_reg      sbpm_reg; /**< mfm_reg - hpkt register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_sbmm_reg structure is used to store the access
+ * register sbmm command parameters
+ */
+struct ku_access_sbmm_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_sbmm_reg      sbmm_reg; /**< mfm_reg - hpkt register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_pptb_reg structure is used to store the access
+ * register pptb command parameters
+ */
+struct ku_access_pptb_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_pptb_reg      pptb_reg; /**< mfm_reg - hpkt register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_smid_reg structure is used to store the access
+ * register smid command parameters
+ */
+struct ku_access_smid_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_smid_reg      smid_reg; /**< smid_reg - smid register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_spms_reg structure is used to store the access
+ * register spms command parameters
+ */
+struct ku_access_spms_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_spms_reg      spms_reg; /**< mfm_reg - hpkt register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_spvid_reg structure is used to store the access
+ * register spvid command parameters
+ */
+struct ku_access_spvid_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_spvid_reg     spvid_reg; /**< mfm_reg - hpkt register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_spvid_reg structure is used to store the access
+ * register spvid command parameters
+ */
+struct ku_access_sfgc_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_sfgc_reg      sfgc_reg; /**< mfm_reg - hpkt register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_qpbr_reg structure is used to store the access
+ * register qpbr command parameters
+ */
+struct ku_access_qpbr_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_qpbr_reg      qpbr_reg; /**< mfm_reg - hpkt register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_plbf_reg structure is used to store the access
+ * register PLBF command parameters
+ */
+struct ku_access_plbf_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_plbf_reg      plbf_reg; /**< plbf_reg - plbf register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_sgcr_reg structure is used to store the access
+ * register SGCR command parameters
+ */
+struct ku_access_sgcr_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_sgcr_reg      sgcr_reg; /**< sgcr_reg - sgcr register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_msci_reg structure is used to store the access
+ * register MSCI command parameters
+ */
+struct ku_access_msci_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_msci_reg      msci_reg; /**< msci_reg - msci register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_mrsr_reg structure is used to store the access
+ * register MRSR command parameters
+ */
+struct ku_access_mrsr_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_mrsr_reg      mrsr_reg; /**< mrsr_reg - mrsr register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_mpsc_reg structure is used to store the access
+ * register MPSC command parameters
+ */
+struct ku_access_mpsc_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_mpsc_reg      mpsc_reg; /**< mpsc_reg - mpsc register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_access_cwgc_reg structure is used to store the access
+ * register cwgc command parameters
+ */
+struct ku_cwgcr_reg {
+    uint8_t en;
+    uint8_t scd;
+    uint8_t aqs_weight;
+    uint8_t aqs_time;
+    uint8_t cece;
+};
+
+/**
+ * ku_cwcp_profile structure is used with ku_access_cwcp_reg as a profile param
+ * register cwcp command parameters
+ */
+struct ku_cw_profile {
+    uint32_t profile_i_min;
+    uint8_t  profile_i_percent;
+    uint32_t profile_i_max;
+};
+/**
+ * ku_access_cwcp_reg structure is used to store the access
+ * register cwcp command parameters
+ */
+struct ku_cwtp_reg {
+    uint8_t              local_port;
+    uint8_t              traffic_class;
+    uint8_t              mode;
+    struct ku_cw_profile profiles[REDECN_NUM_HW_PROFILES];
+};
+
+/**
+ * ku_access_cwcpm_reg structure is used to store the access
+ * register ku_access_cwcpm_reg command parameters
+ */
+struct ku_cwtpm_reg {
+    uint8_t local_port;
+    uint8_t traffic_class;
+    uint8_t ew;
+    uint8_t ee;
+    uint8_t tcp_g;
+    uint8_t tcp_y;
+    uint8_t tcp_r;
+    uint8_t ntcp_g;
+    uint8_t ntcp_y;
+    uint8_t ntcp_r;
+};
+/**
+ * ku_access_cwpp_reg structure is used to store the access
+ * register cwpp command parameters
+ */
+struct ku_cwpp_reg {
+    uint8_t              pool;
+    struct ku_cw_profile profiles[REDECN_NUM_HW_PROFILES];
+};
+
+/**
+ * ku_access_cwppm_reg structure is used to store the access
+ * register cwppm command parameters
+ */
+struct ku_cwppm_reg {
+    uint8_t pool;
+    uint8_t tcp_g;
+    uint8_t tcp_y;
+    uint8_t tcp_r;
+    uint8_t ntcp_g;
+    uint8_t ntcp_y;
+    uint8_t ntcp_r;
+};
+
+/**
+ * ku_access_cwprp_reg structure is used to store the access
+ * register cwprp command parameters
+ */
+struct ku_cpqe_reg {
+    uint8_t egress_local_port;
+    uint8_t element_hierarchy;
+    uint8_t element_index;
+    uint8_t profile_percent;
+};
+/**
+ * ku_swid_config structure is used to store the swid attributes for the set profile command
+ */
+struct ku_swid_config {
+    uint8_t           mask; /**< mask - Modify Switch Partition Configuration mask */
+    enum ku_swid_type type; /**< type - Switch Partition type */
+    uint8_t           properties; /**< properties - For IB switch partition bit 0 - IPoIB Router Port Enable */
+};
+
+/**
+ * ku_profile structure is used to store the profile attributes for the set profile command
+ */
+struct ku_profile {
+    uint8_t               dev_id; /**< dev_id - device id */
+    uint64_t              set_mask_0_63; /**< set_mask_0_63 - Capability bitmask for Set() command */
+    uint64_t              set_mask_64_127; /**< set_mask_64_127 - Capability bitmask for Set() command */
+    uint8_t               max_vepa_channels; /**< max_vepa_channels - Maximum Number of VEPA Channels per port (0 through 16) */
+    uint16_t              max_lag; /**< max_lag - Maximum number of LAG IDs requested */
+    uint16_t              max_port_per_lag; /**< max_port_per_lag - Maximum number of ports per LAG requested */
+    uint16_t              max_mid; /**< max_mid - Maximum Multicast IDs Multicast IDs are allocated from 0 to max_mid-1 */
+    uint16_t              max_pgt; /**< max_pgt - Maximum records in the Port Group Table per Switch Partition */
+    uint16_t              max_system_port; /**< max_system_port - The maximum number of system ports that can be allocated */
+    uint16_t              max_active_vlans; /**< max_active_vlans - Maximum number of active VLANs */
+    uint16_t              max_regions; /**< max_regions - Maximum number of TCAM Regions */
+    uint8_t               max_flood_tables; /**< max_flood_tables - Maximum number of Flooding Tables. */
+    uint8_t               max_per_vid_flood_tables; /**< max_per_vid_flood_tables - Maximum number of Flooding Tables Per VID (A2 only in mixed mode ). */
+    uint8_t               flood_mode; /**< flood_mode - Flood Tables Mode: single, per vid, mixed (mixed supported in A2 only) */
+    uint8_t               max_fid_offset_flood_tables; /**< max_fid_offset_flood_tables - Maxium number of FID-Offset Flooding Tables */
+    uint16_t              fid_offset_table_size; /**< fid_offset_table_size - number of entries in each FID-Offset Flooding Table */
+    uint8_t               max_per_fid_flood_table; /**< max_per_fid_flood_table - Maxium number of FID Flooding Tables */
+    uint16_t              per_fid_table_size; /**< per_fid_table_size - Number of entries in each FID Flooding Table; */
+    uint16_t              max_ib_mc; /**< max_ib_mc - Maximum number of multicast FDB records for IB FDB (in 512 chunks) per IB Switch Partition */
+    uint16_t              max_pkey; /**< max_pkey - Maximum per port pkey table size (for pkey enforcement) */
+    uint8_t               ar_sec; /**< ar_sec - Primary / Secondary Capability */
+    uint16_t              adaptive_routing_group_cap; /**< adaptive_routing_group_cap - Adaptive Routing Group Capability */
+    uint8_t               arn; /**< arn - Adaptive Routing Notification Enable */
+    uint32_t              kvd_linear_size; /**The size of KVD linear*/
+    uint32_t              kvd_hash_single_size; /**The size of KVD hash single*/
+    uint32_t              kvd_hash_double_size; /**The size of KVD hash single*/
+    struct ku_swid_config swid0_config_type; /**< swid0_config_type - Configuration for Switch Partition 0 */
+    struct ku_swid_config swid1_config_type; /**< swid1_config_type - Configuration for Switch Partition 1 */
+    struct ku_swid_config swid2_config_type; /**< swid2_config_type - Configuration for Switch Partition 2 */
+    struct ku_swid_config swid3_config_type; /**< swid3_config_type - Configuration for Switch Partition 3 */
+    struct ku_swid_config swid4_config_type; /**< swid4_config_type - Configuration for Switch Partition 4 */
+    struct ku_swid_config swid5_config_type; /**< swid5_config_type - Configuration for Switch Partition 5 */
+    struct ku_swid_config swid6_config_type; /**< swid6_config_type - Configuration for Switch Partition 6 */
+    struct ku_swid_config swid7_config_type; /**< swid7_config_type - Configuration for Switch Partition 7 */
+    uint32_t              reserved1; 		/**< reserved1 */
+
+    /* this array will contain supported revisions per type */
+    uint64_t            sup_revs_by_type[SXD_CHIP_TYPES_MAX];
+    enum sxd_chip_types chip_type;
+    uint8_t             do_not_config_profile_to_device;
+};
+
+/**
+ * ku_swid_details is used to store the swid details for the enable/disable swid ioctls
+ */
+struct ku_swid_details {
+    uint8_t  dev_id;    /**< dev_id - device id */
+    uint8_t  swid;    /**< swid - swid (0-7, or 255=Don't care) */
+    uint16_t iptrap_synd;    /**< iptrap_synd - syndrome for the IPTRAPs */
+    uint64_t mac;    /**< mac - MAC address of the device */
+};
+
+/**
+ * ku_trap_filter_data is used to store the data of the trap_filter change ioctl
+ */
+struct ku_trap_filter_data {
+    uint16_t trap_id;    /**< trap_id - trap ID to filter on */
+    uint8_t  is_lag;    /**< is_lag - is the port a lag port */
+    uint16_t sysport;    /**< sysport - system port (valid if is_lag == 0) */
+    uint16_t lag_id;    /**< lag_id - LAG ID (valid if is_lag == 1) */
+};
+
+/**
+ * ku_default_vid_data is used to store the data of the default vid change ioctl
+ */
+struct ku_default_vid_data {
+    uint8_t  dev_id;    /**< dev_id - device id */
+    uint8_t  is_lag;    /**< is_lag - is the vid belongs to a lag */
+    uint16_t sysport;    /**< sysport - system port (valid if is_lag == 0) */
+    uint16_t lag_id;    /**< lag_id - LAG ID (valid if is_lag == 1) */
+    uint16_t default_vid;    /**< default_vid - the new default VLAN ID of the port/lag */
+};
+
+/**
+ * ku_default_vid_data is used to store the data of the default vid change ioctl
+ */
+struct ku_vid_membership_data {
+    uint8_t  dev_id;    /**< dev_id - device id */
+    uint8_t  is_lag;    /**< is_lag - is the vid belongs to a lag */
+    uint16_t phy_port;    /**< phy_port - physical port (valid if is_lag == 0) */
+    uint16_t lag_id;    /**< lag_id - LAG ID (valid if is_lag == 1) */
+    uint16_t vid;    /**< vid - the new default VLAN ID of the port/lag */
+    uint8_t  is_tagged;    /**< is_lag - is the vid belongs to a lag */
+};
+
+/**
+ * ku_default_vid_data is used to store the data of the default vid change ioctl
+ */
+struct ku_prio_tagging_data {
+    uint8_t  dev_id;    /**< dev_id - device id */
+    uint8_t  is_lag;    /**< is_lag - is the vid belongs to a lag */
+    uint16_t phy_port;    /**< phy_port - physical port (valid if is_lag == 0) */
+    uint16_t lag_id;    /**< lag_id - LAG ID (valid if is_lag == 1) */
+    uint8_t  is_prio_tagged;    /**< is_lag - is the vid belongs to a lag */
+};
+
+/**
+ * ku_default_vid_data is used to store the data of the default vid change ioctl
+ */
+struct ku_prio_to_tc_data {
+    uint8_t  dev_id;    /**< dev_id - device id */
+    uint8_t  is_lag;    /**< is_lag - is the vid belongs to a lag */
+    uint16_t phy_port;    /**< phy_port - physical port (valid if is_lag == 0) */
+    uint16_t lag_id;    /**< lag_id - LAG ID (valid if is_lag == 1) */
+    uint8_t  priority;
+    uint8_t  traffic_class;
+};
+
+/**
+ * ku_port_swid_data is used to store the data of the port swid change ioctl
+ */
+struct ku_local_port_swid_data {
+    uint8_t  dev_id;    /**< dev_id - device id */
+    uint16_t local_port;    /**<  local port - system port (valid if is_lag == 0) */
+    uint16_t swid;    /**< swid - the new SWID of the port/lag */
+};
+
+/**
+ * ku_ib_local_port_data is used to store the data of the port swid change ioctl
+ */
+struct ku_ib_local_port_data {
+    uint8_t  dev_id;    /**< dev_id - device id */
+    uint16_t local_port;    /**<  local port - system port (valid if is_lag == 0) */
+    uint16_t ib_port;    /**< ib_port - IB port  */
+};
+
+/**
+ * ku_system_local_port_data is used to store the data of the port swid change ioctl
+ */
+struct ku_system_local_port_data {
+    uint8_t  dev_id;    /**< dev_id - device id */
+    uint16_t local_port;    /**<  local port - local port (valid if is_lag == 0) */
+    uint16_t system_port;    /**< system_port - system port  */
+};
+
+/**
+ * ku_default_vid_data is used to store the data of the default vid change ioctl
+ */
+struct ku_port_rp_mode_data {
+    uint8_t  dev_id;    /**< dev_id - device id */
+    uint8_t  is_lag;    /**< is_lag - is the vid belongs to a lag */
+    uint16_t sysport;    /**< sysport - system port (valid if is_lag == 0) */
+    uint16_t vlan_id;    /**< lan id  */
+    uint16_t lag_id;    /**< lag_id - LAG ID (valid if is_lag == 1) */
+    uint8_t  is_rp;    /**< is_rp - is valid rif */
+    uint8_t  opcode;    /**< opcode - create / delete rif */
+    uint16_t rif_id;    /**< rif_id - RIF ID */
+};
+
+/**
+ * ku_port_vlan_to_fid_map_data is used to store the data of the port,vlasn mapping ioctl
+ */
+struct ku_port_vlan_to_fid_map_data
+{
+        uint8_t dev_id; /**< dev_id - device id */
+        uint16_t local_port; /**< port - local port */
+        uint16_t vid; /**< vid - vlan id  */
+        uint8_t is_mapped_to_fid; /**< is_mapped_to_fid  */
+        uint16_t fid; /**< fid - bridge id */
+};
+
+/**
+ * ku_default_vid_data is used to store the data of the default vid change ioctl
+ */
+struct ku_local_port_to_lag_data {
+    uint8_t  dev_id;    /**< dev_id - device id */
+    uint8_t  is_lag;    /**< is_lag - is the vid belongs to a lag */
+    uint16_t local_port;    /**< sysport - system port (valid if is_lag == 0) */
+    uint16_t lag_id;    /**< lag_id - LAG ID (valid if is_lag == 1) */
+    uint16_t lag_port_index;    /**< lag_id - LAG ID (valid if is_lag == 1) */
+};
+
+
+/**
+ * ku_access_mhsr_reg structure is used to store the access register MHSR command parameters
+ */
+struct ku_access_mhsr_reg {
+    struct ku_operation_tlv op_tlv; /**< op_tlv - operation tlv struct */
+    struct ku_mhsr_reg      mhsr_reg; /**< mhsr_reg - mhsr register tlv */
+    uint8_t                 dev_id; /**< dev_id - device id */
+};
+
+/**
+ * ku_vid2ip_data is used to store the data of the default vid change ioctl
+ */
+struct ku_vid2ip_data {
+    uint16_t vid;       /**< vid - the new default VLAN ID of the port/lag */
+    uint32_t ip_addr;
+    uint8_t  valid;     /**< valid bit which define if valid */
+};
+
+
+#endif /* KERNEL_USER_H_ */
+
+/************************************************
+ *                  EOF                         *
+ ***********************************************/
diff --git a/linux/include/linux/mlx_sx/mlx_sx/device.h b/linux/include/linux/mlx_sx/mlx_sx/device.h
new file mode 100644
index 0000000..f686ade
--- /dev/null
+++ b/linux/include/linux/mlx_sx/mlx_sx/device.h
@@ -0,0 +1,308 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+#ifndef SX_DEVICE_H
+#define SX_DEVICE_H
+
+#include <linux/types.h>
+#include <linux/fs.h>
+#include <linux/device.h>
+#include <linux/cdev.h>
+#include <linux/mlx_sx/kernel_user.h>
+
+#ifndef SYSTEM_PCI
+#define NO_PCI
+#endif
+
+/* According to CQe */
+enum sx_packet_type {
+	PKT_TYPE_IB_Raw		= 0,
+	PKT_TYPE_IB_non_Raw	= 1,
+	PKT_TYPE_ETH		= 2,
+	PKT_TYPE_FC			= 3,
+	PKT_TYPE_FCoIB		= 4,
+	PKT_TYPE_FCoETH		= 5,
+	PKT_TYPE_ETHoIB		= 6,
+	PKT_TYPE_NUM
+};
+
+static const char *sx_cqe_packet_type_str[] = {
+	"PKT_TYPE_IB_Raw",
+	"PKT_TYPE_IB_non_Raw",
+	"PKT_TYPE_ETH",
+	"PKT_TYPE_FC",
+	"PKT_TYPE_FCoIB",
+	"PKT_TYPE_FCoETH",
+	"PKT_TYPE_ETHoIB"
+};
+
+static const int sx_cqe_packet_type_str_len =
+		sizeof(sx_cqe_packet_type_str)/sizeof(char *);
+
+enum l2_type {
+	L2_TYPE_DONT_CARE	= -1,
+	L2_TYPE_IB			= 0,
+	L2_TYPE_ETH			= 1,
+	L2_TYPE_FC			= 2
+};
+
+enum sx_event {
+	SX_EVENT_TYPE_COMP				= 0x00,
+	SX_EVENT_TYPE_CMD				= 0x0a,
+	SX_EVENT_TYPE_INTERNAL_ERROR	= 0x08
+};
+
+enum {
+	SX_DBELL_REGION_SIZE		= 0xc00
+};
+
+struct completion_info {
+	u8							swid;
+	u16 						sysport;
+	u16 						hw_synd;
+	u8							is_send;
+	enum sx_packet_type			pkt_type;
+	struct sk_buff				*skb;
+	union ku_filter_critireas	info;
+	u8							is_lag;
+	u8							lag_subport;
+	u8							is_tagged;
+	u16							vid;
+	void						*context;
+	struct sx_dev    		    *dev;
+    u32                         original_packet_size;
+    u16                         bridge_id;
+};
+
+typedef void (*cq_handler)(struct completion_info*, void *);
+
+struct listener_entry {
+	u8							swid;
+	enum l2_type				listener_type;
+	u8							is_default; /*is a default listener */
+	union ku_filter_critireas	critireas;  /*more filter critireas */
+	cq_handler					handler;    /*The completion handler*/
+	void						*context;   /*to pass to the handler*/
+	u64							rx_pkts;	/* rx pkts */
+	struct list_head			list;
+};
+
+struct sx_stats{
+	u64	rx_by_pkt_type[NUMBER_OF_SWIDS+1][PKT_TYPE_NUM];
+	u64	tx_by_pkt_type[NUMBER_OF_SWIDS+1][PKT_TYPE_NUM];
+	u64	rx_by_synd[NUMBER_OF_SWIDS+1][NUM_HW_SYNDROMES+1];
+	u64	tx_by_synd[NUMBER_OF_SWIDS+1][NUM_HW_SYNDROMES+1];
+	u64	rx_unconsumed_by_synd[NUM_HW_SYNDROMES+1][PKT_TYPE_NUM];
+	u64	rx_eventlist_drops_by_synd[NUM_HW_SYNDROMES+1];
+};
+
+struct sx_dev {
+	struct sx_dev_cap		dev_cap;
+	spinlock_t				profile_lock; /* the profile's lock */
+	struct sx_pci_profile	profile;
+	u8						profile_set;
+    u8 						dev_profile_set;
+    u8                      first_ib_swid;
+	unsigned long			flags;
+	struct pci_dev			*pdev;
+	u64						bar0_dbregs_offset;
+	u8						bar0_dbregs_bar;
+	void __iomem			*db_base;
+	char					board_id[SX_BOARD_ID_LEN];
+	u16						vsd_vendor_id;
+	struct device			dev; /* TBD - do we need it? */
+	u16						device_id;
+	struct list_head		list;
+	u64						fw_ver;
+	u8						dev_stuck;
+	u8						global_flushing;
+	struct cdev             cdev;
+
+	/* multi-dev support */
+	struct sx_stats         stats;
+	u64                     eventlist_drops_counter;
+    u64                     unconsumed_packets_counter;
+    u64                   filtered_lag_packets_counter;
+    u64                   filtered_port_packets_counter;
+	u64					loopback_packets_counter;
+	struct work_struct catas_work;
+	struct workqueue_struct *catas_wq;
+	int         			catas_poll_running;
+};
+
+enum {
+	PPBT_REG_ID = 0x3003,
+	QSPTC_REG_ID = 0x4009,
+	QSTCT_REG_ID = 0x400b,
+	PMLP_REG_ID = 0x5002,
+	PMTU_REG_ID = 0x5003,
+	PTYS_REG_ID = 0x5004,
+	PPAD_REG_ID = 0x5005,
+	PAOS_REG_ID = 0x5006,
+	PUDE_REG_ID = 0x5009,
+	PLIB_REG_ID = 0x500a,
+	PPTB_REG_ID = 0x500B,
+	PSPA_REG_ID = 0x500d,
+	PELC_REG_ID = 0x500e,
+	PVLC_REG_ID = 0x500f,
+	PMPR_REG_ID = 0x5013,
+	SPZR_REG_ID = 0x6002,
+	HCAP_REG_ID = 0x7001,
+	HTGT_REG_ID = 0x7002,
+	HPKT_REG_ID = 0x7003,
+	HDRT_REG_ID = 0x7004,
+	OEPFT_REG_ID = 0x7081,
+	MFCR_REG_ID = 0x9001,
+	MFSC_REG_ID = 0x9002,
+	MFSM_REG_ID = 0x9003,
+	MFSL_REG_ID = 0x9004,
+	MTCAP_REG_ID = 0x9009,
+	MTMP_REG_ID = 0x900a,
+	MFPA_REG_ID = 0x9010,
+	MFBA_REG_ID = 0x9011,
+	MFBE_REG_ID = 0x9012,
+	MCIA_REG_ID = 0x9014,
+	MGIR_REG_ID = 0x9020,
+	PMAOS_REG_ID = 0x5012,
+	MFM_REG_ID = 0x901d,
+        MJTAG_REG_ID = 0x901F,
+        PMPC_REG_ID = 0x501F,
+	MPSC_REG_ID = 0x9080,
+};
+
+enum {
+	TLV_TYPE_END_E,
+	TLV_TYPE_OPERATION_E,
+	TLV_TYPE_DR_E,
+	TLV_TYPE_REG_E,
+	TLV_TYPE_USER_DATA_E
+};
+
+enum {
+	EMAD_METHOD_QUERY = 1,
+	EMAD_METHOD_WRITE = 2,
+	EMAD_METHOD_SEND  = 3,
+	EMAD_METHOD_EVENT = 5,
+};
+
+enum {
+		PORT_OPER_STATUS_UP = 1,
+		PORT_OPER_STATUS_DOWN = 2,
+		PORT_OPER_STATUS_FAILURE = 4,
+};
+
+struct sx_eth_hdr {
+	__be64	dmac_smac1;
+	__be32	smac2;
+	__be16	ethertype;
+	u8		mlx_proto;
+	u8		ver;
+};
+
+struct emad_operation {
+	__be16  type_len;
+	u8      status;
+	u8      reserved1;
+	__be16  register_id;
+	u8      r_method;
+	u8      class;
+	__be64  tid;
+};
+
+struct sx_emad {
+	struct sx_eth_hdr eth_hdr;
+	struct emad_operation emad_op;
+};
+
+#define EMAD_TLV_TYPE_SHIFT (3)
+struct sxd_emad_tlv_reg {
+	u8     type;
+	u8     len;
+	__be16 reserved0;
+};
+
+struct sxd_emad_pude_reg {
+	struct sx_emad emad_header;
+	struct sxd_emad_tlv_reg tlv_header;
+	u8     swid;
+	u8     local_port;
+	u8     admin_status;
+	u8     oper_status;
+	__be32 reserved3[3];
+};
+
+#define SX_PORT_PHY_ID_OFFS     (8)
+#define SX_PORT_PHY_ID_MASK     (0x0000FF00)
+#define SX_PORT_PHY_ID_ISO(id)  ((id) & (SX_PORT_PHY_ID_MASK)) 
+#define SX_PORT_PHY_ID_GET(id)  (SX_PORT_PHY_ID_ISO(id) >> SX_PORT_PHY_ID_OFFS)
+
+#define SX_PORT_DEV_ID_OFFS  (16) 
+#define SX_PORT_DEV_ID_MASK  (0x0FFF0000)
+#define SX_PORT_DEV_ID_ISO(id)  ((id) & (SX_PORT_DEV_ID_MASK))
+#define SX_PORT_DEV_ID_GET(id)  (SX_PORT_DEV_ID_ISO(id) >> SX_PORT_DEV_ID_OFFS)
+
+#define SX_PORT_TYPE_ID_OFFS (28)
+#define SX_PORT_TYPE_ID_MASK (0xF0000000)
+#define SX_PORT_TYPE_ID_ISO(id) ((id) & (SX_PORT_TYPE_ID_MASK))
+#define SX_PORT_TYPE_ID_GET(id) (SX_PORT_TYPE_ID_ISO(id) >> SX_PORT_TYPE_ID_OFFS)
+
+#define SX_PORT_LAG_ID_OFFS  (8)
+#define SX_PORT_LAG_ID_MASK  (0x000FFF00)
+#define SX_PORT_LAG_ID_ISO(id)  ((id) & (SX_PORT_LAG_ID_MASK))
+#define SX_PORT_LAG_ID_GET(id)  (SX_PORT_LAG_ID_ISO(id) >> SX_PORT_LAG_ID_OFFS)
+
+#define CPU_PORT_PHY_ID              (0)
+#define UCROUTE_CPU_PORT_DEV_MASK    (0x0FC0)
+#define UCROUTE_CPU_DEV_BIT_OFFSET   (6)
+#define UCROUTE_DEV_ID_BIT_OFFSET    (10)
+#define UCROUTE_PHY_PORT_BITS_OFFSET (4)
+#define UCROUTE_CPU_PORT_PREFIX      (0xB000)
+
+u16 translate_user_port_to_sysport(struct sx_dev *dev, u32 log_port, int* is_lag);
+u32 translate_sysport_to_user_port(struct sx_dev *dev, u16 port, u8 is_lag);
+
+
+#define SX_TRAP_ID_PUDE  0x08
+
+
+#define NUM_OF_SYSPORT_BITS 16
+#define NUM_OF_LAG_BITS 12
+#define MAX_SYSPORT_NUM (1 << NUM_OF_SYSPORT_BITS)
+#define MAX_PHYPORT_NUM 64
+#define MAX_LAG_NUM MAX_PHYPORT_NUM
+#define MAX_LAG_MEMBERS_NUM 32
+#define MAX_IBPORT_NUM MAX_PHYPORT_NUM
+#define MAX_SYSTEM_PORTS_IN_FILTER 256
+#define MAX_LAG_PORTS_IN_FILTER 256
+#define MAX_PRIO_NUM 15
+#define MAX_VLAN_NUM 4096
+
+/* Bridge Netdev values */
+/* MIN_BRIDGE_ID = 4k */
+#define MIN_BRIDGE_ID 4096
+/* MAX_BRIDGE_ID = (15k - 1) */
+#define MAX_BRIDGE_ID 15359
+/* MAX_BRIDGE_NUM */
+#define MAX_BRIDGE_NUM (MAX_BRIDGE_ID - MIN_BRIDGE_ID + 1)
+
+/** This enum defines bitmask values for combinations of port types */
+enum sx_port_type {
+    SX_PORT_TYPE_NETWORK = 0,
+    SX_PORT_TYPE_LAG = 1,
+    SX_PORT_TYPE_VPORT = 2,
+    SX_PORT_TYPE_MULTICAST = 4,
+    SX_PORT_TYPE_MIN = SX_PORT_TYPE_NETWORK,
+    SX_PORT_TYPE_MAX = SX_PORT_TYPE_MULTICAST,
+};
+
+#endif /* SX_DEVICE_H */
diff --git a/linux/include/linux/mlx_sx/mlx_sx/driver.h b/linux/include/linux/mlx_sx/mlx_sx/driver.h
new file mode 100644
index 0000000..b726ea8
--- /dev/null
+++ b/linux/include/linux/mlx_sx/mlx_sx/driver.h
@@ -0,0 +1,200 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+#ifndef SX_DRIVER_H
+#define SX_DRIVER_H
+
+#include <linux/device.h>
+#include <linux/mlx_sx/device.h>
+#include <linux/mlx_sx/kernel_user.h>
+
+struct sx_dev;
+
+enum sx_dev_event {
+	SX_DEV_EVENT_CATASTROPHIC_ERROR,
+	SX_DEV_EVENT_IB_SWID_UP,
+	SX_DEV_EVENT_ETH_SWID_UP,
+	SX_DEV_EVENT_IB_SWID_DOWN,
+	SX_DEV_EVENT_ETH_SWID_DOWN,
+	SX_DEV_EVENT_OPEN_PORT_NETDEV,
+	SX_DEV_EVENT_CLOSE_PORT_NETDEV,
+	SX_DEV_EVENT_PORT_UP,
+	SX_DEV_EVENT_PORT_DOWN,
+	SX_DEV_EVENT_PORT_REINIT,
+	SX_DEV_EVENT_TYPE_INTERNAL_ERROR,
+	SX_DEV_EVENT_TYPE_TCA_INIT,
+	SX_DEV_EVENT_MAD_IFC_ENABLE,
+	SX_DEV_EVENT_ADD_SYND_NETDEV,
+	SX_DEV_EVENT_REMOVE_SYND_NETDEV,
+	SX_DEV_EVENT_ADD_SYND_IPOIB,
+	SX_DEV_EVENT_REMOVE_SYND_IPOIB,
+	SX_DEV_EVENT_DEBUG_NETDEV,
+	SX_DEV_EVENT_NODE_DESC_UPDATE,
+	SX_DEV_EVENT_ADD_SYND_L2_NETDEV,
+	SX_DEV_EVENT_REMOVE_SYND_L2_NETDEV
+};
+
+#define SX_PAGE_SIZE		4096
+#define SX_PAGE_SHIFT		12
+
+#define ETHTYPE_ARP		0x0806
+#define ETHTYPE_VLAN		0x8100
+#define ETHTYPE_EMAD		0x8932
+#define ETHTYPE_DONT_CARE_VALUE 0
+#define QPN_DONT_CARE_VALUE 	0xffffffff
+#define QPN_MULTICAST_VALUE 	0xffffff
+#define DMAC_DONT_CARE_VALUE 	0
+#define TID_DONT_CARE_VALUE 	0
+#define SYSPORT_DONT_CARE_VALUE 0
+#define FWD_BY_FDB_TRAP_ID	0x01
+#define SWITCHIB_QP0_TRAP_ID		0xf0
+#define SWITCHIB_QP1_TRAP_ID		0xf1
+#define SWITCHIB_OTHER_QP_TRAP_ID	0xf2
+#define PACKET_SAMPLE_TRAP_ID	0x38
+#define ROUTER_QP0_TRAP_ID	0x5e
+#define FDB_TRAP_ID		0x06
+#define ARP_REQUEST_TRAP_ID	0x50
+#define ARP_RESPONSE_TRAP_ID	0x51
+#define ETH_L3_MTUERROR_TRAP_ID	0x52
+#define ETH_L3_TTLERROR_TRAP_ID	0x53
+#define ETH_L3_LBERROR_TRAP_ID  0x54
+#define MIN_IPTRAP_TRAP_ID	0x1C0 /* TODO define which one will be used */
+
+union sx_event_data {
+	struct {
+		int swid;
+		u16 dev_id;
+	} ib_swid_change;
+	struct {
+		int swid;
+		int synd;
+		u64 mac;
+	} eth_swid_up;
+	struct {
+		int swid;
+		int hw_synd;
+	} eth_l3_synd;
+	struct {
+		int swid;
+		int hw_synd;
+	} ipoib_synd;
+	struct {
+		int swid;
+	} eth_swid_down;
+	struct {
+		int swid;
+		u16 sysport;
+		u8 	is_lag;
+		u16 mid;
+		char *name;
+		u8  send_to_rp_as_data_supported;
+	} port_netdev_set;
+	struct {
+		int num_of_ib_swids;
+		u8  swid[NUMBER_OF_SWIDS];
+		u16 max_pkey;
+	} tca_init;
+	struct {
+		uint8_t swid;
+		uint8_t NodeDescription[64];
+	} node_desc_update;
+};
+
+struct sx_interface {
+	void *			(*add)	 (struct sx_dev *dev);
+	void			(*remove)(struct sx_dev *dev, void *context);
+	void			(*event) (struct sx_dev *dev, void *context,
+					enum sx_dev_event event,
+					union sx_event_data *event_data);
+	struct list_head	list;
+};
+
+struct sx_sgmii_ctrl_segment {
+	u8	reserved1;
+	u8	one;
+	__be16	type_sdq_lp;
+	__be32 reserved2[3];
+} __attribute__((packed));
+
+struct sx_ethernet_header {
+	uint8_t dmac[6];
+	uint8_t smac[6];
+	__be16 et;
+	uint8_t mlx_proto;
+	uint8_t ver;
+};
+
+typedef enum check_dup{
+    CHECK_DUP_DISABLED_E = 0,
+    CHECK_DUP_ENABLED_E = 1
+}check_dup_e;
+
+typedef enum is_rp {
+    IS_RP_DONT_CARE_E = 0,
+    IS_RP_FROM_RP_E = 1,
+    IS_RP_NOT_FROM_RP_E = 2,
+} is_rp_e;
+
+typedef enum is_bridge {
+    IS_BRIDGE_DONT_CARE_E = 0,
+    IS_BRIDGE_FROM_BRIDGE_E = 1,
+    IS_BRIDGE_NOT_FROM_BRIDGE_E = 2,
+} is_bridge_e;
+
+int sx_core_flush_synd_by_context(void * context);
+int sx_core_flush_synd_by_handler(cq_handler handler);
+int sx_register_interface(struct sx_interface *intf);
+void sx_unregister_interface(struct sx_interface *intf);
+int sx_core_add_synd(u8 swid, u16 hw_synd, enum l2_type type, u8 is_default,
+	union ku_filter_critireas crit, cq_handler handler, void *context, 
+	check_dup_e check_dup, struct sx_dev* sx_dev);
+int sx_core_remove_synd(u8 swid, u16 hw_synd, enum l2_type type, u8 is_default,
+		union ku_filter_critireas critireas, 
+		void *context, struct sx_dev* sx_dev);
+int sx_core_post_send(struct sx_dev *dev, struct sk_buff *skb,
+			struct isx_meta *meta);
+int __sx_core_post_send(struct sx_dev *dev, struct sk_buff *skb,
+			struct isx_meta *meta);
+void sx_skb_free(struct sk_buff *skb);
+
+int sx_core_get_prio2tc(struct sx_dev *dev,
+               uint16_t port_lag_id, uint8_t is_lag,
+               uint8_t pcp, uint8_t *tc);
+int sx_core_get_pvid(struct sx_dev *dev,
+                     uint16_t       port_lag_id,
+                     uint8_t        is_lag,
+                     uint16_t       *pvid);
+int sx_core_get_vlan_tagging(struct sx_dev *dev,
+               uint16_t port_lag_id, uint8_t is_lag,
+               uint16_t vlan, uint8_t *is_vlan_tagged);
+int sx_core_get_prio_tagging(struct sx_dev *dev,
+               uint16_t port_lag_id, uint8_t is_lag,
+               uint8_t *is_port_prio_tagged);
+int sx_core_get_rp_vlan(struct sx_dev *dev,
+                        struct completion_info *comp_info,
+                        uint16_t *vlan_id);
+int sx_core_get_swid(struct sx_dev *dev,
+                     struct completion_info *comp_info,
+                     uint8_t *swid);
+int sx_core_get_vlan2ip(struct sx_dev *dev,
+               uint16_t vid, uint32_t *ip_addr);
+int sx_core_get_rp_rif_id(struct sx_dev *dev, uint16_t port_lag_id,
+                          uint8_t is_lag, uint16_t vlan_id, uint16_t *rif_id);
+int sx_core_get_rp_mode(struct sx_dev *dev, u8 is_lag, u16 sysport_lag_id,
+                        u16 vlan_id, u8 *is_rp);
+
+int sx_core_get_fid_by_port_vid(struct sx_dev *dev, 
+                            struct completion_info *comp_info, uint16_t *fid);
+int sx_core_get_lag_mid(struct sx_dev *dev, u16 lag_id, u16 *mid);
+
+#endif /* SX_DRIVER_H */
diff --git a/linux/include/linux/mlx_sx/mlx_sx/sx_i2c_if.h b/linux/include/linux/mlx_sx/mlx_sx/sx_i2c_if.h
new file mode 100644
index 0000000..510058d
--- /dev/null
+++ b/linux/include/linux/mlx_sx/mlx_sx/sx_i2c_if.h
@@ -0,0 +1,35 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2008-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+#ifndef __SX_I2C_IF_H_
+#define __SX_I2C_IF_H_
+
+struct sx_i2c_ifc{
+	int (*write)(int i2c_dev_id, int offset, int len, u8 *in_out_buf);
+	int (*read)(int i2c_dev_id, int offset, int len, u8 *in_out_buf);
+	int (*write_dword)(int i2c_dev_id, int offset, u32 val);
+	int (*read_dword)(int i2c_dev_id, int offset, u32 *val);
+	int (*enforce)(int i2c_dev_id);
+	int (*release)(int i2c_dev_id);
+	int (*get_local_mbox)(int i2c_dev_id, u32 *mb_size_in,
+			u32 *mb_offset_in, u32 *mb_size_out, u32 *mb_offset_out);
+	int (*get_fw_rev)(int i2c_dev_id, u64 *fw_rev);
+	int (*set_go_bit_stuck)(int i2c_dev_id);
+	u8 is_registered;
+};
+
+void sx_dpt_reg_i2c_ifc(struct sx_i2c_ifc *i2c_ifc);
+
+void sx_dpt_dereg_i2c_ifc(void);
+
+#endif /* __SX_I2C_IF_ */
diff --git a/linux/include/linux/mlx_sx/sx_i2c_if.h b/linux/include/linux/mlx_sx/sx_i2c_if.h
new file mode 100644
index 0000000..510058d
--- /dev/null
+++ b/linux/include/linux/mlx_sx/sx_i2c_if.h
@@ -0,0 +1,35 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2008-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+#ifndef __SX_I2C_IF_H_
+#define __SX_I2C_IF_H_
+
+struct sx_i2c_ifc{
+	int (*write)(int i2c_dev_id, int offset, int len, u8 *in_out_buf);
+	int (*read)(int i2c_dev_id, int offset, int len, u8 *in_out_buf);
+	int (*write_dword)(int i2c_dev_id, int offset, u32 val);
+	int (*read_dword)(int i2c_dev_id, int offset, u32 *val);
+	int (*enforce)(int i2c_dev_id);
+	int (*release)(int i2c_dev_id);
+	int (*get_local_mbox)(int i2c_dev_id, u32 *mb_size_in,
+			u32 *mb_offset_in, u32 *mb_size_out, u32 *mb_offset_out);
+	int (*get_fw_rev)(int i2c_dev_id, u64 *fw_rev);
+	int (*set_go_bit_stuck)(int i2c_dev_id);
+	u8 is_registered;
+};
+
+void sx_dpt_reg_i2c_ifc(struct sx_i2c_ifc *i2c_ifc);
+
+void sx_dpt_dereg_i2c_ifc(void);
+
+#endif /* __SX_I2C_IF_ */
diff --git a/linux/include/linux/mlx_sx/sx_vtca_kernel_user.h b/linux/include/linux/mlx_sx/sx_vtca_kernel_user.h
new file mode 100644
index 0000000..ac0485d
--- /dev/null
+++ b/linux/include/linux/mlx_sx/sx_vtca_kernel_user.h
@@ -0,0 +1,149 @@
+/*
+ * Copyright (C) Mellanox Technologies, Ltd. 2010-2015 ALL RIGHTS RESERVED.
+ *
+ * This software product is a proprietary product of Mellanox Technologies, Ltd.
+ * (the "Company") and all right, title, and interest in and to the software product,
+ * including all associated intellectual property rights, are and shall
+ * remain exclusively with the Company.
+ *
+ * This software product is governed by the End User License Agreement
+ * provided with the software product.
+ *
+ */
+
+#ifndef SX_VTCA_KERNEL_USER_H_
+#define SX_VTCA_KERNEL_USER_H_
+
+#ifdef __KERNEL__
+#include <linux/uio.h>
+#else
+#include <sys/uio.h>
+#include <stdint.h>
+#endif
+
+#define SX_NODE_DESCRIPTION_SIZE 64
+#define SX_VTCA_GENL_VERSION 0x1
+#define SX_VTCA_GENL_NAME "sx_vtca"
+
+/* attributes */
+enum {
+	SX_VTCA_ATTR_UNSPEC,
+	/* NODE_DESCRIPTION attributes */
+	SX_VTCA_ATTR_NODE_DESC,
+	/* NODE_INFO attributes */
+	SX_VTCA_ATTR_NODE_INFO_FIELD_MASK,
+	SX_VTCA_ATTR_NODE_INFO_NODE_GUID,
+	SX_VTCA_ATTR_NODE_INFO_REV_ID,
+	SX_VTCA_ATTR_NODE_INFO_VENDOR_ID,
+	SX_VTCA_ATTR_NODE_INFO_VENDOR_PART_ID,
+	SX_VTCA_ATTR_NODE_INFO_SYS_IMAGE_GUID,
+	SX_VTCA_ATTR_NODE_INFO_NUM_PORTS,
+	SX_VTCA_ATTR_NODE_INFO_PARTITION_CAP,
+	/* PORT_INFO attributes */
+	SX_VTCA_ATTR_PORT_INFO_FIELD_MASK,
+	SX_VTCA_ATTR_PORT_INFO_PORT,
+	SX_VTCA_ATTR_PORT_INFO_MAX_MTU,
+	SX_VTCA_ATTR_PORT_INFO_VL_CAP,
+	SX_VTCA_ATTR_PORT_INFO_GID_PREFIX,
+	SX_VTCA_ATTR_PORT_INFO_STATE,
+	SX_VTCA_ATTR_PORT_INFO_ACTIVE_MTU,
+	SX_VTCA_ATTR_PORT_INFO_PORT_CAP_FLAGS,
+	SX_VTCA_ATTR_PORT_INFO_BAD_PKEY_COUNTER,
+	SX_VTCA_ATTR_PORT_INFO_QKEY_VIOL_COUNTER,
+	SX_VTCA_ATTR_PORT_INFO_LID,
+	SX_VTCA_ATTR_PORT_INFO_SM_LID,
+	SX_VTCA_ATTR_PORT_INFO_LMC,
+	SX_VTCA_ATTR_PORT_INFO_SM_SL,
+	SX_VTCA_ATTR_PORT_INFO_SUBNET_TIMEOUT,
+	SX_VTCA_ATTR_PORT_INFO_INIT_TYPE_REPLY,
+	SX_VTCA_ATTR_PORT_INFO_ACTIVE_WIDTH,
+	SX_VTCA_ATTR_PORT_INFO_ACTIVE_SPEED,
+	SX_VTCA_ATTR_PORT_INFO_PHYS_STATE,
+	/* GUID_INFO attributes */
+	SX_VTCA_ATTR_GUID_INFO_PORT,
+	SX_VTCA_ATTR_GUID_INFO_INDEX,
+	SX_VTCA_ATTR_GUID_INFO_GID,
+	/* PKEY_INFO attributes */
+	SX_VTCA_ATTR_PKEY_TABLE_PORT,
+	SX_VTCA_ATTR_PKEY_TABLE_INDEX,
+	SX_VTCA_ATTR_PKEY_TABLE_PKEY,
+	/* PORT_COUNTERS attributes */
+	SX_VTCA_ATTR_PORT_COUNTERS_PORT,
+	SX_VTCA_ATTR_PORT_COUNTERS_FIELD_MASK,
+	SX_VTCA_ATTR_PORT_COUNTERS_LINK_DOWNED_COUNTER,
+	SX_VTCA_ATTR_PORT_COUNTERS_PORT_XMIT_DISCARDS,
+	SX_VTCA_ATTR_PORT_COUNTERS_VL15_DROPPED,
+	/* GENERATE_EVENT attributes */
+	SX_VTCA_ATTR_GENERATE_EVENT_PORT,
+	SX_VTCA_ATTR_GENERATE_EVENT_EVENT_TYPE,
+	__SX_VTCA_ATTR_MAX,
+};
+#define SX_VTCA_ATTR_MAX (__SX_VTCA_ATTR_MAX - 1)
+
+/* commands */
+enum {
+	SX_VTCA_CMD_UNSPEC,
+	SX_VTCA_CMD_SET_NODE_DESCRIPTION,
+	SX_VTCA_CMD_GET_NODE_DESCRIPTION,
+	SX_VTCA_CMD_SET_NODE_INFO,
+	SX_VTCA_CMD_GET_NODE_INFO,
+	SX_VTCA_CMD_SET_PORT_INFO,
+	SX_VTCA_CMD_GET_PORT_INFO,
+	SX_VTCA_CMD_SET_GUID_INFO,
+	SX_VTCA_CMD_GET_GUID_INFO,
+	SX_VTCA_CMD_SET_PKEY_TABLE,
+	SX_VTCA_CMD_GET_PKEY_TABLE,
+	SX_VTCA_CMD_RESET_PORT_COUNTERS,
+	SX_VTCA_CMD_GET_PORT_COUNTERS,
+	SX_VTCA_CMD_GET_PORT_SWID_MAP,
+	SX_VTCA_CMD_GENERATE_EVENT,
+	__SX_VTCA_CMD_MAX,
+};
+#define SX_VTCA_CMD_MAX (__SX_VTCA_CMD_MAX - 1)
+
+enum {
+	SX_VTCA_NODE_INFO_NODE_GUID_FLAG	= 1 << 0,
+	SX_VTCA_NODE_INFO_REV_ID_FLAG		= 1 << 1,
+	SX_VTCA_NODE_INFO_VENDOR_ID_FLAG	= 1 << 2,
+	SX_VTCA_NODE_INFO_VENDOR_PART_ID_FLAG	= 1 << 3,
+	SX_VTCA_NODE_INFO_SYS_IMAGE_GUID_FLAG	= 1 << 4,
+	SX_VTCA_NODE_INFO_NUM_PORTS_FLAG	= 1 << 5,
+	SX_VTCA_NODE_INFO_PARTITION_CAP_FLAG	= 1 << 6
+};
+
+enum {
+	SX_VTCA_PORT_INFO_MAX_MTU_FLAG		= 1 << 0,
+	SX_VTCA_PORT_INFO_VL_CAP_FLAG		= 1 << 1,
+	SX_VTCA_PORT_INFO_GID_PREFIX_FLAG	= 1 << 2,
+	SX_VTCA_PORT_INFO_STATE_FLAG		= 1 << 3,
+	SX_VTCA_PORT_INFO_ACTIVE_MTU_FLAG	= 1 << 4,
+	SX_VTCA_PORT_INFO_PORT_CAP_FLAGS_FLAG	= 1 << 5,
+	SX_VTCA_PORT_INFO_BAD_PKEY_CNTR_FLAG	= 1 << 6,
+	SX_VTCA_PORT_INFO_QKEY_VIOL_CNTR_FLAG	= 1 << 7,
+	SX_VTCA_PORT_INFO_LID_FLAG		= 1 << 8,
+	SX_VTCA_PORT_INFO_SM_LID_FLAG		= 1 << 9,
+	SX_VTCA_PORT_INFO_LMC_FLAG		= 1 << 10,
+	SX_VTCA_PORT_INFO_SM_SL_FLAG		= 1 << 11,
+	SX_VTCA_PORT_INFO_SUBNET_TIMEOUT_FLAG	= 1 << 12,
+	SX_VTCA_PORT_INFO_INIT_TIME_REPLY_FLAG	= 1 << 13,
+	SX_VTCA_PORT_INFO_ACTIVE_WIDTH_FLAG	= 1 << 14,
+	SX_VTCA_PORT_INFO_ACTIVE_SPEED_FLAG	= 1 << 15,
+	SX_VTCA_PORT_INFO_PHYS_STATE_FLAG	= 1 << 16
+};
+
+enum {
+	SX_VTCA_PORT_COUNTERS_LINK_DOWNED_COUNTER_FLAG	= 1 << 0,
+	SX_VTCA_PORT_COUNTERS_PORT_XMIT_DISCARDS_FLAG	= 1 << 1,
+	SX_VTCA_PORT_COUNTERS_VL15_DROPPED_FLAG		= 1 << 2,
+};
+
+typedef enum sx_vtca_event_type {
+	SX_VTCA_EVENT_TYPE_PORT_UP,
+	SX_VTCA_EVENT_TYPE_PORT_DOWN,
+	SX_VTCA_EVENT_TYPE_LID_CHANGE,
+	SX_VTCA_EVENT_TYPE_CLIENT_REREGISTER,
+	SX_VTCA_EVENT_TYPE_PKEY_CHANGE,
+	SX_VTCA_EVENT_TYPE_SM_CHANGE
+} sx_vtca_event_type;
+
+#endif /* SX_VTCA_KERNEL_USER_H_ */
-- 
2.1.4

